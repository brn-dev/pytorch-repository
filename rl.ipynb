{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.objectives import ObjectiveLoggingConfig\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.gym.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.a2c.a2c import A2C\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.algorithms import policy_optimization_base\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "from src.weight_initialization import orthogonal_initialization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T14:48:14.429478Z",
     "start_time": "2024-05-26T14:48:04.459960Z"
    }
   },
   "id": "ba8c59a3eba2f172",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "policy_id: str\n",
    "policy: Optional[BasePolicy]\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy = init_policy()\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy\n",
    "\n",
    "def init_policy():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n",
    "        SquashedDiagGaussianActionSelector\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "    from src.networks.skip_nets.additive_skip_net import AdditiveSkipNet, FullyConnectedAdditiveSkipNet, \\\n",
    "        FullyConnectedUnweightedAdditiveSkipNet\n",
    "\n",
    "    # in_size = 376\n",
    "    # action_size = 17\n",
    "    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n",
    "    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n",
    "    \n",
    "    in_size = 17\n",
    "    action_size = 6\n",
    "    \n",
    "    actor_layers = 3\n",
    "    actor_features = 96\n",
    "    \n",
    "    critic_layers = 2\n",
    "    critic_features = 96\n",
    "\n",
    "    # hidden_activation_function = nn.ELU()\n",
    "    actor_hidden_activation_function = nn.Tanh()\n",
    "    critic_hidden_activation_function = nn.ReLU()\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.actor_embedding = nn.Sequential(nn.Linear(in_size, actor_features), actor_hidden_activation_function)\n",
    "            self.actor = FullyConnectedUnweightedAdditiveSkipNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                    nn.Tanh() if is_last_layer else actor_hidden_activation_function,\n",
    "                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                    nn.Tanh() if is_last_layer else actor_hidden_activation_function,\n",
    "                ),\n",
    "                num_layers=actor_layers,\n",
    "                num_features=actor_features,\n",
    "                # weights_trainable=True,\n",
    "                # initial_skip_connection_weight=0.02,\n",
    "            )\n",
    "\n",
    "            self.critic_embedding = nn.Sequential(nn.Linear(in_size, critic_features), critic_hidden_activation_function)\n",
    "            self.critic = FullyConnectedUnweightedAdditiveSkipNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                    critic_hidden_activation_function,\n",
    "                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                    critic_hidden_activation_function,\n",
    "                ),\n",
    "                num_layers=critic_layers,\n",
    "                num_features=critic_features,\n",
    "#                 weights_trainable=True,\n",
    "#                 initial_skip_connection_weight=0.02,\n",
    "            )\n",
    "            self.critic_regressor = nn.Linear(critic_features, 1)\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            actor_out = self.actor(self.actor_embedding(x))\n",
    "            critic_out = self.critic_regressor(self.critic(self.critic_embedding(x)))\n",
    "            return actor_out, critic_out\n",
    "\n",
    "    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n",
    "    #         latent_dim=actor_out_sizes[-1],\n",
    "    #         action_dim=action_size,\n",
    "    #         std=0.1,\n",
    "    #         std_learnable=False,\n",
    "    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "    #     ))\n",
    "    # return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n",
    "    #     latent_dim=actor_out_sizes[-1],\n",
    "    #     action_dim=action_size,\n",
    "    #     initial_std=0.03,\n",
    "    #     squash_output=True,\n",
    "    #     use_full_stds=False,\n",
    "    #     learn_sde_features=False,\n",
    "    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "    # ))\n",
    "    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n",
    "        latent_dim=actor_features,\n",
    "        action_dim=action_size,\n",
    "        base_std=0.2,\n",
    "        squash_output=True,\n",
    "        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n",
    "    ))\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if 'raw_rewards' in info['rollout']:\n",
    "        raw_rewards = info['rollout']['raw_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "            rewards=raw_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_scores = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    global best_iteration_score\n",
    "    iteration_score = episode_scores.mean()\n",
    "    score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    if iteration_score >= best_iteration_score:\n",
    "        best_iteration_score = iteration_score\n",
    "        policy_db.save_model_state_dict(\n",
    "            model=policy,\n",
    "            model_id=policy_id,\n",
    "            parent_model_id=parent_policy_id,\n",
    "            model_info={\n",
    "                'score': iteration_score.item(),\n",
    "                'steps_trained': steps_trained,\n",
    "                'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                'init_policy_source_code': init_policy_source\n",
    "            },\n",
    "        )\n",
    "        \n",
    "    info['episode_scores'] = episode_scores\n",
    "    info['score_moving_average'] = score_moving_average\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    time_taken = stopwatch.reset()\n",
    "    \n",
    "    global steps_trained\n",
    "    steps_trained += rl.buffer.pos\n",
    "    \n",
    "    episode_scores = info['episode_scores']\n",
    "    score_moving_average = info['score_moving_average']\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        rl.weigh_actor_objective(torch.abs(info['raw_actor_objective'])),  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_obj = None if info['weighted_entropy_objective'] is None else format_summary_statics(\n",
    "        info['weighted_entropy_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['weighted_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = format_summary_statics(\n",
    "        rl.buffer.episode_starts.astype(int).sum(axis=0), \n",
    "        mean_format='.2f',\n",
    "        std_format=None,\n",
    "        min_value_format='1d',\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    kl_div = info['actor_kl_divergence'][-1]\n",
    "    grad_norm = format_summary_statics(\n",
    "        info['grad_norm'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    rollout_action_stds = format_summary_statics(\n",
    "        info['rollout']['action_stds'],\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    ppo_epochs = info['nr_ppo_epochs']\n",
    "    ppo_updates = info['nr_ppo_updates']\n",
    "    expl_var = rl.buffer.compute_critic_explained_variance(info['returns'])\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          f\"{advantages = :s}, \"\n",
    "          f\"{abs_actor_obj = :s}, \"\n",
    "          +(f\"{entropy_obj = :s}, \" if entropy_obj is not None else '')+\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{critic_obj = :s}, \"\n",
    "          f\"{expl_var = :.3f}, \"\n",
    "          f\"{kl_div = :.4f}, \"\n",
    "          f\"{ppo_epochs = }, \"\n",
    "          f\"{ppo_updates = }, \"\n",
    "          f\"{grad_norm = :s}, \"\n",
    "          f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f} \\n\")\n",
    "    print()\n",
    "    if not wandb_run.disabled:\n",
    "        wandb_run.log({\n",
    "            'scores': episode_scores,\n",
    "            'advantages': wandb.Histogram(info['advantages']),\n",
    "            'actor_obj': wandb.Histogram(rl.weigh_actor_objective(info['raw_actor_objective'])),\n",
    "            'abs_actor_obj': wandb.Histogram(rl.weigh_actor_objective(torch.abs(info['raw_actor_objective']))),\n",
    "            'critic_obj': wandb.Histogram(info['weighted_critic_objective']),\n",
    "            'expl_var': expl_var,\n",
    "            'kl_div': kl_div,\n",
    "            'ppo_epochs': ppo_epochs,\n",
    "            'ppo_updates': ppo_updates,\n",
    "            'grad_norm': wandb.Histogram(info['grad_norm']),\n",
    "            'resets': wandb.Histogram(rl.buffer.episode_starts.astype(int).sum(axis=0)),\n",
    "            'time_taken': time_taken,\n",
    "        }, step=step)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "def wrap_env(env_):\n",
    "    from src.reinforcement_learning.gym.transform_reward_wrapper import TransformRewardWrapper\n",
    "    from gymnasium.wrappers import RescaleAction\n",
    "    \n",
    "    env_ = TransformRewardWrapper(env_, lambda reward_: 0.01 * reward_)\n",
    "    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n",
    "    return env_\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "num_envs = 32\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "policy_action_std=0.15\n",
    "policy_id, policy = get_policy(create_new_if_exists=True)\n",
    "print(f'{count_parameters(policy) = }')\n",
    "\n",
    "# wandb.init(project=f'rl-{env_name}', config={'policy_id': policy_id})\n",
    "wandb_run = wandb.init(mode='disabled')\n",
    "\n",
    "env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "# print(dict(policy.named_parameters()))\n",
    "try:\n",
    "    env = wrap_env(env)\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        PPO(\n",
    "            env=env,\n",
    "            policy=policy.to(device),\n",
    "            policy_optimizer=lambda pol: optim.AdamW(pol.parameters(), lr=5e-5),\n",
    "            buffer_size=2500,\n",
    "            gamma=0.99,\n",
    "            gae_lambda=0.95,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=NormalizationType.Std,\n",
    "            reduce_actor_objective=lambda obj: antisymmetric_power(obj, 1.5).mean(),\n",
    "            weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "            weigh_entropy_objective=lambda obj: 0.1 * obj.exp(),\n",
    "            weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "            ppo_max_epochs=10,\n",
    "            ppo_kl_target=0.025,\n",
    "            ppo_batch_size=500,\n",
    "            action_ratio_clip_range=0.1,\n",
    "            grad_norm_clip_value=0.5,\n",
    "            sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=PPOLoggingConfig(log_returns=True, log_advantages=True, log_grad_norm=True,\n",
    "                                            log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_actor_kl_divergence=True,\n",
    "                                            actor_objective=ObjectiveLoggingConfig(log_raw=True),\n",
    "                                            entropy_objective=ObjectiveLoggingConfig(log_weighted=True),\n",
    "                                            critic_objective=ObjectiveLoggingConfig(log_weighted=True), ),\n",
    "            torch_device=device,\n",
    "        ).train(5_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-26T14:49:44.557141Z"
    }
   },
   "id": "f71efe062771e81b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "New policy 2024-05-26_16.49.46 created\n",
      "Using policy 2024-05-26_16.49.46 with parent policy None\n",
      "count_parameters(policy) = 97837\n",
      "env = <TransformRewardWrapper<AsyncVectorEnv instance>>, num_envs = 32 \n",
      "\n",
      "\n",
      "================================= Warning ================================= \n",
      " SDE noise sample freq is set to 50 despite not using SDE \n",
      "=========================================================================== \n",
      "\n",
      "\n",
      "step =    2500, scores = -16.125 ± 11.846 [-50.132, 8.898], score_ema = -16.125, advantages = -0.080 ± 1.0 [ -5.780,  4.894], abs_actor_obj =  2.895 ± 3.780, entropy_obj = 0.125 ± 0.001, rollout_stds = 0.201 ± 0.022, critic_obj = 0.016 ± 0.007, expl_var = -35.993, kl_div = 0.0148, ppo_epochs = 10, ppo_updates = 50, grad_norm =  4.348 ± 0.3 [  3.818,  5.119], resets = 2.00 ≥ 2, time = 18.3 \n",
      "\n",
      "step =    5000, scores = -5.114 ± 12.573 [-41.423, 33.268], score_ema = -13.372, advantages = -0.118 ± 1.0 [ -5.852,  5.258], abs_actor_obj =  3.024 ± 3.789, entropy_obj = 0.125 ± 0.001, rollout_stds = 0.203 ± 0.031, critic_obj = 0.006 ± 0.001, expl_var = -25.058, kl_div = 0.0255, ppo_epochs = 10, ppo_updates = 50, grad_norm =  4.906 ± 0.6 [  3.971,  5.907], resets = 3.00 ≥ 3, time = 14.2 \n",
      "\n",
      "step =    7500, scores =  21.345 ± 16.718 [-7.020, 60.185], score_ema = -4.693, advantages = -0.078 ± 1.0 [ -7.793,  5.404], abs_actor_obj =  2.903 ± 3.718, entropy_obj = 0.124 ± 0.002, rollout_stds = 0.206 ± 0.043, critic_obj = 0.004 ± 0.000, expl_var = -15.574, kl_div = 0.0383, ppo_epochs = 10, ppo_updates = 47, grad_norm =  5.820 ± 0.6 [  4.685,  7.098], resets = 2.00 ≥ 2, time = 14.1 \n",
      "\n",
      "step =   10000, scores =  84.273 ± 62.674 [-0.415, 174.898], score_ema =  17.549, advantages =  0.070 ± 1.0 [ -7.196,  6.478], abs_actor_obj =  2.572 ± 3.309, entropy_obj = 0.124 ± 0.002, rollout_stds = 0.208 ± 0.048, critic_obj = 0.004 ± 0.000, expl_var = -10.094, kl_div = 0.0387, ppo_epochs = 8, ppo_updates = 37, grad_norm =  4.841 ± 0.7 [  3.797,  6.039], resets = 3.00 ≥ 3, time = 13.4 \n",
      "\n",
      "step =   12500, scores =  244.870 ± 81.601 [ 109.893, 435.450], score_ema =  74.379, advantages =  0.397 ± 1.0 [ -5.457,  6.408], abs_actor_obj =  2.026 ± 2.581, entropy_obj = 0.128 ± 0.002, rollout_stds = 0.208 ± 0.054, critic_obj = 0.005 ± 0.000, expl_var = -6.223, kl_div = 0.0403, ppo_epochs = 6, ppo_updates = 25, grad_norm =  4.403 ± 1.0 [  3.427,  6.870], resets = 2.00 ≥ 2, time = 11.8 \n",
      "\n",
      "step =   15000, scores =  404.916 ± 287.352 [-0.364, 685.895], score_ema =  157.013, advantages =  0.694 ± 1.0 [ -5.607,  5.680], abs_actor_obj =  1.815 ± 2.256, entropy_obj = 0.130 ± 0.002, rollout_stds = 0.209 ± 0.059, critic_obj = 0.006 ± 0.001, expl_var = -4.164, kl_div = 0.0392, ppo_epochs = 5, ppo_updates = 21, grad_norm =  3.942 ± 0.7 [  2.591,  5.097], resets = 3.00 ≥ 3, time = 11.1 \n",
      "\n",
      "step =   17500, scores =  464.779 ± 186.935 [ 108.651, 816.817], score_ema =  233.955, advantages =  0.510 ± 1.0 [ -6.767,  4.976], abs_actor_obj =  1.987 ± 2.630, entropy_obj = 0.131 ± 0.002, rollout_stds = 0.209 ± 0.062, critic_obj = 0.008 ± 0.001, expl_var = -2.592, kl_div = 0.0386, ppo_epochs = 5, ppo_updates = 21, grad_norm =  3.975 ± 0.8 [  3.054,  5.479], resets = 2.00 ≥ 2, time = 11.2 \n",
      "\n",
      "step =   20000, scores =  248.896 ± 231.664 [-0.609, 823.685], score_ema =  237.690, advantages =  0.114 ± 1.0 [ -5.955,  4.879], abs_actor_obj =  2.538 ± 3.183, entropy_obj = 0.134 ± 0.001, rollout_stds = 0.210 ± 0.068, critic_obj = 0.007 ± 0.001, expl_var = -1.742, kl_div = 0.0271, ppo_epochs = 10, ppo_updates = 50, grad_norm =  5.263 ± 0.8 [  3.847,  6.403], resets = 3.00 ≥ 3, time = 14.3 \n",
      "\n",
      "step =   22500, scores =  345.199 ± 147.782 [ 123.466, 729.233], score_ema =  264.567, advantages =  0.211 ± 1.0 [ -5.619,  5.493], abs_actor_obj =  2.326 ± 2.967, entropy_obj = 0.135 ± 0.002, rollout_stds = 0.214 ± 0.079, critic_obj = 0.006 ± 0.001, expl_var = -0.833, kl_div = 0.0313, ppo_epochs = 10, ppo_updates = 50, grad_norm =  5.173 ± 1.0 [  3.682,  7.329], resets = 2.00 ≥ 2, time = 14.6 \n",
      "\n",
      "step =   25000, scores =  340.318 ± 282.849 [-0.367, 823.787], score_ema =  283.505, advantages =  0.226 ± 1.0 [ -6.116,  5.958], abs_actor_obj =  2.290 ± 2.959, entropy_obj = 0.135 ± 0.002, rollout_stds = 0.217 ± 0.089, critic_obj = 0.006 ± 0.001, expl_var = -0.311, kl_div = 0.0310, ppo_epochs = 10, ppo_updates = 50, grad_norm =  4.935 ± 1.2 [  3.727,  8.065], resets = 3.00 ≥ 3, time = 15.2 \n",
      "\n",
      "step =   27500, scores =  599.952 ± 230.818 [ 210.474, 997.113], score_ema =  362.617, advantages =  0.589 ± 1.0 [ -5.644,  5.035], abs_actor_obj =  1.899 ± 2.422, entropy_obj = 0.136 ± 0.002, rollout_stds = 0.220 ± 0.099, critic_obj = 0.007 ± 0.001, expl_var = -0.466, kl_div = 0.0394, ppo_epochs = 8, ppo_updates = 38, grad_norm =  3.923 ± 0.7 [  2.979,  4.973], resets = 2.00 ≥ 2, time = 14.0 \n",
      "\n",
      "step =   30000, scores =  446.782 ± 380.124 [-0.424, 1116.035], score_ema =  383.658, advantages =  0.293 ± 1.0 [ -6.681,  4.466], abs_actor_obj =  2.203 ± 2.971, entropy_obj = 0.138 ± 0.002, rollout_stds = 0.225 ± 0.110, critic_obj = 0.007 ± 0.001, expl_var = 0.125, kl_div = 0.0376, ppo_epochs = 9, ppo_updates = 42, grad_norm =  5.728 ± 1.4 [  3.470,  8.438], resets = 3.00 ≥ 3, time = 14.0 \n",
      "\n",
      "step =   32500, scores =  688.459 ± 280.992 [ 191.405, 1149.094], score_ema =  459.858, advantages =  0.544 ± 1.0 [ -6.604,  4.428], abs_actor_obj =  1.920 ± 2.620, entropy_obj = 0.140 ± 0.001, rollout_stds = 0.221 ± 0.100, critic_obj = 0.008 ± 0.001, expl_var = 0.077, kl_div = 0.0283, ppo_epochs = 10, ppo_updates = 50, grad_norm =  4.935 ± 2.2 [  2.627,  8.113], resets = 2.00 ≥ 2, time = 14.7 \n",
      "\n",
      "step =   35000, scores =  720.025 ± 537.293 [-0.693, 1289.519], score_ema =  524.900, advantages =  0.580 ± 1.0 [ -6.512,  4.169], abs_actor_obj =  1.918 ± 2.807, entropy_obj = 0.140 ± 0.002, rollout_stds = 0.220 ± 0.099, critic_obj = 0.009 ± 0.002, expl_var = 0.327, kl_div = 0.0376, ppo_epochs = 10, ppo_updates = 46, grad_norm =  5.620 ± 2.4 [  2.302,  8.611], resets = 3.00 ≥ 3, time = 14.4 \n",
      "\n",
      "step =   37500, scores =  743.719 ± 327.894 [ 163.667, 1470.657], score_ema =  579.605, advantages =  0.423 ± 1.0 [ -6.971,  4.871], abs_actor_obj =  2.069 ± 2.812, entropy_obj = 0.143 ± 0.002, rollout_stds = 0.218 ± 0.096, critic_obj = 0.010 ± 0.001, expl_var = 0.600, kl_div = 0.0381, ppo_epochs = 9, ppo_updates = 44, grad_norm =  4.559 ± 2.2 [  2.480,  8.832], resets = 2.00 ≥ 2, time = 14.1 \n",
      "\n",
      "step =   40000, scores =  702.966 ± 620.478 [-0.251, 1602.702], score_ema =  610.445, advantages =  0.364 ± 1.0 [ -6.593,  4.097], abs_actor_obj =  2.099 ± 3.023, entropy_obj = 0.145 ± 0.002, rollout_stds = 0.219 ± 0.102, critic_obj = 0.013 ± 0.002, expl_var = 0.693, kl_div = 0.0386, ppo_epochs = 7, ppo_updates = 30, grad_norm =  7.051 ± 3.1 [  2.784, 15.653], resets = 3.00 ≥ 3, time = 12.3 \n",
      "\n",
      "step =   42500, scores =  992.412 ± 437.523 [ 287.050, 1742.997], score_ema =  705.937, advantages =  0.552 ± 1.0 [ -7.048,  5.084], abs_actor_obj =  1.909 ± 2.792, entropy_obj = 0.148 ± 0.002, rollout_stds = 0.220 ± 0.109, critic_obj = 0.014 ± 0.003, expl_var = 0.646, kl_div = 0.0379, ppo_epochs = 7, ppo_updates = 32, grad_norm =  6.414 ± 3.2 [  3.364, 11.887], resets = 2.00 ≥ 2, time = 12.2 \n",
      "\n",
      "step =   45000, scores =  816.399 ± 708.747 [-0.387, 1891.413], score_ema =  733.552, advantages =  0.300 ± 1.0 [ -7.351,  3.849], abs_actor_obj =  2.149 ± 3.179, entropy_obj = 0.152 ± 0.002, rollout_stds = 0.223 ± 0.117, critic_obj = 0.016 ± 0.002, expl_var = 0.741, kl_div = 0.0377, ppo_epochs = 6, ppo_updates = 29, grad_norm =  6.302 ± 2.7 [  2.384, 11.911], resets = 3.00 ≥ 3, time = 12.0 \n",
      "\n",
      "step =   47500, scores =  1148.033 ± 450.637 [ 309.648, 1828.407], score_ema =  837.172, advantages =  0.394 ± 1.0 [ -8.068,  3.860], abs_actor_obj =  2.018 ± 3.061, entropy_obj = 0.154 ± 0.002, rollout_stds = 0.220 ± 0.109, critic_obj = 0.017 ± 0.003, expl_var = 0.715, kl_div = 0.0394, ppo_epochs = 5, ppo_updates = 23, grad_norm =  6.592 ± 2.6 [  3.430, 10.825], resets = 2.00 ≥ 2, time = 11.4 \n",
      "\n",
      "step =   50000, scores =  772.032 ± 711.116 [-1.015, 1934.402], score_ema =  820.887, advantages =  0.137 ± 1.0 [ -7.375,  4.549], abs_actor_obj =  2.396 ± 3.595, entropy_obj = 0.156 ± 0.002, rollout_stds = 0.222 ± 0.112, critic_obj = 0.021 ± 0.003, expl_var = 0.755, kl_div = 0.0451, ppo_epochs = 6, ppo_updates = 25, grad_norm =  8.823 ± 3.3 [  4.696, 16.490], resets = 3.00 ≥ 3, time = 11.7 \n",
      "\n",
      "step =   52500, scores =  1037.390 ± 494.458 [ 183.445, 1865.305], score_ema =  875.013, advantages =  0.197 ± 1.0 [ -7.335,  4.712], abs_actor_obj =  2.278 ± 3.408, entropy_obj = 0.157 ± 0.002, rollout_stds = 0.220 ± 0.105, critic_obj = 0.019 ± 0.004, expl_var = 0.766, kl_div = 0.0379, ppo_epochs = 8, ppo_updates = 38, grad_norm =  8.496 ± 5.1 [  4.335, 20.756], resets = 2.00 ≥ 2, time = 13.0 \n",
      "\n",
      "step =   55000, scores =  1143.388 ± 870.309 [-0.050, 2014.870], score_ema =  942.107, advantages =  0.267 ± 1.0 [ -8.578,  4.984], abs_actor_obj =  2.079 ± 3.757, entropy_obj = 0.160 ± 0.002, rollout_stds = 0.218 ± 0.103, critic_obj = 0.021 ± 0.006, expl_var = 0.712, kl_div = 0.0408, ppo_epochs = 6, ppo_updates = 26, grad_norm =  11.179 ± 10.2 [  3.158, 40.582], resets = 3.00 ≥ 3, time = 12.2 \n",
      "\n",
      "step =   57500, scores =  1142.154 ± 531.328 [ 148.138, 2063.666], score_ema =  992.118, advantages =  0.257 ± 1.0 [ -9.059,  4.490], abs_actor_obj =  2.175 ± 3.554, entropy_obj = 0.165 ± 0.002, rollout_stds = 0.217 ± 0.099, critic_obj = 0.020 ± 0.005, expl_var = 0.803, kl_div = 0.0386, ppo_epochs = 5, ppo_updates = 24, grad_norm =  6.828 ± 2.9 [  3.599, 11.949], resets = 2.00 ≥ 2, time = 11.9 \n",
      "\n",
      "step =   60000, scores =  1172.667 ± 903.046 [-0.342, 2104.983], score_ema =  1037.256, advantages =  0.233 ± 1.0 [ -8.663,  5.350], abs_actor_obj =  2.097 ± 3.826, entropy_obj = 0.168 ± 0.002, rollout_stds = 0.214 ± 0.093, critic_obj = 0.024 ± 0.007, expl_var = 0.768, kl_div = 0.0383, ppo_epochs = 7, ppo_updates = 30, grad_norm =  8.633 ± 4.4 [  2.760, 16.044], resets = 3.00 ≥ 3, time = 12.3 \n",
      "\n",
      "step =   62500, scores =  941.090 ± 623.007 [ 64.012, 2271.754], score_ema =  1013.214, advantages =  0.062 ± 1.0 [ -7.874,  5.696], abs_actor_obj =  2.536 ± 3.632, entropy_obj = 0.169 ± 0.002, rollout_stds = 0.216 ± 0.095, critic_obj = 0.024 ± 0.003, expl_var = 0.869, kl_div = 0.0376, ppo_epochs = 8, ppo_updates = 38, grad_norm =  9.206 ± 3.6 [  5.632, 21.184], resets = 2.00 ≥ 2, time = 13.5 \n",
      "\n",
      "step =   65000, scores =  831.014 ± 840.519 [-0.703, 2324.165], score_ema =  967.664, advantages =  0.025 ± 1.0 [ -8.531,  6.766], abs_actor_obj =  2.598 ± 3.736, entropy_obj = 0.172 ± 0.003, rollout_stds = 0.218 ± 0.100, critic_obj = 0.024 ± 0.002, expl_var = 0.891, kl_div = 0.0383, ppo_epochs = 9, ppo_updates = 41, grad_norm =  9.041 ± 2.4 [  5.746, 13.216], resets = 3.00 ≥ 3, time = 13.3 \n",
      "\n",
      "step =   67500, scores =  1326.398 ± 676.933 [ 148.364, 2350.111], score_ema =  1057.348, advantages =  0.231 ± 1.0 [ -9.412,  5.909], abs_actor_obj =  2.114 ± 3.633, entropy_obj = 0.175 ± 0.002, rollout_stds = 0.211 ± 0.089, critic_obj = 0.024 ± 0.006, expl_var = 0.864, kl_div = 0.0400, ppo_epochs = 5, ppo_updates = 20, grad_norm =  8.376 ± 3.8 [  4.302, 16.750], resets = 2.00 ≥ 2, time = 11.4 \n",
      "\n",
      "step =   70000, scores =  1251.003 ± 979.223 [-0.092, 2278.144], score_ema =  1105.761, advantages =  0.127 ± 1.0 [ -9.140,  5.389], abs_actor_obj =  2.174 ± 4.070, entropy_obj = 0.176 ± 0.002, rollout_stds = 0.209 ± 0.082, critic_obj = 0.026 ± 0.008, expl_var = 0.857, kl_div = 0.0382, ppo_epochs = 5, ppo_updates = 23, grad_norm =  9.575 ± 6.0 [  2.984, 21.171], resets = 3.00 ≥ 3, time = 11.8 \n",
      "\n",
      "step =   72500, scores =  1336.808 ± 655.609 [ 155.784, 2511.953], score_ema =  1163.523, advantages =  0.198 ± 1.0 [ -9.720,  6.175], abs_actor_obj =  2.177 ± 3.658, entropy_obj = 0.174 ± 0.002, rollout_stds = 0.209 ± 0.079, critic_obj = 0.025 ± 0.006, expl_var = 0.904, kl_div = 0.0395, ppo_epochs = 9, ppo_updates = 42, grad_norm =  7.827 ± 4.2 [  3.769, 15.139], resets = 2.00 ≥ 2, time = 13.9 \n",
      "\n",
      "step =   75000, scores =  1267.057 ± 1111.744 [-0.355, 2574.976], score_ema =  1189.407, advantages =  0.147 ± 1.0 [ -9.178,  5.843], abs_actor_obj =  2.095 ± 4.115, entropy_obj = 0.178 ± 0.002, rollout_stds = 0.206 ± 0.072, critic_obj = 0.029 ± 0.009, expl_var = 0.894, kl_div = 0.0378, ppo_epochs = 5, ppo_updates = 22, grad_norm =  10.954 ± 4.7 [  4.651, 16.992], resets = 3.00 ≥ 3, time = 12.1 \n",
      "\n",
      "step =   77500, scores =  1205.832 ± 749.059 [ 142.050, 2667.796], score_ema =  1193.513, advantages =  0.135 ± 1.0 [ -9.859,  6.400], abs_actor_obj =  2.247 ± 3.817, entropy_obj = 0.183 ± 0.003, rollout_stds = 0.206 ± 0.072, critic_obj = 0.027 ± 0.005, expl_var = 0.932, kl_div = 0.0413, ppo_epochs = 5, ppo_updates = 24, grad_norm =  10.364 ± 4.3 [  5.994, 19.422], resets = 2.00 ≥ 2, time = 12.5 \n",
      "\n",
      "step =   80000, scores =  1276.313 ± 1080.030 [-0.710, 2629.421], score_ema =  1214.213, advantages =  0.103 ± 1.0 [-10.082,  6.173], abs_actor_obj =  2.140 ± 4.209, entropy_obj = 0.190 ± 0.002, rollout_stds = 0.207 ± 0.076, critic_obj = 0.031 ± 0.008, expl_var = 0.913, kl_div = 0.0401, ppo_epochs = 5, ppo_updates = 23, grad_norm =  12.200 ± 6.0 [  4.662, 20.847], resets = 3.00 ≥ 3, time = 12.2 \n",
      "\n",
      "step =   82500, scores =  1580.204 ± 699.461 [ 249.739, 2594.367], score_ema =  1305.711, advantages =  0.161 ± 1.0 [-10.303,  6.659], abs_actor_obj =  2.079 ± 4.019, entropy_obj = 0.190 ± 0.002, rollout_stds = 0.208 ± 0.079, critic_obj = 0.029 ± 0.010, expl_var = 0.900, kl_div = 0.0408, ppo_epochs = 6, ppo_updates = 27, grad_norm =  11.278 ± 5.9 [  5.244, 20.982], resets = 2.00 ≥ 2, time = 12.8 \n",
      "\n",
      "step =   85000, scores =  1575.862 ± 1213.489 [-0.168, 2728.446], score_ema =  1373.248, advantages =  0.103 ± 1.0 [ -9.305,  5.546], abs_actor_obj =  1.989 ± 4.565, entropy_obj = 0.193 ± 0.002, rollout_stds = 0.206 ± 0.076, critic_obj = 0.038 ± 0.016, expl_var = 0.831, kl_div = 0.0377, ppo_epochs = 6, ppo_updates = 26, grad_norm =  12.021 ± 6.2 [  3.434, 20.025], resets = 3.00 ≥ 3, time = 13.2 \n",
      "\n",
      "step =   87500, scores =  1866.800 ± 815.095 [ 186.202, 2944.813], score_ema =  1496.636, advantages =  0.198 ± 1.0 [-10.044,  5.827], abs_actor_obj =  1.883 ± 4.189, entropy_obj = 0.197 ± 0.002, rollout_stds = 0.205 ± 0.076, critic_obj = 0.034 ± 0.018, expl_var = 0.848, kl_div = 0.0401, ppo_epochs = 5, ppo_updates = 23, grad_norm =  10.462 ± 6.8 [  4.060, 22.442], resets = 2.00 ≥ 2, time = 13.1 \n",
      "\n",
      "step =   90000, scores =  1767.152 ± 1418.914 [-0.218, 3267.389], score_ema =  1564.265, advantages =  0.106 ± 1.0 [ -9.891,  5.409], abs_actor_obj =  1.996 ± 4.573, entropy_obj = 0.205 ± 0.002, rollout_stds = 0.206 ± 0.078, critic_obj = 0.049 ± 0.020, expl_var = 0.874, kl_div = 0.0408, ppo_epochs = 4, ppo_updates = 18, grad_norm =  12.612 ± 5.8 [  4.752, 20.906], resets = 3.00 ≥ 3, time = 11.3 \n",
      "\n",
      "step =   92500, scores =  1761.679 ± 971.257 [ 175.730, 3281.139], score_ema =  1613.619, advantages =  0.123 ± 1.0 [-10.802,  6.440], abs_actor_obj =  2.057 ± 4.313, entropy_obj = 0.206 ± 0.002, rollout_stds = 0.206 ± 0.080, critic_obj = 0.042 ± 0.018, expl_var = 0.923, kl_div = 0.0384, ppo_epochs = 4, ppo_updates = 17, grad_norm =  11.492 ± 6.9 [  4.609, 23.675], resets = 2.00 ≥ 2, time = 11.3 \n",
      "\n",
      "step =   95000, scores =  1735.582 ± 1388.723 [-0.293, 3186.082], score_ema =  1644.110, advantages =  0.005 ± 1.0 [ -9.534,  6.098], abs_actor_obj =  2.077 ± 4.734, entropy_obj = 0.206 ± 0.003, rollout_stds = 0.206 ± 0.081, critic_obj = 0.051 ± 0.022, expl_var = 0.882, kl_div = 0.0429, ppo_epochs = 4, ppo_updates = 18, grad_norm =  15.668 ± 8.9 [  4.436, 29.086], resets = 3.00 ≥ 3, time = 11.8 \n",
      "\n",
      "step =   97500, scores =  2097.794 ± 798.948 [ 224.741, 3074.724], score_ema =  1757.531, advantages =  0.062 ± 1.0 [-10.534,  6.034], abs_actor_obj =  2.025 ± 4.654, entropy_obj = 0.206 ± 0.002, rollout_stds = 0.206 ± 0.082, critic_obj = 0.043 ± 0.027, expl_var = 0.750, kl_div = 0.0406, ppo_epochs = 4, ppo_updates = 19, grad_norm =  15.512 ± 13.4 [  3.169, 42.923], resets = 2.00 ≥ 2, time = 11.2 \n",
      "\n",
      "step =  100000, scores =  1792.233 ± 1542.574 [-0.274, 3441.834], score_ema =  1766.206, advantages =  0.029 ± 1.0 [-10.067,  5.451], abs_actor_obj =  1.967 ± 4.820, entropy_obj = 0.212 ± 0.003, rollout_stds = 0.204 ± 0.080, critic_obj = 0.057 ± 0.027, expl_var = 0.889, kl_div = 0.0386, ppo_epochs = 6, ppo_updates = 27, grad_norm =  13.187 ± 6.6 [  4.104, 23.089], resets = 3.00 ≥ 3, time = 12.1 \n",
      "\n",
      "step =  102500, scores =  2089.329 ± 1071.729 [ 171.801, 3436.453], score_ema =  1846.987, advantages =  0.064 ± 1.0 [-10.691,  6.355], abs_actor_obj =  1.945 ± 4.489, entropy_obj = 0.210 ± 0.003, rollout_stds = 0.203 ± 0.079, critic_obj = 0.047 ± 0.025, expl_var = 0.908, kl_div = 0.0389, ppo_epochs = 5, ppo_updates = 21, grad_norm =  11.399 ± 8.4 [  4.117, 26.119], resets = 2.00 ≥ 2, time = 11.8 \n",
      "\n",
      "step =  105000, scores =  1968.256 ± 1563.451 [-0.386, 3504.884], score_ema =  1877.304, advantages = -0.005 ± 1.0 [-10.439,  5.813], abs_actor_obj =  1.937 ± 4.870, entropy_obj = 0.208 ± 0.003, rollout_stds = 0.201 ± 0.075, critic_obj = 0.056 ± 0.028, expl_var = 0.906, kl_div = 0.0392, ppo_epochs = 5, ppo_updates = 22, grad_norm =  13.168 ± 6.5 [  4.150, 22.960], resets = 3.00 ≥ 3, time = 11.9 \n",
      "\n",
      "step =  107500, scores =  2365.018 ± 1065.908 [ 173.986, 3610.715], score_ema =  1999.232, advantages =  0.102 ± 1.0 [-11.415,  6.726], abs_actor_obj =  1.792 ± 4.635, entropy_obj = 0.216 ± 0.003, rollout_stds = 0.199 ± 0.071, critic_obj = 0.048 ± 0.032, expl_var = 0.888, kl_div = 0.0381, ppo_epochs = 6, ppo_updates = 28, grad_norm =  11.892 ± 9.8 [  3.432, 29.131], resets = 2.00 ≥ 2, time = 12.9 \n",
      "\n",
      "step =  110000, scores =  2141.885 ± 1709.124 [-0.301, 3729.170], score_ema =  2034.896, advantages =  0.015 ± 1.0 [ -9.654,  6.223], abs_actor_obj =  1.793 ± 4.887, entropy_obj = 0.217 ± 0.003, rollout_stds = 0.196 ± 0.065, critic_obj = 0.064 ± 0.035, expl_var = 0.877, kl_div = 0.0418, ppo_epochs = 3, ppo_updates = 14, grad_norm =  14.919 ± 8.4 [  3.184, 28.924], resets = 3.00 ≥ 3, time = 10.6 \n",
      "\n",
      "step =  112500, scores =  2113.119 ± 1157.404 [ 186.500, 3847.747], score_ema =  2054.451, advantages =  0.074 ± 1.0 [-11.798,  7.614], abs_actor_obj =  1.983 ± 4.674, entropy_obj = 0.217 ± 0.004, rollout_stds = 0.197 ± 0.066, critic_obj = 0.055 ± 0.026, expl_var = 0.936, kl_div = 0.0441, ppo_epochs = 3, ppo_updates = 14, grad_norm =  17.514 ± 11.6 [  5.373, 42.215], resets = 2.00 ≥ 2, time = 10.3 \n",
      "\n",
      "step =  115000, scores =  1972.846 ± 1729.651 [-0.499, 3926.336], score_ema =  2034.050, advantages = -0.052 ± 1.0 [-10.452,  6.356], abs_actor_obj =  2.068 ± 4.962, entropy_obj = 0.220 ± 0.003, rollout_stds = 0.197 ± 0.067, critic_obj = 0.070 ± 0.031, expl_var = 0.922, kl_div = 0.0401, ppo_epochs = 5, ppo_updates = 20, grad_norm =  14.371 ± 7.5 [  4.828, 24.539], resets = 3.00 ≥ 3, time = 11.4 \n",
      "\n",
      "step =  117500, scores =  2201.166 ± 1050.367 [ 158.870, 3688.100], score_ema =  2075.829, advantages = -0.037 ± 1.0 [-11.082,  6.642], abs_actor_obj =  2.152 ± 4.639, entropy_obj = 0.216 ± 0.003, rollout_stds = 0.197 ± 0.068, critic_obj = 0.058 ± 0.030, expl_var = 0.912, kl_div = 0.0385, ppo_epochs = 5, ppo_updates = 23, grad_norm =  14.137 ± 9.7 [  5.367, 29.012], resets = 2.00 ≥ 2, time = 11.9 \n",
      "\n",
      "step =  120000, scores =  2246.894 ± 1780.352 [-0.223, 3904.266], score_ema =  2118.595, advantages = -0.038 ± 1.0 [-10.202,  6.728], abs_actor_obj =  1.902 ± 4.967, entropy_obj = 0.221 ± 0.003, rollout_stds = 0.195 ± 0.070, critic_obj = 0.073 ± 0.041, expl_var = 0.887, kl_div = 0.0409, ppo_epochs = 4, ppo_updates = 19, grad_norm =  15.906 ± 10.7 [  3.517, 34.458], resets = 3.00 ≥ 3, time = 11.4 \n",
      "\n",
      "step =  122500, scores =  2393.079 ± 1129.925 [ 274.926, 3934.558], score_ema =  2187.216, advantages =  0.019 ± 1.0 [-11.438,  6.842], abs_actor_obj =  1.882 ± 4.798, entropy_obj = 0.227 ± 0.003, rollout_stds = 0.194 ± 0.071, critic_obj = 0.061 ± 0.040, expl_var = 0.901, kl_div = 0.0390, ppo_epochs = 6, ppo_updates = 28, grad_norm =  17.467 ± 15.3 [  4.499, 42.315], resets = 2.00 ≥ 2, time = 12.5 \n",
      "\n",
      "step =  125000, scores =  2206.678 ± 1810.389 [-0.333, 3947.808], score_ema =  2192.082, advantages = -0.062 ± 1.0 [-10.164,  6.057], abs_actor_obj =  1.918 ± 5.191, entropy_obj = 0.228 ± 0.003, rollout_stds = 0.193 ± 0.069, critic_obj = 0.080 ± 0.043, expl_var = 0.892, kl_div = 0.0446, ppo_epochs = 4, ppo_updates = 18, grad_norm =  17.056 ± 9.8 [  4.949, 28.173], resets = 3.00 ≥ 3, time = 12.1 \n",
      "\n",
      "step =  127500, scores =  2557.644 ± 1169.449 [ 153.344, 3954.522], score_ema =  2283.472, advantages = -0.003 ± 1.0 [-11.225,  6.315], abs_actor_obj =  1.869 ± 4.787, entropy_obj = 0.229 ± 0.003, rollout_stds = 0.193 ± 0.069, critic_obj = 0.065 ± 0.046, expl_var = 0.863, kl_div = 0.0392, ppo_epochs = 5, ppo_updates = 21, grad_norm =  12.121 ± 9.7 [  3.331, 28.984], resets = 2.00 ≥ 2, time = 12.3 \n",
      "\n",
      "step =  130000, scores =  2244.963 ± 1751.506 [-0.144, 3929.801], score_ema =  2273.845, advantages = -0.113 ± 1.0 [ -9.888,  5.823], abs_actor_obj =  2.132 ± 5.224, entropy_obj = 0.229 ± 0.003, rollout_stds = 0.193 ± 0.067, critic_obj = 0.087 ± 0.045, expl_var = 0.847, kl_div = 0.0383, ppo_epochs = 5, ppo_updates = 21, grad_norm =  20.220 ± 12.3 [  3.241, 34.425], resets = 3.00 ≥ 3, time = 12.2 \n",
      "\n",
      "step =  132500, scores =  2663.166 ± 1111.975 [ 161.109, 4009.370], score_ema =  2371.175, advantages = -0.034 ± 1.0 [-11.162,  6.776], abs_actor_obj =  1.944 ± 5.091, entropy_obj = 0.231 ± 0.003, rollout_stds = 0.191 ± 0.067, critic_obj = 0.070 ± 0.054, expl_var = 0.786, kl_div = 0.0379, ppo_epochs = 4, ppo_updates = 19, grad_norm =  14.192 ± 12.0 [  3.056, 32.941], resets = 2.00 ≥ 2, time = 11.0 \n",
      "\n",
      "step =  135000, scores =  2171.974 ± 1869.734 [-0.148, 4217.281], score_ema =  2321.375, advantages = -0.095 ± 1.0 [-10.134,  5.828], abs_actor_obj =  2.040 ± 5.146, entropy_obj = 0.232 ± 0.003, rollout_stds = 0.192 ± 0.068, critic_obj = 0.087 ± 0.043, expl_var = 0.912, kl_div = 0.0379, ppo_epochs = 6, ppo_updates = 27, grad_norm =  18.598 ± 9.7 [  5.416, 32.449], resets = 3.00 ≥ 3, time = 12.0 \n",
      "\n",
      "step =  137500, scores =  2742.640 ± 997.082 [ 218.773, 3898.360], score_ema =  2426.691, advantages = -0.045 ± 1.0 [-11.419,  6.230], abs_actor_obj =  1.883 ± 4.995, entropy_obj = 0.227 ± 0.003, rollout_stds = 0.190 ± 0.063, critic_obj = 0.067 ± 0.057, expl_var = 0.660, kl_div = 0.0391, ppo_epochs = 4, ppo_updates = 18, grad_norm =  15.832 ± 15.6 [  2.979, 40.716], resets = 2.00 ≥ 2, time = 11.0 \n",
      "\n",
      "step =  140000, scores =  2315.034 ± 1966.905 [-0.091, 4283.753], score_ema =  2398.777, advantages = -0.086 ± 1.0 [ -9.936,  6.256], abs_actor_obj =  1.906 ± 5.272, entropy_obj = 0.239 ± 0.004, rollout_stds = 0.190 ± 0.064, critic_obj = 0.093 ± 0.053, expl_var = 0.882, kl_div = 0.0397, ppo_epochs = 7, ppo_updates = 30, grad_norm =  13.419 ± 7.6 [  4.171, 24.233], resets = 3.00 ≥ 3, time = 12.0 \n",
      "\n",
      "step =  142500, scores =  2536.887 ± 1304.471 [ 137.195, 4220.618], score_ema =  2433.304, advantages = -0.045 ± 1.0 [-11.411,  6.973], abs_actor_obj =  2.015 ± 4.912, entropy_obj = 0.246 ± 0.003, rollout_stds = 0.189 ± 0.065, critic_obj = 0.070 ± 0.042, expl_var = 0.930, kl_div = 0.0377, ppo_epochs = 6, ppo_updates = 25, grad_norm =  15.444 ± 11.1 [  6.105, 33.422], resets = 2.00 ≥ 2, time = 11.7 \n",
      "\n",
      "step =  145000, scores =  2568.188 ± 1971.120 [-0.073, 4212.803], score_ema =  2467.025, advantages = -0.088 ± 1.0 [-10.137,  6.473], abs_actor_obj =  1.860 ± 5.413, entropy_obj = 0.247 ± 0.003, rollout_stds = 0.186 ± 0.062, critic_obj = 0.096 ± 0.060, expl_var = 0.857, kl_div = 0.0392, ppo_epochs = 4, ppo_updates = 16, grad_norm =  19.484 ± 14.5 [  1.977, 42.161], resets = 3.00 ≥ 3, time = 10.8 \n",
      "\n",
      "step =  147500, scores =  2898.231 ± 1285.137 [ 152.408, 4393.501], score_ema =  2574.827, advantages = -0.001 ± 1.0 [-11.721,  7.290], abs_actor_obj =  1.740 ± 4.990, entropy_obj = 0.248 ± 0.003, rollout_stds = 0.186 ± 0.062, critic_obj = 0.069 ± 0.053, expl_var = 0.927, kl_div = 0.0394, ppo_epochs = 4, ppo_updates = 18, grad_norm =  13.786 ± 10.6 [  2.006, 31.496], resets = 2.00 ≥ 2, time = 11.3 \n",
      "\n",
      "step =  150000, scores =  2328.216 ± 2044.981 [-0.601, 4580.195], score_ema =  2513.174, advantages = -0.081 ± 1.0 [-10.170,  5.988], abs_actor_obj =  1.910 ± 5.323, entropy_obj = 0.253 ± 0.004, rollout_stds = 0.186 ± 0.063, critic_obj = 0.100 ± 0.051, expl_var = 0.924, kl_div = 0.0378, ppo_epochs = 4, ppo_updates = 18, grad_norm =  15.954 ± 7.8 [  5.420, 26.224], resets = 3.00 ≥ 3, time = 11.2 \n",
      "\n",
      "step =  152500, scores =  2647.098 ± 1557.868 [ 141.613, 4612.058], score_ema =  2546.655, advantages = -0.003 ± 1.0 [-11.479,  7.484], abs_actor_obj =  1.804 ± 5.154, entropy_obj = 0.260 ± 0.003, rollout_stds = 0.185 ± 0.064, critic_obj = 0.080 ± 0.055, expl_var = 0.939, kl_div = 0.0377, ppo_epochs = 4, ppo_updates = 19, grad_norm =  16.135 ± 11.9 [  4.138, 32.248], resets = 2.00 ≥ 2, time = 11.1 \n",
      "\n",
      "step =  155000, scores =  2384.146 ± 2206.409 [-0.420, 4691.838], score_ema =  2506.028, advantages = -0.063 ± 1.0 [-10.515,  7.221], abs_actor_obj =  1.816 ± 5.284, entropy_obj = 0.263 ± 0.003, rollout_stds = 0.185 ± 0.062, critic_obj = 0.097 ± 0.051, expl_var = 0.937, kl_div = 0.0396, ppo_epochs = 6, ppo_updates = 25, grad_norm =  22.528 ± 15.3 [  5.621, 66.815], resets = 3.00 ≥ 3, time = 12.9 \n",
      "\n",
      "step =  157500, scores =  2802.838 ± 1562.190 [ 125.570, 4574.602], score_ema =  2580.230, advantages = -0.019 ± 1.0 [-11.785,  7.341], abs_actor_obj =  1.768 ± 5.285, entropy_obj = 0.266 ± 0.003, rollout_stds = 0.183 ± 0.056, critic_obj = 0.084 ± 0.066, expl_var = 0.921, kl_div = 0.0406, ppo_epochs = 3, ppo_updates = 14, grad_norm =  16.002 ± 12.9 [  4.173, 39.705], resets = 2.00 ≥ 2, time = 949.6 \n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from src.reinforcement_learning.gym.envs.singleton_vector_env import as_vec_env\n",
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = wrap_env(record_env)\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env, _ = as_vec_env(record_env)\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            actions_dist, _ = policy.process_obs(torch.tensor(obs, device=device))\n",
    "            actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-26T01:17:23.342941Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bba6ab51a61dd845",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
