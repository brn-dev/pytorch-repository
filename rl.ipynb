{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.model_db.rl_model_info import RLModelInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.objectives import ObjectiveLoggingConfig\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.gym.envs.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, \\\n",
    "    RescaleAction, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.a2c.a2c import A2C\n",
    "from src.reinforcement_learning.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.envs.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.core.rl_base import RLBase\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:51:38.405372Z",
     "start_time": "2024-05-02T14:51:34.249214Z"
    }
   },
   "id": "ba8c59a3eba2f172",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "policy_id: str\n",
    "policy: Optional[BasePolicy]\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy = init_policy()\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy\n",
    "\n",
    "def init_policy():\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            in_size = 376\n",
    "            shared_out_sizes = []\n",
    "            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n",
    "            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n",
    "            \n",
    "            hidden_activation_function = nn.ELU()\n",
    "            actor_out_activation_function = nn.Tanh()\n",
    "            critic_out_activation_function = nn.Identity()\n",
    "            \n",
    "            self.has_shared = len(shared_out_sizes) > 0\n",
    "            \n",
    "            if self.has_shared:\n",
    "                self.shared = SeqNet.from_layer_provider(\n",
    "                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                        nn.Linear(in_features, out_features),\n",
    "                        hidden_activation_function\n",
    "                    ),\n",
    "                    in_size=in_size,\n",
    "                    out_sizes=shared_out_sizes\n",
    "                )\n",
    "            else:\n",
    "                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    actor_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    critic_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            if self.has_shared:\n",
    "                shared_out = self.shared(x)\n",
    "            else:\n",
    "                shared_out = x\n",
    "\n",
    "            return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.1)\n",
    "stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: PPO, step: int, info: dict[str, Any]):   \n",
    "    if 'raw_rewards' in info['rollout']:\n",
    "        unnormalized_rewards = info['rollout']['raw_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards[:len(unnormalized_rewards)]),\n",
    "            rewards=unnormalized_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(unnormalized_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_scores = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    global best_iteration_score\n",
    "    iteration_score = episode_scores.mean()\n",
    "    score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    if iteration_score >= best_iteration_score:\n",
    "        best_iteration_score = iteration_score\n",
    "        policy_db.save_model_state_dict(\n",
    "            model=policy,\n",
    "            model_id=policy_id,\n",
    "            parent_model_id=parent_policy_id,\n",
    "            model_info={\n",
    "                'score': iteration_score.item(),\n",
    "                'steps_trained': steps_trained,\n",
    "                'wrap_env_function': wrap_env_function_source,\n",
    "            },\n",
    "            init_function=init_function_source,\n",
    "        )\n",
    "        \n",
    "    info['episode_scores'] = episode_scores\n",
    "    info['score_moving_average'] = score_moving_average\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any]):\n",
    "    time_taken = stopwatch.reset()\n",
    "    \n",
    "    global steps_trained\n",
    "    steps_trained += rl.buffer.pos\n",
    "    \n",
    "    episode_scores = info['episode_scores']\n",
    "    score_moving_average = info['score_moving_average']\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        rl.weigh_actor_objective(torch.abs(info['raw_actor_objective'])),  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['weighted_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = format_summary_statics(\n",
    "        rl.buffer.episode_starts.astype(int).sum(axis=0), \n",
    "        mean_format='.2f',\n",
    "        std_format=None,\n",
    "        min_value_format='1d',\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # kl_div = format_summary_statics(\n",
    "    #     info['actor_kl_divergence'], \n",
    "    #     mean_format='6.4f',\n",
    "    #     std_format='7.5f',\n",
    "    #     min_value_format=None,\n",
    "    #     max_value_format='6.4f',\n",
    "    # )\n",
    "    kl_div = info['actor_kl_divergence'][-1]\n",
    "    grad_norm = format_summary_statics(\n",
    "        info['grad_norm'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    ppo_epochs = info['nr_ppo_epochs']\n",
    "    ppo_updates = info['nr_ppo_updates']\n",
    "    expl_var = rl.buffer.compute_critic_explained_variance(info['returns'])\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          f\"{advantages = :s}, \"\n",
    "          f\"{abs_actor_obj = :s}, \"\n",
    "          f\"{critic_obj = :s}, \"\n",
    "          f\"{expl_var = :.3f}, \"\n",
    "          f\"{kl_div = :.4f}, \"\n",
    "          f\"{ppo_epochs = }, \"\n",
    "          f\"{ppo_updates = }, \"\n",
    "          f\"{grad_norm = :s}, \"\n",
    "          f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f} \\n\")\n",
    "    print()\n",
    "    if not wandb_run.disabled:\n",
    "        wandb_run.log({\n",
    "            'scores': episode_scores,\n",
    "            'advantages': wandb.Histogram(info['advantages']),\n",
    "            'actor_obj': wandb.Histogram(rl.weigh_actor_objective(info['raw_actor_objective'])),\n",
    "            'abs_actor_obj': wandb.Histogram(rl.weigh_actor_objective(torch.abs(info['raw_actor_objective']))),\n",
    "            'critic_obj': wandb.Histogram(info['weighted_critic_objective']),\n",
    "            'expl_var': expl_var,\n",
    "            'kl_div': kl_div,\n",
    "            'ppo_epochs': ppo_epochs,\n",
    "            'ppo_updates': ppo_updates,\n",
    "            'grad_norm': wandb.Histogram(info['grad_norm']),\n",
    "            'resets': wandb.Histogram(rl.buffer.episode_starts.astype(int).sum(axis=0)),\n",
    "            'time_taken': time_taken,\n",
    "        }, step=step)\n",
    "    \n",
    "    # for param_name, param_grad in get_gradients_per_parameter(rl.policy, param_type='weight'):\n",
    "    #     print(f'{param_name + \".grad\":<50}: ' + format_summary_statics(\n",
    "    #         param_grad,\n",
    "    #         mean_format=' 8.5f',\n",
    "    #         std_format='.5f',\n",
    "    #         min_value_format=' 8.5f',\n",
    "    #         max_value_format='7.5f',\n",
    "    #     ))\n",
    "    # \n",
    "    # print('\\n')\n",
    "\n",
    "device = set_default_torch_device(\"cuda:0\") if True else set_default_torch_device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "def wrap_env(_env: VectorEnv):\n",
    "    # _env = NormalizeRewardWrapper(_env, gamma=gamma)\n",
    "    # _env = TransformObservation(_env, lambda _obs: _obs / 255)\n",
    "    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n",
    "    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n",
    "    _env = ClipAction(_env)\n",
    "    return _env\n",
    "\n",
    "wrap_env_function_source = inspect.getsource(wrap_env)\n",
    "init_function_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'Humanoid-v4'\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "wrap_env_kwargs = {'reward_multiplier': 0.01}\n",
    "num_envs = 4\n",
    "    \n",
    "# policy_db = TinyModelDB[RLModelInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[RLModelInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "policy_action_std=0.15\n",
    "policy_id, policy = get_policy(create_new_if_exists=False)\n",
    "print(f'{count_parameters(policy) = }')\n",
    "\n",
    "gamma = 0.995\n",
    "\n",
    "# wandb.init(project=f'rl-{env_name}', config={'policy_id': policy_id})\n",
    "wandb_run = wandb.init(mode='disabled')\n",
    "\n",
    "env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "try:\n",
    "    env = wrap_env(env)\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "    \n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters(), lr=1e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "        weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.01,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.02,\n",
    "        grad_norm_clip_value=2.0,\n",
    "        callback=Callback(on_rollout_done=on_rollout_done, on_optimization_done=on_optimization_done),\n",
    "        logging_config=PPOLoggingConfig(log_returns=True, log_advantages=True, log_grad_norm=True,\n",
    "                                        log_rollout_infos=True, log_actor_kl_divergence=True,\n",
    "                                        actor_objective=ObjectiveLoggingConfig(log_raw=True), \n",
    "                                        critic_objective=ObjectiveLoggingConfig(log_weighted=True), )\n",
    "    ).train(5_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T14:59:10.948032Z",
     "start_time": "2024-05-02T14:51:38.405372Z"
    }
   },
   "id": "f71efe062771e81b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "No policy in RAM, creating a new one\n",
      "New policy 2024-05-02_16.51.38 created\n",
      "Using policy 2024-05-02_16.51.38 with parent policy None\n",
      "count_parameters(policy) = 1639186\n",
      "env = <ClipAction<RescaleAction<TransformRewardWrapper<AsyncVectorEnv instance>>>>, num_envs = 4 \n",
      "\n",
      "\n",
      "step =    2500, scores =  19.283 ± 3.510 [ 1.470, 35.158], score_ema =  19.283, advantages =  0.641 ± 1.0 [ -8.788,  4.476], abs_actor_obj =  3.757 ± 11.282, critic_obj = 0.011 ± 0.007, expl_var = -10.795, kl_div = 0.0155, ppo_epochs = 7, ppo_updates = 32, grad_norm =  4.539 ± 0.7 [  3.537,  5.970], resets = 91.75 ≥ 91, time = 11.8 \n",
      "\n",
      "\n",
      "step =    5000, scores =  19.272 ± 3.336 [ 1.954, 32.620], score_ema =  19.282, advantages =  0.259 ± 1.0 [ -3.750,  3.468], abs_actor_obj =  5.510 ± 7.327, critic_obj = 0.002 ± 0.001, expl_var = -1.196, kl_div = 0.0159, ppo_epochs = 8, ppo_updates = 38, grad_norm =  1.099 ± 0.1 [  0.926,  1.350], resets = 92.50 ≥ 91, time = 10.1 \n",
      "\n",
      "\n",
      "step =    7500, scores =  21.654 ± 4.295 [ 1.506, 36.940], score_ema =  19.519, advantages =  0.297 ± 1.0 [ -8.019,  4.776], abs_actor_obj =  4.951 ± 7.918, critic_obj = 0.001 ± 0.000, expl_var = 0.131, kl_div = 0.0161, ppo_epochs = 8, ppo_updates = 38, grad_norm =  0.924 ± 0.1 [  0.732,  1.070], resets = 84.00 ≥ 81, time = 10.0 \n",
      "\n",
      "\n",
      "step =   10000, scores =  25.543 ± 6.722 [ 1.499, 71.749], score_ema =  20.122, advantages =  0.436 ± 1.0 [ -3.970,  7.650], abs_actor_obj =  3.718 ± 5.720, critic_obj = 0.002 ± 0.001, expl_var = 0.342, kl_div = 0.0152, ppo_epochs = 8, ppo_updates = 39, grad_norm =  0.636 ± 0.1 [  0.516,  0.769], resets = 73.00 ≥ 71, time =  9.8 \n",
      "\n",
      "\n",
      "step =   12500, scores =  31.533 ± 7.622 [ 2.535, 60.796], score_ema =  21.263, advantages =  0.428 ± 1.0 [ -5.496,  6.023], abs_actor_obj =  3.788 ± 5.677, critic_obj = 0.003 ± 0.000, expl_var = 0.404, kl_div = 0.0152, ppo_epochs = 5, ppo_updates = 21, grad_norm =  0.943 ± 0.3 [  0.660,  1.506], resets = 60.00 ≥ 59, time =  9.5 \n",
      "\n",
      "\n",
      "step =   15000, scores =  43.882 ± 12.575 [ 12.509, 70.480], score_ema =  23.525, advantages =  0.612 ± 1.0 [ -2.778,  4.721], abs_actor_obj =  3.195 ± 4.791, critic_obj = 0.006 ± 0.001, expl_var = 0.321, kl_div = 0.0163, ppo_epochs = 6, ppo_updates = 25, grad_norm =  0.925 ± 0.1 [  0.684,  1.122], resets = 46.00 ≥ 44, time = 11.2 \n",
      "\n",
      "\n",
      "step =   17500, scores =  59.626 ± 13.572 [ 14.131, 84.078], score_ema =  27.135, advantages =  0.611 ± 1.0 [ -3.167,  4.259], abs_actor_obj =  3.771 ± 6.370, critic_obj = 0.007 ± 0.002, expl_var = 0.407, kl_div = 0.0156, ppo_epochs = 5, ppo_updates = 23, grad_norm =  1.139 ± 0.4 [  0.424,  2.151], resets = 36.75 ≥ 36, time = 11.4 \n",
      "\n",
      "\n",
      "step =   20000, scores =  62.709 ± 13.963 [ 1.317, 82.392], score_ema =  30.692, advantages =  0.300 ± 1.0 [ -3.648,  5.569], abs_actor_obj =  4.908 ± 9.193, critic_obj = 0.006 ± 0.002, expl_var = 0.557, kl_div = 0.0161, ppo_epochs = 4, ppo_updates = 18, grad_norm =  1.548 ± 0.6 [  0.843,  2.494], resets = 34.75 ≥ 34, time = 11.0 \n",
      "\n",
      "\n",
      "step =   22500, scores =  63.953 ± 11.739 [ 9.260, 87.195], score_ema =  34.018, advantages = -0.012 ± 1.0 [ -5.195,  6.374], abs_actor_obj =  6.397 ± 10.234, critic_obj = 0.005 ± 0.001, expl_var = 0.605, kl_div = 0.0155, ppo_epochs = 3, ppo_updates = 14, grad_norm =  1.786 ± 0.4 [  1.300,  2.262], resets = 34.25 ≥ 34, time = 11.5 \n",
      "\n",
      "\n",
      "step =   25000, scores =  63.786 ± 11.549 [ 20.001, 84.724], score_ema =  36.995, advantages = -0.032 ± 1.0 [ -4.988,  6.611], abs_actor_obj =  6.398 ± 10.654, critic_obj = 0.005 ± 0.001, expl_var = 0.621, kl_div = 0.0155, ppo_epochs = 4, ppo_updates = 15, grad_norm =  1.838 ± 0.5 [  1.156,  2.444], resets = 35.50 ≥ 35, time = 11.6 \n",
      "\n",
      "\n",
      "step =   27500, scores =  64.692 ± 12.205 [ 7.677, 84.275], score_ema =  39.765, advantages =  0.014 ± 1.0 [ -4.626,  6.312], abs_actor_obj =  6.066 ± 10.736, critic_obj = 0.005 ± 0.001, expl_var = 0.602, kl_div = 0.0166, ppo_epochs = 4, ppo_updates = 15, grad_norm =  1.968 ± 0.1 [  1.673,  2.149], resets = 35.75 ≥ 34, time = 11.4 \n",
      "\n",
      "\n",
      "step =   30000, scores =  68.418 ± 10.961 [ 2.781, 88.202], score_ema =  42.630, advantages =  0.321 ± 1.0 [ -4.264,  6.585], abs_actor_obj =  4.177 ± 7.434, critic_obj = 0.004 ± 0.000, expl_var = 0.701, kl_div = 0.0158, ppo_epochs = 4, ppo_updates = 17, grad_norm =  1.077 ± 0.1 [  0.869,  1.273], resets = 35.50 ≥ 34, time = 11.5 \n",
      "\n",
      "\n",
      "step =   32500, scores =  68.684 ± 9.460 [ 21.609, 93.931], score_ema =  45.235, advantages = -0.059 ± 1.0 [ -6.540,  6.636], abs_actor_obj =  5.822 ± 10.812, critic_obj = 0.004 ± 0.001, expl_var = 0.701, kl_div = 0.0170, ppo_epochs = 4, ppo_updates = 19, grad_norm =  1.777 ± 0.3 [  1.258,  2.170], resets = 34.75 ≥ 33, time = 11.6 \n",
      "\n",
      "\n",
      "step =   35000, scores =  68.086 ± 12.157 [ 2.666, 95.544], score_ema =  47.520, advantages = -0.021 ± 1.0 [ -6.069,  6.665], abs_actor_obj =  5.558 ± 10.624, critic_obj = 0.004 ± 0.002, expl_var = 0.676, kl_div = 0.0176, ppo_epochs = 4, ppo_updates = 16, grad_norm =  1.638 ± 0.6 [  0.943,  2.646], resets = 34.75 ≥ 34, time = 11.4 \n",
      "\n",
      "\n",
      "step =   37500, scores =  67.812 ± 12.487 [ 5.390, 93.733], score_ema =  49.550, advantages =  0.041 ± 1.0 [ -5.895,  6.660], abs_actor_obj =  5.329 ± 10.386, critic_obj = 0.004 ± 0.001, expl_var = 0.688, kl_div = 0.0156, ppo_epochs = 5, ppo_updates = 21, grad_norm =  1.672 ± 0.8 [  0.840,  2.714], resets = 35.00 ≥ 35, time = 11.6 \n",
      "\n",
      "\n",
      "step =   40000, scores =  67.287 ± 10.257 [ 3.805, 81.380], score_ema =  51.323, advantages =  0.034 ± 1.0 [ -6.689,  9.049], abs_actor_obj =  4.953 ± 9.722, critic_obj = 0.003 ± 0.001, expl_var = 0.727, kl_div = 0.0161, ppo_epochs = 5, ppo_updates = 21, grad_norm =  1.778 ± 0.8 [  1.009,  3.270], resets = 35.75 ≥ 35, time = 11.1 \n",
      "\n",
      "\n",
      "step =   42500, scores =  67.520 ± 11.642 [ 11.688, 85.160], score_ema =  52.943, advantages =  0.040 ± 1.0 [ -6.241,  7.480], abs_actor_obj =  4.981 ± 10.292, critic_obj = 0.003 ± 0.001, expl_var = 0.730, kl_div = 0.0153, ppo_epochs = 4, ppo_updates = 18, grad_norm =  1.508 ± 0.7 [  0.931,  2.791], resets = 35.50 ≥ 35, time = 11.7 \n",
      "\n",
      "\n",
      "step =   45000, scores =  68.179 ± 9.823 [ 17.263, 98.852], score_ema =  54.467, advantages =  0.030 ± 1.0 [ -6.375,  6.822], abs_actor_obj =  4.857 ± 10.171, critic_obj = 0.004 ± 0.001, expl_var = 0.723, kl_div = 0.0162, ppo_epochs = 4, ppo_updates = 18, grad_norm =  1.965 ± 0.8 [  1.179,  3.461], resets = 35.75 ≥ 35, time = 12.0 \n",
      "\n",
      "\n",
      "step =   47500, scores =  68.608 ± 10.040 [ 1.986, 107.301], score_ema =  55.881, advantages = -0.015 ± 1.0 [ -6.127,  6.981], abs_actor_obj =  5.311 ± 10.437, critic_obj = 0.003 ± 0.001, expl_var = 0.740, kl_div = 0.0157, ppo_epochs = 5, ppo_updates = 22, grad_norm =  1.697 ± 1.0 [  0.977,  3.528], resets = 36.00 ≥ 35, time = 11.0 \n",
      "\n",
      "\n",
      "step =   50000, scores =  68.094 ± 12.953 [-0.288, 94.911], score_ema =  57.102, advantages = -0.104 ± 1.0 [ -4.994,  4.949], abs_actor_obj =  5.529 ± 13.135, critic_obj = 0.006 ± 0.001, expl_var = 0.559, kl_div = 0.0169, ppo_epochs = 4, ppo_updates = 18, grad_norm =  2.385 ± 0.4 [  1.849,  2.928], resets = 35.50 ≥ 35, time = 11.2 \n",
      "\n",
      "\n",
      "step =   52500, scores =  69.880 ± 9.973 [ 23.193, 95.536], score_ema =  58.380, advantages =  0.163 ± 1.0 [ -5.223,  7.441], abs_actor_obj =  4.529 ± 8.681, critic_obj = 0.004 ± 0.001, expl_var = 0.720, kl_div = 0.0155, ppo_epochs = 5, ppo_updates = 23, grad_norm =  1.577 ± 0.8 [  0.794,  3.288], resets = 35.25 ≥ 35, time = 11.8 \n",
      "\n",
      "\n",
      "step =   55000, scores =  68.611 ± 11.210 [ 7.593, 98.594], score_ema =  59.403, advantages = -0.217 ± 1.0 [ -6.248,  6.490], abs_actor_obj =  7.099 ± 12.597, critic_obj = 0.005 ± 0.003, expl_var = 0.639, kl_div = 0.0165, ppo_epochs = 3, ppo_updates = 14, grad_norm =  2.024 ± 0.7 [  1.338,  2.914], resets = 36.00 ≥ 35, time = 15.9 \n",
      "\n",
      "\n",
      "step =   57500, scores =  68.384 ± 11.286 [ 6.039, 97.961], score_ema =  60.301, advantages =  0.148 ± 1.0 [ -6.125,  6.845], abs_actor_obj =  4.376 ± 10.560, critic_obj = 0.005 ± 0.002, expl_var = 0.647, kl_div = 0.0161, ppo_epochs = 6, ppo_updates = 25, grad_norm =  1.493 ± 0.6 [  0.558,  2.372], resets = 35.25 ≥ 34, time = 17.3 \n",
      "\n",
      "\n",
      "step =   60000, scores =  68.836 ± 11.706 [ 10.706, 98.290], score_ema =  61.154, advantages =  0.050 ± 1.0 [ -4.596,  6.292], abs_actor_obj =  5.056 ± 10.399, critic_obj = 0.004 ± 0.001, expl_var = 0.684, kl_div = 0.0153, ppo_epochs = 5, ppo_updates = 24, grad_norm =  1.900 ± 0.6 [  1.000,  2.861], resets = 35.25 ≥ 34, time = 16.4 \n",
      "\n",
      "\n",
      "step =   62500, scores =  70.047 ± 12.059 [ 18.211, 112.573], score_ema =  62.044, advantages = -0.087 ± 1.0 [ -4.727,  6.062], abs_actor_obj =  6.343 ± 12.180, critic_obj = 0.006 ± 0.002, expl_var = 0.623, kl_div = 0.0169, ppo_epochs = 4, ppo_updates = 19, grad_norm =  2.132 ± 1.1 [  0.818,  3.702], resets = 33.50 ≥ 30, time = 17.1 \n",
      "\n",
      "\n",
      "step =   65000, scores =  70.181 ± 14.402 [ 7.254, 120.702], score_ema =  62.857, advantages =  0.064 ± 1.0 [ -5.351,  7.008], abs_actor_obj =  5.191 ± 9.038, critic_obj = 0.005 ± 0.001, expl_var = 0.661, kl_div = 0.0153, ppo_epochs = 4, ppo_updates = 16, grad_norm =  1.854 ± 0.5 [  1.288,  2.708], resets = 34.50 ≥ 33, time = 16.1 \n",
      "\n",
      "\n",
      "step =   67500, scores =  70.178 ± 14.427 [ 2.491, 100.199], score_ema =  63.589, advantages = -0.046 ± 1.0 [ -5.234,  6.670], abs_actor_obj =  6.009 ± 11.179, critic_obj = 0.005 ± 0.001, expl_var = 0.658, kl_div = 0.0155, ppo_epochs = 5, ppo_updates = 21, grad_norm =  2.044 ± 0.5 [  1.178,  2.865], resets = 34.00 ≥ 33, time = 16.7 \n",
      "\n",
      "\n",
      "step =   70000, scores =  70.422 ± 14.509 [ 6.549, 110.176], score_ema =  64.273, advantages =  0.034 ± 1.0 [ -4.973,  6.206], abs_actor_obj =  5.239 ± 10.834, critic_obj = 0.005 ± 0.001, expl_var = 0.647, kl_div = 0.0169, ppo_epochs = 4, ppo_updates = 17, grad_norm =  1.823 ± 0.6 [  0.861,  2.462], resets = 34.50 ≥ 34, time = 16.3 \n",
      "\n",
      "\n",
      "step =   72500, scores =  70.030 ± 12.415 [ 8.343, 97.902], score_ema =  64.848, advantages = -0.121 ± 1.0 [ -5.208,  5.961], abs_actor_obj =  6.262 ± 11.899, critic_obj = 0.005 ± 0.001, expl_var = 0.646, kl_div = 0.0164, ppo_epochs = 4, ppo_updates = 16, grad_norm =  2.353 ± 0.9 [  0.997,  3.580], resets = 34.75 ≥ 33, time = 14.8 \n",
      "\n",
      "\n",
      "step =   75000, scores =  69.012 ± 12.171 [ 2.039, 93.397], score_ema =  65.265, advantages = -0.041 ± 1.0 [ -5.040,  7.557], abs_actor_obj =  5.798 ± 10.560, critic_obj = 0.004 ± 0.001, expl_var = 0.705, kl_div = 0.0157, ppo_epochs = 4, ppo_updates = 15, grad_norm =  1.909 ± 0.4 [  1.336,  2.433], resets = 34.50 ≥ 32, time = 11.8 \n",
      "\n",
      "\n",
      "step =   77500, scores =  69.308 ± 10.507 [ 17.504, 99.603], score_ema =  65.669, advantages =  0.031 ± 1.0 [ -5.436,  6.921], abs_actor_obj =  5.092 ± 9.399, critic_obj = 0.004 ± 0.001, expl_var = 0.713, kl_div = 0.0159, ppo_epochs = 4, ppo_updates = 19, grad_norm =  1.727 ± 0.9 [  1.061,  3.569], resets = 35.00 ≥ 33, time = 10.7 \n",
      "\n",
      "\n",
      "step =   80000, scores =  70.327 ± 9.564 [ 28.610, 105.864], score_ema =  66.135, advantages =  0.085 ± 1.0 [ -6.573,  8.238], abs_actor_obj =  4.800 ± 7.382, critic_obj = 0.003 ± 0.000, expl_var = 0.776, kl_div = 0.0168, ppo_epochs = 6, ppo_updates = 25, grad_norm =  1.374 ± 0.3 [  0.930,  1.875], resets = 35.00 ≥ 34, time = 11.2 \n",
      "\n",
      "\n",
      "step =   82500, scores =  70.663 ± 9.112 [ 14.844, 91.850], score_ema =  66.588, advantages = -0.017 ± 1.0 [ -6.843,  7.654], abs_actor_obj =  4.994 ± 10.282, critic_obj = 0.003 ± 0.002, expl_var = 0.736, kl_div = 0.0153, ppo_epochs = 5, ppo_updates = 22, grad_norm =  1.678 ± 1.2 [  0.841,  4.282], resets = 34.00 ≥ 33, time = 11.2 \n",
      "\n",
      "\n",
      "step =   85000, scores =  71.630 ± 13.501 [ 2.041, 111.387], score_ema =  67.092, advantages = -0.022 ± 1.0 [ -5.561,  6.464], abs_actor_obj =  5.241 ± 12.420, critic_obj = 0.006 ± 0.002, expl_var = 0.625, kl_div = 0.0157, ppo_epochs = 5, ppo_updates = 21, grad_norm =  1.880 ± 0.7 [  0.973,  2.940], resets = 33.00 ≥ 31, time = 10.8 \n",
      "\n",
      "\n",
      "step =   87500, scores =  69.893 ± 15.266 [ 5.161, 101.958], score_ema =  67.372, advantages = -0.077 ± 1.0 [ -4.860,  7.408], abs_actor_obj =  5.623 ± 12.240, critic_obj = 0.006 ± 0.001, expl_var = 0.633, kl_div = 0.0172, ppo_epochs = 5, ppo_updates = 21, grad_norm =  1.897 ± 0.7 [  0.676,  2.703], resets = 33.25 ≥ 33, time = 11.2 \n",
      "\n",
      "\n",
      "step =   90000, scores =  70.157 ± 11.851 [ 11.191, 101.394], score_ema =  67.651, advantages =  0.053 ± 1.0 [ -5.984,  7.959], abs_actor_obj =  5.132 ± 8.664, critic_obj = 0.003 ± 0.001, expl_var = 0.760, kl_div = 0.0154, ppo_epochs = 6, ppo_updates = 27, grad_norm =  1.613 ± 0.9 [  0.885,  3.314], resets = 34.25 ≥ 33, time = 12.3 \n",
      "\n",
      "\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n",
      "done\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-0.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-1.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-2.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-3.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-4.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-5.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-5.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-6.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-6.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard interrupt\n",
      "closing record_env\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-6.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-6.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-28_14.27.40\\rl-video-episode-6.mp4\n",
      "record_env closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = wrap_env(record_env)\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            actions_dist = policy.predict_actions(obs)\n",
    "            actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(10000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T12:27:46.929424Z",
     "start_time": "2024-04-28T12:27:39.289650Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), f'saved_models/rl/{env_name}/{get_current_timestamp()}---6x96_6x64-elu--state_dict.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T01:34:57.846923Z",
     "start_time": "2024-04-27T01:34:57.746930Z"
    }
   },
   "id": "459d7865a53e3600",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(lambda: gym.make('CartPole-v1', render_mode='rgb_array'), n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=2)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "for _ in range(100_000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266d91aeee84c4cd",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-0.mp4\n",
      "closing record_env\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-1.mp4\n",
      "record_env closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "record_env = gymnasium.make(\"ALE/Pacman-ram-v5\", render_mode='rgb_array')\n",
    "try:\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=r'C:\\Users\\domin\\Videos\\rl\\2024-04-24.1', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            # actions_dist = policy.predict_actions(obs)\n",
    "            # actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            \n",
    "            actions = 2\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    \n",
    "    record(10000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T15:57:07.218154Z",
     "start_time": "2024-04-24T15:57:06.685399Z"
    }
   },
   "id": "1a6d8096efe48ca3",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OrderEnforcing<PassiveEnvChecker<AtariEnv<ALE/Pacman-ram-v5>>>>\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T16:29:07.007390Z",
     "start_time": "2024-04-24T16:29:06.925013Z"
    }
   },
   "id": "bba6ab51a61dd845",
   "execution_count": 10
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
