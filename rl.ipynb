{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.objectives import ObjectiveLoggingConfig\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.gym.envs.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.a2c.a2c import A2C\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.envs.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.algorithms import policy_optimization_base\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "from src.weight_initialization import orthogonal_initialization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T15:07:15.095884Z",
     "start_time": "2024-05-25T15:07:14.951458Z"
    }
   },
   "id": "ba8c59a3eba2f172",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "policy_id: str\n",
    "policy: Optional[BasePolicy]\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy = init_policy()\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy\n",
    "\n",
    "def init_policy():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n",
    "        SquashedDiagGaussianActionSelector\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "\n",
    "    # in_size = 376\n",
    "    # action_size = 17\n",
    "    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n",
    "    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n",
    "    \n",
    "    in_size = 17\n",
    "    action_size = 6\n",
    "    actor_out_sizes = [128, 128, 128, 128]\n",
    "    critic_out_sizes = [128, 128, 128, 1]\n",
    "\n",
    "    # hidden_activation_function = nn.ELU()\n",
    "    hidden_activation_function = nn.ELU()\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                    nn.Tanh() if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                    nn.Identity() if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            return self.actor(x), self.critic(x)\n",
    "\n",
    "    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n",
    "    #         latent_dim=actor_out_sizes[-1],\n",
    "    #         action_dim=action_size,\n",
    "    #         std=0.1,\n",
    "    #         std_learnable=False,\n",
    "    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "    #     ))\n",
    "    # return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n",
    "    #     latent_dim=actor_out_sizes[-1],\n",
    "    #     action_dim=action_size,\n",
    "    #     initial_std=0.03,\n",
    "    #     squash_output=True,\n",
    "    #     use_full_stds=False,\n",
    "    #     learn_sde_features=False,\n",
    "    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "    # ))\n",
    "    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n",
    "        latent_dim=actor_out_sizes[-1],\n",
    "        action_dim=action_size,\n",
    "        base_std=0.2,\n",
    "        squash_output=True,\n",
    "        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n",
    "    ))\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):   \n",
    "    \n",
    "    print(f'{step}: {scheduler_values}')\n",
    "    \n",
    "    if 'raw_rewards' in info['rollout']:\n",
    "        raw_rewards = info['rollout']['raw_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "            rewards=raw_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_scores = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    global best_iteration_score\n",
    "    iteration_score = episode_scores.mean()\n",
    "    score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    if iteration_score >= best_iteration_score:\n",
    "        best_iteration_score = iteration_score\n",
    "        policy_db.save_model_state_dict(\n",
    "            model=policy,\n",
    "            model_id=policy_id,\n",
    "            parent_model_id=parent_policy_id,\n",
    "            model_info={\n",
    "                'score': iteration_score.item(),\n",
    "                'steps_trained': steps_trained,\n",
    "                'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                'init_policy_source_code': init_policy_source\n",
    "            },\n",
    "        )\n",
    "        \n",
    "    info['episode_scores'] = episode_scores\n",
    "    info['score_moving_average'] = score_moving_average\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    time_taken = stopwatch.reset()\n",
    "    \n",
    "    global steps_trained\n",
    "    steps_trained += rl.buffer.pos\n",
    "    \n",
    "    episode_scores = info['episode_scores']\n",
    "    score_moving_average = info['score_moving_average']\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        rl.weigh_actor_objective(torch.abs(info['raw_actor_objective'])),  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_obj = None if info['weighted_entropy_objective'] is None else format_summary_statics(\n",
    "        info['weighted_entropy_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['weighted_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = format_summary_statics(\n",
    "        rl.buffer.episode_starts.astype(int).sum(axis=0), \n",
    "        mean_format='.2f',\n",
    "        std_format=None,\n",
    "        min_value_format='1d',\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    kl_div = info['actor_kl_divergence'][-1]\n",
    "    grad_norm = format_summary_statics(\n",
    "        info['grad_norm'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    rollout_action_stds = format_summary_statics(\n",
    "        info['rollout']['action_stds'],\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    ppo_epochs = info['nr_ppo_epochs']\n",
    "    ppo_updates = info['nr_ppo_updates']\n",
    "    expl_var = rl.buffer.compute_critic_explained_variance(info['returns'])\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          f\"{advantages = :s}, \"\n",
    "          f\"{abs_actor_obj = :s}, \"\n",
    "          f\"{entropy_obj = :s}, \" if entropy_obj is not None else ''\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{critic_obj = :s}, \"\n",
    "          f\"{expl_var = :.3f}, \"\n",
    "          f\"{kl_div = :.4f}, \"\n",
    "          f\"{ppo_epochs = }, \"\n",
    "          f\"{ppo_updates = }, \"\n",
    "          f\"{grad_norm = :s}, \"\n",
    "          f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f} \\n\")\n",
    "    print()\n",
    "    if not wandb_run.disabled:\n",
    "        wandb_run.log({\n",
    "            'scores': episode_scores,\n",
    "            'advantages': wandb.Histogram(info['advantages']),\n",
    "            'actor_obj': wandb.Histogram(rl.weigh_actor_objective(info['raw_actor_objective'])),\n",
    "            'abs_actor_obj': wandb.Histogram(rl.weigh_actor_objective(torch.abs(info['raw_actor_objective']))),\n",
    "            'critic_obj': wandb.Histogram(info['weighted_critic_objective']),\n",
    "            'expl_var': expl_var,\n",
    "            'kl_div': kl_div,\n",
    "            'ppo_epochs': ppo_epochs,\n",
    "            'ppo_updates': ppo_updates,\n",
    "            'grad_norm': wandb.Histogram(info['grad_norm']),\n",
    "            'resets': wandb.Histogram(rl.buffer.episode_starts.astype(int).sum(axis=0)),\n",
    "            'time_taken': time_taken,\n",
    "        }, step=step)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "def wrap_env(env_):\n",
    "    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n",
    "    from gymnasium.wrappers import RescaleAction\n",
    "    \n",
    "    env_ = TransformRewardWrapper(env_, lambda reward_: 0.01 * reward_)\n",
    "    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n",
    "    return env_\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "num_envs = 32\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "policy_action_std=0.15\n",
    "policy_id, policy = get_policy(create_new_if_exists=True)\n",
    "print(f'{count_parameters(policy) = }')\n",
    "\n",
    "# wandb.init(project=f'rl-{env_name}', config={'policy_id': policy_id})\n",
    "wandb_run = wandb.init(mode='disabled')\n",
    "\n",
    "env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "try:\n",
    "    env = wrap_env(env)\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "    \n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.AdamW(pol.parameters(), lr=5e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        reduce_actor_objective=lambda obj: antisymmetric_power(obj, 1.5).mean(),\n",
    "        weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "        weigh_entropy_objective=lambda obj: 0.1 * obj.exp(),\n",
    "        weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.025,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.1,\n",
    "        grad_norm_clip_value=0.5,\n",
    "        sde_noise_sample_freq=50,\n",
    "        callback=Callback(\n",
    "            on_rollout_done=on_rollout_done,\n",
    "            rollout_schedulers={},\n",
    "            on_optimization_done=on_optimization_done,\n",
    "            optimization_schedulers={},\n",
    "        ),\n",
    "        logging_config=PPOLoggingConfig(log_returns=True, log_advantages=True, log_grad_norm=True,\n",
    "                                        log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                        log_actor_kl_divergence=True,\n",
    "                                        actor_objective=ObjectiveLoggingConfig(log_raw=True),\n",
    "                                        entropy_objective=ObjectiveLoggingConfig(log_weighted=True),\n",
    "                                        critic_objective=ObjectiveLoggingConfig(log_weighted=True), ),\n",
    "        torch_device=device,\n",
    "    ).train(5_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T15:13:11.016213Z",
     "start_time": "2024-05-25T15:10:41.414986Z"
    }
   },
   "id": "f71efe062771e81b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "New policy 2024-05-25_17.10.41 created\n",
      "Using policy 2024-05-25_17.10.41 with parent policy None\n",
      "count_parameters(policy) = 88845\n",
      "env = <TransformRewardWrapper<AsyncVectorEnv instance>>, num_envs = 32 \n",
      "\n",
      "\n",
      "================================= Warning ================================= \n",
      " SDE noise sample freq is set to 50 despite not using SDE \n",
      "=========================================================================== \n",
      "\n",
      "\n",
      "2500: {'test1': 1.0, 'test2': 10}\n",
      "step =    2500, scores = -16.735 ± 12.985 [-50.186, 3.615], score_ema = -16.735, advantages = -0.012 ± 1.0 [ -5.151,  4.731], abs_actor_obj =  2.756 ± 3.477, entropy_obj = 0.125 ± 0.001, rollout_stds = 0.201 ± 0.012, critic_obj = 0.089 ± 0.063, expl_var = -67.422, kl_div = 0.0131, ppo_epochs = 9, ppo_updates = 50, grad_norm =  2.883 ± 0.5 [  2.272,  4.340], resets = 2.00 ≥ 2, time = 13.2 \n",
      "\n",
      "5000: {'test1': 1.0, 'test2': 5.0}\n",
      "step =    5000, scores = -5.622 ± 11.288 [-40.549, 15.258], score_ema = -13.957, advantages = -0.039 ± 1.0 [ -9.787,  6.199], abs_actor_obj =  2.692 ± 3.746, entropy_obj = 0.124 ± 0.001, rollout_stds = 0.203 ± 0.016, critic_obj = 0.022 ± 0.005, expl_var = -36.901, kl_div = 0.0121, ppo_epochs = 9, ppo_updates = 50, grad_norm =  3.393 ± 0.6 [  2.553,  4.214], resets = 3.00 ≥ 3, time =  8.0 \n",
      "\n",
      "7500: {'test1': 1.0, 'test2': 2.5}\n",
      "step =    7500, scores =  19.482 ± 14.076 [-13.571, 56.714], score_ema = -5.597, advantages = -0.031 ± 1.0 [-10.683,  6.421], abs_actor_obj =  2.627 ± 3.974, entropy_obj = 0.124 ± 0.001, rollout_stds = 0.203 ± 0.019, critic_obj = 0.014 ± 0.002, expl_var = -29.811, kl_div = 0.0133, ppo_epochs = 9, ppo_updates = 50, grad_norm =  3.551 ± 0.5 [  2.898,  4.360], resets = 2.00 ≥ 2, time =  8.2 \n",
      "\n",
      "10000: {'test1': 1.0, 'test2': 1.25}\n",
      "step =   10000, scores =  46.181 ± 35.005 [-0.358, 102.917], score_ema =  7.347, advantages = -0.006 ± 1.0 [ -6.958,  7.515], abs_actor_obj =  2.584 ± 3.855, entropy_obj = 0.125 ± 0.001, rollout_stds = 0.204 ± 0.023, critic_obj = 0.011 ± 0.001, expl_var = -28.420, kl_div = 0.0161, ppo_epochs = 9, ppo_updates = 50, grad_norm =  3.762 ± 0.2 [  3.295,  4.350], resets = 3.00 ≥ 3, time =  8.0 \n",
      "\n",
      "12500: {'test1': 1.0, 'test2': 0.625}\n",
      "step =   12500, scores =  85.146 ± 34.859 [ 19.534, 150.692], score_ema =  26.797, advantages =  0.026 ± 1.0 [ -7.117,  6.002], abs_actor_obj =  2.564 ± 3.718, entropy_obj = 0.126 ± 0.001, rollout_stds = 0.204 ± 0.029, critic_obj = 0.010 ± 0.001, expl_var = -24.034, kl_div = 0.0120, ppo_epochs = 9, ppo_updates = 50, grad_norm =  3.669 ± 0.5 [  3.180,  4.835], resets = 2.00 ≥ 2, time =  8.0 \n",
      "\n",
      "15000: {'test1': 5.0, 'test2': 0.3125}\n",
      "step =   15000, scores =  90.644 ± 67.174 [-0.594, 208.022], score_ema =  42.759, advantages =  0.033 ± 1.0 [ -7.836,  6.890], abs_actor_obj =  2.578 ± 3.569, entropy_obj = 0.126 ± 0.001, rollout_stds = 0.205 ± 0.032, critic_obj = 0.009 ± 0.001, expl_var = -22.599, kl_div = 0.0141, ppo_epochs = 9, ppo_updates = 50, grad_norm =  3.265 ± 0.2 [  2.861,  3.772], resets = 3.00 ≥ 3, time =  8.0 \n",
      "\n",
      "17500: {'test1': 6.0, 'test2': 0.15625}\n",
      "step =   17500, scores =  134.601 ± 55.156 [ 49.550, 246.741], score_ema =  65.719, advantages =  0.087 ± 1.0 [ -6.774,  6.495], abs_actor_obj =  2.492 ± 3.357, entropy_obj = 0.126 ± 0.001, rollout_stds = 0.206 ± 0.037, critic_obj = 0.007 ± 0.001, expl_var = -14.833, kl_div = 0.0214, ppo_epochs = 9, ppo_updates = 50, grad_norm =  3.256 ± 0.3 [  2.692,  3.949], resets = 2.00 ≥ 2, time =  8.3 \n",
      "\n",
      "20000: {'test1': 6.0, 'test2': 0.078125}\n",
      "step =   20000, scores =  195.481 ± 141.342 [-0.777, 388.751], score_ema =  98.160, advantages =  0.203 ± 1.0 [ -6.203,  6.913], abs_actor_obj =  2.289 ± 3.073, entropy_obj = 0.127 ± 0.001, rollout_stds = 0.205 ± 0.038, critic_obj = 0.007 ± 0.001, expl_var = -9.315, kl_div = 0.0319, ppo_epochs = 9, ppo_updates = 50, grad_norm =  2.876 ± 0.2 [  2.585,  3.299], resets = 3.00 ≥ 3, time =  8.3 \n",
      "\n",
      "22500: {'test1': 6.0, 'test2': 0.0390625}\n",
      "step =   22500, scores =  330.269 ± 118.442 [ 150.736, 518.823], score_ema =  156.187, advantages =  0.372 ± 1.0 [ -8.289,  6.519], abs_actor_obj =  2.041 ± 2.772, entropy_obj = 0.129 ± 0.002, rollout_stds = 0.205 ± 0.041, critic_obj = 0.007 ± 0.001, expl_var = -6.011, kl_div = 0.0385, ppo_epochs = 8, ppo_updates = 40, grad_norm =  2.823 ± 0.2 [  2.424,  3.447], resets = 2.00 ≥ 2, time =  8.2 \n",
      "\n",
      "25000: {'test1': 6.0, 'test2': 0.01953125}\n",
      "step =   25000, scores =  477.097 ± 345.135 [-0.785, 848.115], score_ema =  236.414, advantages =  0.572 ± 1.0 [ -7.286,  5.856], abs_actor_obj =  1.875 ± 2.520, entropy_obj = 0.132 ± 0.002, rollout_stds = 0.204 ± 0.043, critic_obj = 0.008 ± 0.001, expl_var = -3.246, kl_div = 0.0376, ppo_epochs = 6, ppo_updates = 34, grad_norm =  2.582 ± 0.4 [  1.801,  3.485], resets = 3.00 ≥ 3, time =  7.8 \n",
      "\n",
      "27500: {'test1': 10.0, 'test2': 0.009765625}\n",
      "step =   27500, scores =  607.692 ± 209.930 [ 369.512, 947.823], score_ema =  329.234, advantages =  0.659 ± 1.0 [ -7.076,  6.597], abs_actor_obj =  1.811 ± 2.348, entropy_obj = 0.132 ± 0.002, rollout_stds = 0.203 ± 0.040, critic_obj = 0.007 ± 0.001, expl_var = -2.450, kl_div = 0.0390, ppo_epochs = 8, ppo_updates = 41, grad_norm =  2.462 ± 0.5 [  1.843,  3.341], resets = 2.00 ≥ 2, time =  9.3 \n",
      "\n",
      "30000: {'test1': 10.0, 'test2': 0.0048828125}\n",
      "step =   30000, scores =  679.688 ± 518.731 [-0.512, 1207.590], score_ema =  416.847, advantages =  0.729 ± 1.0 [ -6.199,  6.480], abs_actor_obj =  1.840 ± 2.442, entropy_obj = 0.136 ± 0.002, rollout_stds = 0.199 ± 0.038, critic_obj = 0.009 ± 0.002, expl_var = -0.212, kl_div = 0.0382, ppo_epochs = 8, ppo_updates = 40, grad_norm =  2.280 ± 0.6 [  1.491,  3.120], resets = 3.00 ≥ 3, time =  8.1 \n",
      "\n",
      "32500: {'test1': 10.0, 'test2': 0.00244140625}\n",
      "step =   32500, scores =  839.923 ± 297.956 [ 229.993, 1305.376], score_ema =  522.616, advantages =  0.687 ± 1.0 [ -6.363,  6.769], abs_actor_obj =  1.869 ± 2.583, entropy_obj = 0.137 ± 0.002, rollout_stds = 0.197 ± 0.038, critic_obj = 0.008 ± 0.002, expl_var = 0.313, kl_div = 0.0386, ppo_epochs = 7, ppo_updates = 39, grad_norm =  2.808 ± 1.4 [  1.507,  5.453], resets = 2.00 ≥ 2, time =  8.2 \n",
      "\n",
      "35000: {'test1': 10.0, 'test2': 0.001220703125}\n",
      "step =   35000, scores =  881.127 ± 626.013 [-0.253, 1445.320], score_ema =  612.244, advantages =  0.741 ± 1.0 [ -7.330,  5.746], abs_actor_obj =  1.816 ± 2.947, entropy_obj = 0.139 ± 0.002, rollout_stds = 0.198 ± 0.038, critic_obj = 0.010 ± 0.003, expl_var = -0.130, kl_div = 0.0396, ppo_epochs = 5, ppo_updates = 27, grad_norm =  3.415 ± 1.7 [  1.189,  5.346], resets = 3.00 ≥ 3, time =  8.1 \n",
      "\n",
      "37500: {'test1': 10.0, 'test2': 0.0006103515625}\n",
      "step =   37500, scores =  1028.793 ± 374.186 [ 229.104, 1534.347], score_ema =  716.381, advantages =  0.725 ± 1.0 [ -7.959,  6.020], abs_actor_obj =  1.817 ± 2.893, entropy_obj = 0.141 ± 0.002, rollout_stds = 0.197 ± 0.034, critic_obj = 0.009 ± 0.003, expl_var = 0.261, kl_div = 0.0392, ppo_epochs = 6, ppo_updates = 30, grad_norm =  3.197 ± 1.9 [  1.285,  5.726], resets = 2.00 ≥ 2, time =  8.2 \n",
      "\n",
      "40000: {'test1': 10.0, 'test2': 0.00030517578125}\n",
      "step =   40000, scores =  1053.543 ± 808.650 [-0.712, 1844.519], score_ema =  800.672, advantages =  0.629 ± 1.0 [ -8.064,  5.639], abs_actor_obj =  1.896 ± 3.240, entropy_obj = 0.144 ± 0.002, rollout_stds = 0.196 ± 0.035, critic_obj = 0.014 ± 0.004, expl_var = 0.553, kl_div = 0.0407, ppo_epochs = 7, ppo_updates = 35, grad_norm =  4.149 ± 2.0 [  1.317,  6.647], resets = 3.00 ≥ 3, time =  8.2 \n",
      "\n",
      "42500: {'test1': 10.0, 'test2': 0.000152587890625}\n",
      "step =   42500, scores =  1266.646 ± 481.798 [ 42.505, 1919.214], score_ema =  917.165, advantages =  0.656 ± 1.0 [ -8.584,  5.743], abs_actor_obj =  1.797 ± 3.380, entropy_obj = 0.148 ± 0.002, rollout_stds = 0.195 ± 0.039, critic_obj = 0.013 ± 0.005, expl_var = 0.354, kl_div = 0.0378, ppo_epochs = 6, ppo_updates = 34, grad_norm =  3.850 ± 2.5 [  1.074,  8.235], resets = 2.00 ≥ 2, time =  8.2 \n",
      "\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n",
      "done\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "from src.reinforcement_learning.gym.envs.singleton_vector_env import as_vec_env\n",
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = wrap_env(record_env)\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env, _ = as_vec_env(record_env)\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            actions_dist, _ = policy.process_obs(torch.tensor(obs, device=device))\n",
    "            actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-24T18:19:28.972829Z",
     "start_time": "2024-05-24T18:19:07.520281Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\monitoring\\video_recorder.py:178: UserWarning: \u001B[33mWARN: Unable to save last video! Did you call close()?\u001B[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-0.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-1.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-24_20.19.07\\rl-video-episode-2.mp4\n",
      "keyboard interrupt\n",
      "closing record_env\n",
      "record_env closed\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T16:29:07.007390Z",
     "start_time": "2024-04-24T16:29:06.925013Z"
    }
   },
   "id": "bba6ab51a61dd845",
   "execution_count": 10,
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
