{"_default": {"1": {"model_id": "2024-05-24_00.21.16", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_00.21.16--state_dict.pth", "model_info": {"score": 911.7609883791652, "steps_trained": 660000, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n            latent_dim=actor_out_sizes[-1],\n            action_dim=action_size,\n            std=0.1,\n            std_learnable=False,\n            action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        ))\n    # return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n    #     latent_dim=actor_out_sizes[-1],\n    #     action_dim=action_size,\n    #     # std=0.1,\n    #     # std_learnable=False,\n    #     initial_std=0.01,\n    #     squash_output=True,\n    #     learn_sde_features=False,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # ))\n"}, "last_update_time": "2024-05-24 01:05:50.767822"}, "2": {"model_id": "2024-05-24_01.06.04", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_01.06.04--state_dict.pth", "model_info": {"score": -0.20304526890038163, "steps_trained": 12500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.01,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n"}, "last_update_time": "2024-05-24 01:06:55.915606"}, "3": {"model_id": "2024-05-24_01.07.26", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_01.07.26--state_dict.pth", "model_info": {"score": 2013.492012615352, "steps_trained": 220000, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.05,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n"}, "last_update_time": "2024-05-24 01:19:22.027554"}, "4": {"model_id": "2024-05-24_16.15.39", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_16.15.39--state_dict.pth", "model_info": {"score": 1952.59244335741, "steps_trained": 125000, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.05,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n"}, "last_update_time": "2024-05-24 16:22:22.324389"}, "5": {"model_id": "2024-05-24_16.41.38", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_16.41.38--state_dict.pth", "model_info": {"score": -0.9835651657351084, "steps_trained": 62500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.05,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n"}, "last_update_time": "2024-05-24 16:45:02.029021"}, "6": {"model_id": "2024-05-24_16.55.49", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_16.55.49--state_dict.pth", "model_info": {"score": -277.8690138160899, "steps_trained": 22500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.05,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n"}, "last_update_time": "2024-05-24 16:57:09.724586"}, "7": {"model_id": "2024-05-24_16.58.27", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_16.58.27--state_dict.pth", "model_info": {"score": -0.8531376091214774, "steps_trained": 12500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.01,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n"}, "last_update_time": "2024-05-24 16:59:18.377054"}, "8": {"model_id": "2024-05-24_17.07.06", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_17.07.06--state_dict.pth", "model_info": {"score": -0.3853402303533032, "steps_trained": 10000, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.01,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.03),\n    ))\n"}, "last_update_time": "2024-05-24 17:07:50.277327"}, "9": {"model_id": "2024-05-24_17.08.21", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_17.08.21--state_dict.pth", "model_info": {"score": -0.3387969670511786, "steps_trained": 12500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.01,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.03),\n    ))\n"}, "last_update_time": "2024-05-24 17:09:11.350479"}, "10": {"model_id": "2024-05-24_17.12.10", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_17.12.10--state_dict.pth", "model_info": {"score": 2197.6409732913294, "steps_trained": 582500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        # std=0.1,\n        # std_learnable=False,\n        initial_std=0.03,\n        squash_output=True,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n"}, "last_update_time": "2024-05-24 17:43:21.784736"}, "11": {"model_id": "2024-05-24_19.31.23", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_19.31.23--state_dict.pth", "model_info": {"score": -40.68252195186079, "steps_trained": 22500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    # return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n    #     latent_dim=actor_out_sizes[-1],\n    #     action_dim=action_size,\n    #     initial_std=0.03,\n    #     squash_output=True,\n    #     learn_sde_features=False,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # ))\n    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01)\n    ))\n"}, "last_update_time": "2024-05-24 19:33:19.026756"}, "12": {"model_id": "2024-05-24_19.37.16", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_19.37.16--state_dict.pth", "model_info": {"score": -49.0195710542998, "steps_trained": 10000, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    # return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n    #     latent_dim=actor_out_sizes[-1],\n    #     action_dim=action_size,\n    #     initial_std=0.03,\n    #     squash_output=True,\n    #     learn_sde_features=False,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # ))\n    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_activation=lambda log_std: log_std - 1,\n    ))\n"}, "last_update_time": "2024-05-24 19:37:59.353029"}, "13": {"model_id": "2024-05-24_19.38.46", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_19.38.46--state_dict.pth", "model_info": {"score": 2267.4550616632564, "steps_trained": 135000, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    # return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n    #     latent_dim=actor_out_sizes[-1],\n    #     action_dim=action_size,\n    #     initial_std=0.03,\n    #     squash_output=True,\n    #     learn_sde_features=False,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # ))\n    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n        log_std_activation=lambda log_std: log_std - 1,\n    ))\n"}, "last_update_time": "2024-05-24 19:45:59.114714"}, "14": {"model_id": "2024-05-24_19.46.17", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_19.46.17--state_dict.pth", "model_info": {"score": 4049.8851119541823, "steps_trained": 457500, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    # return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n    #     latent_dim=actor_out_sizes[-1],\n    #     action_dim=action_size,\n    #     initial_std=0.03,\n    #     squash_output=True,\n    #     learn_sde_features=False,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # ))\n    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        initial_std=0.2,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    ))\n"}, "last_update_time": "2024-05-24 20:16:13.632534"}, "15": {"model_id": "2024-05-24_20.24.07", "parent_model_id": null, "state_dict_path": "saved_models/rl/HalfCheetah-v4\\2024-05-24_20.24.07--state_dict.pth", "model_info": {"score": -32.903142828749104, "steps_trained": 0, "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    \n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    # _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    return _env\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n    \n    in_size = 17\n    action_size = 6\n    actor_out_sizes = [128, 128, 128, 128]\n    critic_out_sizes = [128, 128, 128, 1]\n\n    # hidden_activation_function = nn.ELU()\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    # return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n    #         latent_dim=actor_out_sizes[-1],\n    #         action_dim=action_size,\n    #         std=0.1,\n    #         std_learnable=False,\n    #         action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     ))\n    return ActorCriticPolicy(A2CNetwork(), StateDependentNoiseActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        initial_std=0.03,\n        squash_output=True,\n        use_full_stds=False,\n        learn_sde_features=False,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    ))\n    # return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n    #     latent_dim=actor_out_sizes[-1],\n    #     action_dim=action_size,\n    #     initial_std=0.2,\n    #     squash_output=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    #     log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    # ))\n"}, "last_update_time": "2024-05-24 20:24:48.010283"}}}