{"_default": {"1": {"model_id": "2024-04-30_22.08.06", "parent_model_id": null, "state_dict_path": "saved_models/rl/Humanoid-v4\\2024-04-30_22.08.06--state_dict.pth", "model_info": {"score": 0.28508483138860485, "wrap_env_function": "def wrap_env(_env: Env):\n    # _env = NormalizeRewardWrapper(_env, gamma=gamma)\n    # _env = TransformObservation(_env, lambda _obs: _obs / 255)\n    _env = TransformReward(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "init_function": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "last_update_time": "2024-04-30 22:08:56.378810"}, "2": {"model_id": "2024-04-30_22.09.28", "parent_model_id": null, "state_dict_path": "saved_models/rl/Humanoid-v4\\2024-04-30_22.09.28--state_dict.pth", "model_info": {"score": 1.0121089434686323, "wrap_env_function": "def wrap_env(_env: Env):\n    # _env = NormalizeRewardWrapper(_env, gamma=gamma)\n    # _env = TransformObservation(_env, lambda _obs: _obs / 255)\n    _env = TransformReward(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "init_function": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "last_update_time": "2024-04-30 23:01:38.796099"}}}