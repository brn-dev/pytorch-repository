{"_default": {"1": {"model_id": "2024-05-26_22.07.50", "parent_model_id": null, "state_dict_path": "saved_models/rl/MultiAgentCartPole/3\\2024-05-26_22.07.50--state_dict.pth", "model_info": {"score": 0.14598843455314636, "steps_trained": 102500, "wrap_env_source_code": "def wrap_env(env_):\n    from src.reinforcement_learning.gym.transform_reward_wrapper import TransformRewardWrapper\n    \n    # env_ = TransformRewardWrapper(env_, lambda reward_: 0.01 * reward_)\n    return env_\n", "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n    from src.networks.skip_nets.additive_skip_net import AdditiveSkipNet, FullyConnectedAdditiveSkipNet, \\\n        FullyConnectedUnweightedAdditiveSkipNet\n    \n    in_size = 8\n    action_size = 2\n    \n    actor_layers = 3\n    actor_features = 96\n    \n    critic_layers = 2\n    critic_features = 96\n\n    # hidden_activation_function = nn.ELU()\n    actor_hidden_activation_function = nn.Tanh()\n    critic_hidden_activation_function = nn.ReLU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n            self.actor_embedding = nn.Sequential(nn.Linear(in_size, actor_features), actor_hidden_activation_function)\n            self.actor = FullyConnectedUnweightedAdditiveSkipNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    MultiheadSelfAttention(\n                        embed_dim=in_features,\n                        num_heads=4,\n                        batch_first=True,\n                    ),\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else actor_hidden_activation_function,\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    nn.Tanh() if is_last_layer else actor_hidden_activation_function,\n                ),\n                num_layers=actor_layers,\n                num_features=actor_features,\n            )\n\n            self.critic_embedding = nn.Sequential(nn.Linear(in_size, critic_features), critic_hidden_activation_function)\n            self.critic = FullyConnectedUnweightedAdditiveSkipNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    MultiheadSelfAttention(\n                        embed_dim=in_features,\n                        num_heads=4,\n                        batch_first=True,\n                    ),\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    critic_hidden_activation_function,\n                    orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n                    critic_hidden_activation_function,\n                ),\n                num_layers=critic_layers,\n                num_features=critic_features,\n            )\n            self.critic_regressor = nn.Linear(critic_features, 1)\n\n        def forward(self, x: torch.Tensor):\n            *batch_shape, nr_actors, nr_features = x.shape\n            x = torch.flatten(x, end_dim=-3)\n            \n            actor_out: torch.Tensor = self.actor(self.actor_embedding(x))\n            critic_out: torch.Tensor = self.critic_regressor(self.critic(self.critic_embedding(x)).sum(dim=-2))\n            \n            actor_out = actor_out.unflatten(dim=0, sizes=batch_shape)\n            critic_out = critic_out.unflatten(dim=0, sizes=batch_shape)\n            \n            return actor_out, critic_out\n        \n    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n        latent_dim=actor_features,\n        action_dim=action_size,\n        base_std=0.5,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    ))\n"}, "last_update_time": "2024-05-26 23:05:17.843519"}}}