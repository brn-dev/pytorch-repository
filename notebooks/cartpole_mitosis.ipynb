{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-09T17:37:12.732160Z",
     "start_time": "2024-06-09T17:37:09.648336Z"
    }
   },
   "source": [
    "import sys\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import AsymmetricExponentialMovingAverage\n",
    "from src.np_functions import softmax\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.async_policy_mitosis import AsyncPolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis_base import TrainInfo, TrainResultInfo\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPOLoggingConfig, PPO\n",
    "from src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.core.policy_construction import PolicyConstruction, \\\n",
    "    create_policy_initialization_info, InitActionSelectorFunction\n",
    "from src.reinforcement_learning.core.policy_construction import PolicyConstructionOverride\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.torch_device import get_torch_device, optimizer_to_device\n",
    "from src.torch_functions import antisymmetric_power\n",
    "from src.trees import Forest\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "nr_carts = 6\n",
    "\n",
    "def make_multi_agent_cart_pole_env(render_mode: str | None = None, time_limit: float | None = None):\n",
    "    from src.reinforcement_learning.gym.envs.multi_agent_cartpole3d import MultiAgentCartPole3D\n",
    "    return MultiAgentCartPole3D(\n",
    "        nr_carts=nr_carts,\n",
    "        cart_size=0.25,\n",
    "        force_magnitude=500,\n",
    "        physics_steps_per_step=10,\n",
    "        reset_position_radius=1.25,\n",
    "        reset_randomize_position_angle_offset=True,\n",
    "        reset_position_randomization_magnitude=0.1,\n",
    "        reset_hinge_randomization_magnitude=0.05,\n",
    "        slide_range=2,\n",
    "        hinge_range=1.2,\n",
    "        time_limit=time_limit or 60.0,\n",
    "        step_reward_function=lambda time_, action, state, prev_state: 0.01 ,\n",
    "        out_ouf_range_reward_function=lambda time_, action, state: 0.0,# -10 + time_ * 3,\n",
    "        time_limit_reward_function=lambda time_, action, state: 10,\n",
    "        render_mode=render_mode,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-09T17:37:12.816295Z",
     "start_time": "2024-06-09T17:37:12.733158Z"
    }
   },
   "id": "102df9c436801920",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-09T17:37:12.817293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n",
    "    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "    from src.weight_initialization import orthogonal_initialization\n",
    "\n",
    "    return PredictedStdActionSelector(\n",
    "        latent_dim=latent_dim,\n",
    "        action_dim=action_dim,\n",
    "        base_std=0.15,\n",
    "        squash_output=True,\n",
    "        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n",
    "    )\n",
    "\n",
    "\n",
    "# Type hinting as string, so they don't cause an error when running outside this context\n",
    "def init_policy(init_action_selector: 'InitActionSelectorFunction', hyper_parameters: dict[str, 'Any']) -> 'BasePolicy':\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.net import Net\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n",
    "    from src.weight_initialization import orthogonal_initialization\n",
    "    from src.networks.multihead_self_attention import MultiheadSelfAttention\n",
    "    \n",
    "    in_size = 8\n",
    "    action_size = 2\n",
    "    \n",
    "    actor_layers = 3\n",
    "    actor_features = 48\n",
    "    \n",
    "    critic_layers = 2\n",
    "    critic_features = 48\n",
    "\n",
    "    actor_hidden_activation_function = nn.ELU\n",
    "    critic_hidden_activation_function = nn.ELU\n",
    "    \n",
    "    actor_hidden_initialization = lambda module: orthogonal_initialization(module, gain=np.sqrt(2))\n",
    "    critic_hidden_initialization = lambda module: orthogonal_initialization(module, gain=np.sqrt(2))\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.actor_embedding = nn.Sequential(nn.Linear(in_size, actor_features), actor_hidden_activation_function())\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.seq_as_net(\n",
    "                        actor_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        actor_hidden_activation_function(),\n",
    "                        actor_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        nn.Tanh() if is_last_layer else actor_hidden_activation_function(),\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=actor_layers,\n",
    "                num_features=actor_features,\n",
    "            )\n",
    "\n",
    "            self.critic_embedding = nn.Sequential(nn.Linear(in_size, critic_features), critic_hidden_activation_function())\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.seq_as_net(\n",
    "                        critic_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        critic_hidden_activation_function(),\n",
    "                        critic_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        critic_hidden_activation_function(),\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=critic_layers,\n",
    "                num_features=critic_features,\n",
    "            )\n",
    "            self.critic_regressor = nn.Linear(critic_features, 1)\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            *batch_shape, nr_actors, nr_features = x.shape\n",
    "            x = torch.flatten(x, end_dim=-3)\n",
    "            \n",
    "            actor_out: torch.Tensor = self.actor(self.actor_embedding(x))\n",
    "            critic_out: torch.Tensor = self.critic_regressor(self.critic(self.critic_embedding(x)).sum(dim=-2))\n",
    "            \n",
    "            actor_out = actor_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            critic_out = critic_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            \n",
    "            return actor_out, critic_out\n",
    "\n",
    "    actor_selector = init_action_selector(actor_features, action_size, hyper_parameters)\n",
    "        \n",
    "    return ActorCriticPolicy(A2CNetwork(), actor_selector)\n",
    "\n",
    "def init_optimizer(policy: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n",
    "    import torch.optim\n",
    "    return torch.optim.AdamW(policy.parameters(), lr=1e-5)\n",
    "\n",
    "def wrap_env(env_, hyper_parameters: dict[str, 'Any']):\n",
    "    return env_\n",
    "\n",
    "def train_func(train_info: TrainInfo) -> TrainResultInfo:\n",
    "    policy = train_info['policy']\n",
    "    optimizer = train_info['optimizer']\n",
    "    env = train_info['env']\n",
    "    policy_info = train_info['policy_info']\n",
    "    \n",
    "    score = 0.0\n",
    "    score_ema = AsymmetricExponentialMovingAverage(up_alpha=0.2, down_alpha=0.5)\n",
    "    rollout_stopwatch = Stopwatch()\n",
    "    def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):   \n",
    "        \n",
    "        if 'raw_rewards' in info['rollout']:\n",
    "            raw_rewards = info['rollout']['raw_rewards']\n",
    "            _, gamma_1_returns = compute_gae_and_returns(\n",
    "                value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "                rewards=raw_rewards,\n",
    "                episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "                last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_rewards=None,\n",
    "                normalize_advantages=None,\n",
    "            )\n",
    "        else:\n",
    "            _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "                last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_advantages=None,\n",
    "                normalize_rewards=None,\n",
    "            )\n",
    "        \n",
    "        episode_scores = gamma_1_returns[\n",
    "            rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "        ]\n",
    "        \n",
    "        nonlocal score, score_ema\n",
    "        score = episode_scores.mean()\n",
    "        \n",
    "        current_score_ema = None\n",
    "        if not np.isnan(score):\n",
    "            current_score_ema = score_ema.update(score)\n",
    "        else:\n",
    "            print(f'================================= Warning ================================= \\n'\n",
    "                  f' Score is NaN! There was likely no episode start/end in the rollout buffer \\n'\n",
    "                  f'=========================================================================== \\n\\n\\n')\n",
    "        \n",
    "        rollout_time = rollout_stopwatch.reset()\n",
    "        \n",
    "        resets: np.ndarray = rl.buffer.episode_starts.astype(int).sum(axis=0)\n",
    "        resets_mean = resets.mean()\n",
    "        resets_min = resets.min()\n",
    "        \n",
    "        print(f'{policy_info[\"policy_id\"]}  {step:>6}: '\n",
    "              f'{score = :9.3f}, '\n",
    "              f'score_ema = {current_score_ema or score_ema.get():9.3f}, '\n",
    "              f'time = {rollout_time:5.2f}, '\n",
    "              f'resets = {resets_mean:5.2f} >= {resets_min:5.2f}')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    optimizations_done = 0\n",
    "    def on_optimization_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "        nonlocal optimizations_done\n",
    "        optimizations_done += 1\n",
    "\n",
    "    policy_info_str = ('('\n",
    "          f'policy_id = {policy_info[\"policy_id\"]}, '\n",
    "          f'parent_id = {policy_info[\"parent_policy_id\"]}, '\n",
    "          f'num_parameters = {count_parameters(policy)}, '\n",
    "          f'previous_steps = {policy_info[\"steps_trained\"]}, '\n",
    "          f'previous_score = {policy_info[\"score\"]:9.3f}'\n",
    "          ')')\n",
    "    \n",
    "    print(f'Starting PPO with policy {policy_info_str:s} for {steps_per_iteration:_} steps')\n",
    "    mitosis_iteration_stopwatch = Stopwatch()\n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=optimizer_to_device(optimizer, device),\n",
    "        buffer_size=5000,\n",
    "        gamma=0.995,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        weigh_and_reduce_actor_objective=lambda obj: antisymmetric_power(obj, 1.5).mean(),\n",
    "        weigh_and_reduce_entropy_objective=None,  # lambda obj: 1.0 * obj.mean(),\n",
    "        weigh_and_reduce_critic_objective=lambda obj: 0.5 * obj.mean(),\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.025,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.1,\n",
    "        grad_norm_clip_value=1.0,\n",
    "        callback=Callback(\n",
    "            on_rollout_done=on_rollout_done,\n",
    "            on_optimization_done=on_optimization_done,\n",
    "        ),\n",
    "        logging_config=PPOLoggingConfig(log_rollout_infos=True),\n",
    "        torch_device=device,\n",
    "    ).train(steps_per_iteration)\n",
    "    \n",
    "    \n",
    "    print(f'Training finished for policy {policy_info_str:s}, end score = {score:9.3f}, time = {mitosis_iteration_stopwatch.time_passed():6.2f}')\n",
    "    \n",
    "    return {\n",
    "        'steps_trained': steps_per_iteration, \n",
    "        'optimizations_done': optimizations_done, \n",
    "        'score': score_ema.get(),\n",
    "    }\n",
    "\n",
    "def select_policy_selection_probs(policy_infos: Iterable[MitosisPolicyInfo]) -> np.ndarray:\n",
    "    # TODO introduce score change momentum factor, average child score\n",
    "    policy_infos = list(policy_infos)\n",
    "    policy_info_forest = Forest(\n",
    "        policy_infos, \n",
    "        get_id=lambda pi: pi['policy_id'], \n",
    "        get_parent_id=lambda pi: pi['parent_policy_id']\n",
    "    )\n",
    "    \n",
    "    scores = np.array([policy_info['score'] for policy_info in policy_infos], dtype=float)\n",
    "    score_probs = softmax(scores, temperature=2.5 / np.log(len(scores) + 1), normalize=True)\n",
    "    \n",
    "    num_descendants = np.array([\n",
    "        policy_info_forest.compute_num_descendants(policy_info['policy_id'], discount_factor=0.5) \n",
    "        for policy_info in policy_infos\n",
    "    ], dtype=float)\n",
    "    num_descendants_probs = softmax(-num_descendants, temperature=0.5)\n",
    "    \n",
    "    steps_trained = np.array([policy_info['steps_trained'] for policy_info in policy_infos], dtype=float)\n",
    "    steps_trained_probs = softmax(-steps_trained, temperature=0.1, normalize=True)\n",
    "    \n",
    "    score_weight = 1.0\n",
    "    num_descendants_weight = 0.5\n",
    "    steps_trained_weight = 0.5\n",
    "    \n",
    "    probs = (\n",
    "        score_probs**score_weight * \n",
    "        num_descendants_probs**num_descendants_weight * \n",
    "        steps_trained_probs**steps_trained_weight\n",
    "    )\n",
    "    probs /= probs.sum()\n",
    "    \n",
    "    print('policy selection probs = \\n\\t' + '\\n\\t'.join(\n",
    "        f'{(policy_id := policy_infos[i][\"policy_id\"])}: {p = :8.6f}, '\n",
    "        f'score = {policy_infos[i][\"score\"]:7.3f}, '\n",
    "        f'score_prob = {score_probs[i]**score_weight:7.5f}, '\n",
    "        f'num_children = {len(policy_info_forest[policy_id].children)}, '\n",
    "        f'num_descendants = {num_descendants[i]:7.3f}, '\n",
    "        f'descendants_prob = {num_descendants_probs[i]**num_descendants_weight:7.5f}, '\n",
    "        f'steps = {policy_infos[i][\"steps_trained\"]}, '\n",
    "        f'steps_prob = {steps_trained_probs[i]**steps_trained_weight:7.5f}, '\n",
    "        for i, p\n",
    "        in enumerate(probs)\n",
    "    ))\n",
    "    \n",
    "    return probs\n",
    "\n",
    "device = get_torch_device(\"cuda:0\") if True else get_torch_device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "steps_per_iteration = 100_000\n",
    "\n",
    "num_envs = 16\n",
    "\n",
    "# mitosis_id = get_current_timestamp()\n",
    "mitosis_id = '2024-06-04_20.00.00'\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'E:/saved_models/rl/MultiAgentCartPole/{nr_carts}/mitosis-{mitosis_id}')\n",
    "policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'C:/Users/domin/git/pytorch-starter/saved_models/rl/MultiAgentCartPole/{nr_carts}/mitosis-{mitosis_id}')\n",
    "\n",
    "try:\n",
    "    print(f'Starting {nr_carts} agent cartpole mitosis with id {mitosis_id}')\n",
    "    AsyncPolicyMitosis(\n",
    "        num_workers=3,\n",
    "        policy_db=policy_db,\n",
    "        train_policy_function=train_func,\n",
    "        create_env=lambda: parallelize_env_async(lambda: make_multi_agent_cart_pole_env(None), num_envs),\n",
    "        new_policy_initialization_info=create_policy_initialization_info(\n",
    "            init_action_selector=init_action_selector,\n",
    "            init_policy=init_policy,\n",
    "            init_optimizer=init_optimizer,\n",
    "            wrap_env=wrap_env,\n",
    "        ),\n",
    "        new_policy_prob_function=lambda nr_policies, nr_primordial_ancestors: 0.0,\n",
    "        policy_construction_override=PolicyConstructionOverride(),\n",
    "        select_policy_selection_probs=select_policy_selection_probs,\n",
    "        min_primordial_ancestors=5,\n",
    "        rng_seed=None,\n",
    "        initialization_delay=5,\n",
    "        delay_between_workers=20,\n",
    "        save_optimizer_state_dicts=True,\n",
    "        load_optimizer_state_dicts=True,\n",
    "    ).train_with_mitosis(1000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:    \n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "id": "90c056a126f17111",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "Starting 6 agent cartpole mitosis with id 2024-06-04_20.00.00\n",
      "Starting worker 0 with delay = 0\n",
      "Started training iteration for policy: 2024-06-09_19.37.18~ASSNxg, parent policy id: None\n",
      "Starting worker 1 with delay = 20\n",
      "Started training iteration for policy: 2024-06-09_19.37.38~IdmQ2J, parent policy id: None\n",
      "Starting worker 2 with delay = 40\n",
      "Started training iteration for policy: 2024-06-09_19.37.58~4oCJTR, parent policy id: None\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "237c7b802d108464",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f5a34a1dc6074925",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3c4b2598e23a18",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "19a7c08bba2c4d65",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-01T17:22:37.737329Z"
    }
   },
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def record_video():\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "    from gymnasium.wrappers import AutoResetWrapper, RecordVideo\n",
    "\n",
    "    record_env, _ = as_vec_env(make_multi_agent_cart_pole_env(render_mode='rgb_array', time_limit=180.0))\n",
    "    \n",
    "    policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'E:/saved_models/rl/MultiAgentCartPole/6/mitosis-2024-05-28_20.00.00')\n",
    "    print(policy_db)\n",
    "    \n",
    "    # policy_entry = max(policy_db.all_entries(), key=lambda entry: entry['model_info']['score'])\n",
    "    policy_entry = policy_db.fetch_entry('2024-06-01_18.12.50~XFh5RF')\n",
    "    policy_info : MitosisPolicyInfo = policy_entry['model_info']\n",
    "    print(policy_entry)\n",
    "\n",
    "    policy, _, record_env = PolicyConstruction.init_from_info(policy_info['initialization_info'], record_env)\n",
    "\n",
    "    policy_db.load_model_state_dict(policy, policy_entry['model_id'])\n",
    "    \n",
    "    try:\n",
    "        record_env.metadata['render_fps'] = 500 / record_env.physics_steps_per_step\n",
    "        record_env = AutoResetWrapper(\n",
    "            RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "        )\n",
    "        \n",
    "        def record(max_steps: int):\n",
    "            with torch.no_grad():\n",
    "                obs, info = record_env.reset()\n",
    "                for step in tqdm(range(max_steps)):\n",
    "                    actions_dist, _ = policy.process_obs(torch.tensor(obs, device='cpu'))\n",
    "                    actions = actions_dist.get_actions(deterministic=True).cpu().numpy()\n",
    "                    obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "        \n",
    "        record(50_000)\n",
    "    except KeyboardInterrupt:\n",
    "        print('keyboard interrupt')\n",
    "    finally:\n",
    "        print('closing record_env')\n",
    "        record_env.close()\n",
    "        print('record_env closed')\n",
    "\n",
    "record_video()"
   ],
   "id": "bb9ea562feef0c28",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T23:19:39.805681Z",
     "start_time": "2024-05-28T23:19:39.700649Z"
    }
   },
   "id": "6339c31b7c72572e",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from tinydb import Query\n",
    "\n",
    "with TinyModelDB[MitosisPolicyInfo](base_path=f'E:/saved_models/rl/MultiAgentCartPole/6/mitosis-2024-05-28_20.00.00') as policy_db:\n",
    "    # ids = [\n",
    "    # ]\n",
    "    # for id in ids:\n",
    "    #     policy_db.delete_entry(id, delete_state_dict=True)\n",
    "    \n",
    "    for entry in policy_db.all_entries():\n",
    "        entry['model_info']['optimizations_done'] = 0\n",
    "        policy_db.db.update({'model_info': entry['model_info']}, Query().model_id == entry['model_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T15:42:10.781709Z",
     "start_time": "2024-05-29T15:42:10.232410Z"
    }
   },
   "id": "c034467ae224547a",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "56540b7fff3998cb",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
