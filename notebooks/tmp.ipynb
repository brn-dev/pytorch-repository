{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T16:16:59.862463Z",
     "start_time": "2024-05-17T16:16:59.846803Z"
    }
   },
   "source": [
    "from typing import SupportsFloat, Any\n",
    "\n",
    "from gymnasium.core import ActType, ObsType\n",
    "\n",
    "from tmp_mp import main"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T16:17:15.230794Z",
     "start_time": "2024-05-17T16:17:01.129793Z"
    }
   },
   "id": "83a784f8b65b74f8",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(\"HalfCheetah-v4\", n_envs=4)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, use_sde=True, sde_sample_freq=100, verbose=2)\n",
    "model.learn(total_timesteps=250000)\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "#     vec_env.render(\"human\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T22:15:54.226301Z",
     "start_time": "2024-05-23T22:14:18.948367Z"
    }
   },
   "id": "6d26ca5d3fba5ab7",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "rollout_data = np.random.random((2500, 32, 17))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T22:42:33.450537Z",
     "start_time": "2024-05-23T22:42:33.437024Z"
    }
   },
   "id": "8f542b5b7a30c20d",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%timeit\n",
    "for i in range(len(rollout_data)):\n",
    "    tensor = torch.tensor(rollout_data[i], device='cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-23T22:42:46.607754Z",
     "start_time": "2024-05-23T22:42:33.589059Z"
    }
   },
   "id": "7e926ae1d8feae57",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import types\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "class A(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(17, 17)\n",
    "        \n",
    "        self.forward = self.linear.forward\n",
    "        \n",
    "    # def forward(self, x):\n",
    "    #     return self.linear(x)\n",
    "    \n",
    "test_data = torch.tensor(np.random.random((2500, 1024, 17))).float()\n",
    "\n",
    "a = A()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:25:49.731860Z",
     "start_time": "2024-05-28T17:25:49.343973Z"
    }
   },
   "id": "18b462eb301c3290",
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%timeit\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    a(test_data[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:26:00.167853Z",
     "start_time": "2024-05-28T17:25:49.732855Z"
    }
   },
   "id": "25fc61df57f13567",
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%timeit\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    a(test_data[i]).detach()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:26:11.629808Z",
     "start_time": "2024-05-28T17:26:00.168853Z"
    }
   },
   "id": "3e64a75ffcb3f542",
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from src.torch_device import get_torch_device\n",
    "\n",
    "get_torch_device()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T19:20:16.958767Z",
     "start_time": "2024-05-29T19:20:11.568280Z"
    }
   },
   "id": "c22c1a2827faef5",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium\n",
    "\n",
    "gymnasium.make('Ant-v4',)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11fe74cc94873a0b",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def record_video():\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "    import gymnasium\n",
    "    from gymnasium.wrappers import AutoResetWrapper, RecordVideo    \n",
    "    from src.model_db.tiny_model_db import TinyModelDB\n",
    "    from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "    from src.reinforcement_learning.core.policy_construction import PolicyConstruction\n",
    "    from src.datetime import get_current_timestamp\n",
    "\n",
    "    record_env, _ = as_vec_env(gymnasium.make('Ant-v4', render_mode='rgb_array'))\n",
    "\n",
    "    policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'E:/saved_models/rl/Ant-v4/mitosis-2024-06-10_19.43.13')\n",
    "    print(policy_db)\n",
    "    \n",
    "    policy_entry = list(sorted(policy_db.all_entries(), key=lambda entry: entry['model_info']['score']))[-2]\n",
    "    # policy_entry = policy_db.fetch_entry('2024-06-10_22.13.57~PJHPLG')\n",
    "    policy_info : MitosisPolicyInfo = policy_entry['model_info']\n",
    "    print(policy_entry)\n",
    "\n",
    "    policy, _, record_env = PolicyConstruction.init_from_info(policy_info['initialization_info'], record_env)\n",
    "\n",
    "    policy_db.load_model_state_dict(policy_entry['model_id'], policy)\n",
    "    \n",
    "    try:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "        record_env = AutoResetWrapper(\n",
    "            RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "        )\n",
    "        \n",
    "        def record(max_steps: int):\n",
    "            with torch.no_grad():\n",
    "                obs, info = record_env.reset()\n",
    "                for step in tqdm(range(max_steps)):\n",
    "                    actions_dist, _ = policy.process_obs(torch.tensor(obs, device='cpu'))\n",
    "                    actions = actions_dist.get_actions(deterministic=True).cpu().numpy()\n",
    "                    obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "        \n",
    "        record(50_000)\n",
    "    except KeyboardInterrupt:\n",
    "        print('keyboard interrupt')\n",
    "    finally:\n",
    "        print('closing record_env')\n",
    "        record_env.close()\n",
    "        print('record_env closed')\n",
    "\n",
    "record_video()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-10T21:30:42.251803Z"
    }
   },
   "id": "9413fb35d2e62be7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from gymnasium.wrappers import AutoResetWrapper, TimeLimit\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium\n",
    "from gymnasium.core import ActType, ObsType\n",
    "from typing import Any, SupportsFloat\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\n",
    "\n",
    "\n",
    "class TestEnv(gymnasium.Env):\n",
    "    \n",
    "    counter: int\n",
    "    \n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[ObsType, dict[str, Any]]:\n",
    "        self.counter = 0\n",
    "        return np.array([self.counter]), {}\n",
    "    \n",
    "    def step(\n",
    "        self, action: ActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        self.counter += 1\n",
    "        done_ = self.counter >= 10\n",
    "        \n",
    "        if done_:\n",
    "            reward_ = 5.0\n",
    "        else:\n",
    "            reward_ = 0.0\n",
    "        \n",
    "        return np.array([self.counter]), reward_, done_, False, {}\n",
    "\n",
    "\n",
    "\n",
    "env = AutoResetWrapper(TimeLimit(TestEnv(), 100))\n",
    "obs, _ = env.reset()\n",
    "episode_starts = True\n",
    "\n",
    "obs_l = []\n",
    "rewards = []\n",
    "dones = []\n",
    "\n",
    "for i in range(50):\n",
    "    next_obs, reward, terminated, truncated, _ = env.step(None)\n",
    "    obs_l.append(obs)\n",
    "    rewards.append(reward)\n",
    "    dones.append(episode_starts)\n",
    "    episode_starts = np.logical_or(terminated, truncated)\n",
    "    obs = next_obs\n",
    "    \n",
    "\n",
    "returns, episode_returns = compute_episode_returns(\n",
    "    rewards=np.array(rewards)[:, np.newaxis],\n",
    "    episode_starts=np.array(dones)[:, np.newaxis],\n",
    "    last_episode_starts=np.array([episode_starts]),\n",
    "    gamma=1.0,\n",
    "    gae_lambda=1.0,\n",
    "    normalize_rewards=None,\n",
    "    remove_unfinished_episodes=True\n",
    ")    \n",
    "\n",
    "for i, (obs_, reward_, return_, done) in enumerate(zip(obs_l, rewards, returns, dones)):\n",
    "    print(f'{i}; {obs_}; {reward_}; {return_}; {done}')\n",
    "print(obs, episode_starts)\n",
    "print()\n",
    "\n",
    "print(len(episode_returns), np.mean(episode_returns), np.std(episode_returns))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:37:33.198877Z",
     "start_time": "2024-06-14T18:37:33.185470Z"
    }
   },
   "id": "70f22b7cb50ff18a",
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "232fbbf514e29e1d",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
