{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:12:23.719310Z",
     "start_time": "2024-10-10T18:12:18.995294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from sac import init_policy, init_action_selector\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import stable_baselines3 as sb\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "import gymnasium\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "env_kwargs = {}\n",
    "num_envs = 1\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gymnasium.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "env = create_env(render_mode=None)\n",
    "\n",
    "sb_sac = sb.SAC(\"MlpPolicy\", env, verbose=10, learning_starts=10000, stats_window_size=1) # , seed=594371)"
   ],
   "id": "47c2f325a8cba618",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T18:12:23.735587Z",
     "start_time": "2024-10-10T18:12:23.720312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.reinforcement_learning.core.polyak_update import polyak_update\n",
    "from src.reinforcement_learning.core.buffers.replay.base_replay_buffer import ReplayBufferSamples\n",
    "from src.hyper_parameters import HyperParameters\n",
    "import torch\n",
    "from src.reinforcement_learning.core.type_aliases import TensorObs\n",
    "from typing import Optional\n",
    "from src.reinforcement_learning.core.policies.components.feature_extractors import FeatureExtractor\n",
    "import copy\n",
    "from src.console import print_warning\n",
    "from src.tags import Tags\n",
    "from src.reinforcement_learning.core.policies.components.actor import Actor\n",
    "from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "import stable_baselines3 as sb\n",
    "\n",
    "\n",
    "class DebugSACPolicy(BasePolicy):\n",
    "    \n",
    "    actor: sb.sac.policies.Actor\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            actor: Actor,\n",
    "            critic: QCritic,\n",
    "            shared_feature_extractor: Optional[FeatureExtractor] = None\n",
    "    ):\n",
    "        super().__init__(actor, shared_feature_extractor)\n",
    "        self.actor = sb_sac.actor\n",
    "        self.critic = sb_sac.critic\n",
    "\n",
    "        self._build_target()\n",
    "\n",
    "        self._check_action_selector()\n",
    "        \n",
    "    @property\n",
    "    def uses_sde(self):\n",
    "        return False\n",
    "        \n",
    "    def act(self, obs: TensorObs) -> torch.Tensor:\n",
    "        return self.actor(obs, False)\n",
    "    \n",
    "    def reset_sde_noise(self, batch_size: int) -> None:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def collect_hyper_parameters(self) -> HyperParameters:\n",
    "        return {}\n",
    "\n",
    "    def collect_tags(self) -> Tags:\n",
    "        return []\n",
    "\n",
    "    def _check_action_selector(self):\n",
    "        # if not isinstance(self.actor.action_selector, (PredictedStdActionSelector, StateDependentNoiseActionSelector)):\n",
    "        #     print_warning('SAC not being used with PredictedStdAction Selector or gSDE. LogStds should be clamped!')\n",
    "        pass\n",
    "\n",
    "    def _build_target(self):\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.target_critic.set_training_mode(False)\n",
    "\n",
    "        self.target_shared_feature_extractor = copy.deepcopy(self.shared_feature_extractor)\n",
    "        self.target_shared_feature_extractor.set_trainable(False)\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError('forward is not used in SACPolicy')\n",
    "\n",
    "    def compute_target_values(\n",
    "            self,\n",
    "            replay_samples: ReplayBufferSamples,\n",
    "            entropy_coef: torch.Tensor,\n",
    "            gamma: float,\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            next_observations = replay_samples.next_observations\n",
    "\n",
    "            next_actions, next_actions_log_prob = self.actor.action_log_prob(\n",
    "                self.shared_feature_extractor(next_observations)\n",
    "            )\n",
    "\n",
    "            next_q_values = torch.cat(\n",
    "                self.target_critic(self.target_shared_feature_extractor(next_observations), next_actions),\n",
    "                dim=-1\n",
    "            )\n",
    "            next_q_values, _ = torch.min(next_q_values, dim=-1, keepdim=True)\n",
    "            next_q_values = next_q_values - entropy_coef * next_actions_log_prob.reshape(-1, 1)\n",
    "\n",
    "            target_q_values = replay_samples.rewards + (1 - replay_samples.dones) * gamma * next_q_values\n",
    "\n",
    "            return target_q_values\n",
    "\n",
    "\n",
    "    def perform_polyak_update(self, tau: float):\n",
    "        polyak_update(self.critic.parameters(), self.target_critic.parameters(), tau)\n",
    "        polyak_update(\n",
    "            self.shared_feature_extractor.parameters(),\n",
    "            self.target_shared_feature_extractor.parameters(),\n",
    "            tau\n",
    "        )\n",
    "\n",
    "    def set_train_mode(self, mode: bool) -> None:\n",
    "        self.actor.set_training_mode(mode)\n",
    "        self.critic.set_training_mode(mode)\n",
    "        # Leaving target_critic on train_mode = False\n",
    "\n",
    "        self.shared_feature_extractor.set_train_mode(mode)\n",
    "        # Leaving target_shared_feature_extractor on train_mode = False\n",
    "\n",
    "        self.train_mode = mode\n"
   ],
   "id": "1948bbcf34ea601e",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from src.reinforcement_learning.algorithms.sac.sac import SACLoggingConfig, SAC\n",
    "from dataclasses import dataclass\n",
    "from typing import Type, Optional, Any, Literal\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from src.function_types import TorchTensorFn\n",
    "from src.module_analysis import calculate_grad_norm\n",
    "from src.hyper_parameters import HyperParameters\n",
    "from src.reinforcement_learning.algorithms.base.base_algorithm import PolicyProvider\n",
    "from src.reinforcement_learning.algorithms.base.off_policy_algorithm import OffPolicyAlgorithm, ReplayBuf\n",
    "from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n",
    "from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n",
    "from src.reinforcement_learning.core.action_noise import ActionNoise\n",
    "from src.reinforcement_learning.core.buffers.replay.base_replay_buffer import BaseReplayBuffer, ReplayBufferSamples\n",
    "from src.reinforcement_learning.core.buffers.replay.replay_buffer import ReplayBuffer\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.infos import InfoDict, concat_infos\n",
    "from src.reinforcement_learning.core.logging import LoggingConfig, log_if_enabled\n",
    "from src.reinforcement_learning.core.loss_config import weigh_and_reduce_loss, LossLoggingConfig\n",
    "from src.reinforcement_learning.core.type_aliases import OptimizerProvider, TensorObs, detach_obs\n",
    "from src.reinforcement_learning.gym.env_analysis import get_single_action_space\n",
    "from src.tags import Tags\n",
    "from src.torch_device import TorchDevice\n",
    "from src.torch_functions import identity\n",
    "from src.repr_utils import func_repr\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "SAC_DEFAULT_OPTIMIZER_PROVIDER = lambda params: optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\n",
    "AUTO_TARGET_ENTROPY = 'auto'\n",
    "\n",
    "\n",
    "class SACDebug(SAC):\n",
    "    \n",
    "    @property\n",
    "    def replay_buffer(self):\n",
    "        return self.buffer\n",
    "\n",
    "    buffer: BaseReplayBuffer\n",
    "    target_entropy: float\n",
    "    log_ent_coef: Optional[torch.Tensor]\n",
    "    entropy_coef_optimizer: Optional[optim.Optimizer]\n",
    "    entropy_coef_tensor: Optional[torch.Tensor]\n",
    "    \n",
    "    def collect_hyper_parameters(self) -> HyperParameters:\n",
    "        print(f'{type(self.policy) = }, {type(self.policy.actor) = }, {type(self.policy.critic) = }, {type(self.policy.target_critic) = }, {type(self.buffer) = }')\n",
    "        return super().collect_hyper_parameters()\n",
    "    \n",
    "    \n",
    "    def _setup_entropy_optimization(\n",
    "            self,\n",
    "            entropy_coef: float,\n",
    "            target_entropy: float | Literal['auto'],\n",
    "            entropy_coef_optimizer_provider: Optional[OptimizerProvider],\n",
    "    ):\n",
    "        if target_entropy == 'auto':\n",
    "            self.target_entropy = float(-np.prod(get_single_action_space(self.env).shape).astype(np.float32))\n",
    "        else:\n",
    "            self.target_entropy = float(target_entropy)\n",
    "\n",
    "        if entropy_coef_optimizer_provider is not None:\n",
    "            self.log_ent_coef = torch.log(\n",
    "                torch.tensor([entropy_coef], device=self.torch_device, dtype=self.torch_dtype)\n",
    "            ).requires_grad_(True)\n",
    "            self.entropy_coef_optimizer = entropy_coef_optimizer_provider([self.log_ent_coef])\n",
    "            self.entropy_coef_tensor = None\n",
    "        else:\n",
    "            self.log_ent_coef = None\n",
    "            self.entropy_coef_optimizer = None\n",
    "            self.entropy_coef_tensor = torch.tensor(entropy_coef, device=self.torch_device, dtype=self.torch_dtype)\n",
    "\n",
    "    # def get_and_optimize_entropy_coef(\n",
    "    #         self,\n",
    "    #         actions_pi_log_prob: torch.Tensor,\n",
    "    #         info: InfoDict\n",
    "    # ) -> torch.Tensor:\n",
    "    #     if self.entropy_coef_optimizer is not None:\n",
    "    #         entropy_coef = torch.exp(self.log_ent_coef.detach())\n",
    "    # \n",
    "    #         entropy_coef_loss = weigh_and_reduce_loss(\n",
    "    #             raw_loss=-self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach(),\n",
    "    #             weigh_and_reduce_function=self.weigh_and_reduce_entropy_coef_loss,\n",
    "    #             info=info,\n",
    "    #             loss_name='entropy_coef_loss',\n",
    "    #             logging_config=self.logging_config.entropy_coef_loss\n",
    "    #         )\n",
    "    #         self.entropy_coef_optimizer.zero_grad()\n",
    "    #         entropy_coef_loss.backward()\n",
    "    #         self.entropy_coef_optimizer.step()\n",
    "    # \n",
    "    #         return entropy_coef\n",
    "    #     else:\n",
    "    #         return self.entropy_coef_tensor\n",
    "    # \n",
    "    # def calculate_critic_loss(\n",
    "    #         self,\n",
    "    #         observation_features: TensorObs,\n",
    "    #         replay_samples: ReplayBufferSamples,\n",
    "    #         entropy_coef: torch.Tensor,\n",
    "    #         info: InfoDict,\n",
    "    # ):\n",
    "    #     target_q_values = self.policy.compute_target_values(\n",
    "    #         replay_samples=replay_samples,\n",
    "    #         entropy_coef=entropy_coef,\n",
    "    #         gamma=self.gamma,\n",
    "    #     )\n",
    "    #     # critic loss should not influence shared feature extractor\n",
    "    #     current_q_values = self.critic(detach_obs(observation_features), replay_samples.actions)\n",
    "    # \n",
    "    #     # noinspection PyTypeChecker\n",
    "    #     critic_loss: torch.Tensor = 0.5 * sum(\n",
    "    #         F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n",
    "    #     )\n",
    "    #     critic_loss = weigh_and_reduce_loss(\n",
    "    #         raw_loss=critic_loss,\n",
    "    #         weigh_and_reduce_function=self.weigh_critic_loss,\n",
    "    #         info=info,\n",
    "    #         loss_name='critic_loss',\n",
    "    #         logging_config=self.logging_config.critic_loss,\n",
    "    #     )\n",
    "    #     return critic_loss\n",
    "    # \n",
    "    # def calculate_actor_loss(\n",
    "    #         self,\n",
    "    #         observation_features: TensorObs,\n",
    "    #         actions_pi: torch.Tensor,\n",
    "    #         actions_pi_log_prob: torch.Tensor,\n",
    "    #         entropy_coef: torch.Tensor,\n",
    "    #         info: InfoDict,\n",
    "    # ) -> torch.Tensor:\n",
    "    #     q_values_pi = torch.cat(self.critic(observation_features, actions_pi), dim=-1)\n",
    "    #     min_q_values_pi, _ = torch.min(q_values_pi, dim=-1, keepdim=True)\n",
    "    #     actor_loss = entropy_coef * actions_pi_log_prob - min_q_values_pi\n",
    "    # \n",
    "    #     actor_loss = weigh_and_reduce_loss(\n",
    "    #         raw_loss=actor_loss,\n",
    "    #         weigh_and_reduce_function=self.weigh_and_reduce_actor_loss,\n",
    "    #         info=info,\n",
    "    #         loss_name='actor_loss',\n",
    "    #         logging_config=self.logging_config.actor_loss,\n",
    "    #     )\n",
    "    # \n",
    "    #     return actor_loss\n",
    "\n",
    "    def optimize(self, last_obs: np.ndarray, last_episode_starts: np.ndarray, info: InfoDict) -> None:\n",
    "        ent_coef_losses, ent_coefs = [], []\n",
    "        actor_losses, critic_losses = [], []\n",
    "\n",
    "        for gradient_step in range(self.gradient_steps):\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(self.optimization_batch_size, env=None)  # type: ignore[union-attr]\n",
    "\n",
    "            # We need to sample because `log_std` may have changed between two gradient steps\n",
    "            # if self.sde_noise_sample_freq:\n",
    "            #     self.actor.reset_noise()\n",
    "\n",
    "            # Action by the current actor for the sampled state\n",
    "            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n",
    "            log_prob = log_prob.reshape(-1, 1)\n",
    "\n",
    "            ent_coef_loss = None\n",
    "            if self.entropy_coef_optimizer is not None and self.log_ent_coef is not None:\n",
    "                # Important: detach the variable from the graph\n",
    "                # so we don't change it with other losses\n",
    "                # see https://github.com/rail-berkeley/softlearning/issues/60\n",
    "                ent_coef = torch.exp(self.log_ent_coef.detach())\n",
    "                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n",
    "                ent_coef_losses.append(ent_coef_loss.item())\n",
    "            else:\n",
    "                ent_coef = self.entropy_coef_tensor\n",
    "\n",
    "            ent_coefs.append(ent_coef.item())\n",
    "\n",
    "            # Optimize entropy coefficient, also called\n",
    "            # entropy temperature or alpha in the paper\n",
    "            if ent_coef_loss is not None and self.entropy_coef_optimizer is not None:\n",
    "                self.entropy_coef_optimizer.zero_grad()\n",
    "                ent_coef_loss.backward()\n",
    "                self.entropy_coef_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Select action according to policy\n",
    "                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n",
    "                # Compute the next Q values: min over all critics targets\n",
    "                next_q_values = torch.cat(self.policy.target_critic(replay_data.next_observations, next_actions), dim=1)\n",
    "                next_q_values, _ = torch.min(next_q_values, dim=1, keepdim=True)\n",
    "                # add entropy term\n",
    "                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n",
    "                # td error + entropy term\n",
    "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q-values estimates for each critic network\n",
    "            # using action from the replay buffer\n",
    "            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n",
    "            assert isinstance(critic_loss, torch.Tensor)  # for type checker\n",
    "            critic_losses.append(critic_loss.item())  # type: ignore[union-attr]\n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic.optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic.optimizer.step()\n",
    "\n",
    "            # Compute actor loss\n",
    "            # Alternative: actor_loss = torch.mean(log_prob - qf1_pi)\n",
    "            # Min over all critic networks\n",
    "            q_values_pi = torch.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n",
    "            min_qf_pi, _ = torch.min(q_values_pi, dim=1, keepdim=True)\n",
    "            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n",
    "            actor_losses.append(actor_loss.item())\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "\n",
    "            # Update target networks\n",
    "            if gradient_step % self.target_update_interval == 0:\n",
    "                polyak_update(self.critic.parameters(), self.policy.target_critic.parameters(), self.tau)\n",
    "                # Copy running stats, see GH issue #996\n",
    "        \n",
    "        info['entropy_coef'] = np.array(ent_coefs)\n",
    "        info['final_entropy_coef_loss'] = np.array(ent_coef_losses)\n",
    "        info['final_actor_loss'] = np.array(actor_losses)\n",
    "        info['final_critic_loss'] = np.array(critic_losses)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T18:12:23.778122Z",
     "start_time": "2024-10-10T18:12:23.736591Z"
    }
   },
   "id": "dae89c724469f5ba",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import time\n",
    "\n",
    "from gymnasium import Env\n",
    "\n",
    "from sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.core.policy_construction import PolicyConstruction\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from typing import Any\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T18:12:23.911081Z",
     "start_time": "2024-10-10T18:12:23.778122Z"
    }
   },
   "id": "2051c5de0ad8edfb",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from src.summary_statistics import maybe_compute_summary_statistics\n",
    "from src.reinforcement_learning.core.loss_config import LossLoggingConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\n",
    "def get_setup() -> dict[str, str]:\n",
    "    import inspect\n",
    "    import sac\n",
    "    return {\n",
    "        'sac.py': inspect.getsource(sac),\n",
    "        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n",
    "    }\n",
    "\n",
    "policy_id: str\n",
    "policy: BasePolicy\n",
    "optimizer: optim.Optimizer\n",
    "wrapped_env: Env\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n",
    "            env=env,\n",
    "            info=PolicyConstruction.create_policy_initialization_info(\n",
    "                init_action_selector=init_action_selector,\n",
    "                init_policy=init_policy,\n",
    "                init_optimizer=init_optimizer,\n",
    "                wrap_env=wrap_env,\n",
    "                hyper_parameters=policy_construction_hyper_parameter,\n",
    "            ),\n",
    "        )\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "step_stopwatch = Stopwatch()\n",
    "total_stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    # tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    # rewards = rl.buffer.rewards[tail_indices]\n",
    "    # if 'raw_rewards' in info['rollout']:\n",
    "    #     rewards = info['rollout']['raw_rewards']\n",
    "    \n",
    "    # episode_scores = compute_episode_returns(\n",
    "    #     rewards=rewards,\n",
    "    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n",
    "    #     last_episode_starts=info['last_episode_starts'],\n",
    "    #     gamma=1.0,\n",
    "    #     gae_lambda=1.0,\n",
    "    #     normalize_rewards=None,\n",
    "    #     remove_unfinished_episodes=True,\n",
    "    # )\n",
    "    \n",
    "    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n",
    "    # \n",
    "    # if len(episode_scores) > 0:\n",
    "    # \n",
    "    #     global best_iteration_score\n",
    "    #     iteration_score = episode_scores.mean()\n",
    "    #     score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    #     if iteration_score >= best_iteration_score:\n",
    "    #         best_iteration_score = iteration_score\n",
    "    #         policy_db.save_model_state_dict(\n",
    "    #             model_id=policy_id,\n",
    "    #             parent_model_id=parent_policy_id,\n",
    "    #             model_info={\n",
    "    #                 'score': iteration_score.item(),\n",
    "    #                 'steps_trained': steps_trained,\n",
    "    #                 'wrap_env_source_code': wrap_env_source_code_source,\n",
    "    #                 'init_policy_source_code': init_policy_source\n",
    "    #             },\n",
    "    #             model=policy,\n",
    "    #             optimizer=optimizer,\n",
    "    #         )\n",
    "    #     info['score_moving_average'] = score_moving_average\n",
    "    # \n",
    "    # info['episode_scores'] = episode_scores\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    # global steps_trained\n",
    "    # steps_trained += rl.buffer.pos\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    num_env_steps = step * rl.num_envs\n",
    "    \n",
    "    step_time = step_stopwatch.reset()\n",
    "    total_time = total_stopwatch.time_passed()\n",
    "    \n",
    "    # TODO!!\n",
    "    # tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    # episode_scores = info.get('episode_scores')\n",
    "    score_moving_average = info.get('score_moving_average') or 0.0\n",
    "    \n",
    "    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n",
    "    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "        n_format='>2'\n",
    "    )\n",
    "    # scores2 = format_summary_statics(\n",
    "    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='4.3f',\n",
    "    #     min_value_format=' 6.3f',\n",
    "    #     max_value_format='5.3f',\n",
    "    #     n_format='>2'\n",
    "    # )\n",
    "    # advantages = format_summary_statics(\n",
    "    #     rl.buffer.advantages, \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    actor_loss = format_summary_statics(\n",
    "        info['final_actor_loss'],  \n",
    "        mean_format=' 5.3f',\n",
    "        # std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # actor_loss_raw = format_summary_statics(\n",
    "    #     info['raw_actor_loss'],  \n",
    "    #     mean_format=' 5.3f',\n",
    "    #     std_format='5.3f',\n",
    "    #     min_value_format=None,\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n",
    "        info['final_entropy_coef_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_loss = format_summary_statics(\n",
    "        info['final_critic_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_coef = format_summary_statics(\n",
    "        info['entropy_coef'],\n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # resets = format_summary_statics(\n",
    "    #     rl.buffer.dones.astype(int).sum(axis=0), \n",
    "    #     mean_format='.2f',\n",
    "    #     std_format=None,\n",
    "    #     min_value_format='1d',\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    # kl_div = info['actor_kl_divergence'][-1]\n",
    "    # grad_norm = format_summary_statics(\n",
    "    #     info['grad_norm'], \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    action_stds = info['rollout'].get('action_stds')\n",
    "    if action_stds is not None:\n",
    "        rollout_action_stds = format_summary_statics(\n",
    "            action_stds,\n",
    "            mean_format='5.3f',\n",
    "            std_format='5.3f',\n",
    "            min_value_format=None,\n",
    "            max_value_format=None,\n",
    "        )\n",
    "    else:\n",
    "        rollout_action_stds = 'N/A'\n",
    "    action_magnitude = format_summary_statics(\n",
    "        np.abs(rl.buffer.actions[tail_indices]),\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # ppo_epochs = info['nr_ppo_epochs']\n",
    "    # ppo_updates = info['nr_ppo_updates']\n",
    "    # expl_var = rl.buffer.compute_critic_explained_variance()\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{num_env_steps = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          # f\"{scores2 = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          # f\"{advantages = :s}, \"\n",
    "          f\"{actor_loss = :s}, \"\n",
    "          # f\"{actor_loss_raw = :s}, \"\n",
    "          f\"{critic_loss = :s}, \"\n",
    "          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n",
    "          f\"{entropy_coef = :s}, \"\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{action_magnitude = :s}, \"\n",
    "          # f\"{expl_var = :.3f}, \"\n",
    "          # f\"{kl_div = :.4f}, \"\n",
    "          # f\"{ppo_epochs = }, \"\n",
    "          # f\"{ppo_updates = }, \"\n",
    "          # f\"{grad_norm = :s}, \"\n",
    "          f\"n_updates = {rl.gradient_steps_performed}, \"\n",
    "          # f\"{resets = :s}, \"\n",
    "          f\"time = {step_time:4.1f}, \"\n",
    "          f\"total_time = {total_time:4.1f} \\n\"\n",
    "          )\n",
    "    logger.add_item({\n",
    "        'step': step,\n",
    "        'num_env_steps': num_env_steps,\n",
    "        'scores': maybe_compute_summary_statistics(episode_scores),\n",
    "        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n",
    "        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n",
    "        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n",
    "        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n",
    "        'action_stds': maybe_compute_summary_statistics(action_stds),\n",
    "        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n",
    "        'num_gradient_steps': rl.gradient_steps_performed,\n",
    "        'step_time': step_time,\n",
    "        'total_time': total_time\n",
    "    })\n",
    "    if step % 10000 == 0:\n",
    "        logger.save_experiment_log()\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n",
    "    #     logger.save_experiment_log()\n",
    "    #     raise ValueError('Score too low, policy probably fucked :(')\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "env_kwargs = {}\n",
    "num_envs = 1\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "\n",
    "# TODO\n",
    "# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "env = create_env(render_mode=None)\n",
    "\n",
    "logger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "try:\n",
    "    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = }')\n",
    "        \n",
    "    with ((torch.autograd.set_detect_anomaly(False))):\n",
    "        algo = SACDebug(\n",
    "            env=wrapped_env,\n",
    "            policy=DebugSACPolicy(policy.actor, policy.critic),\n",
    "            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n",
    "            # weigh_critic_loss=lambda l: 1 * l,\n",
    "            buffer_size=1_000_000,\n",
    "            reward_scale=1,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n",
    "            entropy_coef=1.0,\n",
    "            rollout_steps=1,\n",
    "            gradient_steps=1,\n",
    "            warmup_steps=10_000,\n",
    "            optimization_batch_size=256,\n",
    "            target_update_interval=1,\n",
    "            # sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_last_obs=True, log_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n",
    "                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n",
    "                                            critic_loss=LossLoggingConfig(log_final=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        \n",
    "        # Todo!\n",
    "        algo.buffer = sb_sac.replay_buffer\n",
    "        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        total_stopwatch.reset()\n",
    "        with log_experiment(\n",
    "            logger,\n",
    "            experiment_tags=algo.collect_tags() + ['Debug'],\n",
    "            hyper_parameters=algo.collect_hyper_parameters(),\n",
    "            setup=get_setup(),\n",
    "        ) as x:\n",
    "            logger.save_experiment_log()\n",
    "            print('\\nStarting Training\\n\\n')\n",
    "            # import cProfile\n",
    "            # pr = cProfile.Profile()\n",
    "            # pr.enable()\n",
    "            algo.learn(5_000_000)\n",
    "            # pr.disable()  \n",
    "            # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T18:34:49.487982Z",
     "start_time": "2024-10-10T18:12:23.912083Z"
    }
   },
   "id": "2bf98616d22b5cdc",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "logger.experiment_log['experiment_id']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-25T19:28:38.199879Z",
     "start_time": "2024-09-25T19:28:37.969996Z"
    }
   },
   "id": "bc8c5c974988ba38",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T17:58:26.515534Z",
     "start_time": "2024-10-10T17:58:24.654973Z"
    }
   },
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "algo.buffer.observations.shape"
   ],
   "id": "66c40f43fdaba700",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "11ef78ceafe9a42",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
