{
    "experiment_id": "2024-09-23_22-43-51_643276~0LpLwL",
    "experiment_tags": [
        "SAC",
        "HalfCheetah-v4"
    ],
    "start_time": "2024-09-23 22:43:51.643276",
    "end_time": "2024-09-23 22:46:24.605850",
    "model_db_reference": null,
    "hyper_parameters": {
        "__type": "SAC",
        "env": "<RescaleAction<TransformRewardWrapper<AsyncVectorEnv instance>>>",
        "num_envs": 16,
        "policy": {
            "__type": "SACPolicy",
            "parameter_count": 217870,
            "feature_extractor": {
                "__type": "IdentityExtractor",
                "parameter_count": 0
            },
            "actor": {
                "__type": "Actor",
                "parameter_count": 73484,
                "feature_extractor": {
                    "__type": "IdentityExtractor",
                    "parameter_count": 0
                },
                "network": "Sequential(\n  (0): Linear(in_features=17, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ELU(alpha=1.0)\n)",
                "action_selector": "PredictedStdActionSelector(\n  (action_net): Linear(in_features=256, out_features=6, bias=True)\n  (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n)"
            },
            "critic": {
                "__type": "QCritic",
                "parameter_count": 144386,
                "feature_extractor": {
                    "__type": "IdentityExtractor",
                    "parameter_count": 0
                },
                "n_critics": 2,
                "q_network": "Sequential(\n  (0): Linear(in_features=23, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=256, out_features=1, bias=True)\n)"
            }
        },
        "policy_parameter_count": 217870,
        "buffer": {
            "buffer_size": 15000,
            "num_envs": 16,
            "total_buffer_size": 240000,
            "torch_device": "cuda:0",
            "torch_dtype": "torch.float32",
            "np_dtype": "<class 'numpy.float32'>",
            "optimize_memory_usage": false
        },
        "buffer_step_size": 15000,
        "buffer_total_size": 240000,
        "gamma": 0.99,
        "sde_noise_sample_freq": null,
        "torch_device": "cuda:0",
        "torch_dtype": "torch.float32",
        "tau": 0.005,
        "rollout_steps": 2,
        "gradient_steps": 2,
        "optimization_batch_size": 256,
        "action_noise": null,
        "warmup_steps": 500,
        "actor_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.5, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "critic_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.5, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "entropy_coef_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object at 0x00007FF833D262F0>",
        "weigh_and_reduce_actor_loss": "<function <lambda> at 0x0000025309739F80>",
        "weigh_critic_loss": "<function <lambda> at 0x000002530AF491C0>",
        "target_update_interval": 1,
        "target_entropy": -6.0,
        "entropy_coef": "Dynamic"
    },
    "setup": {
        "sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ELU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n#             BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n",
        "notebook": "import inspect\nimport os\nimport time\nfrom pathlib import Path\n\nimport gymnasium\nfrom gymnasium import Env\nfrom gymnasium.vector import VectorEnv\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.model_db import ModelDB\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.np_functions import inv_symmetric_log\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.model_db.tiny_model_db import TinyModelDB\nfrom src.module_analysis import count_parameters, get_gradients_per_parameter\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\nfrom src.reinforcement_learning.gym.envs.test_env import TestEnv\nfrom src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\nfrom typing import Any, SupportsFloat, Optional\nfrom gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\nfrom src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\nfrom src.reinforcement_learning.core.normalization import NormalizationType\nfrom src.torch_device import set_default_torch_device, optimizer_to_device\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nfrom torch.distributions import Normal, Categorical\n\nimport torch\nfrom torch import optim, nn\nimport torch.distributions as dist\nimport gymnasium as gym\nimport numpy as np\n\nfrom src.torch_functions import antisymmetric_power\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    episode_scores = compute_episode_returns(\n        rewards=rewards,\n        episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n        last_episode_starts=info['last_episode_starts'],\n        gamma=1.0,\n        gae_lambda=1.0,\n        normalize_rewards=None,\n        remove_unfinished_episodes=True,\n    )\n    \n    global best_iteration_score\n    iteration_score = episode_scores.mean()\n    score_moving_average = score_mean_ema.update(iteration_score)\n    if iteration_score >= best_iteration_score:\n        best_iteration_score = iteration_score\n        policy_db.save_model_state_dict(\n            model_id=policy_id,\n            parent_model_id=parent_policy_id,\n            model_info={\n                'score': iteration_score.item(),\n                'steps_trained': steps_trained,\n                'wrap_env_source_code': wrap_env_source_code_source,\n                'init_policy_source_code': init_policy_source\n            },\n            model=policy,\n            optimizer=optimizer,\n        )\n    \n    info['episode_scores'] = episode_scores\n    info['score_moving_average'] = score_moving_average\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    \n    time_taken = stopwatch.reset()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info['episode_scores']\n    score_moving_average = info['score_moving_average']\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {time_taken:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'scores': compute_summary_statistics(episode_scores),\n        'actor_loss': compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': compute_summary_statistics(info['final_entropy_coef_loss']),\n        'critic_loss': compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': compute_summary_statistics(info['entropy_coef']),\n        'action_stds': compute_summary_statistics(action_stds),\n        'action_magnitude': compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'gradient_step': rl.gradient_steps_performed,\n        'time_taken': time_taken,\n    })\n    if step % 10000 == 0:\n        logger.save_experiment()\n    print()\n    \n    if episode_scores.mean().item() < -500:\n        logger.save_experiment()\n        raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 16\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\nenv = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n\nlogger = ExperimentLogger('experiment_logs/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = } \\n\\n')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=15_000,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=2,\n            gradient_steps=2,\n            warmup_steps=500,\n            learning_starts=500,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        with log_experiment(\n            logger,\n            experiment_tags=[type(algo).__name__, env_name],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(1_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"
    },
    "notes": [],
    "logs_by_category": {
        "__default": [
            {
                "step": 1000,
                "scores": {
                    "n": 16,
                    "mean": -265.806884765625,
                    "std": 80.67440795898438,
                    "min_value": -464.0863952636719,
                    "max_value": -118.64876556396484
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -12.770866394042969,
                    "std": 0.25636816024780273,
                    "min_value": -12.952146530151367,
                    "max_value": -12.589587211608887
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -1.5039641857147217,
                    "std": 0.006748637650161982,
                    "min_value": -1.5087361335754395,
                    "max_value": -1.4991921186447144
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 2.8609108924865723,
                    "std": 0.009312767535448074,
                    "min_value": 2.854325771331787,
                    "max_value": 2.8674960136413574
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.8607233762741089,
                    "std": 0.00018215867748949677,
                    "min_value": 0.8605945706367493,
                    "max_value": 0.8608521819114685
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.8965272307395935,
                    "std": 0.02858094684779644,
                    "min_value": 0.8125565052032471,
                    "max_value": 0.9728070497512817
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5087224245071411,
                    "std": 0.28718748688697815,
                    "min_value": 5.593523383140564e-06,
                    "max_value": 0.9999889731407166
                },
                "gradient_step": 502,
                "time_taken": 8.83817458152771,
                "__timestamp": "2024-09-23 22:43:57.544819"
            },
            {
                "step": 2000,
                "scores": {
                    "n": 16,
                    "mean": -217.6732177734375,
                    "std": 104.03636932373047,
                    "min_value": -435.629638671875,
                    "max_value": -14.266961097717285
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -22.386253356933594,
                    "std": 0.15396074950695038,
                    "min_value": -22.495119094848633,
                    "max_value": -22.277385711669922
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -4.452518463134766,
                    "std": 0.019844084978103638,
                    "min_value": -4.466550350189209,
                    "max_value": -4.438486576080322
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 2.693404197692871,
                    "std": 0.33888494968414307,
                    "min_value": 2.4537763595581055,
                    "max_value": 2.9330320358276367
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.6382334232330322,
                    "std": 0.0001343220064882189,
                    "min_value": 0.638138473033905,
                    "max_value": 0.6383284330368042
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.9219450950622559,
                    "std": 0.033230047672986984,
                    "min_value": 0.8464251756668091,
                    "max_value": 1.0074009895324707
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5290288925170898,
                    "std": 0.28715887665748596,
                    "min_value": 5.662441253662109e-07,
                    "max_value": 0.9994552135467529
                },
                "gradient_step": 1502,
                "time_taken": 10.383097887039185,
                "__timestamp": "2024-09-23 22:44:07.926916"
            },
            {
                "step": 3000,
                "scores": {
                    "n": 16,
                    "mean": -221.53518676757812,
                    "std": 44.725040435791016,
                    "min_value": -305.75567626953125,
                    "max_value": -143.95924377441406
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -28.400279998779297,
                    "std": 0.18577925860881805,
                    "min_value": -28.531644821166992,
                    "max_value": -28.26891326904297
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -7.355504989624023,
                    "std": 0.0941419005393982,
                    "min_value": -7.4220733642578125,
                    "max_value": -7.288936614990234
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 6.88176155090332,
                    "std": 3.7050974369049072,
                    "min_value": 4.261862277984619,
                    "max_value": 9.50166130065918
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.4740011692047119,
                    "std": 9.99512558337301e-05,
                    "min_value": 0.4739305078983307,
                    "max_value": 0.4740718603134155
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.9274339079856873,
                    "std": 0.031366508454084396,
                    "min_value": 0.846158504486084,
                    "max_value": 1.0022279024124146
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5386736392974854,
                    "std": 0.2891548275947571,
                    "min_value": 5.185604095458984e-06,
                    "max_value": 0.9996739029884338
                },
                "gradient_step": 2502,
                "time_taken": 10.18294644355774,
                "__timestamp": "2024-09-23 22:44:18.109863"
            },
            {
                "step": 4000,
                "scores": {
                    "n": 16,
                    "mean": -155.17222595214844,
                    "std": 44.05441665649414,
                    "min_value": -215.90240478515625,
                    "max_value": -75.63682556152344
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -31.431760787963867,
                    "std": 0.10964924097061157,
                    "min_value": -31.509294509887695,
                    "max_value": -31.35422706604004
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -10.199262619018555,
                    "std": 0.018586423248052597,
                    "min_value": -10.21240520477295,
                    "max_value": -10.18612003326416
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 8.425232887268066,
                    "std": 6.5841450691223145,
                    "min_value": 3.7695395946502686,
                    "max_value": 13.080926895141602
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.35156428813934326,
                    "std": 7.413630373775959e-05,
                    "min_value": 0.3515118658542633,
                    "max_value": 0.3516167104244232
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.9387943744659424,
                    "std": 0.031775832176208496,
                    "min_value": 0.8030151128768921,
                    "max_value": 1.066074252128601
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5435746908187866,
                    "std": 0.2894075810909271,
                    "min_value": 3.933906555175781e-05,
                    "max_value": 0.99953293800354
                },
                "gradient_step": 3502,
                "time_taken": 10.194700956344604,
                "__timestamp": "2024-09-23 22:44:28.304563"
            },
            {
                "step": 5000,
                "scores": {
                    "n": 16,
                    "mean": -169.1009063720703,
                    "std": 77.68004608154297,
                    "min_value": -343.26116943359375,
                    "max_value": -37.52760314941406
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -33.20985412597656,
                    "std": 0.3849106431007385,
                    "min_value": -33.482025146484375,
                    "max_value": -32.937679290771484
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -12.919437408447266,
                    "std": 0.08304142951965332,
                    "min_value": -12.978157043457031,
                    "max_value": -12.860718727111816
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 6.2125115394592285,
                    "std": 0.47200626134872437,
                    "min_value": 5.878752708435059,
                    "max_value": 6.546270370483398
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.26111918687820435,
                    "std": 5.487519592861645e-05,
                    "min_value": 0.26108038425445557,
                    "max_value": 0.2611579895019531
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.920483410358429,
                    "std": 0.03854285553097725,
                    "min_value": 0.7380815744400024,
                    "max_value": 1.016363263130188
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.550813615322113,
                    "std": 0.2910541892051697,
                    "min_value": 5.066394805908203e-07,
                    "max_value": 0.9996846914291382
                },
                "gradient_step": 4502,
                "time_taken": 10.702468872070312,
                "__timestamp": "2024-09-23 22:44:39.007032"
            },
            {
                "step": 6000,
                "scores": {
                    "n": 16,
                    "mean": -163.9908447265625,
                    "std": 55.27410125732422,
                    "min_value": -243.41845703125,
                    "max_value": -13.578792572021484
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -33.063011169433594,
                    "std": 0.19911114871501923,
                    "min_value": -33.20380401611328,
                    "max_value": -32.922218322753906
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -15.52012825012207,
                    "std": 0.051475126296281815,
                    "min_value": -15.556527137756348,
                    "max_value": -15.48373031616211
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 7.255551338195801,
                    "std": 4.908332824707031,
                    "min_value": 3.7848358154296875,
                    "max_value": 10.726266860961914
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.19430583715438843,
                    "std": 4.069278293172829e-05,
                    "min_value": 0.1942770630121231,
                    "max_value": 0.19433461129665375
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.9266057014465332,
                    "std": 0.05659891292452812,
                    "min_value": 0.7186852097511292,
                    "max_value": 1.041556477546692
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.554966390132904,
                    "std": 0.29108792543411255,
                    "min_value": 1.8537044525146484e-05,
                    "max_value": 0.9998070001602173
                },
                "gradient_step": 5502,
                "time_taken": 11.060746669769287,
                "__timestamp": "2024-09-23 22:44:50.068779"
            },
            {
                "step": 7000,
                "scores": {
                    "n": 16,
                    "mean": -213.8698272705078,
                    "std": 54.42584228515625,
                    "min_value": -298.18109130859375,
                    "max_value": -113.18950653076172
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -32.16425704956055,
                    "std": 0.1963975727558136,
                    "min_value": -32.303131103515625,
                    "max_value": -32.02538299560547
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -17.592647552490234,
                    "std": 0.06339021027088165,
                    "min_value": -17.63747215270996,
                    "max_value": -17.54782485961914
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 6.404329776763916,
                    "std": 4.747836112976074,
                    "min_value": 3.047102689743042,
                    "max_value": 9.761556625366211
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.14500993490219116,
                    "std": 2.9724067644565366e-05,
                    "min_value": 0.14498890936374664,
                    "max_value": 0.1450309455394745
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.9335089325904846,
                    "std": 0.056946732103824615,
                    "min_value": 0.8069431781768799,
                    "max_value": 1.1152160167694092
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5582903027534485,
                    "std": 0.2922581732273102,
                    "min_value": 6.556510925292969e-07,
                    "max_value": 0.9998222589492798
                },
                "gradient_step": 6502,
                "time_taken": 10.536365032196045,
                "__timestamp": "2024-09-23 22:45:00.605144"
            },
            {
                "step": 8000,
                "scores": {
                    "n": 16,
                    "mean": -206.8668670654297,
                    "std": 41.03275680541992,
                    "min_value": -267.89666748046875,
                    "max_value": -102.93427276611328
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -30.92472267150879,
                    "std": 0.09922919422388077,
                    "min_value": -30.994888305664062,
                    "max_value": -30.854557037353516
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -19.1733341217041,
                    "std": 0.4512450695037842,
                    "min_value": -19.492412567138672,
                    "max_value": -18.85425567626953
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 3.266089677810669,
                    "std": 0.7600233554840088,
                    "min_value": 2.7286720275878906,
                    "max_value": 3.8035073280334473
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.108685702085495,
                    "std": 2.1768846636405215e-05,
                    "min_value": 0.10867030918598175,
                    "max_value": 0.10870109498500824
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.901233434677124,
                    "std": 0.06431292742490768,
                    "min_value": 0.7306398749351501,
                    "max_value": 1.0550533533096313
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.567270815372467,
                    "std": 0.2932969331741333,
                    "min_value": 3.6507844924926758e-06,
                    "max_value": 0.9998197555541992
                },
                "gradient_step": 7502,
                "time_taken": 10.36348271369934,
                "__timestamp": "2024-09-23 22:45:10.967626"
            },
            {
                "step": 9000,
                "scores": {
                    "n": 16,
                    "mean": -207.66098022460938,
                    "std": 36.0258903503418,
                    "min_value": -283.6171875,
                    "max_value": -139.8069610595703
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -29.232769012451172,
                    "std": 0.06368017941713333,
                    "min_value": -29.27779769897461,
                    "max_value": -29.187740325927734
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -20.32584571838379,
                    "std": 0.17858125269412994,
                    "min_value": -20.45212173461914,
                    "max_value": -20.199569702148438
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 2.5955724716186523,
                    "std": 0.38446423411369324,
                    "min_value": 2.3237152099609375,
                    "max_value": 2.867429733276367
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.08190001547336578,
                    "std": 1.6389856682508253e-05,
                    "min_value": 0.08188842982053757,
                    "max_value": 0.0819116085767746
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.8807573914527893,
                    "std": 0.0760982558131218,
                    "min_value": 0.6273053884506226,
                    "max_value": 1.0267754793167114
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5779423117637634,
                    "std": 0.29390203952789307,
                    "min_value": 3.311038017272949e-05,
                    "max_value": 0.999939501285553
                },
                "gradient_step": 8502,
                "time_taken": 10.397179365158081,
                "__timestamp": "2024-09-23 22:45:21.364806"
            },
            {
                "step": 10000,
                "scores": {
                    "n": 16,
                    "mean": -149.270751953125,
                    "std": 25.83329200744629,
                    "min_value": -192.07730102539062,
                    "max_value": -99.77009582519531
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -27.675315856933594,
                    "std": 0.5515464544296265,
                    "min_value": -28.065319061279297,
                    "max_value": -27.285314559936523
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -21.807262420654297,
                    "std": 0.3604007363319397,
                    "min_value": -22.062105178833008,
                    "max_value": -21.55242156982422
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 2.5856502056121826,
                    "std": 0.44993850588798523,
                    "min_value": 2.267495632171631,
                    "max_value": 2.9038047790527344
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.06199897825717926,
                    "std": 1.2428052286850289e-05,
                    "min_value": 0.06199019029736519,
                    "max_value": 0.06200776621699333
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.8822140693664551,
                    "std": 0.08286961168050766,
                    "min_value": 0.6458141803741455,
                    "max_value": 1.0642675161361694
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5901533365249634,
                    "std": 0.2952873408794403,
                    "min_value": 4.172325134277344e-06,
                    "max_value": 0.9998813271522522
                },
                "gradient_step": 9502,
                "time_taken": 10.24520492553711,
                "__timestamp": "2024-09-23 22:45:31.610011"
            },
            {
                "step": 11000,
                "scores": {
                    "n": 16,
                    "mean": -135.90145874023438,
                    "std": 40.784828186035156,
                    "min_value": -238.9197998046875,
                    "max_value": -68.43657684326172
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -25.506175994873047,
                    "std": 0.2580479681491852,
                    "min_value": -25.688644409179688,
                    "max_value": -25.32370948791504
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -21.338821411132812,
                    "std": 0.379643976688385,
                    "min_value": -21.607269287109375,
                    "max_value": -21.070371627807617
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 1.9662976264953613,
                    "std": 0.2611284852027893,
                    "min_value": 1.7816518545150757,
                    "max_value": 2.1509432792663574
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.04711387678980827,
                    "std": 9.26177017390728e-06,
                    "min_value": 0.04710732772946358,
                    "max_value": 0.04712042585015297
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.8278501033782959,
                    "std": 0.09356268495321274,
                    "min_value": 0.5907946825027466,
                    "max_value": 1.0259281396865845
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.606033205986023,
                    "std": 0.2961345314979553,
                    "min_value": 3.427267074584961e-06,
                    "max_value": 0.9999873638153076
                },
                "gradient_step": 10502,
                "time_taken": 10.892597436904907,
                "__timestamp": "2024-09-23 22:45:42.503609"
            },
            {
                "step": 12000,
                "scores": {
                    "n": 16,
                    "mean": -124.13121795654297,
                    "std": 74.94818115234375,
                    "min_value": -398.53570556640625,
                    "max_value": -60.64802169799805
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -23.52752113342285,
                    "std": 0.381511926651001,
                    "min_value": -23.797290802001953,
                    "max_value": -23.25775146484375
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -19.952482223510742,
                    "std": 0.537594199180603,
                    "min_value": -20.332618713378906,
                    "max_value": -19.572345733642578
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 1.9589430093765259,
                    "std": 0.4056183993816376,
                    "min_value": 1.6721274852752686,
                    "max_value": 2.245758533477783
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.03591935709118843,
                    "std": 6.780374405934708e-06,
                    "min_value": 0.03591456264257431,
                    "max_value": 0.03592415153980255
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.7885103225708008,
                    "std": 0.09381718188524246,
                    "min_value": 0.5720422863960266,
                    "max_value": 0.9997533559799194
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.6255432367324829,
                    "std": 0.29662370681762695,
                    "min_value": 2.5033950805664062e-06,
                    "max_value": 0.9999246597290039
                },
                "gradient_step": 11502,
                "time_taken": 10.314099311828613,
                "__timestamp": "2024-09-23 22:45:52.817707"
            },
            {
                "step": 13000,
                "scores": {
                    "n": 16,
                    "mean": -74.725830078125,
                    "std": 100.27033996582031,
                    "min_value": -447.16064453125,
                    "max_value": -5.243732929229736
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -21.765668869018555,
                    "std": 0.052121829241514206,
                    "min_value": -21.80252456665039,
                    "max_value": -21.72881317138672
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -18.343843460083008,
                    "std": 0.7549451589584351,
                    "min_value": -18.877670288085938,
                    "max_value": -17.810016632080078
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 2.327890157699585,
                    "std": 0.585739016532898,
                    "min_value": 1.913710117340088,
                    "max_value": 2.742070198059082
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.02760089561343193,
                    "std": 5.077378318674164e-06,
                    "min_value": 0.027597304433584213,
                    "max_value": 0.0276044849306345
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.7455820441246033,
                    "std": 0.09591703116893768,
                    "min_value": 0.5279726386070251,
                    "max_value": 0.9716378450393677
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.6444481611251831,
                    "std": 0.29584506154060364,
                    "min_value": 5.125999450683594e-06,
                    "max_value": 0.999943196773529
                },
                "gradient_step": 12502,
                "time_taken": 10.48564076423645,
                "__timestamp": "2024-09-23 22:46:03.303348"
            },
            {
                "step": 14000,
                "scores": {
                    "n": 16,
                    "mean": 96.90275573730469,
                    "std": 42.479644775390625,
                    "min_value": 22.744640350341797,
                    "max_value": 164.81155395507812
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -20.0970458984375,
                    "std": 0.0947919711470604,
                    "min_value": -20.164073944091797,
                    "max_value": -20.030017852783203
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -16.877796173095703,
                    "std": 0.34530338644981384,
                    "min_value": -17.121963500976562,
                    "max_value": -16.633630752563477
                },
                "critic_loss": {
                    "n": 2,
                    "mean": 1.8902256488800049,
                    "std": 0.2906244397163391,
                    "min_value": 1.684723138809204,
                    "max_value": 2.0957281589508057
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.02136109583079815,
                    "std": 3.695751729537733e-06,
                    "min_value": 0.021358482539653778,
                    "max_value": 0.02136370912194252
                },
                "action_stds": {
                    "n": 192,
                    "mean": 0.7036519646644592,
                    "std": 0.08135566115379333,
                    "min_value": 0.5057803392410278,
                    "max_value": 0.9989621043205261
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.674105703830719,
                    "std": 0.292773962020874,
                    "min_value": 2.0772218704223633e-05,
                    "max_value": 0.9999697804450989
                },
                "gradient_step": 13502,
                "time_taken": 10.545144319534302,
                "__timestamp": "2024-09-23 22:46:13.847492"
            },
            {
                "step": 15000,
                "scores": {
                    "n": 16,
                    "mean": -511.8850402832031,
                    "std": 33.628570556640625,
                    "min_value": -547.4173583984375,
                    "max_value": -456.1907958984375
                },
                "actor_loss": {
                    "n": 2,
                    "mean": -1.3767793232429711e+18,
                    "std": 2.482711428910285e+16,
                    "min_value": -1.3943346756478566e+18,
                    "max_value": -1.3592238333991322e+18
                },
                "entropy_coef_loss": {
                    "n": 2,
                    "mean": -6.425454473355513e+20,
                    "std": 1.1586897625084854e+19,
                    "min_value": -6.507386209576451e+20,
                    "max_value": -6.343522737134575e+20
                },
                "critic_loss": {
                    "n": 2,
                    "mean": Infinity,
                    "std": NaN,
                    "min_value": Infinity,
                    "max_value": Infinity
                },
                "entropy_coef": {
                    "n": 2,
                    "mean": 0.009890968911349773,
                    "std": 0.0,
                    "min_value": 0.009890968911349773,
                    "max_value": 0.009890968911349773
                },
                "action_stds": {
                    "n": 192,
                    "mean": 1.231547236442566,
                    "std": 2.760921001434326,
                    "min_value": 4.539993096841499e-05,
                    "max_value": 7.389056205749512
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.9428292512893677,
                    "std": 0.1734551191329956,
                    "min_value": 6.828457117080688e-05,
                    "max_value": 1.0
                },
                "gradient_step": 14502,
                "time_taken": 10.754356861114502,
                "__timestamp": "2024-09-23 22:46:24.601850"
            }
        ]
    }
}