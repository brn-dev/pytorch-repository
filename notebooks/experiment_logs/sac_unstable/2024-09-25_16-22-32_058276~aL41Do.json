{
    "experiment_id": "2024-09-25_16-22-32_058276~aL41Do",
    "experiment_tags": [
        "SACDebug",
        "HalfCheetah-v4"
    ],
    "start_time": "2024-09-25 16:22:32.058276",
    "end_time": "2024-09-25 16:22:32.388737",
    "end_exception": "Traceback (most recent call last):\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\experiment_logging\\experiment_logger.py\", line 144, in log_experiment\n    yield experiment_logger\n  File \"C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_13976\\2629730856.py\", line 332, in <module>\n    algo.learn(1_000_000)\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\reinforcement_learning\\algorithms\\base\\base_algorithm.py\", line 125, in learn\n    steps_performed, obs, episode_starts = self.perform_rollout(\n                                           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\reinforcement_learning\\algorithms\\base\\off_policy_algorithm.py\", line 161, in perform_rollout\n    obs, episode_starts, step_info = self.rollout_step(obs)\n                                     ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\reinforcement_learning\\algorithms\\base\\off_policy_algorithm.py\", line 110, in rollout_step\n    actions = self.sample_actions(obs, info)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\reinforcement_learning\\algorithms\\base\\off_policy_algorithm.py\", line 90, in sample_actions\n    action_selector = self.policy.act(\n                      ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\nAttributeError: 'SACPolicy' object has no attribute 'act'\n",
    "model_db_reference": null,
    "hyper_parameters": {
        "_type": "SACDebug",
        "_type_fq": "__main__.SACDebug",
        "env": "<RescaleAction<TransformRewardWrapper<AsyncVectorEnv instance>>>",
        "num_envs": 16,
        "policy": "SACPolicy(\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (critic_target): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)",
        "policy_parameter_count": 362256,
        "policy_repr": "SACPolicy(\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (critic_target): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)",
        "buffer": {
            "_type": "ReplayBuffer",
            "_type_fq": "src.reinforcement_learning.core.buffers.replay.replay_buffer.ReplayBuffer",
            "buffer_size": 50000,
            "num_envs": 16,
            "total_buffer_size": 800000,
            "torch_device": "cuda:0",
            "torch_dtype": "torch.float32",
            "np_dtype": "<class 'numpy.float32'>",
            "optimize_memory_usage": false
        },
        "buffer_step_size": 50000,
        "buffer_total_size": 800000,
        "gamma": 0.99,
        "sde_noise_sample_freq": null,
        "torch_device": "cuda:0",
        "torch_dtype": "torch.float32",
        "tau": 0.005,
        "rollout_steps": 1,
        "gradient_steps": 1,
        "optimization_batch_size": 256,
        "action_noise": null,
        "warmup_steps": 100,
        "actor_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "critic_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "entropy_coef_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object at 0x00007FFC517F62F0>",
        "weigh_and_reduce_actor_loss": "<function <lambda> at 0x0000017CE30B4220>",
        "weigh_critic_loss": "<function <lambda> at 0x0000017CE30B4180>",
        "target_update_interval": 1,
        "target_entropy": -6.0,
        "entropy_coef": "dynamic"
    },
    "setup": {
        "sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n",
        "notebook": "from dataclasses import dataclass\nfrom typing import Type, Optional, Any, Literal\n\nimport gymnasium\nimport numpy as np\nimport stable_baselines3 as sb\nimport torch\nimport torch.nn.functional as F\nfrom stable_baselines3.common.policies import ContinuousCritic\nfrom torch import optim\n\nfrom src.function_types import TorchTensorFn\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.algorithms.base.base_algorithm import PolicyProvider\nfrom src.reinforcement_learning.algorithms.base.off_policy_algorithm import OffPolicyAlgorithm, ReplayBuf\nfrom src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\nfrom src.reinforcement_learning.core.action_noise import ActionNoise\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import BaseReplayBuffer, ReplayBufferSamples\nfrom src.reinforcement_learning.core.buffers.replay.replay_buffer import ReplayBuffer\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.core.infos import InfoDict, concat_infos\nfrom src.reinforcement_learning.core.logging import LoggingConfig, log_if_enabled\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.core.polyak_update import polyak_update\nfrom src.reinforcement_learning.core.type_aliases import OptimizerProvider, TensorObs, detach_obs\nfrom src.reinforcement_learning.gym.env_analysis import get_single_action_space\nfrom src.torch_device import TorchDevice\nfrom src.torch_functions import identity\n\nSAC_DEFAULT_OPTIMIZER_PROVIDER = lambda params: optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nAUTO_TARGET_ENTROPY = 'auto'\n\n\n@dataclass\nclass SACLoggingConfig(LoggingConfig):\n\n    log_entropy_coef: bool = False\n    entropy_coef_loss: LossLoggingConfig = None\n    actor_loss: LossLoggingConfig = None\n    critic_loss: LossLoggingConfig = None\n\n    def __post_init__(self):\n        if self.actor_loss is None:\n            self.actor_loss = LossLoggingConfig()\n        if self.entropy_coef_loss is None:\n            self.entropy_loss = LossLoggingConfig()\n        if self.critic_loss is None:\n            self.critic_loss = LossLoggingConfig()\n\n        super().__post_init__()\n\n\"\"\"\n\n        Soft Actor-Critic:\n        Off-Policy Maximum Entropy Deep Reinforcement\n        Learning with a Stochastic Actor\n        https://arxiv.org/pdf/1801.01290\n\n\"\"\"\nclass SACDebug(OffPolicyAlgorithm[sb.sac.sac.SACPolicy, ReplayBuf, SACLoggingConfig]):\n    \n    policy: sb.sac.sac.SACPolicy\n    actor: sb.sac.sac.Actor\n    critic: ContinuousCritic\n    buffer: BaseReplayBuffer\n    target_entropy: float\n    log_ent_coef: Optional[torch.Tensor]\n    entropy_coef_optimizer: Optional[optim.Optimizer]\n    entropy_coef_tensor: Optional[torch.Tensor]\n\n    def __init__(\n            self,\n            env: gymnasium.Env,\n            policy: sb.sac.sac.SACPolicy | PolicyProvider[sb.sac.sac.SACPolicy],\n            actor_optimizer_provider: OptimizerProvider = SAC_DEFAULT_OPTIMIZER_PROVIDER,\n            critic_optimizer_provider: OptimizerProvider = SAC_DEFAULT_OPTIMIZER_PROVIDER,\n            weigh_and_reduce_actor_loss: TorchTensorFn = torch.mean,\n            weigh_critic_loss: TorchTensorFn = identity,\n            buffer_type: Type[ReplayBuf] = ReplayBuffer,\n            buffer_size: int = 100_000,\n            buffer_kwargs: dict[str, Any] = None,\n            gamma: float = 0.99,\n            tau: float = 0.005,\n            rollout_steps: int = 100,\n            gradient_steps: int = 1,\n            optimization_batch_size: int = 256,\n            target_update_interval: int = 1,\n            entropy_coef: float = 1.0,\n            target_entropy: float | Literal['auto'] = AUTO_TARGET_ENTROPY,\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider] = None,\n            weigh_and_reduce_entropy_coef_loss: TorchTensorFn = torch.mean,\n            action_noise: Optional[ActionNoise] = None,\n            warmup_steps: int = 100,\n            learning_starts: int = 100,\n            sde_noise_sample_freq: Optional[int] = None,\n            callback: Callback['SAC'] = None,\n            logging_config: SACLoggingConfig = None,\n            torch_device: TorchDevice = 'auto',\n            torch_dtype: torch.dtype = torch.float32,\n    ):\n        super().__init__(\n            env=env,\n            policy=policy,\n            buffer=buffer_type.for_env(env, buffer_size, torch_device, torch_dtype, **(buffer_kwargs or {})),\n            gamma=gamma,\n            tau=tau,\n            rollout_steps=rollout_steps,\n            gradient_steps=gradient_steps,\n            optimization_batch_size=optimization_batch_size,\n            action_noise=action_noise,\n            warmup_steps=warmup_steps,\n            learning_starts=learning_starts,\n            sde_noise_sample_freq=sde_noise_sample_freq,\n            callback=callback or Callback(),\n            logging_config=logging_config or LoggingConfig(),\n            torch_device=torch_device,\n            torch_dtype=torch_dtype,\n        )\n\n        self.actor = self.policy.actor\n        self.critic = self.policy.critic\n        # self.shared_feature_extractor = self.policy.shared_feature_extractor\n\n        self.actor_optimizer = actor_optimizer_provider(\n            # self.chain_parameters(self.actor, self.shared_feature_extractor)\n            self.actor.parameters()\n        )\n        self.critic_optimizer = critic_optimizer_provider(self.critic.parameters())\n\n        self.weigh_and_reduce_entropy_coef_loss = weigh_and_reduce_entropy_coef_loss\n        self.weigh_and_reduce_actor_loss = weigh_and_reduce_actor_loss\n        self.weigh_critic_loss = weigh_critic_loss\n\n        self.target_update_interval = target_update_interval\n        self.gradient_steps_performed = 0\n\n        self._setup_entropy_optimization(entropy_coef, target_entropy, entropy_coef_optimizer_provider)\n\n        # CrossQ doesn't use a target critic\n        if isinstance(self.policy, SACCrossQPolicy):\n            self.tau = 0\n            self.target_update_interval = 0\n\n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return self.update_hps(super().collect_hyper_parameters(), {\n            'actor_optimizer': str(self.actor_optimizer),\n            'critic_optimizer': str(self.critic_optimizer),\n            'entropy_coef_optimizer': str(self.entropy_coef_optimizer),\n            'weigh_and_reduce_entropy_coef_loss': str(self.weigh_and_reduce_entropy_coef_loss),\n            'weigh_and_reduce_actor_loss': str(self.weigh_and_reduce_actor_loss),\n            'weigh_critic_loss': str(self.weigh_critic_loss),\n            'target_update_interval': self.target_update_interval,\n            'target_entropy': self.target_entropy,\n            'entropy_coef': self.entropy_coef_tensor.item() if self.entropy_coef_tensor is not None else 'dynamic',\n        })\n\n    def _setup_entropy_optimization(\n            self,\n            entropy_coef: float,\n            target_entropy: float | Literal['auto'],\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider],\n    ):\n        if target_entropy == 'auto':\n            self.target_entropy = float(-np.prod(get_single_action_space(self.env).shape).astype(np.float32))\n        else:\n            self.target_entropy = float(target_entropy)\n\n        if entropy_coef_optimizer_provider is not None:\n            self.log_ent_coef = torch.log(\n                torch.tensor([entropy_coef], device=self.torch_device, dtype=self.torch_dtype)\n            ).requires_grad_(True)\n            self.entropy_coef_optimizer = entropy_coef_optimizer_provider([self.log_ent_coef])\n            self.entropy_coef_tensor = None\n        else:\n            self.log_ent_coef = None\n            self.entropy_coef_optimizer = None\n            self.entropy_coef_tensor = torch.tensor(entropy_coef, device=self.torch_device, dtype=self.torch_dtype)\n\n    def get_and_optimize_entropy_coef(\n            self,\n            actions_pi_log_prob: torch.Tensor,\n            info: InfoDict\n    ) -> torch.Tensor:\n        if self.entropy_coef_optimizer is not None:\n            entropy_coef = torch.exp(self.log_ent_coef.detach())\n\n            # TODO!\n            # entropy_coef_loss = weigh_and_reduce_loss(\n            #     raw_loss=-self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach(),\n            #     weigh_and_reduce_function=self.weigh_and_reduce_entropy_coef_loss,\n            #     info=info,\n            #     loss_name='entropy_coef_loss',\n            #     logging_config=self.logging_config.entropy_coef_loss\n            # )\n\n            entropy_coef_loss = -(self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach()).mean()\n            info['final_entropy_coef_loss'] = entropy_coef_loss.detach()\n\n            self.entropy_coef_optimizer.zero_grad()\n            entropy_coef_loss.backward()\n            self.entropy_coef_optimizer.step()\n\n            return entropy_coef\n        else:\n            return self.entropy_coef_tensor\n\n    def calculate_critic_loss(\n            self,\n            observation_features: TensorObs,\n            replay_samples: ReplayBufferSamples,\n            entropy_coef: torch.Tensor,\n            info: InfoDict,\n    ):\n        with torch.no_grad():\n                # Select action according to policy\n            next_actions, next_log_prob = self.actor.action_log_prob(replay_samples.next_observations)\n            # Compute the next Q values: min over all critics targets\n            next_q_values = torch.cat(self.policy.critic_target(replay_samples.next_observations, next_actions), dim=1)\n            next_q_values, _ = torch.min(next_q_values, dim=1, keepdim=True)\n            # add entropy term\n            next_q_values = next_q_values - entropy_coef * next_log_prob.reshape(-1, 1)\n            # td error + entropy term\n            target_q_values = replay_samples.rewards + (1 - replay_samples.dones) * self.gamma * next_q_values\n\n        # target_q_values = self.policy.compute_target_values(\n        #     replay_samples=replay_samples,\n        #     entropy_coef=entropy_coef,\n        #     gamma=self.gamma,\n        # )\n        # critic loss should not influence shared feature extractor\n        current_q_values = self.critic(detach_obs(observation_features), replay_samples.actions)\n\n        # noinspection PyTypeChecker\n        critic_loss: torch.Tensor = 0.5 * sum(\n            F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n        )\n        # TODO!\n        # critic_loss = weigh_and_reduce_loss(\n        #     raw_loss=critic_loss,\n        #     weigh_and_reduce_function=self.weigh_critic_loss,\n        #     info=info,\n        #     loss_name='critic_loss',\n        #     logging_config=self.logging_config.critic_loss,\n        # )\n\n        info['final_critic_loss'] = critic_loss.detach()\n        return critic_loss\n\n    def calculate_actor_loss(\n            self,\n            observation_features: TensorObs,\n            actions_pi: torch.Tensor,\n            actions_pi_log_prob: torch.Tensor,\n            entropy_coef: torch.Tensor,\n            info: InfoDict,\n    ) -> torch.Tensor:\n        q_values_pi = torch.cat(self.critic(observation_features, actions_pi), dim=-1)\n        min_q_values_pi, _ = torch.min(q_values_pi, dim=-1, keepdim=True)\n        actor_loss = (entropy_coef * actions_pi_log_prob - min_q_values_pi).mean()  # TODO!\n\n        # TODO!\n        # actor_loss = weigh_and_reduce_loss(\n        #     raw_loss=actor_loss,\n        #     weigh_and_reduce_function=self.weigh_and_reduce_actor_loss,\n        #     info=info,\n        #     loss_name='actor_loss',\n        #     logging_config=self.logging_config.actor_loss,\n        # )\n\n        info['final_actor_loss'] = actor_loss.detach()\n        return actor_loss\n\n    def optimize(self, last_obs: np.ndarray, last_episode_starts: np.ndarray, info: InfoDict) -> None:\n        gradient_step_infos: list[InfoDict] = []\n\n        for gradient_step in range(self.gradient_steps):\n            step_info: InfoDict = {}\n            replay_samples = self.buffer.sample(self.optimization_batch_size)\n\n            # self.actor.reset_sde_noise()  # TODO: set batch size?\n\n            # observation_features = self.shared_feature_extractor(replay_samples.observations)\n            # observation_features = replay_samples.observations\n            actions_pi, actions_pi_log_prob = self.actor.action_log_prob(replay_samples.observations)\n            actions_pi_log_prob = actions_pi_log_prob.reshape(-1, 1)\n\n            entropy_coef = self.get_and_optimize_entropy_coef(actions_pi_log_prob, step_info)\n            log_if_enabled(step_info, 'entropy_coef', entropy_coef, self.logging_config.log_entropy_coef)\n\n            critic_loss = self.calculate_critic_loss(\n                observation_features=replay_samples.observations,\n                replay_samples=replay_samples,\n                entropy_coef=entropy_coef,\n                info=step_info\n            )\n\n            self.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic_optimizer.step()\n\n            actor_loss = self.calculate_actor_loss(\n                observation_features=replay_samples.observations,\n                actions_pi=actions_pi,\n                actions_pi_log_prob=actions_pi_log_prob,\n                entropy_coef=entropy_coef,\n                info=step_info\n            )\n\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            self.gradient_steps_performed += 1\n            if self.target_update_interval > 0 and self.gradient_steps_performed % self.target_update_interval == 0:\n                # self.policy.perform_polyak_update(self.tau)\n                polyak_update(self.critic.parameters(), self.policy.critic_target.parameters(), self.tau)\n            gradient_step_infos.append(step_info)\n        info.update(concat_infos(gradient_step_infos))\n\n\nfrom stable_baselines3.common.env_util import make_vec_env\nimport stable_baselines3 as sb\n\n\nenv = make_vec_env(\"HalfCheetah-v4\", n_envs=16)\n\nsb_sac = sb.sac.SAC(\"MlpPolicy\", env, verbose=1)\n# model.learn(total_timesteps=10_000_000, log_interval=16)\n\n\nimport inspect\nimport time\n\nfrom gymnasium import Env\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.module_analysis import count_parameters\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import PolicyConstruction\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom typing import Any\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n\nimport torch\nfrom torch import optim\nimport gymnasium as gym\nimport numpy as np\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n\\n' + _ih[2] + '\\n\\n\\n' + _ih[3] + '\\n\\n\\n' + _ih[-1],\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    # return policy_id, policy, optimizer, wrapped_env, steps_trained\n    return policy_id, sb_sac.policy, None, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SACDebug, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    \n    if len(episode_scores) > 0:\n    \n        global best_iteration_score\n        iteration_score = episode_scores.mean()\n        score_moving_average = score_mean_ema.update(iteration_score)\n        if iteration_score >= best_iteration_score:\n            best_iteration_score = iteration_score\n            policy_db.save_model_state_dict(\n                model_id=policy_id,\n                parent_model_id=parent_policy_id,\n                model_info={\n                    'score': iteration_score.item(),\n                    'steps_trained': steps_trained,\n                    'wrap_env_source_code': wrap_env_source_code_source,\n                    'init_policy_source_code': init_policy_source\n                },\n                model=policy,\n                optimizer=optimizer,\n            )\n        info['score_moving_average'] = score_moving_average\n    \n    info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SACDebug, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    total_step = step * rl.num_envs\n    \n    time_taken = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{total_step = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {time_taken:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'total_step': total_step,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info['final_entropy_coef_loss']),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'gradient_step': rl.gradient_steps_performed,\n        'time_taken': time_taken,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment()\n    print()\n    \n    if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n        logger.save_experiment()\n        raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 16\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\nenv = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n\nlogger = ExperimentLogger('experiment_logs/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n    # print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = } \\n\\n')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=50_000,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=100,\n            learning_starts=100,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=[type(algo).__name__, env_name],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(1_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"
    },
    "notes": [],
    "logs_by_category": {}
}