{"experiment_id": "2024-10-10_14-56-40_531038~eXbike", "experiment_tags": ["SACDebug", "HalfCheetah-v4", "Debug"], "start_time": "2024-10-10 14:56:40.531038", "end_time": "2024-10-10 15:37:09.415455", "end_exception": null, "hyper_parameters": {"_type": "SACDebug", "_type_fq": "__main__.SACDebug", "env": "<SingletonVectorEnv instance>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {}, "policy_parameter_count": 362256, "policy_repr": "DebugSACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 9 3900X 12-Core Processor", "cores": 12, "logical_cores": 24, "speed": "3793 MHz"}, "gpu": [{"name": "NVIDIA GeForce RTX 3070", "video_processor": "NVIDIA GeForce RTX 3070", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA", "memory": "8192 MB", "memory_clock": "810 MHz", "compute_capability": "8.6"}], "ram_speed": "3600 MHz", "ram": "64 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "from sac import init_policy, init_action_selector\nfrom stable_baselines3.common.env_util import make_vec_env\nimport stable_baselines3 as sb\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nimport gymnasium\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n\ndef create_env(render_mode: str | None):\n    return gymnasium.make(env_name, render_mode=render_mode, **env_kwargs)\n\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nsb_sac = sb.SAC(\"MlpPolicy\", env, verbose=10, learning_starts=10000, stats_window_size=1) # , seed=594371)\n\nfrom src.reinforcement_learning.algorithms.sac.sac import SACLoggingConfig, SAC\nfrom dataclasses import dataclass\nfrom typing import Type, Optional, Any, Literal\n\nimport gymnasium\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom src.function_types import TorchTensorFn\nfrom src.module_analysis import calculate_grad_norm\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.algorithms.base.base_algorithm import PolicyProvider\nfrom src.reinforcement_learning.algorithms.base.off_policy_algorithm import OffPolicyAlgorithm, ReplayBuf\nfrom src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\nfrom src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\nfrom src.reinforcement_learning.core.action_noise import ActionNoise\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import BaseReplayBuffer, ReplayBufferSamples\nfrom src.reinforcement_learning.core.buffers.replay.replay_buffer import ReplayBuffer\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.core.infos import InfoDict, concat_infos\nfrom src.reinforcement_learning.core.logging import LoggingConfig, log_if_enabled\nfrom src.reinforcement_learning.core.loss_config import weigh_and_reduce_loss, LossLoggingConfig\nfrom src.reinforcement_learning.core.type_aliases import OptimizerProvider, TensorObs, detach_obs\nfrom src.reinforcement_learning.gym.env_analysis import get_single_action_space\nfrom src.tags import Tags\nfrom src.torch_device import TorchDevice\nfrom src.torch_functions import identity\nfrom src.repr_utils import func_repr\n\nfrom typing import Literal\n\nSAC_DEFAULT_OPTIMIZER_PROVIDER = lambda params: optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nAUTO_TARGET_ENTROPY = 'auto'\n\n\nclass SACDebug(SAC):\n    \n    @property\n    def replay_buffer(self):\n        return self.buffer\n\n    buffer: BaseReplayBuffer\n    target_entropy: float\n    log_ent_coef: Optional[torch.Tensor]\n    entropy_coef_optimizer: Optional[optim.Optimizer]\n    entropy_coef_tensor: Optional[torch.Tensor]\n    \n    def collect_hyper_parameters(self) -> HyperParameters:\n        print(f'{type(self.policy) = }, {type(self.policy.actor) = }, {type(self.policy.critic) = }, {type(self.policy.target_critic) = }, {type(self.buffer) = }')\n        return super().collect_hyper_parameters()\n    \n    \n    def _setup_entropy_optimization(\n            self,\n            entropy_coef: float,\n            target_entropy: float | Literal['auto'],\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider],\n    ):\n        if target_entropy == 'auto':\n            self.target_entropy = float(-np.prod(get_single_action_space(self.env).shape).astype(np.float32))\n        else:\n            self.target_entropy = float(target_entropy)\n\n        if entropy_coef_optimizer_provider is not None:\n            self.log_ent_coef = torch.log(\n                torch.tensor([entropy_coef], device=self.torch_device, dtype=self.torch_dtype)\n            ).requires_grad_(True)\n            self.entropy_coef_optimizer = entropy_coef_optimizer_provider([self.log_ent_coef])\n            self.entropy_coef_tensor = None\n        else:\n            self.log_ent_coef = None\n            self.entropy_coef_optimizer = None\n            self.entropy_coef_tensor = torch.tensor(entropy_coef, device=self.torch_device, dtype=self.torch_dtype)\n\n    # def get_and_optimize_entropy_coef(\n    #         self,\n    #         actions_pi_log_prob: torch.Tensor,\n    #         info: InfoDict\n    # ) -> torch.Tensor:\n    #     if self.entropy_coef_optimizer is not None:\n    #         entropy_coef = torch.exp(self.log_ent_coef.detach())\n    # \n    #         entropy_coef_loss = weigh_and_reduce_loss(\n    #             raw_loss=-self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach(),\n    #             weigh_and_reduce_function=self.weigh_and_reduce_entropy_coef_loss,\n    #             info=info,\n    #             loss_name='entropy_coef_loss',\n    #             logging_config=self.logging_config.entropy_coef_loss\n    #         )\n    #         self.entropy_coef_optimizer.zero_grad()\n    #         entropy_coef_loss.backward()\n    #         self.entropy_coef_optimizer.step()\n    # \n    #         return entropy_coef\n    #     else:\n    #         return self.entropy_coef_tensor\n    # \n    # def calculate_critic_loss(\n    #         self,\n    #         observation_features: TensorObs,\n    #         replay_samples: ReplayBufferSamples,\n    #         entropy_coef: torch.Tensor,\n    #         info: InfoDict,\n    # ):\n    #     target_q_values = self.policy.compute_target_values(\n    #         replay_samples=replay_samples,\n    #         entropy_coef=entropy_coef,\n    #         gamma=self.gamma,\n    #     )\n    #     # critic loss should not influence shared feature extractor\n    #     current_q_values = self.critic(detach_obs(observation_features), replay_samples.actions)\n    # \n    #     # noinspection PyTypeChecker\n    #     critic_loss: torch.Tensor = 0.5 * sum(\n    #         F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n    #     )\n    #     critic_loss = weigh_and_reduce_loss(\n    #         raw_loss=critic_loss,\n    #         weigh_and_reduce_function=self.weigh_critic_loss,\n    #         info=info,\n    #         loss_name='critic_loss',\n    #         logging_config=self.logging_config.critic_loss,\n    #     )\n    #     return critic_loss\n    # \n    # def calculate_actor_loss(\n    #         self,\n    #         observation_features: TensorObs,\n    #         actions_pi: torch.Tensor,\n    #         actions_pi_log_prob: torch.Tensor,\n    #         entropy_coef: torch.Tensor,\n    #         info: InfoDict,\n    # ) -> torch.Tensor:\n    #     q_values_pi = torch.cat(self.critic(observation_features, actions_pi), dim=-1)\n    #     min_q_values_pi, _ = torch.min(q_values_pi, dim=-1, keepdim=True)\n    #     actor_loss = entropy_coef * actions_pi_log_prob - min_q_values_pi\n    # \n    #     actor_loss = weigh_and_reduce_loss(\n    #         raw_loss=actor_loss,\n    #         weigh_and_reduce_function=self.weigh_and_reduce_actor_loss,\n    #         info=info,\n    #         loss_name='actor_loss',\n    #         logging_config=self.logging_config.actor_loss,\n    #     )\n    # \n    #     return actor_loss\n\n    def optimize(self, last_obs: np.ndarray, last_episode_starts: np.ndarray, info: InfoDict) -> None:\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(self.gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(self.optimization_batch_size, env=None)  # type: ignore[union-attr]\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            # if self.sde_noise_sample_freq:\n            #     self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.entropy_coef_optimizer is not None and self.log_ent_coef is not None:\n                # Important: detach the variable from the graph\n                # so we don't change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = torch.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.entropy_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None and self.entropy_coef_optimizer is not None:\n                self.entropy_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.entropy_coef_optimizer.step()\n\n            with torch.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the next Q values: min over all critics targets\n                next_q_values = torch.cat(self.policy.target_critic(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = torch.min(next_q_values, dim=1, keepdim=True)\n                # add entropy term\n                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n                # td error + entropy term\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            # using action from the replay buffer\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            assert isinstance(critic_loss, torch.Tensor)  # for type checker\n            critic_losses.append(critic_loss.item())  # type: ignore[union-attr]\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = torch.mean(log_prob - qf1_pi)\n            # Min over all critic networks\n            q_values_pi = torch.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n            min_qf_pi, _ = torch.min(q_values_pi, dim=1, keepdim=True)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                polyak_update(self.critic.parameters(), self.policy.target_critic.parameters(), self.tau)\n                # Copy running stats, see GH issue #996\n        \n        info['entropy_coef'] = np.array(ent_coefs)\n        info['final_entropy_coef_loss'] = np.array(ent_coef_losses)\n        info['final_actor_loss'] = np.array(actor_losses)\n        info['final_critic_loss'] = np.array(critic_losses)\n\nimport inspect\nimport time\n\nfrom gymnasium import Env\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.module_analysis import count_parameters\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import PolicyConstruction\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom typing import Any\nfrom src.reinforcement_learning.core.callback import Callback\n\nimport torch\nfrom torch import optim\nimport gymnasium as gym\nimport numpy as np\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -2.3225e+02}, "actor_loss": {"n": 1, "mean": -1.8105e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.0255e+00}, "critic_loss": {"n": 1, "mean": 1.9089e+00}, "entropy_coef": {"n": 1, "mean": 7.4078e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.2509e-01, "std": 2.8524e-01, "min_value": 1.5177e-05, "max_value": 9.9875e-01}, "num_gradient_steps": 0, "step_time": 1.3658e+01, "total_time": 1.3648e+01, "__timestamp": "2024-10-10 14:56:54.178770"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -2.0809e+02}, "actor_loss": {"n": 1, "mean": -2.5600e+01}, "entropy_coef_loss": {"n": 1, "mean": -6.0183e+00}, "critic_loss": {"n": 1, "mean": 1.3677e+00}, "entropy_coef": {"n": 1, "mean": 5.4885e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.2910e-01, "std": 2.8700e-01, "min_value": 2.1887e-04, "max_value": 9.9864e-01}, "num_gradient_steps": 0, "step_time": 9.9027e+00, "total_time": 2.3550e+01, "__timestamp": "2024-10-10 14:57:04.082438"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -2.0523e+02}, "actor_loss": {"n": 1, "mean": -3.0115e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.8311e+00}, "critic_loss": {"n": 1, "mean": 1.8124e+00}, "entropy_coef": {"n": 1, "mean": 4.0747e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4346e-01, "std": 2.8966e-01, "min_value": 8.7798e-05, "max_value": 9.9894e-01}, "num_gradient_steps": 0, "step_time": 9.5508e+00, "total_time": 3.3101e+01, "__timestamp": "2024-10-10 14:57:13.633223"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -1.7353e+02}, "actor_loss": {"n": 1, "mean": -3.2524e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1430e+01}, "critic_loss": {"n": 1, "mean": 1.8849e+00}, "entropy_coef": {"n": 1, "mean": 3.0303e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4589e-01, "std": 2.9058e-01, "min_value": 1.0345e-04, "max_value": 9.9874e-01}, "num_gradient_steps": 0, "step_time": 1.0463e+01, "total_time": 4.3565e+01, "__timestamp": "2024-10-10 14:57:24.095659"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -2.5581e+02}, "actor_loss": {"n": 1, "mean": -3.3189e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3887e+01}, "critic_loss": {"n": 1, "mean": 1.7588e+00}, "entropy_coef": {"n": 1, "mean": 2.2611e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.5644e-01, "std": 2.9078e-01, "min_value": 6.2895e-04, "max_value": 9.9900e-01}, "num_gradient_steps": 0, "step_time": 9.6080e+00, "total_time": 5.3173e+01, "__timestamp": "2024-10-10 14:57:33.704651"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -2.6902e+02}, "actor_loss": {"n": 1, "mean": -3.3663e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5624e+01}, "critic_loss": {"n": 1, "mean": 5.2147e+00}, "entropy_coef": {"n": 1, "mean": 1.6953e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.5978e-01, "std": 2.9386e-01, "min_value": 5.0985e-04, "max_value": 9.9945e-01}, "num_gradient_steps": 0, "step_time": 9.9116e+00, "total_time": 6.3084e+01, "__timestamp": "2024-10-10 14:57:43.616251"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -2.1036e+02}, "actor_loss": {"n": 1, "mean": -3.3591e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6593e+01}, "critic_loss": {"n": 1, "mean": 1.8464e+00}, "entropy_coef": {"n": 1, "mean": 1.2814e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.7863e-01, "std": 2.9395e-01, "min_value": 2.2262e-05, "max_value": 9.9950e-01}, "num_gradient_steps": 0, "step_time": 1.0467e+01, "total_time": 7.3552e+01, "__timestamp": "2024-10-10 14:57:54.083556"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -2.7725e+02}, "actor_loss": {"n": 1, "mean": -3.2549e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6513e+01}, "critic_loss": {"n": 1, "mean": 1.8325e+00}, "entropy_coef": {"n": 1, "mean": 9.7772e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9830e-01, "std": 2.9149e-01, "min_value": 1.3644e-04, "max_value": 9.9954e-01}, "num_gradient_steps": 0, "step_time": 1.0022e+01, "total_time": 8.3574e+01, "__timestamp": "2024-10-10 14:58:04.104832"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -2.7899e+02}, "actor_loss": {"n": 1, "mean": -3.1956e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5712e+01}, "critic_loss": {"n": 1, "mean": 1.6449e+00}, "entropy_coef": {"n": 1, "mean": 7.5249e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0445e-01, "std": 2.9613e-01, "min_value": 2.8205e-04, "max_value": 9.9964e-01}, "num_gradient_steps": 0, "step_time": 9.8828e+00, "total_time": 9.3457e+01, "__timestamp": "2024-10-10 14:58:13.988588"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -1.9391e+02}, "actor_loss": {"n": 1, "mean": -3.1608e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4428e+01}, "critic_loss": {"n": 1, "mean": 1.8286e+00}, "entropy_coef": {"n": 1, "mean": 5.8351e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1318e-01, "std": 2.9218e-01, "min_value": 1.6814e-04, "max_value": 9.9938e-01}, "num_gradient_steps": 0, "step_time": 1.0360e+01, "total_time": 1.0382e+02, "__timestamp": "2024-10-10 14:58:24.348737"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -3.0089e+02}, "actor_loss": {"n": 1, "mean": -3.1148e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.2745e+01}, "critic_loss": {"n": 1, "mean": 1.8455e+00}, "entropy_coef": {"n": 1, "mean": 4.5627e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1965e-01, "std": 2.9926e-01, "min_value": 2.0921e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 9.7412e+00, "total_time": 1.1356e+02, "__timestamp": "2024-10-10 14:58:34.088898"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -1.8090e+02}, "actor_loss": {"n": 1, "mean": -3.1492e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.4721e+00}, "critic_loss": {"n": 1, "mean": 5.6716e+00}, "entropy_coef": {"n": 1, "mean": 3.6121e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2208e-01, "std": 3.0158e-01, "min_value": 6.0418e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 9.6415e+00, "total_time": 1.2320e+02, "__timestamp": "2024-10-10 14:58:43.731389"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -2.1309e+02}, "actor_loss": {"n": 1, "mean": -2.9924e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.3867e+00}, "critic_loss": {"n": 1, "mean": 6.4263e+00}, "entropy_coef": {"n": 1, "mean": 2.9004e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.3303e-01, "std": 3.0070e-01, "min_value": 1.3862e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 9.6145e+00, "total_time": 1.3281e+02, "__timestamp": "2024-10-10 14:58:53.344916"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -2.5514e+02}, "actor_loss": {"n": 1, "mean": -2.9846e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.8015e+00}, "critic_loss": {"n": 1, "mean": 2.9599e+00}, "entropy_coef": {"n": 1, "mean": 2.3661e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.5242e-01, "std": 2.9839e-01, "min_value": 1.6251e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.5473e+00, "total_time": 1.4236e+02, "__timestamp": "2024-10-10 14:59:02.892266"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": -2.4967e+02}, "actor_loss": {"n": 1, "mean": -2.9197e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.2662e+00}, "critic_loss": {"n": 1, "mean": 2.0620e+00}, "entropy_coef": {"n": 1, "mean": 1.9758e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4985e-01, "std": 2.9761e-01, "min_value": 1.6332e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.5500e+00, "total_time": 1.5191e+02, "__timestamp": "2024-10-10 14:59:12.442264"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": -5.4952e+02}, "actor_loss": {"n": 1, "mean": -2.9807e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.9766e+00}, "critic_loss": {"n": 1, "mean": 2.5733e+00}, "entropy_coef": {"n": 1, "mean": 1.6610e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1949e-01, "std": 2.8762e-01, "min_value": 8.9452e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.9030e+00, "total_time": 1.6181e+02, "__timestamp": "2024-10-10 14:59:22.345261"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": -2.3849e+02}, "actor_loss": {"n": 1, "mean": -3.0482e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.1690e+00}, "critic_loss": {"n": 1, "mean": 2.1844e+00}, "entropy_coef": {"n": 1, "mean": 1.3544e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2183e-01, "std": 2.9784e-01, "min_value": 3.5056e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0012e+01, "total_time": 1.7183e+02, "__timestamp": "2024-10-10 14:59:32.357071"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": -2.5039e+01}, "actor_loss": {"n": 1, "mean": -2.8719e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.0533e+00}, "critic_loss": {"n": 1, "mean": 2.5678e+00}, "entropy_coef": {"n": 1, "mean": 1.1997e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0608e-01, "std": 2.9720e-01, "min_value": 4.7076e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.6457e+00, "total_time": 1.8147e+02, "__timestamp": "2024-10-10 14:59:42.003731"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": 4.2406e+02}, "actor_loss": {"n": 1, "mean": -3.0094e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.0478e+00}, "critic_loss": {"n": 1, "mean": 2.4970e+00}, "entropy_coef": {"n": 1, "mean": 1.1366e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4899e-01, "std": 3.0359e-01, "min_value": 2.2630e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.7411e+00, "total_time": 1.9121e+02, "__timestamp": "2024-10-10 14:59:51.743827"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": 2.2790e+02}, "actor_loss": {"n": 1, "mean": -2.9638e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.1217e+00}, "critic_loss": {"n": 1, "mean": 2.6182e+00}, "entropy_coef": {"n": 1, "mean": 1.1303e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.5501e-01, "std": 2.9874e-01, "min_value": 9.4585e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.7012e+00, "total_time": 2.0091e+02, "__timestamp": "2024-10-10 15:00:01.445062"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": 6.6651e+02}, "actor_loss": {"n": 1, "mean": -3.0556e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.5647e-01}, "critic_loss": {"n": 1, "mean": 2.0427e+00}, "entropy_coef": {"n": 1, "mean": 1.0615e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.6779e-01, "std": 3.0221e-01, "min_value": 2.1428e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.8083e+00, "total_time": 2.1072e+02, "__timestamp": "2024-10-10 15:00:11.254404"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": 5.6819e+02}, "actor_loss": {"n": 1, "mean": -2.7025e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.4920e-02}, "critic_loss": {"n": 1, "mean": 2.3587e+00}, "entropy_coef": {"n": 1, "mean": 1.0624e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.5702e-01, "std": 3.0284e-01, "min_value": 1.1993e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.8577e+00, "total_time": 2.2058e+02, "__timestamp": "2024-10-10 15:00:21.111134"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 8.0917e+02}, "actor_loss": {"n": 1, "mean": -2.8306e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.8181e+00}, "critic_loss": {"n": 1, "mean": 2.7727e+00}, "entropy_coef": {"n": 1, "mean": 1.0620e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4735e-01, "std": 3.0391e-01, "min_value": 1.6897e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1169e+01, "total_time": 2.3175e+02, "__timestamp": "2024-10-10 15:00:32.280201"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 1.0082e+03}, "actor_loss": {"n": 1, "mean": -2.8849e+01}, "entropy_coef_loss": {"n": 1, "mean": 8.0070e-01}, "critic_loss": {"n": 1, "mean": 2.4086e+00}, "entropy_coef": {"n": 1, "mean": 1.1028e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1762e-01, "std": 3.0397e-01, "min_value": 2.6817e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0358e+01, "total_time": 2.4211e+02, "__timestamp": "2024-10-10 15:00:42.637988"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 1.1485e+03}, "actor_loss": {"n": 1, "mean": -3.0722e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.3314e+00}, "critic_loss": {"n": 1, "mean": 2.5484e+00}, "entropy_coef": {"n": 1, "mean": 1.1075e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1069e-01, "std": 3.0514e-01, "min_value": 1.0511e-05, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 9.9496e+00, "total_time": 2.5206e+02, "__timestamp": "2024-10-10 15:00:52.587575"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 9.9839e+02}, "actor_loss": {"n": 1, "mean": -2.8833e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.8586e-01}, "critic_loss": {"n": 1, "mean": 2.4514e+00}, "entropy_coef": {"n": 1, "mean": 1.1761e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4519e-01, "std": 3.0225e-01, "min_value": 7.2271e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.7283e+00, "total_time": 2.6178e+02, "__timestamp": "2024-10-10 15:01:02.316916"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 5.7598e+02}, "actor_loss": {"n": 1, "mean": -3.0416e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.9212e+00}, "critic_loss": {"n": 1, "mean": 6.9472e+00}, "entropy_coef": {"n": 1, "mean": 1.2052e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.5712e-01, "std": 3.0270e-01, "min_value": 3.0908e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.8640e+00, "total_time": 2.7165e+02, "__timestamp": "2024-10-10 15:01:12.179892"}, {"step": 38000, "num_env_steps": 38000, "scores": {"n": 1, "mean": 1.4991e+03}, "actor_loss": {"n": 1, "mean": -3.2010e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.9443e+00}, "critic_loss": {"n": 1, "mean": 3.0201e+00}, "entropy_coef": {"n": 1, "mean": 1.2706e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.6286e-01, "std": 3.0240e-01, "min_value": 3.6448e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0222e+01, "total_time": 2.8187e+02, "__timestamp": "2024-10-10 15:01:22.402451"}, {"step": 39000, "num_env_steps": 39000, "scores": {"n": 1, "mean": 1.7977e+03}, "actor_loss": {"n": 1, "mean": -3.4746e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.6240e-01}, "critic_loss": {"n": 1, "mean": 2.1262e+00}, "entropy_coef": {"n": 1, "mean": 1.3828e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.8752e-01, "std": 2.9871e-01, "min_value": 1.1650e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0034e+01, "total_time": 2.9190e+02, "__timestamp": "2024-10-10 15:01:32.436007"}, {"step": 40000, "num_env_steps": 40000, "scores": {"n": 1, "mean": 1.8486e+03}, "actor_loss": {"n": 1, "mean": -3.2847e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.8227e+00}, "critic_loss": {"n": 1, "mean": 2.8319e+00}, "entropy_coef": {"n": 1, "mean": 1.5386e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0733e-01, "std": 2.9655e-01, "min_value": 7.5102e-06, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0474e+01, "total_time": 3.0238e+02, "__timestamp": "2024-10-10 15:01:42.908700"}, {"step": 41000, "num_env_steps": 41000, "scores": {"n": 1, "mean": 1.7300e+03}, "actor_loss": {"n": 1, "mean": -3.7828e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.8441e-01}, "critic_loss": {"n": 1, "mean": 2.9603e+00}, "entropy_coef": {"n": 1, "mean": 1.7455e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0903e-01, "std": 2.9279e-01, "min_value": 2.8670e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0121e+01, "total_time": 3.1250e+02, "__timestamp": "2024-10-10 15:01:53.030615"}, {"step": 42000, "num_env_steps": 42000, "scores": {"n": 1, "mean": 1.8782e+03}, "actor_loss": {"n": 1, "mean": -3.8966e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.6244e+00}, "critic_loss": {"n": 1, "mean": 3.1633e+00}, "entropy_coef": {"n": 1, "mean": 1.9139e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1505e-01, "std": 2.9498e-01, "min_value": 8.3372e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.5742e+00, "total_time": 3.2207e+02, "__timestamp": "2024-10-10 15:02:02.604776"}, {"step": 43000, "num_env_steps": 43000, "scores": {"n": 1, "mean": -6.0974e+01}, "actor_loss": {"n": 1, "mean": -4.3034e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.4927e+00}, "critic_loss": {"n": 1, "mean": 3.5927e+00}, "entropy_coef": {"n": 1, "mean": 2.0563e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.8737e-01, "std": 3.0002e-01, "min_value": 2.0179e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0031e+01, "total_time": 3.3210e+02, "__timestamp": "2024-10-10 15:02:12.636028"}, {"step": 44000, "num_env_steps": 44000, "scores": {"n": 1, "mean": 3.0894e+02}, "actor_loss": {"n": 1, "mean": -4.0911e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.4306e-01}, "critic_loss": {"n": 1, "mean": 3.7462e+00}, "entropy_coef": {"n": 1, "mean": 2.0630e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.9704e-01, "std": 2.9926e-01, "min_value": 2.1403e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0578e+01, "total_time": 3.4268e+02, "__timestamp": "2024-10-10 15:02:23.214020"}, {"step": 45000, "num_env_steps": 45000, "scores": {"n": 1, "mean": 1.9098e+03}, "actor_loss": {"n": 1, "mean": -4.2002e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.9168e-01}, "critic_loss": {"n": 1, "mean": 4.3161e+00}, "entropy_coef": {"n": 1, "mean": 2.1040e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2137e-01, "std": 2.9468e-01, "min_value": 9.1773e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0789e+01, "total_time": 3.5347e+02, "__timestamp": "2024-10-10 15:02:34.001967"}, {"step": 46000, "num_env_steps": 46000, "scores": {"n": 1, "mean": 2.0409e+03}, "actor_loss": {"n": 1, "mean": -4.5111e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.5388e+00}, "critic_loss": {"n": 1, "mean": 4.0875e+00}, "entropy_coef": {"n": 1, "mean": 2.2268e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2059e-01, "std": 2.9819e-01, "min_value": 4.2081e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0241e+01, "total_time": 3.6371e+02, "__timestamp": "2024-10-10 15:02:44.244283"}, {"step": 47000, "num_env_steps": 47000, "scores": {"n": 1, "mean": 2.5072e+03}, "actor_loss": {"n": 1, "mean": -4.7238e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.7505e-01}, "critic_loss": {"n": 1, "mean": 2.8218e+01}, "entropy_coef": {"n": 1, "mean": 2.2118e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4067e-01, "std": 2.8804e-01, "min_value": 6.4364e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0493e+01, "total_time": 3.7421e+02, "__timestamp": "2024-10-10 15:02:54.736554"}, {"step": 48000, "num_env_steps": 48000, "scores": {"n": 1, "mean": 2.2638e+03}, "actor_loss": {"n": 1, "mean": -5.2564e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.4461e+00}, "critic_loss": {"n": 1, "mean": 4.7762e+00}, "entropy_coef": {"n": 1, "mean": 2.2749e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3234e-01, "std": 2.8859e-01, "min_value": 1.1238e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0870e+01, "total_time": 3.8508e+02, "__timestamp": "2024-10-10 15:03:05.607836"}, {"step": 49000, "num_env_steps": 49000, "scores": {"n": 1, "mean": 2.2388e+03}, "actor_loss": {"n": 1, "mean": -5.1708e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.6357e-01}, "critic_loss": {"n": 1, "mean": 5.4425e+00}, "entropy_coef": {"n": 1, "mean": 2.3627e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2840e-01, "std": 2.9155e-01, "min_value": 1.1754e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0210e+01, "total_time": 3.9529e+02, "__timestamp": "2024-10-10 15:03:15.816715"}, {"step": 50000, "num_env_steps": 50000, "scores": {"n": 1, "mean": 2.2413e+03}, "actor_loss": {"n": 1, "mean": -5.6063e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.7773e-02}, "critic_loss": {"n": 1, "mean": 4.7567e+00}, "entropy_coef": {"n": 1, "mean": 2.4444e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2186e-01, "std": 2.9431e-01, "min_value": 8.2195e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0404e+01, "total_time": 4.0569e+02, "__timestamp": "2024-10-10 15:03:26.220990"}, {"step": 51000, "num_env_steps": 51000, "scores": {"n": 1, "mean": 2.4080e+03}, "actor_loss": {"n": 1, "mean": -5.7940e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.8883e-01}, "critic_loss": {"n": 1, "mean": 2.6582e+01}, "entropy_coef": {"n": 1, "mean": 2.5160e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3161e-01, "std": 2.8951e-01, "min_value": 9.7430e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0019e+01, "total_time": 4.1571e+02, "__timestamp": "2024-10-10 15:03:36.241404"}, {"step": 52000, "num_env_steps": 52000, "scores": {"n": 1, "mean": 2.3134e+03}, "actor_loss": {"n": 1, "mean": -5.9325e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.9414e-01}, "critic_loss": {"n": 1, "mean": 6.3892e+00}, "entropy_coef": {"n": 1, "mean": 2.5964e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3191e-01, "std": 2.9128e-01, "min_value": 4.1157e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0582e+01, "total_time": 4.2629e+02, "__timestamp": "2024-10-10 15:03:46.822124"}, {"step": 53000, "num_env_steps": 53000, "scores": {"n": 1, "mean": 2.5373e+03}, "actor_loss": {"n": 1, "mean": -5.9609e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.3399e+00}, "critic_loss": {"n": 1, "mean": 4.1324e+00}, "entropy_coef": {"n": 1, "mean": 2.6898e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3743e-01, "std": 2.8577e-01, "min_value": 4.5165e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.8213e+00, "total_time": 4.3611e+02, "__timestamp": "2024-10-10 15:03:56.643421"}, {"step": 54000, "num_env_steps": 54000, "scores": {"n": 1, "mean": 2.4496e+03}, "actor_loss": {"n": 1, "mean": -6.5347e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.8900e-01}, "critic_loss": {"n": 1, "mean": 4.9609e+01}, "entropy_coef": {"n": 1, "mean": 2.6672e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3023e-01, "std": 2.8772e-01, "min_value": 9.8018e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.8599e+00, "total_time": 4.4597e+02, "__timestamp": "2024-10-10 15:04:06.504293"}, {"step": 55000, "num_env_steps": 55000, "scores": {"n": 1, "mean": 1.8830e+03}, "actor_loss": {"n": 1, "mean": -6.8267e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.7271e-01}, "critic_loss": {"n": 1, "mean": 4.5084e+00}, "entropy_coef": {"n": 1, "mean": 2.7623e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1009e-01, "std": 2.9482e-01, "min_value": 1.6424e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.9151e+00, "total_time": 4.5589e+02, "__timestamp": "2024-10-10 15:04:16.419360"}, {"step": 56000, "num_env_steps": 56000, "scores": {"n": 1, "mean": 2.5781e+03}, "actor_loss": {"n": 1, "mean": -6.9590e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.3884e+00}, "critic_loss": {"n": 1, "mean": 7.6019e+00}, "entropy_coef": {"n": 1, "mean": 2.8271e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2737e-01, "std": 2.8910e-01, "min_value": 2.4252e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.9565e+00, "total_time": 4.6584e+02, "__timestamp": "2024-10-10 15:04:26.374818"}, {"step": 57000, "num_env_steps": 57000, "scores": {"n": 1, "mean": 2.8109e+03}, "actor_loss": {"n": 1, "mean": -7.5330e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.9308e+00}, "critic_loss": {"n": 1, "mean": 7.4382e+00}, "entropy_coef": {"n": 1, "mean": 2.9233e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3654e-01, "std": 2.8641e-01, "min_value": 3.8800e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0477e+01, "total_time": 4.7632e+02, "__timestamp": "2024-10-10 15:04:36.852060"}, {"step": 58000, "num_env_steps": 58000, "scores": {"n": 1, "mean": 2.6619e+03}, "actor_loss": {"n": 1, "mean": -7.9364e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2643e-01}, "critic_loss": {"n": 1, "mean": 3.4215e+01}, "entropy_coef": {"n": 1, "mean": 3.0083e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2047e-01, "std": 2.9149e-01, "min_value": 3.7974e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.6111e+00, "total_time": 4.8593e+02, "__timestamp": "2024-10-10 15:04:46.463149"}, {"step": 59000, "num_env_steps": 59000, "scores": {"n": 1, "mean": 2.8798e+03}, "actor_loss": {"n": 1, "mean": -7.3711e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.5882e-01}, "critic_loss": {"n": 1, "mean": 6.8667e+00}, "entropy_coef": {"n": 1, "mean": 3.2409e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3703e-01, "std": 2.9131e-01, "min_value": 1.9240e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.8915e+00, "total_time": 4.9582e+02, "__timestamp": "2024-10-10 15:04:56.355682"}, {"step": 60000, "num_env_steps": 60000, "scores": {"n": 1, "mean": 3.0525e+03}, "actor_loss": {"n": 1, "mean": -7.9117e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.7434e-01}, "critic_loss": {"n": 1, "mean": 4.9421e+00}, "entropy_coef": {"n": 1, "mean": 3.3344e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3327e-01, "std": 2.9268e-01, "min_value": 6.2548e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6743e+00, "total_time": 5.0550e+02, "__timestamp": "2024-10-10 15:05:06.030000"}, {"step": 61000, "num_env_steps": 61000, "scores": {"n": 1, "mean": 2.9767e+03}, "actor_loss": {"n": 1, "mean": -8.4894e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.8772e-02}, "critic_loss": {"n": 1, "mean": 6.6984e+00}, "entropy_coef": {"n": 1, "mean": 3.5155e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4214e-01, "std": 2.9254e-01, "min_value": 6.1101e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.5984e+00, "total_time": 5.1510e+02, "__timestamp": "2024-10-10 15:05:15.628410"}, {"step": 62000, "num_env_steps": 62000, "scores": {"n": 1, "mean": 3.1574e+03}, "actor_loss": {"n": 1, "mean": -9.4648e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.5374e-01}, "critic_loss": {"n": 1, "mean": 9.4870e+00}, "entropy_coef": {"n": 1, "mean": 3.5747e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5296e-01, "std": 2.8624e-01, "min_value": 6.3300e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0344e+01, "total_time": 5.2544e+02, "__timestamp": "2024-10-10 15:05:25.971448"}, {"step": 63000, "num_env_steps": 63000, "scores": {"n": 1, "mean": 3.1290e+03}, "actor_loss": {"n": 1, "mean": -9.4442e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.9221e-01}, "critic_loss": {"n": 1, "mean": 8.3422e+00}, "entropy_coef": {"n": 1, "mean": 3.6621e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5484e-01, "std": 2.8936e-01, "min_value": 1.2862e-03, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 9.4830e+00, "total_time": 5.3492e+02, "__timestamp": "2024-10-10 15:05:35.454400"}, {"step": 64000, "num_env_steps": 64000, "scores": {"n": 1, "mean": 3.4554e+03}, "actor_loss": {"n": 1, "mean": -9.2250e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.3545e-01}, "critic_loss": {"n": 1, "mean": 7.2804e+01}, "entropy_coef": {"n": 1, "mean": 3.7103e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6860e-01, "std": 2.8253e-01, "min_value": 3.7304e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.2769e+00, "total_time": 5.4420e+02, "__timestamp": "2024-10-10 15:05:44.732307"}, {"step": 65000, "num_env_steps": 65000, "scores": {"n": 1, "mean": 3.2595e+03}, "actor_loss": {"n": 1, "mean": -1.0252e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.8380e-01}, "critic_loss": {"n": 1, "mean": 8.7049e+00}, "entropy_coef": {"n": 1, "mean": 3.9528e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5415e-01, "std": 2.9156e-01, "min_value": 1.1269e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.5438e+00, "total_time": 5.5374e+02, "__timestamp": "2024-10-10 15:05:54.276104"}, {"step": 66000, "num_env_steps": 66000, "scores": {"n": 1, "mean": 3.0565e+03}, "actor_loss": {"n": 1, "mean": -1.0442e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5463e+00}, "critic_loss": {"n": 1, "mean": 7.4831e+00}, "entropy_coef": {"n": 1, "mean": 4.0178e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4740e-01, "std": 2.9146e-01, "min_value": 9.7728e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0279e+01, "total_time": 5.6402e+02, "__timestamp": "2024-10-10 15:06:04.554510"}, {"step": 67000, "num_env_steps": 67000, "scores": {"n": 1, "mean": 3.3388e+03}, "actor_loss": {"n": 1, "mean": -9.8414e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.8010e+00}, "critic_loss": {"n": 1, "mean": 6.6461e+00}, "entropy_coef": {"n": 1, "mean": 4.1721e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6165e-01, "std": 2.8468e-01, "min_value": 2.5305e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0400e+01, "total_time": 5.7442e+02, "__timestamp": "2024-10-10 15:06:14.954399"}, {"step": 68000, "num_env_steps": 68000, "scores": {"n": 1, "mean": 3.4723e+03}, "actor_loss": {"n": 1, "mean": -1.0480e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.6549e-01}, "critic_loss": {"n": 1, "mean": 7.9904e+00}, "entropy_coef": {"n": 1, "mean": 4.4000e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7254e-01, "std": 2.8735e-01, "min_value": 4.5896e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.7051e+00, "total_time": 5.8413e+02, "__timestamp": "2024-10-10 15:06:24.660512"}, {"step": 69000, "num_env_steps": 69000, "scores": {"n": 1, "mean": 3.2282e+03}, "actor_loss": {"n": 1, "mean": -1.0803e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4761e-01}, "critic_loss": {"n": 1, "mean": 8.6078e+00}, "entropy_coef": {"n": 1, "mean": 4.4154e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5506e-01, "std": 2.8966e-01, "min_value": 6.3404e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6370e+00, "total_time": 5.9377e+02, "__timestamp": "2024-10-10 15:06:34.297547"}, {"step": 70000, "num_env_steps": 70000, "scores": {"n": 1, "mean": 3.3870e+03}, "actor_loss": {"n": 1, "mean": -1.1266e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.9550e+00}, "critic_loss": {"n": 1, "mean": 8.4307e+00}, "entropy_coef": {"n": 1, "mean": 4.6444e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6497e-01, "std": 2.8577e-01, "min_value": 6.6757e-06, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0119e+01, "total_time": 6.0388e+02, "__timestamp": "2024-10-10 15:06:44.415384"}, {"step": 71000, "num_env_steps": 71000, "scores": {"n": 1, "mean": 3.2660e+03}, "actor_loss": {"n": 1, "mean": -1.1324e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.5077e-01}, "critic_loss": {"n": 1, "mean": 9.1044e+00}, "entropy_coef": {"n": 1, "mean": 4.7609e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6190e-01, "std": 2.8750e-01, "min_value": 1.6038e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0392e+01, "total_time": 6.1428e+02, "__timestamp": "2024-10-10 15:06:54.808084"}, {"step": 72000, "num_env_steps": 72000, "scores": {"n": 1, "mean": 3.3943e+03}, "actor_loss": {"n": 1, "mean": -1.1617e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5130e-01}, "critic_loss": {"n": 1, "mean": 1.0635e+01}, "entropy_coef": {"n": 1, "mean": 4.8761e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5971e-01, "std": 2.8694e-01, "min_value": 5.3528e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.5900e+00, "total_time": 6.2387e+02, "__timestamp": "2024-10-10 15:07:04.398115"}, {"step": 73000, "num_env_steps": 73000, "scores": {"n": 1, "mean": 3.4484e+03}, "actor_loss": {"n": 1, "mean": -1.1538e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2640e+00}, "critic_loss": {"n": 1, "mean": 1.0002e+01}, "entropy_coef": {"n": 1, "mean": 5.1279e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6159e-01, "std": 2.8792e-01, "min_value": 2.5277e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.7154e+00, "total_time": 6.3358e+02, "__timestamp": "2024-10-10 15:07:14.113556"}, {"step": 74000, "num_env_steps": 74000, "scores": {"n": 1, "mean": 3.4559e+03}, "actor_loss": {"n": 1, "mean": -1.2840e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2608e+00}, "critic_loss": {"n": 1, "mean": 1.2513e+01}, "entropy_coef": {"n": 1, "mean": 5.1939e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5779e-01, "std": 2.8858e-01, "min_value": 8.9394e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.6136e+00, "total_time": 6.4320e+02, "__timestamp": "2024-10-10 15:07:23.726172"}, {"step": 75000, "num_env_steps": 75000, "scores": {"n": 1, "mean": 3.4953e+03}, "actor_loss": {"n": 1, "mean": -1.2691e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.6163e+00}, "critic_loss": {"n": 1, "mean": 9.1960e+00}, "entropy_coef": {"n": 1, "mean": 5.2011e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5360e-01, "std": 2.8809e-01, "min_value": 7.9951e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.9157e+00, "total_time": 6.5311e+02, "__timestamp": "2024-10-10 15:07:33.641868"}, {"step": 76000, "num_env_steps": 76000, "scores": {"n": 1, "mean": 3.4076e+03}, "actor_loss": {"n": 1, "mean": -1.3221e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.9123e+00}, "critic_loss": {"n": 1, "mean": 9.7416e+00}, "entropy_coef": {"n": 1, "mean": 5.3688e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6301e-01, "std": 2.8471e-01, "min_value": 8.3321e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.9226e+00, "total_time": 6.6303e+02, "__timestamp": "2024-10-10 15:07:43.565447"}, {"step": 77000, "num_env_steps": 77000, "scores": {"n": 1, "mean": 3.7265e+03}, "actor_loss": {"n": 1, "mean": -1.2254e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.3390e-01}, "critic_loss": {"n": 1, "mean": 9.7578e+01}, "entropy_coef": {"n": 1, "mean": 5.5129e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6593e-01, "std": 2.8278e-01, "min_value": 2.4780e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 9.2237e+00, "total_time": 6.7226e+02, "__timestamp": "2024-10-10 15:07:52.788184"}, {"step": 78000, "num_env_steps": 78000, "scores": {"n": 1, "mean": 3.9133e+03}, "actor_loss": {"n": 1, "mean": -1.3664e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.1082e-01}, "critic_loss": {"n": 1, "mean": 9.4771e+00}, "entropy_coef": {"n": 1, "mean": 5.6866e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6901e-01, "std": 2.8403e-01, "min_value": 5.6289e-05, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.6963e+00, "total_time": 6.8195e+02, "__timestamp": "2024-10-10 15:08:02.485446"}, {"step": 79000, "num_env_steps": 79000, "scores": {"n": 1, "mean": 3.4259e+03}, "actor_loss": {"n": 1, "mean": -1.3661e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.9458e-01}, "critic_loss": {"n": 1, "mean": 1.1236e+01}, "entropy_coef": {"n": 1, "mean": 5.7669e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6729e-01, "std": 2.8139e-01, "min_value": 5.9451e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0211e+01, "total_time": 6.9216e+02, "__timestamp": "2024-10-10 15:08:12.695985"}, {"step": 80000, "num_env_steps": 80000, "scores": {"n": 1, "mean": 3.5224e+03}, "actor_loss": {"n": 1, "mean": -1.3552e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1775e+00}, "critic_loss": {"n": 1, "mean": 1.0814e+01}, "entropy_coef": {"n": 1, "mean": 5.8210e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6433e-01, "std": 2.8200e-01, "min_value": 1.0262e-03, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0354e+01, "total_time": 7.0252e+02, "__timestamp": "2024-10-10 15:08:23.050423"}, {"step": 81000, "num_env_steps": 81000, "scores": {"n": 1, "mean": 3.5783e+03}, "actor_loss": {"n": 1, "mean": -1.4211e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.7415e-01}, "critic_loss": {"n": 1, "mean": 1.3243e+02}, "entropy_coef": {"n": 1, "mean": 6.0008e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7028e-01, "std": 2.8137e-01, "min_value": 7.5907e-05, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.5444e+00, "total_time": 7.1206e+02, "__timestamp": "2024-10-10 15:08:32.593844"}, {"step": 82000, "num_env_steps": 82000, "scores": {"n": 1, "mean": 3.8099e+03}, "actor_loss": {"n": 1, "mean": -1.4611e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.0620e-01}, "critic_loss": {"n": 1, "mean": 1.1706e+01}, "entropy_coef": {"n": 1, "mean": 6.0259e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6371e-01, "std": 2.8165e-01, "min_value": 4.7144e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.4770e+00, "total_time": 7.2154e+02, "__timestamp": "2024-10-10 15:08:42.070801"}, {"step": 83000, "num_env_steps": 83000, "scores": {"n": 1, "mean": 4.0064e+03}, "actor_loss": {"n": 1, "mean": -1.5138e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.1644e-01}, "critic_loss": {"n": 1, "mean": 1.0279e+01}, "entropy_coef": {"n": 1, "mean": 6.1540e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6792e-01, "std": 2.8700e-01, "min_value": 3.8240e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 9.4936e+00, "total_time": 7.3103e+02, "__timestamp": "2024-10-10 15:08:51.564374"}, {"step": 84000, "num_env_steps": 84000, "scores": {"n": 1, "mean": 3.7473e+03}, "actor_loss": {"n": 1, "mean": -1.5430e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.8192e-01}, "critic_loss": {"n": 1, "mean": 1.4381e+01}, "entropy_coef": {"n": 1, "mean": 6.3058e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6575e-01, "std": 2.8567e-01, "min_value": 4.9293e-05, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 9.2885e+00, "total_time": 7.4032e+02, "__timestamp": "2024-10-10 15:09:00.853877"}, {"step": 85000, "num_env_steps": 85000, "scores": {"n": 1, "mean": 3.8642e+03}, "actor_loss": {"n": 1, "mean": -1.4476e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.8529e-01}, "critic_loss": {"n": 1, "mean": 1.0075e+01}, "entropy_coef": {"n": 1, "mean": 6.4010e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6906e-01, "std": 2.8040e-01, "min_value": 1.1964e-03, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.5536e+00, "total_time": 7.4988e+02, "__timestamp": "2024-10-10 15:09:10.407482"}, {"step": 86000, "num_env_steps": 86000, "scores": {"n": 1, "mean": 3.4719e+03}, "actor_loss": {"n": 1, "mean": -1.5863e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.6389e-01}, "critic_loss": {"n": 1, "mean": 1.0264e+01}, "entropy_coef": {"n": 1, "mean": 6.6192e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6544e-01, "std": 2.8216e-01, "min_value": 5.1635e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 9.8139e+00, "total_time": 7.5969e+02, "__timestamp": "2024-10-10 15:09:20.221352"}, {"step": 87000, "num_env_steps": 87000, "scores": {"n": 1, "mean": 3.6840e+03}, "actor_loss": {"n": 1, "mean": -1.6172e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.9725e-01}, "critic_loss": {"n": 1, "mean": 1.1992e+01}, "entropy_coef": {"n": 1, "mean": 6.8278e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7336e-01, "std": 2.7982e-01, "min_value": 4.3188e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 9.6036e+00, "total_time": 7.6929e+02, "__timestamp": "2024-10-10 15:09:29.824997"}, {"step": 88000, "num_env_steps": 88000, "scores": {"n": 1, "mean": 3.7756e+03}, "actor_loss": {"n": 1, "mean": -1.5889e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3721e+00}, "critic_loss": {"n": 1, "mean": 1.2528e+01}, "entropy_coef": {"n": 1, "mean": 6.7165e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7170e-01, "std": 2.7892e-01, "min_value": 3.6830e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0100e+01, "total_time": 7.7939e+02, "__timestamp": "2024-10-10 15:09:39.924304"}, {"step": 89000, "num_env_steps": 89000, "scores": {"n": 1, "mean": 4.0282e+03}, "actor_loss": {"n": 1, "mean": -1.5697e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.1039e-01}, "critic_loss": {"n": 1, "mean": 1.6712e+01}, "entropy_coef": {"n": 1, "mean": 6.9323e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7211e-01, "std": 2.8111e-01, "min_value": 7.4258e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 9.7528e+00, "total_time": 7.8915e+02, "__timestamp": "2024-10-10 15:09:49.678058"}, {"step": 90000, "num_env_steps": 90000, "scores": {"n": 1, "mean": 4.1293e+03}, "actor_loss": {"n": 1, "mean": -1.6483e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.8391e-01}, "critic_loss": {"n": 1, "mean": 1.1595e+01}, "entropy_coef": {"n": 1, "mean": 7.1438e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7383e-01, "std": 2.8127e-01, "min_value": 7.0423e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.4964e+00, "total_time": 7.9864e+02, "__timestamp": "2024-10-10 15:09:59.173425"}, {"step": 91000, "num_env_steps": 91000, "scores": {"n": 1, "mean": 4.1285e+03}, "actor_loss": {"n": 1, "mean": -1.6082e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.7115e-01}, "critic_loss": {"n": 1, "mean": 2.4663e+01}, "entropy_coef": {"n": 1, "mean": 7.2822e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7258e-01, "std": 2.8261e-01, "min_value": 3.9801e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.5519e+00, "total_time": 8.0819e+02, "__timestamp": "2024-10-10 15:10:08.726370"}, {"step": 92000, "num_env_steps": 92000, "scores": {"n": 1, "mean": 4.0253e+03}, "actor_loss": {"n": 1, "mean": -1.6611e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5983e+00}, "critic_loss": {"n": 1, "mean": 1.3746e+02}, "entropy_coef": {"n": 1, "mean": 7.3442e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7045e-01, "std": 2.8538e-01, "min_value": 2.2091e-05, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0049e+01, "total_time": 8.1824e+02, "__timestamp": "2024-10-10 15:10:18.775336"}, {"step": 93000, "num_env_steps": 93000, "scores": {"n": 1, "mean": 4.1772e+03}, "actor_loss": {"n": 1, "mean": -1.6936e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1544e-02}, "critic_loss": {"n": 1, "mean": 1.6165e+01}, "entropy_coef": {"n": 1, "mean": 7.1681e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8263e-01, "std": 2.7601e-01, "min_value": 3.1394e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0705e+01, "total_time": 8.2895e+02, "__timestamp": "2024-10-10 15:10:29.478897"}, {"step": 94000, "num_env_steps": 94000, "scores": {"n": 1, "mean": 4.3264e+03}, "actor_loss": {"n": 1, "mean": -1.5995e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.4587e-01}, "critic_loss": {"n": 1, "mean": 1.2494e+01}, "entropy_coef": {"n": 1, "mean": 7.2745e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8819e-01, "std": 2.7488e-01, "min_value": 3.1109e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0852e+01, "total_time": 8.3980e+02, "__timestamp": "2024-10-10 15:10:40.332208"}, {"step": 95000, "num_env_steps": 95000, "scores": {"n": 1, "mean": 3.8510e+03}, "actor_loss": {"n": 1, "mean": -1.6641e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2938e-01}, "critic_loss": {"n": 1, "mean": 2.0087e+02}, "entropy_coef": {"n": 1, "mean": 7.6043e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7388e-01, "std": 2.7715e-01, "min_value": 8.6941e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0439e+01, "total_time": 8.5024e+02, "__timestamp": "2024-10-10 15:10:50.770901"}, {"step": 96000, "num_env_steps": 96000, "scores": {"n": 1, "mean": 4.2521e+03}, "actor_loss": {"n": 1, "mean": -1.6881e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.9269e-01}, "critic_loss": {"n": 1, "mean": 1.6944e+01}, "entropy_coef": {"n": 1, "mean": 7.7411e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9069e-01, "std": 2.7071e-01, "min_value": 5.0768e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 9.5417e+00, "total_time": 8.5978e+02, "__timestamp": "2024-10-10 15:11:00.311607"}, {"step": 97000, "num_env_steps": 97000, "scores": {"n": 1, "mean": 4.0380e+03}, "actor_loss": {"n": 1, "mean": -1.7907e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.5294e+00}, "critic_loss": {"n": 1, "mean": 1.6122e+01}, "entropy_coef": {"n": 1, "mean": 7.9017e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8134e-01, "std": 2.7335e-01, "min_value": 5.6624e-05, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0190e+01, "total_time": 8.6997e+02, "__timestamp": "2024-10-10 15:11:10.501314"}, {"step": 98000, "num_env_steps": 98000, "scores": {"n": 1, "mean": 4.3674e+03}, "actor_loss": {"n": 1, "mean": -1.8058e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.0112e-01}, "critic_loss": {"n": 1, "mean": 1.3177e+01}, "entropy_coef": {"n": 1, "mean": 8.0231e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9807e-01, "std": 2.6376e-01, "min_value": 3.7706e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.9836e+00, "total_time": 8.7995e+02, "__timestamp": "2024-10-10 15:11:20.485925"}, {"step": 99000, "num_env_steps": 99000, "scores": {"n": 1, "mean": 4.3034e+03}, "actor_loss": {"n": 1, "mean": -1.8143e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.8654e-01}, "critic_loss": {"n": 1, "mean": 1.4245e+01}, "entropy_coef": {"n": 1, "mean": 8.1653e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8346e-01, "std": 2.7326e-01, "min_value": 2.7437e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.0253e+01, "total_time": 8.9021e+02, "__timestamp": "2024-10-10 15:11:30.739404"}, {"step": 100000, "num_env_steps": 100000, "scores": {"n": 1, "mean": 4.3706e+03}, "actor_loss": {"n": 1, "mean": -1.7686e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.9817e-01}, "critic_loss": {"n": 1, "mean": 1.5180e+01}, "entropy_coef": {"n": 1, "mean": 8.2473e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0089e-01, "std": 2.6155e-01, "min_value": 8.5811e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1021e+01, "total_time": 9.0123e+02, "__timestamp": "2024-10-10 15:11:41.759303"}, {"step": 101000, "num_env_steps": 101000, "scores": {"n": 1, "mean": 4.5965e+03}, "actor_loss": {"n": 1, "mean": -1.7917e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.1725e-01}, "critic_loss": {"n": 1, "mean": 1.3952e+01}, "entropy_coef": {"n": 1, "mean": 8.2146e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0189e-01, "std": 2.6454e-01, "min_value": 5.2490e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0343e+01, "total_time": 9.1157e+02, "__timestamp": "2024-10-10 15:11:52.102918"}, {"step": 102000, "num_env_steps": 102000, "scores": {"n": 1, "mean": 2.0850e+03}, "actor_loss": {"n": 1, "mean": -1.8673e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.7488e-01}, "critic_loss": {"n": 1, "mean": 1.9048e+01}, "entropy_coef": {"n": 1, "mean": 8.5174e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1422e-01, "std": 2.9187e-01, "min_value": 1.9416e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0238e+01, "total_time": 9.2181e+02, "__timestamp": "2024-10-10 15:12:02.340717"}, {"step": 103000, "num_env_steps": 103000, "scores": {"n": 1, "mean": 4.5861e+03}, "actor_loss": {"n": 1, "mean": -1.8552e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4760e+00}, "critic_loss": {"n": 1, "mean": 1.4729e+01}, "entropy_coef": {"n": 1, "mean": 8.4022e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9309e-01, "std": 2.6653e-01, "min_value": 2.3445e-03, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 9.8710e+00, "total_time": 9.3168e+02, "__timestamp": "2024-10-10 15:12:12.211716"}, {"step": 104000, "num_env_steps": 104000, "scores": {"n": 1, "mean": 4.2705e+03}, "actor_loss": {"n": 1, "mean": -1.7552e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0789e+00}, "critic_loss": {"n": 1, "mean": 1.4564e+01}, "entropy_coef": {"n": 1, "mean": 8.4817e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9476e-01, "std": 2.6377e-01, "min_value": 5.0361e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 9.7138e+00, "total_time": 9.4139e+02, "__timestamp": "2024-10-10 15:12:21.924527"}, {"step": 105000, "num_env_steps": 105000, "scores": {"n": 1, "mean": 4.1803e+03}, "actor_loss": {"n": 1, "mean": -1.8195e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.4499e-01}, "critic_loss": {"n": 1, "mean": 1.9378e+01}, "entropy_coef": {"n": 1, "mean": 8.5195e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9938e-01, "std": 2.6019e-01, "min_value": 2.2941e-03, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.8133e+00, "total_time": 9.5121e+02, "__timestamp": "2024-10-10 15:12:31.737816"}, {"step": 106000, "num_env_steps": 106000, "scores": {"n": 1, "mean": 4.5025e+03}, "actor_loss": {"n": 1, "mean": -1.8269e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.0797e-01}, "critic_loss": {"n": 1, "mean": 2.0880e+01}, "entropy_coef": {"n": 1, "mean": 8.8050e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0125e-01, "std": 2.6222e-01, "min_value": 1.9007e-03, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 9.7382e+00, "total_time": 9.6094e+02, "__timestamp": "2024-10-10 15:12:41.476020"}, {"step": 107000, "num_env_steps": 107000, "scores": {"n": 1, "mean": 4.2531e+03}, "actor_loss": {"n": 1, "mean": -1.8844e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.2438e-02}, "critic_loss": {"n": 1, "mean": 1.5603e+01}, "entropy_coef": {"n": 1, "mean": 8.8881e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8973e-01, "std": 2.6775e-01, "min_value": 1.2184e-03, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 9.6421e+00, "total_time": 9.7059e+02, "__timestamp": "2024-10-10 15:12:51.119164"}, {"step": 108000, "num_env_steps": 108000, "scores": {"n": 1, "mean": 4.6060e+03}, "actor_loss": {"n": 1, "mean": -1.8564e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.3248e-01}, "critic_loss": {"n": 1, "mean": 1.3725e+01}, "entropy_coef": {"n": 1, "mean": 8.9967e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8935e-01, "std": 2.6903e-01, "min_value": 1.0096e-03, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 9.7327e+00, "total_time": 9.8032e+02, "__timestamp": "2024-10-10 15:13:00.851818"}, {"step": 109000, "num_env_steps": 109000, "scores": {"n": 1, "mean": 4.8058e+03}, "actor_loss": {"n": 1, "mean": -1.8780e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0342e+00}, "critic_loss": {"n": 1, "mean": 1.3337e+01}, "entropy_coef": {"n": 1, "mean": 9.3580e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0684e-01, "std": 2.6088e-01, "min_value": 1.6448e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0449e+01, "total_time": 9.9077e+02, "__timestamp": "2024-10-10 15:13:11.299529"}, {"step": 110000, "num_env_steps": 110000, "scores": {"n": 1, "mean": 5.1742e+03}, "actor_loss": {"n": 1, "mean": -1.9727e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4312e+00}, "critic_loss": {"n": 1, "mean": 1.7228e+01}, "entropy_coef": {"n": 1, "mean": 9.1237e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1953e-01, "std": 2.5241e-01, "min_value": 2.6083e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 9.6577e+00, "total_time": 1.0004e+03, "__timestamp": "2024-10-10 15:13:20.957216"}, {"step": 111000, "num_env_steps": 111000, "scores": {"n": 1, "mean": 4.7901e+03}, "actor_loss": {"n": 1, "mean": -1.8771e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0272e+00}, "critic_loss": {"n": 1, "mean": 1.9226e+01}, "entropy_coef": {"n": 1, "mean": 9.2946e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0015e-01, "std": 2.6619e-01, "min_value": 7.0471e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 9.4971e+00, "total_time": 1.0099e+03, "__timestamp": "2024-10-10 15:13:30.455364"}, {"step": 112000, "num_env_steps": 112000, "scores": {"n": 1, "mean": 4.7415e+03}, "actor_loss": {"n": 1, "mean": -1.8573e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.9098e-01}, "critic_loss": {"n": 1, "mean": 1.4340e+01}, "entropy_coef": {"n": 1, "mean": 9.3618e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1048e-01, "std": 2.5544e-01, "min_value": 6.4495e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 9.6936e+00, "total_time": 1.0196e+03, "__timestamp": "2024-10-10 15:13:40.147949"}, {"step": 113000, "num_env_steps": 113000, "scores": {"n": 1, "mean": 9.9967e+02}, "actor_loss": {"n": 1, "mean": -1.9863e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.6621e-01}, "critic_loss": {"n": 1, "mean": 2.5245e+02}, "entropy_coef": {"n": 1, "mean": 9.5477e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.5454e-01, "std": 3.0016e-01, "min_value": 3.5235e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0124e+01, "total_time": 1.0297e+03, "__timestamp": "2024-10-10 15:13:50.272118"}, {"step": 114000, "num_env_steps": 114000, "scores": {"n": 1, "mean": 5.1623e+03}, "actor_loss": {"n": 1, "mean": -2.0984e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.5234e-01}, "critic_loss": {"n": 1, "mean": 2.0122e+01}, "entropy_coef": {"n": 1, "mean": 9.7085e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2198e-01, "std": 2.5071e-01, "min_value": 2.5325e-03, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0352e+01, "total_time": 1.0401e+03, "__timestamp": "2024-10-10 15:14:00.624066"}, {"step": 115000, "num_env_steps": 115000, "scores": {"n": 1, "mean": 4.6982e+03}, "actor_loss": {"n": 1, "mean": -2.0434e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.4042e-01}, "critic_loss": {"n": 1, "mean": 1.5219e+01}, "entropy_coef": {"n": 1, "mean": 9.9950e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0535e-01, "std": 2.5790e-01, "min_value": 2.7615e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.4875e+00, "total_time": 1.0496e+03, "__timestamp": "2024-10-10 15:14:10.111558"}, {"step": 116000, "num_env_steps": 116000, "scores": {"n": 1, "mean": 1.8573e+03}, "actor_loss": {"n": 1, "mean": -2.1086e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1972e+00}, "critic_loss": {"n": 1, "mean": 2.5618e+02}, "entropy_coef": {"n": 1, "mean": 9.7978e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7876e-01, "std": 3.0198e-01, "min_value": 3.7886e-05, "max_value": 9.9962e-01}, "num_gradient_steps": 0, "step_time": 9.8003e+00, "total_time": 1.0594e+03, "__timestamp": "2024-10-10 15:14:19.912822"}, {"step": 117000, "num_env_steps": 117000, "scores": {"n": 1, "mean": 4.8908e+03}, "actor_loss": {"n": 1, "mean": -1.8751e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.9400e+00}, "critic_loss": {"n": 1, "mean": 1.3215e+01}, "entropy_coef": {"n": 1, "mean": 9.8771e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4292e-01, "std": 2.3049e-01, "min_value": 5.1008e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0224e+01, "total_time": 1.0696e+03, "__timestamp": "2024-10-10 15:14:30.136505"}, {"step": 118000, "num_env_steps": 118000, "scores": {"n": 1, "mean": 4.9238e+03}, "actor_loss": {"n": 1, "mean": -1.9444e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.3065e-01}, "critic_loss": {"n": 1, "mean": 1.6543e+02}, "entropy_coef": {"n": 1, "mean": 1.0080e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3000e-01, "std": 2.3946e-01, "min_value": 3.9262e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0782e+01, "total_time": 1.0804e+03, "__timestamp": "2024-10-10 15:14:40.917473"}, {"step": 119000, "num_env_steps": 119000, "scores": {"n": 1, "mean": 4.6212e+03}, "actor_loss": {"n": 1, "mean": -2.0977e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.9399e-01}, "critic_loss": {"n": 1, "mean": 1.7601e+01}, "entropy_coef": {"n": 1, "mean": 1.0276e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1686e-01, "std": 2.4740e-01, "min_value": 6.5901e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0553e+01, "total_time": 1.0909e+03, "__timestamp": "2024-10-10 15:14:51.470040"}, {"step": 120000, "num_env_steps": 120000, "scores": {"n": 1, "mean": 4.7270e+03}, "actor_loss": {"n": 1, "mean": -2.0572e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.0638e-02}, "critic_loss": {"n": 1, "mean": 1.6613e+01}, "entropy_coef": {"n": 1, "mean": 1.0491e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2059e-01, "std": 2.4628e-01, "min_value": 4.0343e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 9.7175e+00, "total_time": 1.1007e+03, "__timestamp": "2024-10-10 15:15:01.187492"}, {"step": 121000, "num_env_steps": 121000, "scores": {"n": 1, "mean": 4.7378e+03}, "actor_loss": {"n": 1, "mean": -2.0693e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.0581e-01}, "critic_loss": {"n": 1, "mean": 1.8233e+01}, "entropy_coef": {"n": 1, "mean": 1.0435e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2169e-01, "std": 2.4317e-01, "min_value": 3.1653e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.9144e+00, "total_time": 1.1106e+03, "__timestamp": "2024-10-10 15:15:11.102896"}, {"step": 122000, "num_env_steps": 122000, "scores": {"n": 1, "mean": 4.9355e+03}, "actor_loss": {"n": 1, "mean": -2.0446e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.4632e-01}, "critic_loss": {"n": 1, "mean": 2.0342e+01}, "entropy_coef": {"n": 1, "mean": 1.0751e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4387e-01, "std": 2.2974e-01, "min_value": 2.9066e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.6802e+00, "total_time": 1.1203e+03, "__timestamp": "2024-10-10 15:15:20.782083"}, {"step": 123000, "num_env_steps": 123000, "scores": {"n": 1, "mean": 5.0391e+03}, "actor_loss": {"n": 1, "mean": -2.1197e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.4485e-01}, "critic_loss": {"n": 1, "mean": 1.6862e+01}, "entropy_coef": {"n": 1, "mean": 1.0870e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5797e-01, "std": 2.1606e-01, "min_value": 1.2599e-03, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.1015e+01, "total_time": 1.1313e+03, "__timestamp": "2024-10-10 15:15:31.798174"}, {"step": 124000, "num_env_steps": 124000, "scores": {"n": 1, "mean": 4.9299e+03}, "actor_loss": {"n": 1, "mean": -2.1872e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.6554e-01}, "critic_loss": {"n": 1, "mean": 6.8408e+01}, "entropy_coef": {"n": 1, "mean": 1.1126e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3650e-01, "std": 2.3899e-01, "min_value": 2.5739e-03, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.0819e+01, "total_time": 1.1421e+03, "__timestamp": "2024-10-10 15:15:42.616789"}, {"step": 125000, "num_env_steps": 125000, "scores": {"n": 1, "mean": 4.7396e+03}, "actor_loss": {"n": 1, "mean": -1.9728e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2600e+00}, "critic_loss": {"n": 1, "mean": 3.9225e+02}, "entropy_coef": {"n": 1, "mean": 1.1059e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4075e-01, "std": 2.3013e-01, "min_value": 1.0213e-03, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.1053e+01, "total_time": 1.1531e+03, "__timestamp": "2024-10-10 15:15:53.669396"}, {"step": 126000, "num_env_steps": 126000, "scores": {"n": 1, "mean": 5.1932e+03}, "actor_loss": {"n": 1, "mean": -2.2427e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.5071e+00}, "critic_loss": {"n": 1, "mean": 2.1456e+01}, "entropy_coef": {"n": 1, "mean": 1.1303e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5008e-01, "std": 2.2494e-01, "min_value": 6.5857e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.1820e+01, "total_time": 1.1650e+03, "__timestamp": "2024-10-10 15:16:05.489461"}, {"step": 127000, "num_env_steps": 127000, "scores": {"n": 1, "mean": 5.0252e+03}, "actor_loss": {"n": 1, "mean": -2.0423e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.4604e-01}, "critic_loss": {"n": 1, "mean": 1.5529e+01}, "entropy_coef": {"n": 1, "mean": 1.1597e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5028e-01, "std": 2.2320e-01, "min_value": 1.2119e-03, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 1.1540e+01, "total_time": 1.1765e+03, "__timestamp": "2024-10-10 15:16:17.028487"}, {"step": 128000, "num_env_steps": 128000, "scores": {"n": 1, "mean": 4.9308e+03}, "actor_loss": {"n": 1, "mean": -2.0764e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.3811e-01}, "critic_loss": {"n": 1, "mean": 1.9716e+01}, "entropy_coef": {"n": 1, "mean": 1.1599e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3304e-01, "std": 2.4007e-01, "min_value": 1.3752e-03, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0738e+01, "total_time": 1.1872e+03, "__timestamp": "2024-10-10 15:16:27.766960"}, {"step": 129000, "num_env_steps": 129000, "scores": {"n": 1, "mean": 5.2699e+03}, "actor_loss": {"n": 1, "mean": -2.1547e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.2420e-01}, "critic_loss": {"n": 1, "mean": 1.8438e+01}, "entropy_coef": {"n": 1, "mean": 1.1649e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5045e-01, "std": 2.2270e-01, "min_value": 2.6229e-03, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.0690e+01, "total_time": 1.1979e+03, "__timestamp": "2024-10-10 15:16:38.457738"}, {"step": 130000, "num_env_steps": 130000, "scores": {"n": 1, "mean": 5.1080e+03}, "actor_loss": {"n": 1, "mean": -2.2564e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.1527e-02}, "critic_loss": {"n": 1, "mean": 2.2459e+01}, "entropy_coef": {"n": 1, "mean": 1.1874e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4630e-01, "std": 2.2717e-01, "min_value": 5.0583e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0641e+01, "total_time": 1.2086e+03, "__timestamp": "2024-10-10 15:16:49.097987"}, {"step": 131000, "num_env_steps": 131000, "scores": {"n": 1, "mean": 5.0982e+03}, "actor_loss": {"n": 1, "mean": -2.1904e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4299e+00}, "critic_loss": {"n": 1, "mean": 2.1818e+01}, "entropy_coef": {"n": 1, "mean": 1.1944e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4448e-01, "std": 2.2802e-01, "min_value": 1.7314e-03, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0777e+01, "total_time": 1.2193e+03, "__timestamp": "2024-10-10 15:16:59.875169"}, {"step": 132000, "num_env_steps": 132000, "scores": {"n": 1, "mean": 5.1912e+03}, "actor_loss": {"n": 1, "mean": -2.1880e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.2745e-01}, "critic_loss": {"n": 1, "mean": 1.8430e+01}, "entropy_coef": {"n": 1, "mean": 1.2509e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4754e-01, "std": 2.2721e-01, "min_value": 4.1604e-05, "max_value": 9.9955e-01}, "num_gradient_steps": 0, "step_time": 1.0676e+01, "total_time": 1.2300e+03, "__timestamp": "2024-10-10 15:17:10.551016"}, {"step": 133000, "num_env_steps": 133000, "scores": {"n": 1, "mean": 5.2167e+03}, "actor_loss": {"n": 1, "mean": -2.3087e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.5302e-01}, "critic_loss": {"n": 1, "mean": 2.2178e+01}, "entropy_coef": {"n": 1, "mean": 1.2424e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4507e-01, "std": 2.2894e-01, "min_value": 5.0196e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0598e+01, "total_time": 1.2406e+03, "__timestamp": "2024-10-10 15:17:21.149394"}, {"step": 134000, "num_env_steps": 134000, "scores": {"n": 1, "mean": 5.2230e+03}, "actor_loss": {"n": 1, "mean": -2.2701e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.3276e-02}, "critic_loss": {"n": 1, "mean": 2.6736e+01}, "entropy_coef": {"n": 1, "mean": 1.2529e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5115e-01, "std": 2.2511e-01, "min_value": 8.2979e-04, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.0658e+01, "total_time": 1.2513e+03, "__timestamp": "2024-10-10 15:17:31.807105"}, {"step": 135000, "num_env_steps": 135000, "scores": {"n": 1, "mean": 5.1777e+03}, "actor_loss": {"n": 1, "mean": -2.1352e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.2571e-02}, "critic_loss": {"n": 1, "mean": 2.2280e+01}, "entropy_coef": {"n": 1, "mean": 1.2505e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4049e-01, "std": 2.3091e-01, "min_value": 1.4288e-03, "max_value": 9.9964e-01}, "num_gradient_steps": 0, "step_time": 1.0670e+01, "total_time": 1.2619e+03, "__timestamp": "2024-10-10 15:17:42.478513"}, {"step": 136000, "num_env_steps": 136000, "scores": {"n": 1, "mean": 5.2071e+03}, "actor_loss": {"n": 1, "mean": -2.2819e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0990e-01}, "critic_loss": {"n": 1, "mean": 1.8719e+01}, "entropy_coef": {"n": 1, "mean": 1.2486e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3265e-01, "std": 2.3347e-01, "min_value": 1.8264e-03, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.0628e+01, "total_time": 1.2726e+03, "__timestamp": "2024-10-10 15:17:53.106438"}, {"step": 137000, "num_env_steps": 137000, "scores": {"n": 1, "mean": 5.0846e+03}, "actor_loss": {"n": 1, "mean": -2.2438e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.5690e-01}, "critic_loss": {"n": 1, "mean": 1.8474e+01}, "entropy_coef": {"n": 1, "mean": 1.3079e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3030e-01, "std": 2.3736e-01, "min_value": 1.5333e-03, "max_value": 9.9964e-01}, "num_gradient_steps": 0, "step_time": 1.0733e+01, "total_time": 1.2833e+03, "__timestamp": "2024-10-10 15:18:03.838494"}, {"step": 138000, "num_env_steps": 138000, "scores": {"n": 1, "mean": 5.0971e+03}, "actor_loss": {"n": 1, "mean": -2.2772e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.8753e-01}, "critic_loss": {"n": 1, "mean": 2.1946e+02}, "entropy_coef": {"n": 1, "mean": 1.2848e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3580e-01, "std": 2.3331e-01, "min_value": 9.4612e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0688e+01, "total_time": 1.2940e+03, "__timestamp": "2024-10-10 15:18:14.526842"}, {"step": 139000, "num_env_steps": 139000, "scores": {"n": 1, "mean": 3.9571e+03}, "actor_loss": {"n": 1, "mean": -2.3301e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.2963e-01}, "critic_loss": {"n": 1, "mean": 1.4902e+01}, "entropy_coef": {"n": 1, "mean": 1.3191e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6810e-01, "std": 2.7506e-01, "min_value": 4.6700e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0738e+01, "total_time": 1.3047e+03, "__timestamp": "2024-10-10 15:18:25.265896"}, {"step": 140000, "num_env_steps": 140000, "scores": {"n": 1, "mean": 5.1455e+03}, "actor_loss": {"n": 1, "mean": -2.3312e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.0007e-01}, "critic_loss": {"n": 1, "mean": 1.8408e+01}, "entropy_coef": {"n": 1, "mean": 1.3429e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3517e-01, "std": 2.3799e-01, "min_value": 7.8272e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0629e+01, "total_time": 1.3154e+03, "__timestamp": "2024-10-10 15:18:35.893580"}, {"step": 141000, "num_env_steps": 141000, "scores": {"n": 1, "mean": 3.1261e+03}, "actor_loss": {"n": 1, "mean": -2.2785e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.2120e-01}, "critic_loss": {"n": 1, "mean": 1.5964e+01}, "entropy_coef": {"n": 1, "mean": 1.3364e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2352e-01, "std": 2.9131e-01, "min_value": 2.3952e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0734e+01, "total_time": 1.3261e+03, "__timestamp": "2024-10-10 15:18:46.629065"}, {"step": 142000, "num_env_steps": 142000, "scores": {"n": 1, "mean": 5.2803e+03}, "actor_loss": {"n": 1, "mean": -2.2149e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4548e-01}, "critic_loss": {"n": 1, "mean": 1.8196e+01}, "entropy_coef": {"n": 1, "mean": 1.3370e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3807e-01, "std": 2.3380e-01, "min_value": 1.4111e-03, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.0734e+01, "total_time": 1.3368e+03, "__timestamp": "2024-10-10 15:18:57.361712"}, {"step": 143000, "num_env_steps": 143000, "scores": {"n": 1, "mean": 5.1700e+03}, "actor_loss": {"n": 1, "mean": -2.2684e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.8282e-02}, "critic_loss": {"n": 1, "mean": 2.4602e+02}, "entropy_coef": {"n": 1, "mean": 1.3517e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1314e-01, "std": 2.5120e-01, "min_value": 9.0107e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0653e+01, "total_time": 1.3475e+03, "__timestamp": "2024-10-10 15:19:08.015249"}, {"step": 144000, "num_env_steps": 144000, "scores": {"n": 1, "mean": 5.6376e+03}, "actor_loss": {"n": 1, "mean": -2.3084e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.0707e-01}, "critic_loss": {"n": 1, "mean": 2.0340e+01}, "entropy_coef": {"n": 1, "mean": 1.3679e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3746e-01, "std": 2.3617e-01, "min_value": 3.0308e-03, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.1570e+01, "total_time": 1.3591e+03, "__timestamp": "2024-10-10 15:19:19.585089"}, {"step": 145000, "num_env_steps": 145000, "scores": {"n": 1, "mean": 5.3408e+03}, "actor_loss": {"n": 1, "mean": -2.2845e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.5361e-01}, "critic_loss": {"n": 1, "mean": 2.2051e+01}, "entropy_coef": {"n": 1, "mean": 1.3762e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2763e-01, "std": 2.4084e-01, "min_value": 1.0366e-04, "max_value": 9.9965e-01}, "num_gradient_steps": 0, "step_time": 1.1332e+01, "total_time": 1.3704e+03, "__timestamp": "2024-10-10 15:19:30.916051"}, {"step": 146000, "num_env_steps": 146000, "scores": {"n": 1, "mean": 5.2423e+03}, "actor_loss": {"n": 1, "mean": -2.2576e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.9643e-01}, "critic_loss": {"n": 1, "mean": 1.9035e+01}, "entropy_coef": {"n": 1, "mean": 1.3921e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2557e-01, "std": 2.4231e-01, "min_value": 3.7260e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.1523e+01, "total_time": 1.3819e+03, "__timestamp": "2024-10-10 15:19:42.439027"}, {"step": 147000, "num_env_steps": 147000, "scores": {"n": 1, "mean": 5.7398e+03}, "actor_loss": {"n": 1, "mean": -2.3313e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.7624e-01}, "critic_loss": {"n": 1, "mean": 1.9757e+01}, "entropy_coef": {"n": 1, "mean": 1.4355e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3623e-01, "std": 2.3463e-01, "min_value": 9.3077e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.1292e+01, "total_time": 1.3932e+03, "__timestamp": "2024-10-10 15:19:53.731906"}, {"step": 148000, "num_env_steps": 148000, "scores": {"n": 1, "mean": 5.3875e+03}, "actor_loss": {"n": 1, "mean": -2.1727e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2040e+00}, "critic_loss": {"n": 1, "mean": 2.6713e+01}, "entropy_coef": {"n": 1, "mean": 1.4189e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2256e-01, "std": 2.4384e-01, "min_value": 3.2146e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0717e+01, "total_time": 1.4039e+03, "__timestamp": "2024-10-10 15:20:04.448385"}, {"step": 149000, "num_env_steps": 149000, "scores": {"n": 1, "mean": 1.1354e+03}, "actor_loss": {"n": 1, "mean": -2.3476e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.2869e-01}, "critic_loss": {"n": 1, "mean": 1.7220e+01}, "entropy_coef": {"n": 1, "mean": 1.4381e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1077e-01, "std": 3.0274e-01, "min_value": 1.1280e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.0897e+01, "total_time": 1.4148e+03, "__timestamp": "2024-10-10 15:20:15.346776"}, {"step": 150000, "num_env_steps": 150000, "scores": {"n": 1, "mean": 5.3823e+03}, "actor_loss": {"n": 1, "mean": -2.3933e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.5544e-01}, "critic_loss": {"n": 1, "mean": 3.9552e+02}, "entropy_coef": {"n": 1, "mean": 1.4341e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2550e-01, "std": 2.4034e-01, "min_value": 6.9479e-04, "max_value": 9.9972e-01}, "num_gradient_steps": 0, "step_time": 1.1574e+01, "total_time": 1.4264e+03, "__timestamp": "2024-10-10 15:20:26.919411"}, {"step": 151000, "num_env_steps": 151000, "scores": {"n": 1, "mean": 5.5470e+03}, "actor_loss": {"n": 1, "mean": -2.3206e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.9614e-02}, "critic_loss": {"n": 1, "mean": 1.6871e+01}, "entropy_coef": {"n": 1, "mean": 1.4479e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3153e-01, "std": 2.3825e-01, "min_value": 1.1940e-03, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.1402e+01, "total_time": 1.4378e+03, "__timestamp": "2024-10-10 15:20:38.322440"}, {"step": 152000, "num_env_steps": 152000, "scores": {"n": 1, "mean": 5.3303e+03}, "actor_loss": {"n": 1, "mean": -2.3979e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.9326e-01}, "critic_loss": {"n": 1, "mean": 1.5237e+01}, "entropy_coef": {"n": 1, "mean": 1.4474e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2527e-01, "std": 2.3996e-01, "min_value": 1.8395e-04, "max_value": 9.9969e-01}, "num_gradient_steps": 0, "step_time": 1.1787e+01, "total_time": 1.4496e+03, "__timestamp": "2024-10-10 15:20:50.108029"}, {"step": 153000, "num_env_steps": 153000, "scores": {"n": 1, "mean": 5.1117e+03}, "actor_loss": {"n": 1, "mean": -2.2635e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.2615e-01}, "critic_loss": {"n": 1, "mean": 1.9913e+01}, "entropy_coef": {"n": 1, "mean": 1.4689e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1679e-01, "std": 2.4717e-01, "min_value": 1.2609e-03, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.1371e+01, "total_time": 1.4609e+03, "__timestamp": "2024-10-10 15:21:01.479137"}, {"step": 154000, "num_env_steps": 154000, "scores": {"n": 1, "mean": 5.3044e+03}, "actor_loss": {"n": 1, "mean": -2.4054e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.8310e-01}, "critic_loss": {"n": 1, "mean": 4.4728e+02}, "entropy_coef": {"n": 1, "mean": 1.4711e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2387e-01, "std": 2.4487e-01, "min_value": 5.9158e-06, "max_value": 9.9967e-01}, "num_gradient_steps": 0, "step_time": 1.1615e+01, "total_time": 1.4726e+03, "__timestamp": "2024-10-10 15:21:13.094480"}, {"step": 155000, "num_env_steps": 155000, "scores": {"n": 1, "mean": 5.3406e+03}, "actor_loss": {"n": 1, "mean": -2.5620e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1356e+00}, "critic_loss": {"n": 1, "mean": 2.0949e+01}, "entropy_coef": {"n": 1, "mean": 1.4821e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2256e-01, "std": 2.4398e-01, "min_value": 6.2823e-05, "max_value": 9.9956e-01}, "num_gradient_steps": 0, "step_time": 1.1557e+01, "total_time": 1.4841e+03, "__timestamp": "2024-10-10 15:21:24.651699"}, {"step": 156000, "num_env_steps": 156000, "scores": {"n": 1, "mean": 5.4100e+03}, "actor_loss": {"n": 1, "mean": -2.4321e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.5584e-01}, "critic_loss": {"n": 1, "mean": 1.7624e+01}, "entropy_coef": {"n": 1, "mean": 1.4813e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1687e-01, "std": 2.4655e-01, "min_value": 1.0182e-03, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.1490e+01, "total_time": 1.4956e+03, "__timestamp": "2024-10-10 15:21:36.142861"}, {"step": 157000, "num_env_steps": 157000, "scores": {"n": 1, "mean": 5.2705e+03}, "actor_loss": {"n": 1, "mean": -2.4384e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.4763e-01}, "critic_loss": {"n": 1, "mean": 1.9909e+01}, "entropy_coef": {"n": 1, "mean": 1.5225e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3130e-01, "std": 2.3668e-01, "min_value": 1.1826e-03, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.1669e+01, "total_time": 1.5073e+03, "__timestamp": "2024-10-10 15:21:47.810515"}, {"step": 158000, "num_env_steps": 158000, "scores": {"n": 1, "mean": 5.2663e+03}, "actor_loss": {"n": 1, "mean": -2.4167e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.1892e-01}, "critic_loss": {"n": 1, "mean": 1.8513e+01}, "entropy_coef": {"n": 1, "mean": 1.5350e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1994e-01, "std": 2.4887e-01, "min_value": 2.7782e-04, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.1659e+01, "total_time": 1.5189e+03, "__timestamp": "2024-10-10 15:21:59.469136"}, {"step": 159000, "num_env_steps": 159000, "scores": {"n": 1, "mean": 5.0794e+03}, "actor_loss": {"n": 1, "mean": -2.4192e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.4309e-02}, "critic_loss": {"n": 1, "mean": 2.5307e+01}, "entropy_coef": {"n": 1, "mean": 1.5474e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1283e-01, "std": 2.4978e-01, "min_value": 9.5376e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.1771e+01, "total_time": 1.5307e+03, "__timestamp": "2024-10-10 15:22:11.240881"}, {"step": 160000, "num_env_steps": 160000, "scores": {"n": 1, "mean": 5.2280e+03}, "actor_loss": {"n": 1, "mean": -2.3269e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.9236e-01}, "critic_loss": {"n": 1, "mean": 5.3294e+01}, "entropy_coef": {"n": 1, "mean": 1.5688e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2105e-01, "std": 2.4688e-01, "min_value": 1.4184e-03, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 1.1737e+01, "total_time": 1.5424e+03, "__timestamp": "2024-10-10 15:22:22.977445"}, {"step": 161000, "num_env_steps": 161000, "scores": {"n": 1, "mean": 5.2780e+03}, "actor_loss": {"n": 1, "mean": -2.4715e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2048e-03}, "critic_loss": {"n": 1, "mean": 2.6144e+01}, "entropy_coef": {"n": 1, "mean": 1.5707e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2107e-01, "std": 2.4357e-01, "min_value": 2.4670e-04, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.1561e+01, "total_time": 1.5540e+03, "__timestamp": "2024-10-10 15:22:34.538283"}, {"step": 162000, "num_env_steps": 162000, "scores": {"n": 1, "mean": 5.3809e+03}, "actor_loss": {"n": 1, "mean": -2.4248e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.2605e-01}, "critic_loss": {"n": 1, "mean": 2.2204e+01}, "entropy_coef": {"n": 1, "mean": 1.5668e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2766e-01, "std": 2.4224e-01, "min_value": 1.6227e-03, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.1273e+01, "total_time": 1.5653e+03, "__timestamp": "2024-10-10 15:22:45.811363"}, {"step": 163000, "num_env_steps": 163000, "scores": {"n": 1, "mean": 5.4122e+03}, "actor_loss": {"n": 1, "mean": -2.3362e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.4526e-01}, "critic_loss": {"n": 1, "mean": 5.8677e+01}, "entropy_coef": {"n": 1, "mean": 1.6074e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2418e-01, "std": 2.4467e-01, "min_value": 3.9585e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.1162e+01, "total_time": 1.5764e+03, "__timestamp": "2024-10-10 15:22:56.973484"}, {"step": 164000, "num_env_steps": 164000, "scores": {"n": 1, "mean": 5.1881e+03}, "actor_loss": {"n": 1, "mean": -2.4071e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.6111e-01}, "critic_loss": {"n": 1, "mean": 1.7086e+01}, "entropy_coef": {"n": 1, "mean": 1.5833e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1668e-01, "std": 2.4885e-01, "min_value": 1.6123e-04, "max_value": 9.9957e-01}, "num_gradient_steps": 0, "step_time": 1.1318e+01, "total_time": 1.5878e+03, "__timestamp": "2024-10-10 15:23:08.291849"}, {"step": 165000, "num_env_steps": 165000, "scores": {"n": 1, "mean": 5.4250e+03}, "actor_loss": {"n": 1, "mean": -2.4860e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.5647e-02}, "critic_loss": {"n": 1, "mean": 1.6981e+01}, "entropy_coef": {"n": 1, "mean": 1.6068e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2691e-01, "std": 2.4358e-01, "min_value": 2.9303e-04, "max_value": 9.9955e-01}, "num_gradient_steps": 0, "step_time": 1.1564e+01, "total_time": 1.5993e+03, "__timestamp": "2024-10-10 15:23:19.856301"}, {"step": 166000, "num_env_steps": 166000, "scores": {"n": 1, "mean": 5.3246e+03}, "actor_loss": {"n": 1, "mean": -2.4535e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.8617e-01}, "critic_loss": {"n": 1, "mean": 1.9840e+01}, "entropy_coef": {"n": 1, "mean": 1.6125e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2016e-01, "std": 2.4696e-01, "min_value": 3.3802e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1438e+01, "total_time": 1.6108e+03, "__timestamp": "2024-10-10 15:23:31.293774"}, {"step": 167000, "num_env_steps": 167000, "scores": {"n": 1, "mean": 5.4662e+03}, "actor_loss": {"n": 1, "mean": -2.5714e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2059e+00}, "critic_loss": {"n": 1, "mean": 1.5277e+01}, "entropy_coef": {"n": 1, "mean": 1.6037e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1903e-01, "std": 2.4736e-01, "min_value": 3.8876e-04, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.1320e+01, "total_time": 1.6221e+03, "__timestamp": "2024-10-10 15:23:42.613989"}, {"step": 168000, "num_env_steps": 168000, "scores": {"n": 1, "mean": 5.3746e+03}, "actor_loss": {"n": 1, "mean": -2.5292e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.9533e-01}, "critic_loss": {"n": 1, "mean": 2.0138e+01}, "entropy_coef": {"n": 1, "mean": 1.6047e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2039e-01, "std": 2.4691e-01, "min_value": 1.4400e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.1161e+01, "total_time": 1.6332e+03, "__timestamp": "2024-10-10 15:23:53.775899"}, {"step": 169000, "num_env_steps": 169000, "scores": {"n": 1, "mean": 5.3849e+03}, "actor_loss": {"n": 1, "mean": -2.4716e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.4791e-01}, "critic_loss": {"n": 1, "mean": 2.5133e+01}, "entropy_coef": {"n": 1, "mean": 1.6507e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3108e-01, "std": 2.3934e-01, "min_value": 2.6076e-03, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 1.1500e+01, "total_time": 1.6447e+03, "__timestamp": "2024-10-10 15:24:05.275276"}, {"step": 170000, "num_env_steps": 170000, "scores": {"n": 1, "mean": 5.4937e+03}, "actor_loss": {"n": 1, "mean": -2.4824e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.3565e-02}, "critic_loss": {"n": 1, "mean": 1.9795e+01}, "entropy_coef": {"n": 1, "mean": 1.6654e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2952e-01, "std": 2.4079e-01, "min_value": 8.3506e-05, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.1448e+01, "total_time": 1.6562e+03, "__timestamp": "2024-10-10 15:24:16.723962"}, {"step": 171000, "num_env_steps": 171000, "scores": {"n": 1, "mean": 5.3787e+03}, "actor_loss": {"n": 1, "mean": -2.4178e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0768e-01}, "critic_loss": {"n": 1, "mean": 1.6517e+01}, "entropy_coef": {"n": 1, "mean": 1.6728e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1122e-01, "std": 2.5478e-01, "min_value": 1.4886e-04, "max_value": 9.9946e-01}, "num_gradient_steps": 0, "step_time": 1.1357e+01, "total_time": 1.6675e+03, "__timestamp": "2024-10-10 15:24:28.079735"}, {"step": 172000, "num_env_steps": 172000, "scores": {"n": 1, "mean": 5.3473e+03}, "actor_loss": {"n": 1, "mean": -2.5719e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4711e+00}, "critic_loss": {"n": 1, "mean": 1.4456e+03}, "entropy_coef": {"n": 1, "mean": 1.6745e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2205e-01, "std": 2.4285e-01, "min_value": 1.6220e-04, "max_value": 9.9961e-01}, "num_gradient_steps": 0, "step_time": 1.1433e+01, "total_time": 1.6790e+03, "__timestamp": "2024-10-10 15:24:39.513920"}, {"step": 173000, "num_env_steps": 173000, "scores": {"n": 1, "mean": 5.5328e+03}, "actor_loss": {"n": 1, "mean": -2.5043e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.9957e-01}, "critic_loss": {"n": 1, "mean": 1.9523e+01}, "entropy_coef": {"n": 1, "mean": 1.7044e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2444e-01, "std": 2.4532e-01, "min_value": 4.3461e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.1116e+01, "total_time": 1.6901e+03, "__timestamp": "2024-10-10 15:24:50.629350"}, {"step": 174000, "num_env_steps": 174000, "scores": {"n": 1, "mean": 5.6830e+03}, "actor_loss": {"n": 1, "mean": -2.5269e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.3017e-01}, "critic_loss": {"n": 1, "mean": 1.4134e+01}, "entropy_coef": {"n": 1, "mean": 1.7301e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3393e-01, "std": 2.3848e-01, "min_value": 3.0496e-04, "max_value": 9.9936e-01}, "num_gradient_steps": 0, "step_time": 1.3988e+01, "total_time": 1.7041e+03, "__timestamp": "2024-10-10 15:25:04.617134"}, {"step": 175000, "num_env_steps": 175000, "scores": {"n": 1, "mean": 5.6660e+03}, "actor_loss": {"n": 1, "mean": -2.5043e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.1764e-01}, "critic_loss": {"n": 1, "mean": 1.9159e+01}, "entropy_coef": {"n": 1, "mean": 1.6958e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2645e-01, "std": 2.4544e-01, "min_value": 1.4320e-05, "max_value": 9.9956e-01}, "num_gradient_steps": 0, "step_time": 1.1452e+01, "total_time": 1.7155e+03, "__timestamp": "2024-10-10 15:25:16.069317"}, {"step": 176000, "num_env_steps": 176000, "scores": {"n": 1, "mean": 5.4543e+03}, "actor_loss": {"n": 1, "mean": -2.5753e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.6795e-01}, "critic_loss": {"n": 1, "mean": 1.4194e+01}, "entropy_coef": {"n": 1, "mean": 1.7062e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2786e-01, "std": 2.4021e-01, "min_value": 6.9528e-04, "max_value": 9.9936e-01}, "num_gradient_steps": 0, "step_time": 1.1196e+01, "total_time": 1.7267e+03, "__timestamp": "2024-10-10 15:25:27.266174"}, {"step": 177000, "num_env_steps": 177000, "scores": {"n": 1, "mean": 5.5764e+03}, "actor_loss": {"n": 1, "mean": -2.5644e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4528e+00}, "critic_loss": {"n": 1, "mean": 1.8055e+01}, "entropy_coef": {"n": 1, "mean": 1.6848e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2549e-01, "std": 2.4381e-01, "min_value": 1.2858e-04, "max_value": 9.9940e-01}, "num_gradient_steps": 0, "step_time": 1.1051e+01, "total_time": 1.7378e+03, "__timestamp": "2024-10-10 15:25:38.316649"}, {"step": 178000, "num_env_steps": 178000, "scores": {"n": 1, "mean": 5.6010e+03}, "actor_loss": {"n": 1, "mean": -2.5098e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.1455e-01}, "critic_loss": {"n": 1, "mean": 2.0221e+01}, "entropy_coef": {"n": 1, "mean": 1.7156e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3296e-01, "std": 2.3810e-01, "min_value": 2.7279e-03, "max_value": 9.9967e-01}, "num_gradient_steps": 0, "step_time": 1.0887e+01, "total_time": 1.7487e+03, "__timestamp": "2024-10-10 15:25:49.203932"}, {"step": 179000, "num_env_steps": 179000, "scores": {"n": 1, "mean": 5.4812e+03}, "actor_loss": {"n": 1, "mean": -2.4810e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1706e-01}, "critic_loss": {"n": 1, "mean": 2.0426e+01}, "entropy_coef": {"n": 1, "mean": 1.7123e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2146e-01, "std": 2.4698e-01, "min_value": 4.2781e-05, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 1.0786e+01, "total_time": 1.7595e+03, "__timestamp": "2024-10-10 15:25:59.989528"}, {"step": 180000, "num_env_steps": 180000, "scores": {"n": 1, "mean": 5.1330e+03}, "actor_loss": {"n": 1, "mean": -2.5697e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.2006e-01}, "critic_loss": {"n": 1, "mean": 1.9398e+01}, "entropy_coef": {"n": 1, "mean": 1.7197e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1019e-01, "std": 2.4953e-01, "min_value": 1.2315e-03, "max_value": 9.9954e-01}, "num_gradient_steps": 0, "step_time": 1.0786e+01, "total_time": 1.7702e+03, "__timestamp": "2024-10-10 15:26:10.775196"}, {"step": 181000, "num_env_steps": 181000, "scores": {"n": 1, "mean": 5.3403e+03}, "actor_loss": {"n": 1, "mean": -2.5377e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.0897e-01}, "critic_loss": {"n": 1, "mean": 1.7942e+01}, "entropy_coef": {"n": 1, "mean": 1.7723e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3266e-01, "std": 2.3531e-01, "min_value": 2.1232e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0966e+01, "total_time": 1.7812e+03, "__timestamp": "2024-10-10 15:26:21.740923"}, {"step": 182000, "num_env_steps": 182000, "scores": {"n": 1, "mean": 5.4789e+03}, "actor_loss": {"n": 1, "mean": -2.4890e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2089e-01}, "critic_loss": {"n": 1, "mean": 1.8434e+01}, "entropy_coef": {"n": 1, "mean": 1.7641e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2922e-01, "std": 2.3844e-01, "min_value": 1.1190e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0758e+01, "total_time": 1.7920e+03, "__timestamp": "2024-10-10 15:26:32.499043"}, {"step": 183000, "num_env_steps": 183000, "scores": {"n": 1, "mean": 5.6436e+03}, "actor_loss": {"n": 1, "mean": -2.5652e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.7105e-01}, "critic_loss": {"n": 1, "mean": 1.6320e+01}, "entropy_coef": {"n": 1, "mean": 1.7524e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2842e-01, "std": 2.3942e-01, "min_value": 9.3635e-04, "max_value": 9.9939e-01}, "num_gradient_steps": 0, "step_time": 1.0739e+01, "total_time": 1.8027e+03, "__timestamp": "2024-10-10 15:26:43.237754"}, {"step": 184000, "num_env_steps": 184000, "scores": {"n": 1, "mean": 5.5376e+03}, "actor_loss": {"n": 1, "mean": -2.5909e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.7843e-01}, "critic_loss": {"n": 1, "mean": 1.8333e+01}, "entropy_coef": {"n": 1, "mean": 1.7587e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1901e-01, "std": 2.4265e-01, "min_value": 3.7515e-04, "max_value": 9.9957e-01}, "num_gradient_steps": 0, "step_time": 1.0688e+01, "total_time": 1.8134e+03, "__timestamp": "2024-10-10 15:26:53.927220"}, {"step": 185000, "num_env_steps": 185000, "scores": {"n": 1, "mean": 5.6192e+03}, "actor_loss": {"n": 1, "mean": -2.5419e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.3559e-01}, "critic_loss": {"n": 1, "mean": 3.9927e+02}, "entropy_coef": {"n": 1, "mean": 1.7604e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3074e-01, "std": 2.3889e-01, "min_value": 8.5686e-04, "max_value": 9.9943e-01}, "num_gradient_steps": 0, "step_time": 1.0665e+01, "total_time": 1.8241e+03, "__timestamp": "2024-10-10 15:27:04.590979"}, {"step": 186000, "num_env_steps": 186000, "scores": {"n": 1, "mean": 5.7114e+03}, "actor_loss": {"n": 1, "mean": -2.3830e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1535e+00}, "critic_loss": {"n": 1, "mean": 1.8994e+01}, "entropy_coef": {"n": 1, "mean": 1.7805e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3827e-01, "std": 2.3472e-01, "min_value": 4.1395e-05, "max_value": 9.9933e-01}, "num_gradient_steps": 0, "step_time": 1.0632e+01, "total_time": 1.8347e+03, "__timestamp": "2024-10-10 15:27:15.223146"}, {"step": 187000, "num_env_steps": 187000, "scores": {"n": 1, "mean": 5.6733e+03}, "actor_loss": {"n": 1, "mean": -2.5538e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.6123e-02}, "critic_loss": {"n": 1, "mean": 1.7210e+01}, "entropy_coef": {"n": 1, "mean": 1.8005e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3293e-01, "std": 2.3567e-01, "min_value": 8.5069e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.0638e+01, "total_time": 1.8453e+03, "__timestamp": "2024-10-10 15:27:25.860897"}, {"step": 188000, "num_env_steps": 188000, "scores": {"n": 1, "mean": 5.8265e+03}, "actor_loss": {"n": 1, "mean": -2.5424e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.3889e-01}, "critic_loss": {"n": 1, "mean": 2.7189e+02}, "entropy_coef": {"n": 1, "mean": 1.7816e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2574e-01, "std": 2.4638e-01, "min_value": 2.8439e-05, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 1.0657e+01, "total_time": 1.8560e+03, "__timestamp": "2024-10-10 15:27:36.518265"}, {"step": 189000, "num_env_steps": 189000, "scores": {"n": 1, "mean": 5.6615e+03}, "actor_loss": {"n": 1, "mean": -2.4903e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.4622e-01}, "critic_loss": {"n": 1, "mean": 1.4437e+01}, "entropy_coef": {"n": 1, "mean": 1.8023e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3132e-01, "std": 2.3843e-01, "min_value": 2.7418e-05, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0602e+01, "total_time": 1.8666e+03, "__timestamp": "2024-10-10 15:27:47.120970"}, {"step": 190000, "num_env_steps": 190000, "scores": {"n": 1, "mean": 5.4434e+03}, "actor_loss": {"n": 1, "mean": -2.5758e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.0255e-01}, "critic_loss": {"n": 1, "mean": 1.6954e+01}, "entropy_coef": {"n": 1, "mean": 1.7938e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2385e-01, "std": 2.4261e-01, "min_value": 7.3843e-04, "max_value": 9.9955e-01}, "num_gradient_steps": 0, "step_time": 1.0688e+01, "total_time": 1.8773e+03, "__timestamp": "2024-10-10 15:27:57.809331"}, {"step": 191000, "num_env_steps": 191000, "scores": {"n": 1, "mean": 5.3742e+03}, "actor_loss": {"n": 1, "mean": -2.5199e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.7351e-01}, "critic_loss": {"n": 1, "mean": 1.1858e+01}, "entropy_coef": {"n": 1, "mean": 1.8090e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1940e-01, "std": 2.4609e-01, "min_value": 1.0765e-03, "max_value": 9.9957e-01}, "num_gradient_steps": 0, "step_time": 1.0618e+01, "total_time": 1.8879e+03, "__timestamp": "2024-10-10 15:28:08.426322"}, {"step": 192000, "num_env_steps": 192000, "scores": {"n": 1, "mean": 5.6638e+03}, "actor_loss": {"n": 1, "mean": -2.6292e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2762e-01}, "critic_loss": {"n": 1, "mean": 2.0179e+01}, "entropy_coef": {"n": 1, "mean": 1.8603e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3145e-01, "std": 2.3833e-01, "min_value": 8.9735e-05, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.0715e+01, "total_time": 1.8986e+03, "__timestamp": "2024-10-10 15:28:19.141521"}, {"step": 193000, "num_env_steps": 193000, "scores": {"n": 1, "mean": 5.2219e+03}, "actor_loss": {"n": 1, "mean": -2.5524e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.7127e-02}, "critic_loss": {"n": 1, "mean": 1.7544e+01}, "entropy_coef": {"n": 1, "mean": 1.8696e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1894e-01, "std": 2.4191e-01, "min_value": 1.0146e-03, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.0647e+01, "total_time": 1.9093e+03, "__timestamp": "2024-10-10 15:28:29.788953"}, {"step": 194000, "num_env_steps": 194000, "scores": {"n": 1, "mean": 5.6328e+03}, "actor_loss": {"n": 1, "mean": -2.4801e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.2351e-01}, "critic_loss": {"n": 1, "mean": 2.9846e+01}, "entropy_coef": {"n": 1, "mean": 1.9027e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3562e-01, "std": 2.3088e-01, "min_value": 1.3655e-04, "max_value": 9.9961e-01}, "num_gradient_steps": 0, "step_time": 1.0677e+01, "total_time": 1.9199e+03, "__timestamp": "2024-10-10 15:28:40.465877"}, {"step": 195000, "num_env_steps": 195000, "scores": {"n": 1, "mean": 5.6327e+03}, "actor_loss": {"n": 1, "mean": -2.6514e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.4725e-01}, "critic_loss": {"n": 1, "mean": 1.9794e+01}, "entropy_coef": {"n": 1, "mean": 1.8637e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3518e-01, "std": 2.3336e-01, "min_value": 8.8282e-05, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 1.0728e+01, "total_time": 1.9307e+03, "__timestamp": "2024-10-10 15:28:51.194678"}, {"step": 196000, "num_env_steps": 196000, "scores": {"n": 1, "mean": 5.6836e+03}, "actor_loss": {"n": 1, "mean": -2.5201e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4141e+00}, "critic_loss": {"n": 1, "mean": 1.9417e+01}, "entropy_coef": {"n": 1, "mean": 1.9094e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3270e-01, "std": 2.3281e-01, "min_value": 6.5650e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.0596e+01, "total_time": 1.9413e+03, "__timestamp": "2024-10-10 15:29:01.789867"}, {"step": 197000, "num_env_steps": 197000, "scores": {"n": 1, "mean": 5.7344e+03}, "actor_loss": {"n": 1, "mean": -2.6349e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.5475e-01}, "critic_loss": {"n": 1, "mean": 1.5803e+01}, "entropy_coef": {"n": 1, "mean": 1.8906e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3587e-01, "std": 2.2983e-01, "min_value": 1.0611e-03, "max_value": 9.9943e-01}, "num_gradient_steps": 0, "step_time": 1.0622e+01, "total_time": 1.9519e+03, "__timestamp": "2024-10-10 15:29:12.412188"}, {"step": 198000, "num_env_steps": 198000, "scores": {"n": 1, "mean": 5.5329e+03}, "actor_loss": {"n": 1, "mean": -2.6346e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.9898e-01}, "critic_loss": {"n": 1, "mean": 1.4834e+01}, "entropy_coef": {"n": 1, "mean": 1.9014e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2653e-01, "std": 2.3590e-01, "min_value": 1.2434e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0711e+01, "total_time": 1.9626e+03, "__timestamp": "2024-10-10 15:29:23.124429"}, {"step": 199000, "num_env_steps": 199000, "scores": {"n": 1, "mean": 5.6675e+03}, "actor_loss": {"n": 1, "mean": -2.5289e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1938e-01}, "critic_loss": {"n": 1, "mean": 3.4044e+01}, "entropy_coef": {"n": 1, "mean": 1.9585e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2965e-01, "std": 2.3584e-01, "min_value": 3.2681e-04, "max_value": 9.9962e-01}, "num_gradient_steps": 0, "step_time": 1.0780e+01, "total_time": 1.9734e+03, "__timestamp": "2024-10-10 15:29:33.903436"}, {"step": 200000, "num_env_steps": 200000, "scores": {"n": 1, "mean": 5.7322e+03}, "actor_loss": {"n": 1, "mean": -2.5657e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.2393e-01}, "critic_loss": {"n": 1, "mean": 1.6459e+01}, "entropy_coef": {"n": 1, "mean": 1.9303e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3583e-01, "std": 2.3012e-01, "min_value": 1.1174e-03, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0702e+01, "total_time": 1.9841e+03, "__timestamp": "2024-10-10 15:29:44.605600"}, {"step": 201000, "num_env_steps": 201000, "scores": {"n": 1, "mean": 5.6339e+03}, "actor_loss": {"n": 1, "mean": -2.6037e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.7562e-01}, "critic_loss": {"n": 1, "mean": 1.6970e+01}, "entropy_coef": {"n": 1, "mean": 1.9272e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3374e-01, "std": 2.3120e-01, "min_value": 3.7250e-04, "max_value": 9.9953e-01}, "num_gradient_steps": 0, "step_time": 1.0633e+01, "total_time": 1.9947e+03, "__timestamp": "2024-10-10 15:29:55.238955"}, {"step": 202000, "num_env_steps": 202000, "scores": {"n": 1, "mean": 5.6226e+03}, "actor_loss": {"n": 1, "mean": -2.5734e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.9702e-01}, "critic_loss": {"n": 1, "mean": 4.1660e+02}, "entropy_coef": {"n": 1, "mean": 1.9312e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2751e-01, "std": 2.3741e-01, "min_value": 2.4136e-03, "max_value": 9.9926e-01}, "num_gradient_steps": 0, "step_time": 1.0662e+01, "total_time": 2.0054e+03, "__timestamp": "2024-10-10 15:30:05.900949"}, {"step": 203000, "num_env_steps": 203000, "scores": {"n": 1, "mean": 5.6883e+03}, "actor_loss": {"n": 1, "mean": -2.6917e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0995e-01}, "critic_loss": {"n": 1, "mean": 2.2084e+01}, "entropy_coef": {"n": 1, "mean": 1.9401e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2186e-01, "std": 2.4655e-01, "min_value": 3.6900e-03, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0628e+01, "total_time": 2.0160e+03, "__timestamp": "2024-10-10 15:30:16.530341"}, {"step": 204000, "num_env_steps": 204000, "scores": {"n": 1, "mean": 5.7901e+03}, "actor_loss": {"n": 1, "mean": -2.6213e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.5063e-01}, "critic_loss": {"n": 1, "mean": 3.0650e+01}, "entropy_coef": {"n": 1, "mean": 1.9667e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2529e-01, "std": 2.4035e-01, "min_value": 1.0116e-03, "max_value": 9.9939e-01}, "num_gradient_steps": 0, "step_time": 1.0804e+01, "total_time": 2.0268e+03, "__timestamp": "2024-10-10 15:30:27.333406"}, {"step": 205000, "num_env_steps": 205000, "scores": {"n": 1, "mean": 5.7596e+03}, "actor_loss": {"n": 1, "mean": -2.5548e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.1195e-02}, "critic_loss": {"n": 1, "mean": 4.5185e+02}, "entropy_coef": {"n": 1, "mean": 1.9429e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2462e-01, "std": 2.4058e-01, "min_value": 2.1362e-03, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.0604e+01, "total_time": 2.0374e+03, "__timestamp": "2024-10-10 15:30:37.938850"}, {"step": 206000, "num_env_steps": 206000, "scores": {"n": 1, "mean": 5.5981e+03}, "actor_loss": {"n": 1, "mean": -2.6581e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.9090e-01}, "critic_loss": {"n": 1, "mean": 2.1058e+01}, "entropy_coef": {"n": 1, "mean": 1.9497e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1973e-01, "std": 2.4109e-01, "min_value": 1.3062e-03, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.0659e+01, "total_time": 2.0481e+03, "__timestamp": "2024-10-10 15:30:48.596912"}, {"step": 207000, "num_env_steps": 207000, "scores": {"n": 1, "mean": 5.6498e+03}, "actor_loss": {"n": 1, "mean": -2.5385e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.3970e-01}, "critic_loss": {"n": 1, "mean": 1.6190e+01}, "entropy_coef": {"n": 1, "mean": 1.9440e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2488e-01, "std": 2.4275e-01, "min_value": 1.0534e-03, "max_value": 9.9951e-01}, "num_gradient_steps": 0, "step_time": 1.0707e+01, "total_time": 2.0588e+03, "__timestamp": "2024-10-10 15:30:59.304274"}, {"step": 208000, "num_env_steps": 208000, "scores": {"n": 1, "mean": 5.8654e+03}, "actor_loss": {"n": 1, "mean": -2.4842e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.4597e-01}, "critic_loss": {"n": 1, "mean": 1.1916e+01}, "entropy_coef": {"n": 1, "mean": 1.9517e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2759e-01, "std": 2.3834e-01, "min_value": 1.0414e-03, "max_value": 9.9930e-01}, "num_gradient_steps": 0, "step_time": 1.0623e+01, "total_time": 2.0694e+03, "__timestamp": "2024-10-10 15:31:09.927144"}, {"step": 209000, "num_env_steps": 209000, "scores": {"n": 1, "mean": 5.7434e+03}, "actor_loss": {"n": 1, "mean": -2.6044e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.4884e-01}, "critic_loss": {"n": 1, "mean": 1.4136e+01}, "entropy_coef": {"n": 1, "mean": 1.9852e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2059e-01, "std": 2.4539e-01, "min_value": 2.7221e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.0579e+01, "total_time": 2.0800e+03, "__timestamp": "2024-10-10 15:31:20.507033"}, {"step": 210000, "num_env_steps": 210000, "scores": {"n": 1, "mean": 5.8022e+03}, "actor_loss": {"n": 1, "mean": -2.5526e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.5363e-01}, "critic_loss": {"n": 1, "mean": 4.2101e+02}, "entropy_coef": {"n": 1, "mean": 1.9689e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1982e-01, "std": 2.4460e-01, "min_value": 1.3502e-04, "max_value": 9.9948e-01}, "num_gradient_steps": 0, "step_time": 1.0702e+01, "total_time": 2.0907e+03, "__timestamp": "2024-10-10 15:31:31.208555"}, {"step": 211000, "num_env_steps": 211000, "scores": {"n": 1, "mean": 5.9293e+03}, "actor_loss": {"n": 1, "mean": -2.6401e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.1018e-03}, "critic_loss": {"n": 1, "mean": 1.5278e+01}, "entropy_coef": {"n": 1, "mean": 1.9910e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3260e-01, "std": 2.3302e-01, "min_value": 3.5394e-03, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.0606e+01, "total_time": 2.1013e+03, "__timestamp": "2024-10-10 15:31:41.813509"}, {"step": 212000, "num_env_steps": 212000, "scores": {"n": 1, "mean": 5.8460e+03}, "actor_loss": {"n": 1, "mean": -2.5903e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.7131e-01}, "critic_loss": {"n": 1, "mean": 1.1853e+01}, "entropy_coef": {"n": 1, "mean": 2.0016e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2669e-01, "std": 2.3689e-01, "min_value": 2.6291e-03, "max_value": 9.9953e-01}, "num_gradient_steps": 0, "step_time": 1.0629e+01, "total_time": 2.1119e+03, "__timestamp": "2024-10-10 15:31:52.443855"}, {"step": 213000, "num_env_steps": 213000, "scores": {"n": 1, "mean": 5.7733e+03}, "actor_loss": {"n": 1, "mean": -2.6066e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2032e-01}, "critic_loss": {"n": 1, "mean": 1.9406e+01}, "entropy_coef": {"n": 1, "mean": 2.0107e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2498e-01, "std": 2.3920e-01, "min_value": 1.9435e-03, "max_value": 9.9924e-01}, "num_gradient_steps": 0, "step_time": 1.0693e+01, "total_time": 2.1226e+03, "__timestamp": "2024-10-10 15:32:03.135738"}, {"step": 214000, "num_env_steps": 214000, "scores": {"n": 1, "mean": 5.8480e+03}, "actor_loss": {"n": 1, "mean": -2.6734e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.8248e-01}, "critic_loss": {"n": 1, "mean": 2.3967e+01}, "entropy_coef": {"n": 1, "mean": 2.0058e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2070e-01, "std": 2.4724e-01, "min_value": 4.2564e-04, "max_value": 9.9957e-01}, "num_gradient_steps": 0, "step_time": 1.0662e+01, "total_time": 2.1333e+03, "__timestamp": "2024-10-10 15:32:13.797951"}, {"step": 215000, "num_env_steps": 215000, "scores": {"n": 1, "mean": 5.8612e+03}, "actor_loss": {"n": 1, "mean": -2.7291e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.2544e-01}, "critic_loss": {"n": 1, "mean": 1.4430e+01}, "entropy_coef": {"n": 1, "mean": 2.0071e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1870e-01, "std": 2.4518e-01, "min_value": 2.5836e-04, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 1.0669e+01, "total_time": 2.1439e+03, "__timestamp": "2024-10-10 15:32:24.467573"}, {"step": 216000, "num_env_steps": 216000, "scores": {"n": 1, "mean": 5.8664e+03}, "actor_loss": {"n": 1, "mean": -2.6536e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.4356e-01}, "critic_loss": {"n": 1, "mean": 1.4756e+01}, "entropy_coef": {"n": 1, "mean": 1.9891e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2517e-01, "std": 2.3919e-01, "min_value": 6.8331e-04, "max_value": 9.9945e-01}, "num_gradient_steps": 0, "step_time": 1.0666e+01, "total_time": 2.1546e+03, "__timestamp": "2024-10-10 15:32:35.133228"}, {"step": 217000, "num_env_steps": 217000, "scores": {"n": 1, "mean": 5.9083e+03}, "actor_loss": {"n": 1, "mean": -2.7090e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.3439e-01}, "critic_loss": {"n": 1, "mean": 1.3621e+01}, "entropy_coef": {"n": 1, "mean": 2.0081e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2142e-01, "std": 2.4026e-01, "min_value": 1.2818e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.0694e+01, "total_time": 2.1653e+03, "__timestamp": "2024-10-10 15:32:45.827603"}, {"step": 218000, "num_env_steps": 218000, "scores": {"n": 1, "mean": 5.9200e+03}, "actor_loss": {"n": 1, "mean": -2.6592e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.4555e-01}, "critic_loss": {"n": 1, "mean": 1.9947e+01}, "entropy_coef": {"n": 1, "mean": 2.0611e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2244e-01, "std": 2.4221e-01, "min_value": 4.8184e-04, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0646e+01, "total_time": 2.1759e+03, "__timestamp": "2024-10-10 15:32:56.472981"}, {"step": 219000, "num_env_steps": 219000, "scores": {"n": 1, "mean": 5.8928e+03}, "actor_loss": {"n": 1, "mean": -2.6911e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.4410e-01}, "critic_loss": {"n": 1, "mean": 2.0563e+01}, "entropy_coef": {"n": 1, "mean": 2.0237e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2479e-01, "std": 2.3950e-01, "min_value": 3.6851e-05, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.0730e+01, "total_time": 2.1867e+03, "__timestamp": "2024-10-10 15:33:07.203627"}, {"step": 220000, "num_env_steps": 220000, "scores": {"n": 1, "mean": 5.8609e+03}, "actor_loss": {"n": 1, "mean": -2.6511e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.1757e-01}, "critic_loss": {"n": 1, "mean": 2.2915e+01}, "entropy_coef": {"n": 1, "mean": 2.0157e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2148e-01, "std": 2.4256e-01, "min_value": 1.4782e-05, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.0757e+01, "total_time": 2.1974e+03, "__timestamp": "2024-10-10 15:33:17.960465"}, {"step": 221000, "num_env_steps": 221000, "scores": {"n": 1, "mean": 6.1598e+03}, "actor_loss": {"n": 1, "mean": -2.7162e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.8071e-01}, "critic_loss": {"n": 1, "mean": 1.5302e+01}, "entropy_coef": {"n": 1, "mean": 2.0434e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2709e-01, "std": 2.4238e-01, "min_value": 1.1012e-04, "max_value": 9.9912e-01}, "num_gradient_steps": 0, "step_time": 1.0689e+01, "total_time": 2.2081e+03, "__timestamp": "2024-10-10 15:33:28.648659"}, {"step": 222000, "num_env_steps": 222000, "scores": {"n": 1, "mean": 6.0646e+03}, "actor_loss": {"n": 1, "mean": -2.6713e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.5201e-01}, "critic_loss": {"n": 1, "mean": 1.8333e+01}, "entropy_coef": {"n": 1, "mean": 2.0252e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3186e-01, "std": 2.3669e-01, "min_value": 1.5222e-03, "max_value": 9.9965e-01}, "num_gradient_steps": 0, "step_time": 1.0721e+01, "total_time": 2.2188e+03, "__timestamp": "2024-10-10 15:33:39.369331"}, {"step": 223000, "num_env_steps": 223000, "scores": {"n": 1, "mean": 5.8191e+03}, "actor_loss": {"n": 1, "mean": -2.6950e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.3844e-01}, "critic_loss": {"n": 1, "mean": 1.8198e+01}, "entropy_coef": {"n": 1, "mean": 2.0396e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1832e-01, "std": 2.4268e-01, "min_value": 2.1504e-03, "max_value": 9.9938e-01}, "num_gradient_steps": 0, "step_time": 1.0698e+01, "total_time": 2.2295e+03, "__timestamp": "2024-10-10 15:33:50.067726"}, {"step": 224000, "num_env_steps": 224000, "scores": {"n": 1, "mean": 5.8960e+03}, "actor_loss": {"n": 1, "mean": -2.5794e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.8186e-01}, "critic_loss": {"n": 1, "mean": 1.6519e+01}, "entropy_coef": {"n": 1, "mean": 2.0235e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1536e-01, "std": 2.4611e-01, "min_value": 4.3154e-05, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0664e+01, "total_time": 2.2402e+03, "__timestamp": "2024-10-10 15:34:00.731570"}, {"step": 225000, "num_env_steps": 225000, "scores": {"n": 1, "mean": 5.8726e+03}, "actor_loss": {"n": 1, "mean": -2.6251e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.8624e-01}, "critic_loss": {"n": 1, "mean": 1.3476e+01}, "entropy_coef": {"n": 1, "mean": 2.0324e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2501e-01, "std": 2.3646e-01, "min_value": 2.7373e-05, "max_value": 9.9949e-01}, "num_gradient_steps": 0, "step_time": 1.1019e+01, "total_time": 2.2512e+03, "__timestamp": "2024-10-10 15:34:11.751933"}, {"step": 226000, "num_env_steps": 226000, "scores": {"n": 1, "mean": 6.1725e+03}, "actor_loss": {"n": 1, "mean": -2.6449e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.0911e-01}, "critic_loss": {"n": 1, "mean": 4.4053e+02}, "entropy_coef": {"n": 1, "mean": 2.0679e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2080e-01, "std": 2.4473e-01, "min_value": 2.9931e-03, "max_value": 9.9961e-01}, "num_gradient_steps": 0, "step_time": 1.2147e+01, "total_time": 2.2634e+03, "__timestamp": "2024-10-10 15:34:23.899294"}, {"step": 227000, "num_env_steps": 227000, "scores": {"n": 1, "mean": 6.0174e+03}, "actor_loss": {"n": 1, "mean": -2.6281e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4916e-01}, "critic_loss": {"n": 1, "mean": 1.4076e+01}, "entropy_coef": {"n": 1, "mean": 2.1217e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2119e-01, "std": 2.4422e-01, "min_value": 2.4826e-04, "max_value": 9.9935e-01}, "num_gradient_steps": 0, "step_time": 1.1819e+01, "total_time": 2.2752e+03, "__timestamp": "2024-10-10 15:34:35.718023"}, {"step": 228000, "num_env_steps": 228000, "scores": {"n": 1, "mean": 5.7842e+03}, "actor_loss": {"n": 1, "mean": -2.6667e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.5680e-01}, "critic_loss": {"n": 1, "mean": 1.9014e+01}, "entropy_coef": {"n": 1, "mean": 2.1259e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2178e-01, "std": 2.4168e-01, "min_value": 1.0015e-04, "max_value": 9.9964e-01}, "num_gradient_steps": 0, "step_time": 1.1707e+01, "total_time": 2.2869e+03, "__timestamp": "2024-10-10 15:34:47.425190"}, {"step": 229000, "num_env_steps": 229000, "scores": {"n": 1, "mean": 6.1724e+03}, "actor_loss": {"n": 1, "mean": -2.6500e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4966e+00}, "critic_loss": {"n": 1, "mean": 1.9510e+01}, "entropy_coef": {"n": 1, "mean": 2.1802e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2039e-01, "std": 2.4720e-01, "min_value": 3.8335e-04, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.1224e+01, "total_time": 2.2981e+03, "__timestamp": "2024-10-10 15:34:58.649525"}, {"step": 230000, "num_env_steps": 230000, "scores": {"n": 1, "mean": 5.8860e+03}, "actor_loss": {"n": 1, "mean": -2.7527e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5573e-01}, "critic_loss": {"n": 1, "mean": 1.4920e+01}, "entropy_coef": {"n": 1, "mean": 2.1415e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2491e-01, "std": 2.3768e-01, "min_value": 3.2395e-05, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 1.1384e+01, "total_time": 2.3095e+03, "__timestamp": "2024-10-10 15:35:10.033908"}, {"step": 231000, "num_env_steps": 231000, "scores": {"n": 1, "mean": 5.9350e+03}, "actor_loss": {"n": 1, "mean": -2.6496e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.6715e-01}, "critic_loss": {"n": 1, "mean": 1.4637e+01}, "entropy_coef": {"n": 1, "mean": 2.1788e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2422e-01, "std": 2.3944e-01, "min_value": 1.0385e-03, "max_value": 9.9935e-01}, "num_gradient_steps": 0, "step_time": 1.2262e+01, "total_time": 2.3218e+03, "__timestamp": "2024-10-10 15:35:22.295512"}, {"step": 232000, "num_env_steps": 232000, "scores": {"n": 1, "mean": 6.1529e+03}, "actor_loss": {"n": 1, "mean": -2.6270e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.0161e-01}, "critic_loss": {"n": 1, "mean": 2.0472e+01}, "entropy_coef": {"n": 1, "mean": 2.1691e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2530e-01, "std": 2.3925e-01, "min_value": 2.0902e-04, "max_value": 9.9975e-01}, "num_gradient_steps": 0, "step_time": 1.1773e+01, "total_time": 2.3335e+03, "__timestamp": "2024-10-10 15:35:34.068660"}, {"step": 233000, "num_env_steps": 233000, "scores": {"n": 1, "mean": 5.8622e+03}, "actor_loss": {"n": 1, "mean": -2.7131e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.3027e-01}, "critic_loss": {"n": 1, "mean": 1.6732e+01}, "entropy_coef": {"n": 1, "mean": 2.1663e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2485e-01, "std": 2.4061e-01, "min_value": 1.6821e-03, "max_value": 9.9944e-01}, "num_gradient_steps": 0, "step_time": 1.1875e+01, "total_time": 2.3454e+03, "__timestamp": "2024-10-10 15:35:45.943985"}, {"step": 234000, "num_env_steps": 234000, "scores": {"n": 1, "mean": 5.8838e+03}, "actor_loss": {"n": 1, "mean": -2.7133e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.0647e-01}, "critic_loss": {"n": 1, "mean": 4.3741e+02}, "entropy_coef": {"n": 1, "mean": 2.1912e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1985e-01, "std": 2.4288e-01, "min_value": 1.1046e-03, "max_value": 9.9965e-01}, "num_gradient_steps": 0, "step_time": 1.1867e+01, "total_time": 2.3573e+03, "__timestamp": "2024-10-10 15:35:57.811407"}, {"step": 235000, "num_env_steps": 235000, "scores": {"n": 1, "mean": 6.0331e+03}, "actor_loss": {"n": 1, "mean": -2.6856e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1836e+00}, "critic_loss": {"n": 1, "mean": 1.2328e+01}, "entropy_coef": {"n": 1, "mean": 2.1556e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2871e-01, "std": 2.3376e-01, "min_value": 5.6200e-04, "max_value": 9.9956e-01}, "num_gradient_steps": 0, "step_time": 1.1455e+01, "total_time": 2.3687e+03, "__timestamp": "2024-10-10 15:36:09.266428"}, {"step": 236000, "num_env_steps": 236000, "scores": {"n": 1, "mean": 6.1009e+03}, "actor_loss": {"n": 1, "mean": -2.7187e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0053e-01}, "critic_loss": {"n": 1, "mean": 1.9317e+01}, "entropy_coef": {"n": 1, "mean": 2.1973e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2461e-01, "std": 2.3780e-01, "min_value": 1.0314e-03, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.2744e+01, "total_time": 2.3815e+03, "__timestamp": "2024-10-10 15:36:22.008935"}, {"step": 237000, "num_env_steps": 237000, "scores": {"n": 1, "mean": 5.9153e+03}, "actor_loss": {"n": 1, "mean": -2.7015e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1543e-01}, "critic_loss": {"n": 1, "mean": 1.4411e+01}, "entropy_coef": {"n": 1, "mean": 2.1509e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2193e-01, "std": 2.4050e-01, "min_value": 3.3933e-04, "max_value": 9.9964e-01}, "num_gradient_steps": 0, "step_time": 1.1772e+01, "total_time": 2.3932e+03, "__timestamp": "2024-10-10 15:36:33.781819"}, {"step": 238000, "num_env_steps": 238000, "scores": {"n": 1, "mean": 5.8539e+03}, "actor_loss": {"n": 1, "mean": -2.5977e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1341e+00}, "critic_loss": {"n": 1, "mean": 1.4592e+01}, "entropy_coef": {"n": 1, "mean": 2.2041e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2236e-01, "std": 2.3606e-01, "min_value": 8.8096e-05, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.1594e+01, "total_time": 2.4048e+03, "__timestamp": "2024-10-10 15:36:45.375291"}, {"step": 239000, "num_env_steps": 239000, "scores": {"n": 1, "mean": 6.1718e+03}, "actor_loss": {"n": 1, "mean": -2.7325e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.8983e-01}, "critic_loss": {"n": 1, "mean": 1.3801e+01}, "entropy_coef": {"n": 1, "mean": 2.1467e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2275e-01, "std": 2.3807e-01, "min_value": 1.3357e-03, "max_value": 9.9955e-01}, "num_gradient_steps": 0, "step_time": 1.2089e+01, "total_time": 2.4169e+03, "__timestamp": "2024-10-10 15:36:57.465549"}, {"step": 240000, "num_env_steps": 240000, "scores": {"n": 1, "mean": 5.9678e+03}, "actor_loss": {"n": 1, "mean": -2.7605e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0467e+00}, "critic_loss": {"n": 1, "mean": 1.2174e+01}, "entropy_coef": {"n": 1, "mean": 2.1534e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2124e-01, "std": 2.3799e-01, "min_value": 9.9003e-05, "max_value": 9.9976e-01}, "num_gradient_steps": 0, "step_time": 1.0762e+01, "total_time": 2.4277e+03, "__timestamp": "2024-10-10 15:37:08.227542"}]}}