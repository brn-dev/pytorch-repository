{"experiment_id": "2024-10-10_18-03-09_473267~SeHwzc", "experiment_tags": ["SACDebug", "HalfCheetah-v4", "Debug"], "start_time": "2024-10-10 18:03:09.473267", "end_time": "2024-10-10 18:34:47.201310", "end_exception": null, "hyper_parameters": {"_type": "SACDebug", "_type_fq": "__main__.SACDebug", "env": "<SingletonVectorEnv instance>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {}, "policy_parameter_count": 362256, "policy_repr": "DebugSACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 9 3900X 12-Core Processor", "cores": 12, "logical_cores": 24, "speed": "3793 MHz"}, "gpu": [{"name": "NVIDIA GeForce RTX 3070", "video_processor": "NVIDIA GeForce RTX 3070", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA", "memory": "8192 MB", "memory_clock": "405 MHz", "compute_capability": "8.6"}], "ram_speed": "3600 MHz", "ram": "64 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "from sac import init_policy, init_action_selector\nfrom stable_baselines3.common.env_util import make_vec_env\nimport stable_baselines3 as sb\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nimport gymnasium\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n\ndef create_env(render_mode: str | None):\n    return gymnasium.make(env_name, render_mode=render_mode, **env_kwargs)\n\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nsb_sac = sb.SAC(\"MlpPolicy\", env, verbose=10, learning_starts=10000, stats_window_size=1) # , seed=594371)\n\nimport inspect\nimport time\n\nfrom gymnasium import Env\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.module_analysis import count_parameters\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import PolicyConstruction\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom typing import Any\nfrom src.reinforcement_learning.core.callback import Callback\n\nimport torch\nfrom torch import optim\nimport gymnasium as gym\nimport numpy as np\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -4.3450e+02}, "actor_loss": {"n": 1, "mean": -1.7600e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.9931e+00}, "critic_loss": {"n": 1, "mean": 1.0738e+00}, "entropy_coef": {"n": 1, "mean": 7.4084e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.2420e-01, "std": 2.8691e-01, "min_value": 4.2450e-05, "max_value": 9.9886e-01}, "num_gradient_steps": 0, "step_time": 1.4674e+01, "total_time": 1.4665e+01, "__timestamp": "2024-10-10 18:03:24.139154"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -2.2739e+02}, "actor_loss": {"n": 1, "mean": -2.4914e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9720e+00}, "critic_loss": {"n": 1, "mean": 1.3159e+00}, "entropy_coef": {"n": 1, "mean": 5.4941e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3709e-01, "std": 2.8907e-01, "min_value": 2.3305e-05, "max_value": 9.9821e-01}, "num_gradient_steps": 0, "step_time": 9.2070e+00, "total_time": 2.3872e+01, "__timestamp": "2024-10-10 18:03:33.345203"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -2.3229e+02}, "actor_loss": {"n": 1, "mean": -2.9595e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.8752e+00}, "critic_loss": {"n": 1, "mean": 4.0518e+00}, "entropy_coef": {"n": 1, "mean": 4.0736e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3781e-01, "std": 2.8826e-01, "min_value": 2.3623e-04, "max_value": 9.9847e-01}, "num_gradient_steps": 0, "step_time": 9.4520e+00, "total_time": 3.3324e+01, "__timestamp": "2024-10-10 18:03:42.798213"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -1.9556e+02}, "actor_loss": {"n": 1, "mean": -3.2017e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1470e+01}, "critic_loss": {"n": 1, "mean": 1.9499e+00}, "entropy_coef": {"n": 1, "mean": 3.0280e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3944e-01, "std": 2.8946e-01, "min_value": 1.2895e-04, "max_value": 9.9852e-01}, "num_gradient_steps": 0, "step_time": 9.7431e+00, "total_time": 4.3067e+01, "__timestamp": "2024-10-10 18:03:52.540360"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -2.4394e+02}, "actor_loss": {"n": 1, "mean": -3.2992e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3419e+01}, "critic_loss": {"n": 1, "mean": 1.9186e+00}, "entropy_coef": {"n": 1, "mean": 2.2642e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4283e-01, "std": 2.9052e-01, "min_value": 5.1230e-05, "max_value": 9.9891e-01}, "num_gradient_steps": 0, "step_time": 9.2320e+00, "total_time": 5.2299e+01, "__timestamp": "2024-10-10 18:04:01.773384"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -2.2662e+02}, "actor_loss": {"n": 1, "mean": -3.3408e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4908e+01}, "critic_loss": {"n": 1, "mean": 2.0778e+00}, "entropy_coef": {"n": 1, "mean": 1.7069e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4811e-01, "std": 2.8829e-01, "min_value": 7.6890e-06, "max_value": 9.9827e-01}, "num_gradient_steps": 0, "step_time": 9.2220e+00, "total_time": 6.1521e+01, "__timestamp": "2024-10-10 18:04:10.994394"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -2.0504e+02}, "actor_loss": {"n": 1, "mean": -3.3017e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5650e+01}, "critic_loss": {"n": 1, "mean": 4.9232e+00}, "entropy_coef": {"n": 1, "mean": 1.2966e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.6805e-01, "std": 2.9244e-01, "min_value": 1.6809e-05, "max_value": 9.9906e-01}, "num_gradient_steps": 0, "step_time": 9.3439e+00, "total_time": 7.0865e+01, "__timestamp": "2024-10-10 18:04:20.338343"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -1.6159e+02}, "actor_loss": {"n": 1, "mean": -3.3193e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4293e+01}, "critic_loss": {"n": 1, "mean": 4.2708e+00}, "entropy_coef": {"n": 1, "mean": 9.9033e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.6856e-01, "std": 2.9433e-01, "min_value": 4.5785e-04, "max_value": 9.9965e-01}, "num_gradient_steps": 0, "step_time": 1.0218e+01, "total_time": 8.1083e+01, "__timestamp": "2024-10-10 18:04:30.557443"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -1.8584e+02}, "actor_loss": {"n": 1, "mean": -3.2041e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5951e+01}, "critic_loss": {"n": 1, "mean": 4.9304e+00}, "entropy_coef": {"n": 1, "mean": 7.5827e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8857e-01, "std": 2.9349e-01, "min_value": 1.9838e-04, "max_value": 9.9967e-01}, "num_gradient_steps": 0, "step_time": 9.5737e+00, "total_time": 9.0657e+01, "__timestamp": "2024-10-10 18:04:40.130149"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -1.9915e+02}, "actor_loss": {"n": 1, "mean": -3.1217e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4548e+01}, "critic_loss": {"n": 1, "mean": 1.5068e+00}, "entropy_coef": {"n": 1, "mean": 5.8159e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9273e-01, "std": 2.9853e-01, "min_value": 6.7973e-04, "max_value": 9.9946e-01}, "num_gradient_steps": 0, "step_time": 9.7758e+00, "total_time": 1.0043e+02, "__timestamp": "2024-10-10 18:04:49.905990"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -1.5689e+02}, "actor_loss": {"n": 1, "mean": -2.9092e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6078e+01}, "critic_loss": {"n": 1, "mean": 1.3172e+00}, "entropy_coef": {"n": 1, "mean": 4.4785e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0181e-01, "std": 2.9602e-01, "min_value": 1.0461e-05, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.1029e+01, "total_time": 1.1146e+02, "__timestamp": "2024-10-10 18:05:00.935878"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -2.2091e+02}, "actor_loss": {"n": 1, "mean": -2.9296e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3872e+01}, "critic_loss": {"n": 1, "mean": 1.5081e+00}, "entropy_coef": {"n": 1, "mean": 3.4468e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0709e-01, "std": 2.9959e-01, "min_value": 1.0674e-03, "max_value": 9.9976e-01}, "num_gradient_steps": 0, "step_time": 1.0007e+01, "total_time": 1.2147e+02, "__timestamp": "2024-10-10 18:05:10.943017"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -5.4527e+02}, "actor_loss": {"n": 1, "mean": -2.6844e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4334e+01}, "critic_loss": {"n": 1, "mean": 1.4510e+00}, "entropy_coef": {"n": 1, "mean": 2.6455e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1535e-01, "std": 2.9881e-01, "min_value": 6.3694e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 9.9326e+00, "total_time": 1.3140e+02, "__timestamp": "2024-10-10 18:05:20.875588"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -7.1563e+01}, "actor_loss": {"n": 1, "mean": -2.5862e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3085e+01}, "critic_loss": {"n": 1, "mean": 2.1099e+00}, "entropy_coef": {"n": 1, "mean": 2.0379e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.6912e-01, "std": 2.9706e-01, "min_value": 3.9160e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0076e+01, "total_time": 1.4148e+02, "__timestamp": "2024-10-10 18:05:30.952014"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": 7.1641e+02}, "actor_loss": {"n": 1, "mean": -2.5311e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.1357e+00}, "critic_loss": {"n": 1, "mean": 1.7678e+00}, "entropy_coef": {"n": 1, "mean": 1.6060e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4190e-01, "std": 2.9969e-01, "min_value": 1.4931e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0464e+01, "total_time": 1.5194e+02, "__timestamp": "2024-10-10 18:05:41.415292"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": 3.0696e+02}, "actor_loss": {"n": 1, "mean": -2.6422e+01}, "entropy_coef_loss": {"n": 1, "mean": -6.6224e+00}, "critic_loss": {"n": 1, "mean": 1.9241e+00}, "entropy_coef": {"n": 1, "mean": 1.2981e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.6940e-01, "std": 2.9923e-01, "min_value": 3.2008e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 9.9912e+00, "total_time": 1.6193e+02, "__timestamp": "2024-10-10 18:05:51.407537"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": 7.5426e+02}, "actor_loss": {"n": 1, "mean": -2.5240e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.8909e+00}, "critic_loss": {"n": 1, "mean": 1.3485e+00}, "entropy_coef": {"n": 1, "mean": 1.0836e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1742e-01, "std": 3.0299e-01, "min_value": 2.8443e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.4598e+00, "total_time": 1.7139e+02, "__timestamp": "2024-10-10 18:06:00.867381"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": 8.0228e+02}, "actor_loss": {"n": 1, "mean": -2.6798e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.2872e+00}, "critic_loss": {"n": 1, "mean": 1.8974e+00}, "entropy_coef": {"n": 1, "mean": 9.5712e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1818e-01, "std": 3.0749e-01, "min_value": 1.4049e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.4349e+00, "total_time": 1.8083e+02, "__timestamp": "2024-10-10 18:06:10.301277"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": 1.1572e+03}, "actor_loss": {"n": 1, "mean": -2.5965e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.9328e+00}, "critic_loss": {"n": 1, "mean": 4.3120e+00}, "entropy_coef": {"n": 1, "mean": 9.6745e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1463e-01, "std": 2.9924e-01, "min_value": 6.9960e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.1952e+00, "total_time": 1.9002e+02, "__timestamp": "2024-10-10 18:06:19.497460"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": 4.8528e+02}, "actor_loss": {"n": 1, "mean": -2.8830e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.7007e+00}, "critic_loss": {"n": 1, "mean": 2.4379e+00}, "entropy_coef": {"n": 1, "mean": 1.0868e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7791e-01, "std": 3.0453e-01, "min_value": 4.0978e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.7810e+00, "total_time": 1.9980e+02, "__timestamp": "2024-10-10 18:06:29.277415"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": 1.8316e+03}, "actor_loss": {"n": 1, "mean": -2.8067e+01}, "entropy_coef_loss": {"n": 1, "mean": 5.2547e-01}, "critic_loss": {"n": 1, "mean": 2.1888e+00}, "entropy_coef": {"n": 1, "mean": 1.2222e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8992e-01, "std": 2.7582e-01, "min_value": 1.1284e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.9117e+00, "total_time": 2.0972e+02, "__timestamp": "2024-10-10 18:06:39.189106"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": 3.4542e+02}, "actor_loss": {"n": 1, "mean": -3.0148e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.1434e+00}, "critic_loss": {"n": 1, "mean": 2.7806e+00}, "entropy_coef": {"n": 1, "mean": 1.3403e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.8675e-01, "std": 2.9890e-01, "min_value": 7.5334e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0201e+01, "total_time": 2.1992e+02, "__timestamp": "2024-10-10 18:06:49.390425"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 1.1726e+03}, "actor_loss": {"n": 1, "mean": -3.1101e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.9830e+00}, "critic_loss": {"n": 1, "mean": 6.0114e+00}, "entropy_coef": {"n": 1, "mean": 1.4093e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4787e-01, "std": 2.8523e-01, "min_value": 5.7278e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0417e+01, "total_time": 2.3033e+02, "__timestamp": "2024-10-10 18:06:59.808691"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 1.6030e+03}, "actor_loss": {"n": 1, "mean": -3.1551e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.5893e-01}, "critic_loss": {"n": 1, "mean": 2.9903e+00}, "entropy_coef": {"n": 1, "mean": 1.5963e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6918e-01, "std": 2.8171e-01, "min_value": 1.2577e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0019e+01, "total_time": 2.4035e+02, "__timestamp": "2024-10-10 18:07:09.826630"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 1.3135e+03}, "actor_loss": {"n": 1, "mean": -3.1789e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.0377e-01}, "critic_loss": {"n": 1, "mean": 3.0890e+00}, "entropy_coef": {"n": 1, "mean": 1.6833e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4952e-01, "std": 2.8868e-01, "min_value": 4.9695e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0114e+01, "total_time": 2.5047e+02, "__timestamp": "2024-10-10 18:07:19.941927"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 3.7526e+02}, "actor_loss": {"n": 1, "mean": -3.7775e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.4787e+00}, "critic_loss": {"n": 1, "mean": 3.6853e+00}, "entropy_coef": {"n": 1, "mean": 1.7819e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7421e-01, "std": 3.0009e-01, "min_value": 4.1038e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0165e+01, "total_time": 2.6063e+02, "__timestamp": "2024-10-10 18:07:30.106224"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 2.2170e+03}, "actor_loss": {"n": 1, "mean": -3.6063e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.4598e+00}, "critic_loss": {"n": 1, "mean": 1.2701e+01}, "entropy_coef": {"n": 1, "mean": 1.8862e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0379e-01, "std": 2.6950e-01, "min_value": 5.6237e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.9057e+00, "total_time": 2.7054e+02, "__timestamp": "2024-10-10 18:07:40.011973"}, {"step": 38000, "num_env_steps": 38000, "scores": {"n": 1, "mean": 2.2576e+02}, "actor_loss": {"n": 1, "mean": -3.6129e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.4408e-01}, "critic_loss": {"n": 1, "mean": 3.1780e+00}, "entropy_coef": {"n": 1, "mean": 2.0627e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.6249e-01, "std": 3.0019e-01, "min_value": 6.8641e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0032e+01, "total_time": 2.8057e+02, "__timestamp": "2024-10-10 18:07:50.045313"}, {"step": 39000, "num_env_steps": 39000, "scores": {"n": 1, "mean": 2.3617e+03}, "actor_loss": {"n": 1, "mean": -4.0054e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.9267e-01}, "critic_loss": {"n": 1, "mean": 8.9800e+00}, "entropy_coef": {"n": 1, "mean": 2.1078e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2049e-01, "std": 2.5566e-01, "min_value": 8.3598e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0115e+01, "total_time": 2.9069e+02, "__timestamp": "2024-10-10 18:08:00.159356"}, {"step": 40000, "num_env_steps": 40000, "scores": {"n": 1, "mean": 2.3998e+03}, "actor_loss": {"n": 1, "mean": -4.0310e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.5573e-01}, "critic_loss": {"n": 1, "mean": 4.0384e+00}, "entropy_coef": {"n": 1, "mean": 2.3137e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1994e-01, "std": 2.5448e-01, "min_value": 8.6221e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0237e+01, "total_time": 3.0092e+02, "__timestamp": "2024-10-10 18:08:10.397316"}, {"step": 41000, "num_env_steps": 41000, "scores": {"n": 1, "mean": 2.7500e+03}, "actor_loss": {"n": 1, "mean": -4.8052e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.3729e-01}, "critic_loss": {"n": 1, "mean": 5.2435e+00}, "entropy_coef": {"n": 1, "mean": 2.5295e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2931e-01, "std": 2.4806e-01, "min_value": 2.0376e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.7208e+00, "total_time": 3.1064e+02, "__timestamp": "2024-10-10 18:08:20.118084"}, {"step": 42000, "num_env_steps": 42000, "scores": {"n": 1, "mean": 2.5768e+03}, "actor_loss": {"n": 1, "mean": -5.1682e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2044e+00}, "critic_loss": {"n": 1, "mean": 5.0774e+00}, "entropy_coef": {"n": 1, "mean": 2.7307e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1241e-01, "std": 2.5629e-01, "min_value": 7.4113e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.8044e+00, "total_time": 3.2045e+02, "__timestamp": "2024-10-10 18:08:29.922522"}, {"step": 43000, "num_env_steps": 43000, "scores": {"n": 1, "mean": 2.3820e+03}, "actor_loss": {"n": 1, "mean": -5.2141e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.2784e-01}, "critic_loss": {"n": 1, "mean": 4.6461e+00}, "entropy_coef": {"n": 1, "mean": 2.9904e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0022e-01, "std": 2.6146e-01, "min_value": 1.3509e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0051e+01, "total_time": 3.3050e+02, "__timestamp": "2024-10-10 18:08:39.972853"}, {"step": 44000, "num_env_steps": 44000, "scores": {"n": 1, "mean": 2.4557e+03}, "actor_loss": {"n": 1, "mean": -5.3535e+01}, "entropy_coef_loss": {"n": 1, "mean": 6.9075e-01}, "critic_loss": {"n": 1, "mean": 6.4284e+00}, "entropy_coef": {"n": 1, "mean": 3.2395e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0760e-01, "std": 2.5703e-01, "min_value": 4.7523e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0693e+01, "total_time": 3.4119e+02, "__timestamp": "2024-10-10 18:08:50.667146"}, {"step": 45000, "num_env_steps": 45000, "scores": {"n": 1, "mean": 2.5320e+03}, "actor_loss": {"n": 1, "mean": -5.9334e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.9210e-01}, "critic_loss": {"n": 1, "mean": 9.4120e+00}, "entropy_coef": {"n": 1, "mean": 3.5681e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0536e-01, "std": 2.5885e-01, "min_value": 2.7636e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0109e+01, "total_time": 3.5130e+02, "__timestamp": "2024-10-10 18:09:00.775320"}, {"step": 46000, "num_env_steps": 46000, "scores": {"n": 1, "mean": 2.5717e+03}, "actor_loss": {"n": 1, "mean": -6.4643e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.5487e+00}, "critic_loss": {"n": 1, "mean": 4.5143e+01}, "entropy_coef": {"n": 1, "mean": 3.7148e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0225e-01, "std": 2.5435e-01, "min_value": 1.2137e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0353e+01, "total_time": 3.6165e+02, "__timestamp": "2024-10-10 18:09:11.128110"}, {"step": 47000, "num_env_steps": 47000, "scores": {"n": 1, "mean": 2.6109e+03}, "actor_loss": {"n": 1, "mean": -6.2792e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3712e+00}, "critic_loss": {"n": 1, "mean": 2.3133e+01}, "entropy_coef": {"n": 1, "mean": 3.9506e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1374e-01, "std": 2.4882e-01, "min_value": 1.3922e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0503e+01, "total_time": 3.7216e+02, "__timestamp": "2024-10-10 18:09:21.632204"}, {"step": 48000, "num_env_steps": 48000, "scores": {"n": 1, "mean": 2.5793e+03}, "actor_loss": {"n": 1, "mean": -6.8149e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.3884e-01}, "critic_loss": {"n": 1, "mean": 6.6231e+01}, "entropy_coef": {"n": 1, "mean": 4.1714e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1097e-01, "std": 2.4738e-01, "min_value": 2.1935e-03, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0377e+01, "total_time": 3.8254e+02, "__timestamp": "2024-10-10 18:09:32.008330"}, {"step": 49000, "num_env_steps": 49000, "scores": {"n": 1, "mean": 2.6736e+03}, "actor_loss": {"n": 1, "mean": -7.0968e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3238e+00}, "critic_loss": {"n": 1, "mean": 5.4571e+00}, "entropy_coef": {"n": 1, "mean": 4.2265e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0241e-01, "std": 2.5549e-01, "min_value": 8.4662e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.1927e+00, "total_time": 3.9173e+02, "__timestamp": "2024-10-10 18:09:41.201998"}, {"step": 50000, "num_env_steps": 50000, "scores": {"n": 1, "mean": 2.4235e+03}, "actor_loss": {"n": 1, "mean": -7.6374e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1065e+00}, "critic_loss": {"n": 1, "mean": 6.0626e+00}, "entropy_coef": {"n": 1, "mean": 4.5697e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9296e-01, "std": 2.6213e-01, "min_value": 5.9998e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.1801e+00, "total_time": 4.0091e+02, "__timestamp": "2024-10-10 18:09:50.382048"}, {"step": 51000, "num_env_steps": 51000, "scores": {"n": 1, "mean": 2.6759e+03}, "actor_loss": {"n": 1, "mean": -7.2687e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3277e+00}, "critic_loss": {"n": 1, "mean": 5.4953e+00}, "entropy_coef": {"n": 1, "mean": 4.5485e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9528e-01, "std": 2.5922e-01, "min_value": 2.2740e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.1446e+00, "total_time": 4.1005e+02, "__timestamp": "2024-10-10 18:09:59.526674"}, {"step": 52000, "num_env_steps": 52000, "scores": {"n": 1, "mean": 2.8649e+03}, "actor_loss": {"n": 1, "mean": -7.0522e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.9804e+00}, "critic_loss": {"n": 1, "mean": 5.6864e+01}, "entropy_coef": {"n": 1, "mean": 4.7356e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0655e-01, "std": 2.5398e-01, "min_value": 2.0839e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.1288e+00, "total_time": 4.1918e+02, "__timestamp": "2024-10-10 18:10:08.655505"}, {"step": 53000, "num_env_steps": 53000, "scores": {"n": 1, "mean": 2.9117e+03}, "actor_loss": {"n": 1, "mean": -8.7679e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.9808e+00}, "critic_loss": {"n": 1, "mean": 5.6915e+00}, "entropy_coef": {"n": 1, "mean": 4.8803e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9886e-01, "std": 2.5691e-01, "min_value": 2.2612e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 9.2049e+00, "total_time": 4.2839e+02, "__timestamp": "2024-10-10 18:10:17.859363"}, {"step": 54000, "num_env_steps": 54000, "scores": {"n": 1, "mean": 3.0577e+03}, "actor_loss": {"n": 1, "mean": -8.0578e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.0337e+00}, "critic_loss": {"n": 1, "mean": 7.3623e+00}, "entropy_coef": {"n": 1, "mean": 4.9676e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0478e-01, "std": 2.5429e-01, "min_value": 5.2363e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.6681e+00, "total_time": 4.3805e+02, "__timestamp": "2024-10-10 18:10:27.528510"}, {"step": 55000, "num_env_steps": 55000, "scores": {"n": 1, "mean": 2.0058e+03}, "actor_loss": {"n": 1, "mean": -8.0657e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4485e+00}, "critic_loss": {"n": 1, "mean": 6.1183e+00}, "entropy_coef": {"n": 1, "mean": 5.0812e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5624e-01, "std": 2.7632e-01, "min_value": 2.2955e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.8612e+00, "total_time": 4.4792e+02, "__timestamp": "2024-10-10 18:10:37.388706"}, {"step": 56000, "num_env_steps": 56000, "scores": {"n": 1, "mean": 2.7000e+03}, "actor_loss": {"n": 1, "mean": -8.0640e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.8671e+00}, "critic_loss": {"n": 1, "mean": 8.5624e+00}, "entropy_coef": {"n": 1, "mean": 4.9947e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9126e-01, "std": 2.6271e-01, "min_value": 6.4908e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.5643e+00, "total_time": 4.5748e+02, "__timestamp": "2024-10-10 18:10:46.952990"}, {"step": 57000, "num_env_steps": 57000, "scores": {"n": 1, "mean": 2.9065e+03}, "actor_loss": {"n": 1, "mean": -8.4644e+01}, "entropy_coef_loss": {"n": 1, "mean": 8.7694e-01}, "critic_loss": {"n": 1, "mean": 8.8059e+00}, "entropy_coef": {"n": 1, "mean": 5.1983e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9771e-01, "std": 2.5822e-01, "min_value": 6.9928e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.5955e+00, "total_time": 4.6708e+02, "__timestamp": "2024-10-10 18:10:56.549444"}, {"step": 58000, "num_env_steps": 58000, "scores": {"n": 1, "mean": 2.8548e+03}, "actor_loss": {"n": 1, "mean": -9.6995e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.3236e+00}, "critic_loss": {"n": 1, "mean": 5.9434e+00}, "entropy_coef": {"n": 1, "mean": 5.2745e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0380e-01, "std": 2.5231e-01, "min_value": 2.3909e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 9.2446e+00, "total_time": 4.7632e+02, "__timestamp": "2024-10-10 18:11:05.793079"}, {"step": 59000, "num_env_steps": 59000, "scores": {"n": 1, "mean": 2.9077e+03}, "actor_loss": {"n": 1, "mean": -9.2630e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.1569e-02}, "critic_loss": {"n": 1, "mean": 7.4011e+00}, "entropy_coef": {"n": 1, "mean": 5.3624e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0529e-01, "std": 2.5255e-01, "min_value": 5.8671e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 9.4676e+00, "total_time": 4.8579e+02, "__timestamp": "2024-10-10 18:11:15.260695"}, {"step": 60000, "num_env_steps": 60000, "scores": {"n": 1, "mean": 2.8773e+03}, "actor_loss": {"n": 1, "mean": -9.9351e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.3754e-01}, "critic_loss": {"n": 1, "mean": 6.1213e+00}, "entropy_coef": {"n": 1, "mean": 5.6255e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0202e-01, "std": 2.5652e-01, "min_value": 5.3608e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.5177e+00, "total_time": 4.9531e+02, "__timestamp": "2024-10-10 18:11:24.779431"}, {"step": 61000, "num_env_steps": 61000, "scores": {"n": 1, "mean": 2.9727e+03}, "actor_loss": {"n": 1, "mean": -9.4210e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.2180e-01}, "critic_loss": {"n": 1, "mean": 5.5415e+00}, "entropy_coef": {"n": 1, "mean": 5.7496e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0099e-01, "std": 2.5576e-01, "min_value": 6.0654e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 9.4211e+00, "total_time": 5.0473e+02, "__timestamp": "2024-10-10 18:11:34.200522"}, {"step": 62000, "num_env_steps": 62000, "scores": {"n": 1, "mean": 2.8359e+03}, "actor_loss": {"n": 1, "mean": -1.0524e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.6308e-01}, "critic_loss": {"n": 1, "mean": 1.0420e+02}, "entropy_coef": {"n": 1, "mean": 5.8279e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9383e-01, "std": 2.5552e-01, "min_value": 5.8334e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.2695e+00, "total_time": 5.1400e+02, "__timestamp": "2024-10-10 18:11:43.469991"}, {"step": 63000, "num_env_steps": 63000, "scores": {"n": 1, "mean": 2.9576e+03}, "actor_loss": {"n": 1, "mean": -1.0076e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.4205e-01}, "critic_loss": {"n": 1, "mean": 9.6915e+01}, "entropy_coef": {"n": 1, "mean": 6.1153e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9751e-01, "std": 2.5758e-01, "min_value": 5.6243e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.2923e+00, "total_time": 5.2329e+02, "__timestamp": "2024-10-10 18:11:52.761291"}, {"step": 64000, "num_env_steps": 64000, "scores": {"n": 1, "mean": 3.0317e+03}, "actor_loss": {"n": 1, "mean": -9.9399e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3967e+00}, "critic_loss": {"n": 1, "mean": 7.9480e+00}, "entropy_coef": {"n": 1, "mean": 6.0243e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9402e-01, "std": 2.5933e-01, "min_value": 2.6254e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0487e+01, "total_time": 5.3378e+02, "__timestamp": "2024-10-10 18:12:03.248536"}, {"step": 65000, "num_env_steps": 65000, "scores": {"n": 1, "mean": 3.1032e+03}, "actor_loss": {"n": 1, "mean": -1.0797e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1051e+00}, "critic_loss": {"n": 1, "mean": 7.3907e+00}, "entropy_coef": {"n": 1, "mean": 6.1382e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0054e-01, "std": 2.5535e-01, "min_value": 1.7604e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 9.8217e+00, "total_time": 5.4360e+02, "__timestamp": "2024-10-10 18:12:13.070258"}, {"step": 66000, "num_env_steps": 66000, "scores": {"n": 1, "mean": 3.0594e+03}, "actor_loss": {"n": 1, "mean": -1.1452e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.8241e-01}, "critic_loss": {"n": 1, "mean": 7.5610e+00}, "entropy_coef": {"n": 1, "mean": 6.1343e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9344e-01, "std": 2.5998e-01, "min_value": 8.6699e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.0648e+01, "total_time": 5.5424e+02, "__timestamp": "2024-10-10 18:12:23.718806"}, {"step": 67000, "num_env_steps": 67000, "scores": {"n": 1, "mean": 3.0216e+03}, "actor_loss": {"n": 1, "mean": -1.1517e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.7923e-01}, "critic_loss": {"n": 1, "mean": 6.3449e+00}, "entropy_coef": {"n": 1, "mean": 6.3965e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8728e-01, "std": 2.6400e-01, "min_value": 1.3714e-03, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 9.6574e+00, "total_time": 5.6390e+02, "__timestamp": "2024-10-10 18:12:33.376494"}, {"step": 68000, "num_env_steps": 68000, "scores": {"n": 1, "mean": 3.2259e+03}, "actor_loss": {"n": 1, "mean": -1.0994e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.0744e-01}, "critic_loss": {"n": 1, "mean": 7.6904e+00}, "entropy_coef": {"n": 1, "mean": 6.3496e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9338e-01, "std": 2.5681e-01, "min_value": 1.4298e-03, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 1.0491e+01, "total_time": 5.7439e+02, "__timestamp": "2024-10-10 18:12:43.867902"}, {"step": 69000, "num_env_steps": 69000, "scores": {"n": 1, "mean": 2.8965e+03}, "actor_loss": {"n": 1, "mean": -1.1396e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.6248e-01}, "critic_loss": {"n": 1, "mean": 5.6455e+00}, "entropy_coef": {"n": 1, "mean": 6.5332e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7899e-01, "std": 2.6975e-01, "min_value": 5.7895e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0555e+01, "total_time": 5.8495e+02, "__timestamp": "2024-10-10 18:12:54.421607"}, {"step": 70000, "num_env_steps": 70000, "scores": {"n": 1, "mean": 3.2254e+03}, "actor_loss": {"n": 1, "mean": -1.2397e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.7678e-01}, "critic_loss": {"n": 1, "mean": 6.7263e+00}, "entropy_coef": {"n": 1, "mean": 6.6832e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9090e-01, "std": 2.6129e-01, "min_value": 4.3291e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.0597e+01, "total_time": 5.9555e+02, "__timestamp": "2024-10-10 18:13:05.019991"}, {"step": 71000, "num_env_steps": 71000, "scores": {"n": 1, "mean": 3.0348e+03}, "actor_loss": {"n": 1, "mean": -1.1860e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.3043e-01}, "critic_loss": {"n": 1, "mean": 8.1632e+00}, "entropy_coef": {"n": 1, "mean": 6.7292e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8989e-01, "std": 2.5928e-01, "min_value": 7.9244e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0580e+01, "total_time": 6.0613e+02, "__timestamp": "2024-10-10 18:13:15.599871"}, {"step": 72000, "num_env_steps": 72000, "scores": {"n": 1, "mean": 3.1731e+03}, "actor_loss": {"n": 1, "mean": -1.2166e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.6979e-01}, "critic_loss": {"n": 1, "mean": 8.3698e+00}, "entropy_coef": {"n": 1, "mean": 6.7906e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8652e-01, "std": 2.6148e-01, "min_value": 3.7128e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.0620e+01, "total_time": 6.1675e+02, "__timestamp": "2024-10-10 18:13:26.219210"}, {"step": 73000, "num_env_steps": 73000, "scores": {"n": 1, "mean": 3.1558e+03}, "actor_loss": {"n": 1, "mean": -1.1528e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.0193e-01}, "critic_loss": {"n": 1, "mean": 1.1830e+02}, "entropy_coef": {"n": 1, "mean": 6.8085e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8902e-01, "std": 2.6023e-01, "min_value": 4.4517e-05, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0548e+01, "total_time": 6.2729e+02, "__timestamp": "2024-10-10 18:13:36.767462"}, {"step": 74000, "num_env_steps": 74000, "scores": {"n": 1, "mean": 3.2957e+03}, "actor_loss": {"n": 1, "mean": -1.1710e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.8650e-01}, "critic_loss": {"n": 1, "mean": 1.1583e+01}, "entropy_coef": {"n": 1, "mean": 6.9506e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9655e-01, "std": 2.5542e-01, "min_value": 1.7255e-03, "max_value": 9.9967e-01}, "num_gradient_steps": 0, "step_time": 1.0961e+01, "total_time": 6.3826e+02, "__timestamp": "2024-10-10 18:13:47.729860"}, {"step": 75000, "num_env_steps": 75000, "scores": {"n": 1, "mean": 3.5747e+03}, "actor_loss": {"n": 1, "mean": -1.1991e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.3938e-01}, "critic_loss": {"n": 1, "mean": 1.0937e+02}, "entropy_coef": {"n": 1, "mean": 6.9683e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9153e-01, "std": 2.6193e-01, "min_value": 1.3394e-03, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0556e+01, "total_time": 6.4881e+02, "__timestamp": "2024-10-10 18:13:58.284794"}, {"step": 76000, "num_env_steps": 76000, "scores": {"n": 1, "mean": 3.1556e+03}, "actor_loss": {"n": 1, "mean": -1.3046e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.9218e-01}, "critic_loss": {"n": 1, "mean": 7.4556e+00}, "entropy_coef": {"n": 1, "mean": 7.1749e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9857e-01, "std": 2.5161e-01, "min_value": 2.3100e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.0546e+01, "total_time": 6.5936e+02, "__timestamp": "2024-10-10 18:14:08.830944"}, {"step": 77000, "num_env_steps": 77000, "scores": {"n": 1, "mean": 3.2258e+03}, "actor_loss": {"n": 1, "mean": -1.2161e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5877e+00}, "critic_loss": {"n": 1, "mean": 8.1617e+00}, "entropy_coef": {"n": 1, "mean": 7.2121e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9436e-01, "std": 2.5623e-01, "min_value": 3.6661e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.1299e+01, "total_time": 6.7066e+02, "__timestamp": "2024-10-10 18:14:20.130526"}, {"step": 78000, "num_env_steps": 78000, "scores": {"n": 1, "mean": 3.3189e+03}, "actor_loss": {"n": 1, "mean": -1.2460e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.0663e+00}, "critic_loss": {"n": 1, "mean": 6.5351e+00}, "entropy_coef": {"n": 1, "mean": 7.3894e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9249e-01, "std": 2.5778e-01, "min_value": 7.3746e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.1603e+01, "total_time": 6.8226e+02, "__timestamp": "2024-10-10 18:14:31.733478"}, {"step": 79000, "num_env_steps": 79000, "scores": {"n": 1, "mean": 3.4926e+03}, "actor_loss": {"n": 1, "mean": -1.2426e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.3344e-01}, "critic_loss": {"n": 1, "mean": 7.9926e+00}, "entropy_coef": {"n": 1, "mean": 7.5114e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8301e-01, "std": 2.6568e-01, "min_value": 7.5069e-04, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.1359e+01, "total_time": 6.9362e+02, "__timestamp": "2024-10-10 18:14:43.092797"}, {"step": 80000, "num_env_steps": 80000, "scores": {"n": 1, "mean": 3.2572e+03}, "actor_loss": {"n": 1, "mean": -1.3108e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.9217e-01}, "critic_loss": {"n": 1, "mean": 3.8258e+01}, "entropy_coef": {"n": 1, "mean": 7.4920e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8188e-01, "std": 2.6433e-01, "min_value": 4.4972e-05, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.1780e+01, "total_time": 7.0540e+02, "__timestamp": "2024-10-10 18:14:54.872701"}, {"step": 81000, "num_env_steps": 81000, "scores": {"n": 1, "mean": 3.4598e+03}, "actor_loss": {"n": 1, "mean": -1.4230e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.5409e+00}, "critic_loss": {"n": 1, "mean": 1.0218e+01}, "entropy_coef": {"n": 1, "mean": 7.7182e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8075e-01, "std": 2.6172e-01, "min_value": 9.3666e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.1641e+01, "total_time": 7.1704e+02, "__timestamp": "2024-10-10 18:15:06.513744"}, {"step": 82000, "num_env_steps": 82000, "scores": {"n": 1, "mean": 3.5727e+03}, "actor_loss": {"n": 1, "mean": -1.3589e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.9682e+00}, "critic_loss": {"n": 1, "mean": 8.1522e+00}, "entropy_coef": {"n": 1, "mean": 7.5799e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8364e-01, "std": 2.6016e-01, "min_value": 7.3103e-04, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 1.1127e+01, "total_time": 7.2817e+02, "__timestamp": "2024-10-10 18:15:17.640736"}, {"step": 83000, "num_env_steps": 83000, "scores": {"n": 1, "mean": 3.4764e+03}, "actor_loss": {"n": 1, "mean": -1.3023e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.0962e-01}, "critic_loss": {"n": 1, "mean": 8.3186e+00}, "entropy_coef": {"n": 1, "mean": 7.8201e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7989e-01, "std": 2.6498e-01, "min_value": 6.7245e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.1119e+01, "total_time": 7.3929e+02, "__timestamp": "2024-10-10 18:15:28.759761"}, {"step": 84000, "num_env_steps": 84000, "scores": {"n": 1, "mean": 3.3908e+03}, "actor_loss": {"n": 1, "mean": -1.3522e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2006e-01}, "critic_loss": {"n": 1, "mean": 7.6655e+00}, "entropy_coef": {"n": 1, "mean": 7.9527e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8241e-01, "std": 2.6445e-01, "min_value": 3.6435e-04, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.1413e+01, "total_time": 7.5070e+02, "__timestamp": "2024-10-10 18:15:40.171699"}, {"step": 85000, "num_env_steps": 85000, "scores": {"n": 1, "mean": 3.5085e+03}, "actor_loss": {"n": 1, "mean": -1.4220e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.1797e-01}, "critic_loss": {"n": 1, "mean": 9.3144e+01}, "entropy_coef": {"n": 1, "mean": 8.1274e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8114e-01, "std": 2.6331e-01, "min_value": 3.1659e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.1489e+01, "total_time": 7.6219e+02, "__timestamp": "2024-10-10 18:15:51.661007"}, {"step": 86000, "num_env_steps": 86000, "scores": {"n": 1, "mean": 3.3561e+03}, "actor_loss": {"n": 1, "mean": -1.4105e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.8261e-01}, "critic_loss": {"n": 1, "mean": 7.9628e+00}, "entropy_coef": {"n": 1, "mean": 8.1955e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8081e-01, "std": 2.6384e-01, "min_value": 4.1105e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0954e+01, "total_time": 7.7314e+02, "__timestamp": "2024-10-10 18:16:02.615652"}, {"step": 87000, "num_env_steps": 87000, "scores": {"n": 1, "mean": 3.2481e+03}, "actor_loss": {"n": 1, "mean": -1.4411e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.0082e-01}, "critic_loss": {"n": 1, "mean": 6.6189e+00}, "entropy_coef": {"n": 1, "mean": 8.1045e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7155e-01, "std": 2.6724e-01, "min_value": 1.3754e-05, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.2483e+01, "total_time": 7.8562e+02, "__timestamp": "2024-10-10 18:16:15.098248"}, {"step": 88000, "num_env_steps": 88000, "scores": {"n": 1, "mean": 3.4594e+03}, "actor_loss": {"n": 1, "mean": -1.3852e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.3417e+00}, "critic_loss": {"n": 1, "mean": 8.4994e+00}, "entropy_coef": {"n": 1, "mean": 8.4106e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7898e-01, "std": 2.6560e-01, "min_value": 2.7691e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 1.1231e+01, "total_time": 7.9685e+02, "__timestamp": "2024-10-10 18:16:26.327982"}, {"step": 89000, "num_env_steps": 89000, "scores": {"n": 1, "mean": 3.5223e+03}, "actor_loss": {"n": 1, "mean": -1.4227e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.2507e-01}, "critic_loss": {"n": 1, "mean": 8.8404e+00}, "entropy_coef": {"n": 1, "mean": 8.4473e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8927e-01, "std": 2.5710e-01, "min_value": 8.0597e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.1127e+01, "total_time": 8.0798e+02, "__timestamp": "2024-10-10 18:16:37.455876"}, {"step": 90000, "num_env_steps": 90000, "scores": {"n": 1, "mean": 3.3277e+03}, "actor_loss": {"n": 1, "mean": -1.3848e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.6133e-01}, "critic_loss": {"n": 1, "mean": 7.2838e+00}, "entropy_coef": {"n": 1, "mean": 8.4383e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8143e-01, "std": 2.6515e-01, "min_value": 7.6890e-06, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0506e+01, "total_time": 8.1849e+02, "__timestamp": "2024-10-10 18:16:47.960826"}, {"step": 91000, "num_env_steps": 91000, "scores": {"n": 1, "mean": 3.4448e+03}, "actor_loss": {"n": 1, "mean": -1.5137e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.4902e-01}, "critic_loss": {"n": 1, "mean": 9.5855e+00}, "entropy_coef": {"n": 1, "mean": 8.5082e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7874e-01, "std": 2.6516e-01, "min_value": 1.5013e-03, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0550e+01, "total_time": 8.2904e+02, "__timestamp": "2024-10-10 18:16:58.511636"}, {"step": 92000, "num_env_steps": 92000, "scores": {"n": 1, "mean": 3.4600e+03}, "actor_loss": {"n": 1, "mean": -1.4254e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.4083e-01}, "critic_loss": {"n": 1, "mean": 7.4813e+00}, "entropy_coef": {"n": 1, "mean": 8.5406e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8648e-01, "std": 2.6151e-01, "min_value": 7.6272e-05, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0619e+01, "total_time": 8.3966e+02, "__timestamp": "2024-10-10 18:17:09.129495"}, {"step": 93000, "num_env_steps": 93000, "scores": {"n": 1, "mean": 3.6968e+03}, "actor_loss": {"n": 1, "mean": -1.4711e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2399e+00}, "critic_loss": {"n": 1, "mean": 8.5863e+00}, "entropy_coef": {"n": 1, "mean": 8.4865e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8332e-01, "std": 2.6373e-01, "min_value": 1.2788e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.1052e+01, "total_time": 8.5071e+02, "__timestamp": "2024-10-10 18:17:20.182342"}, {"step": 94000, "num_env_steps": 94000, "scores": {"n": 1, "mean": 3.5236e+03}, "actor_loss": {"n": 1, "mean": -1.4627e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.5852e-01}, "critic_loss": {"n": 1, "mean": 8.3864e+00}, "entropy_coef": {"n": 1, "mean": 8.5752e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7135e-01, "std": 2.6833e-01, "min_value": 5.0662e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.1029e+01, "total_time": 8.6174e+02, "__timestamp": "2024-10-10 18:17:31.211329"}, {"step": 95000, "num_env_steps": 95000, "scores": {"n": 1, "mean": 3.5956e+03}, "actor_loss": {"n": 1, "mean": -1.5774e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.5146e-01}, "critic_loss": {"n": 1, "mean": 7.2851e+00}, "entropy_coef": {"n": 1, "mean": 8.8140e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7849e-01, "std": 2.6785e-01, "min_value": 8.5869e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.1166e+01, "total_time": 8.7290e+02, "__timestamp": "2024-10-10 18:17:42.376452"}, {"step": 96000, "num_env_steps": 96000, "scores": {"n": 1, "mean": 3.5649e+03}, "actor_loss": {"n": 1, "mean": -1.5226e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5503e+00}, "critic_loss": {"n": 1, "mean": 8.1800e+00}, "entropy_coef": {"n": 1, "mean": 8.7097e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8221e-01, "std": 2.6256e-01, "min_value": 2.5183e-06, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.0573e+01, "total_time": 8.8348e+02, "__timestamp": "2024-10-10 18:17:52.950685"}, {"step": 97000, "num_env_steps": 97000, "scores": {"n": 1, "mean": 3.5102e+03}, "actor_loss": {"n": 1, "mean": -1.4923e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.9197e-01}, "critic_loss": {"n": 1, "mean": 1.0228e+01}, "entropy_coef": {"n": 1, "mean": 8.8392e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8877e-01, "std": 2.6164e-01, "min_value": 3.4226e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.0649e+01, "total_time": 8.9413e+02, "__timestamp": "2024-10-10 18:18:03.599704"}, {"step": 98000, "num_env_steps": 98000, "scores": {"n": 1, "mean": 3.5808e+03}, "actor_loss": {"n": 1, "mean": -1.5555e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4419e+00}, "critic_loss": {"n": 1, "mean": 8.4778e+00}, "entropy_coef": {"n": 1, "mean": 9.0960e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9246e-01, "std": 2.5609e-01, "min_value": 8.0943e-05, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0646e+01, "total_time": 9.0477e+02, "__timestamp": "2024-10-10 18:18:14.245391"}, {"step": 99000, "num_env_steps": 99000, "scores": {"n": 1, "mean": 3.8894e+03}, "actor_loss": {"n": 1, "mean": -1.5996e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.8573e-01}, "critic_loss": {"n": 1, "mean": 8.6060e+00}, "entropy_coef": {"n": 1, "mean": 9.1292e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9186e-01, "std": 2.6033e-01, "min_value": 3.8984e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0755e+01, "total_time": 9.1553e+02, "__timestamp": "2024-10-10 18:18:25.000748"}, {"step": 100000, "num_env_steps": 100000, "scores": {"n": 1, "mean": 3.7109e+03}, "actor_loss": {"n": 1, "mean": -1.5323e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.5642e-01}, "critic_loss": {"n": 1, "mean": 1.4675e+02}, "entropy_coef": {"n": 1, "mean": 9.2761e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8868e-01, "std": 2.6060e-01, "min_value": 1.5381e-04, "max_value": 9.9975e-01}, "num_gradient_steps": 0, "step_time": 1.1141e+01, "total_time": 9.2667e+02, "__timestamp": "2024-10-10 18:18:36.141153"}, {"step": 101000, "num_env_steps": 101000, "scores": {"n": 1, "mean": 2.4559e+03}, "actor_loss": {"n": 1, "mean": -1.5567e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.4132e-01}, "critic_loss": {"n": 1, "mean": 9.1395e+00}, "entropy_coef": {"n": 1, "mean": 9.3127e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1794e-01, "std": 2.9284e-01, "min_value": 5.4524e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0982e+01, "total_time": 9.3765e+02, "__timestamp": "2024-10-10 18:18:47.123153"}, {"step": 102000, "num_env_steps": 102000, "scores": {"n": 1, "mean": 3.6375e+03}, "actor_loss": {"n": 1, "mean": -1.5327e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.4978e-01}, "critic_loss": {"n": 1, "mean": 1.5311e+01}, "entropy_coef": {"n": 1, "mean": 9.0991e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8196e-01, "std": 2.6614e-01, "min_value": 1.3584e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.1202e+01, "total_time": 9.4885e+02, "__timestamp": "2024-10-10 18:18:58.324068"}, {"step": 103000, "num_env_steps": 103000, "scores": {"n": 1, "mean": 3.7319e+03}, "actor_loss": {"n": 1, "mean": -1.5699e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.7562e-01}, "critic_loss": {"n": 1, "mean": 8.3281e+00}, "entropy_coef": {"n": 1, "mean": 9.2118e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8442e-01, "std": 2.6627e-01, "min_value": 1.2676e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0663e+01, "total_time": 9.5951e+02, "__timestamp": "2024-10-10 18:19:08.987706"}, {"step": 104000, "num_env_steps": 104000, "scores": {"n": 1, "mean": 3.7445e+03}, "actor_loss": {"n": 1, "mean": -1.5373e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.5101e-01}, "critic_loss": {"n": 1, "mean": 1.2670e+01}, "entropy_coef": {"n": 1, "mean": 9.2834e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8011e-01, "std": 2.6591e-01, "min_value": 5.6215e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.0485e+01, "total_time": 9.7000e+02, "__timestamp": "2024-10-10 18:19:19.473087"}, {"step": 105000, "num_env_steps": 105000, "scores": {"n": 1, "mean": 3.7502e+03}, "actor_loss": {"n": 1, "mean": -1.6257e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.7017e-01}, "critic_loss": {"n": 1, "mean": 8.6434e+00}, "entropy_coef": {"n": 1, "mean": 9.4378e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8463e-01, "std": 2.6324e-01, "min_value": 5.8819e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0757e+01, "total_time": 9.8076e+02, "__timestamp": "2024-10-10 18:19:30.229524"}, {"step": 106000, "num_env_steps": 106000, "scores": {"n": 1, "mean": 3.8339e+03}, "actor_loss": {"n": 1, "mean": -1.6270e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.0118e-01}, "critic_loss": {"n": 1, "mean": 9.2084e+00}, "entropy_coef": {"n": 1, "mean": 9.3338e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8550e-01, "std": 2.6443e-01, "min_value": 4.5531e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0654e+01, "total_time": 9.9141e+02, "__timestamp": "2024-10-10 18:19:40.884936"}, {"step": 107000, "num_env_steps": 107000, "scores": {"n": 1, "mean": 3.6872e+03}, "actor_loss": {"n": 1, "mean": -1.5644e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0640e+00}, "critic_loss": {"n": 1, "mean": 1.0369e+02}, "entropy_coef": {"n": 1, "mean": 9.6453e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7981e-01, "std": 2.6787e-01, "min_value": 2.4992e-04, "max_value": 9.9962e-01}, "num_gradient_steps": 0, "step_time": 1.0700e+01, "total_time": 1.0021e+03, "__timestamp": "2024-10-10 18:19:51.583489"}, {"step": 108000, "num_env_steps": 108000, "scores": {"n": 1, "mean": 3.6867e+03}, "actor_loss": {"n": 1, "mean": -1.6384e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.3451e-01}, "critic_loss": {"n": 1, "mean": 8.6212e+00}, "entropy_coef": {"n": 1, "mean": 9.7249e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8604e-01, "std": 2.5926e-01, "min_value": 8.9771e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.1001e+01, "total_time": 1.0131e+03, "__timestamp": "2024-10-10 18:20:02.585753"}, {"step": 109000, "num_env_steps": 109000, "scores": {"n": 1, "mean": 3.9043e+03}, "actor_loss": {"n": 1, "mean": -1.6972e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.9034e-01}, "critic_loss": {"n": 1, "mean": 7.8704e+00}, "entropy_coef": {"n": 1, "mean": 9.7226e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8738e-01, "std": 2.5967e-01, "min_value": 8.7774e-04, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 1.1455e+01, "total_time": 1.0246e+03, "__timestamp": "2024-10-10 18:20:14.040242"}, {"step": 110000, "num_env_steps": 110000, "scores": {"n": 1, "mean": 3.8120e+03}, "actor_loss": {"n": 1, "mean": -1.7621e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5721e-01}, "critic_loss": {"n": 1, "mean": 8.1441e+00}, "entropy_coef": {"n": 1, "mean": 9.6996e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8633e-01, "std": 2.5967e-01, "min_value": 5.6690e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0602e+01, "total_time": 1.0352e+03, "__timestamp": "2024-10-10 18:20:24.643592"}, {"step": 111000, "num_env_steps": 111000, "scores": {"n": 1, "mean": 3.8582e+03}, "actor_loss": {"n": 1, "mean": -1.6958e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.2152e-01}, "critic_loss": {"n": 1, "mean": 1.0758e+01}, "entropy_coef": {"n": 1, "mean": 9.8368e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8591e-01, "std": 2.6403e-01, "min_value": 2.2908e-04, "max_value": 9.9962e-01}, "num_gradient_steps": 0, "step_time": 1.0610e+01, "total_time": 1.0458e+03, "__timestamp": "2024-10-10 18:20:35.253239"}, {"step": 112000, "num_env_steps": 112000, "scores": {"n": 1, "mean": 3.7954e+03}, "actor_loss": {"n": 1, "mean": -1.7350e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.7790e-01}, "critic_loss": {"n": 1, "mean": 7.9151e+00}, "entropy_coef": {"n": 1, "mean": 1.0136e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8507e-01, "std": 2.6238e-01, "min_value": 7.0039e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0563e+01, "total_time": 1.0563e+03, "__timestamp": "2024-10-10 18:20:45.815662"}, {"step": 113000, "num_env_steps": 113000, "scores": {"n": 1, "mean": 3.8273e+03}, "actor_loss": {"n": 1, "mean": -1.7376e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.6779e-01}, "critic_loss": {"n": 1, "mean": 8.1143e+00}, "entropy_coef": {"n": 1, "mean": 1.0294e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8271e-01, "std": 2.6546e-01, "min_value": 1.6415e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.1086e+01, "total_time": 1.0674e+03, "__timestamp": "2024-10-10 18:20:56.901337"}, {"step": 114000, "num_env_steps": 114000, "scores": {"n": 1, "mean": 4.0043e+03}, "actor_loss": {"n": 1, "mean": -1.6804e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.2971e-01}, "critic_loss": {"n": 1, "mean": 8.5472e+00}, "entropy_coef": {"n": 1, "mean": 1.0063e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8228e-01, "std": 2.6534e-01, "min_value": 2.4151e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.0501e+01, "total_time": 1.0779e+03, "__timestamp": "2024-10-10 18:21:07.402953"}, {"step": 115000, "num_env_steps": 115000, "scores": {"n": 1, "mean": 3.9761e+03}, "actor_loss": {"n": 1, "mean": -1.6067e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1852e+00}, "critic_loss": {"n": 1, "mean": 1.5673e+01}, "entropy_coef": {"n": 1, "mean": 1.0263e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8796e-01, "std": 2.5913e-01, "min_value": 9.8410e-04, "max_value": 9.9954e-01}, "num_gradient_steps": 0, "step_time": 1.0666e+01, "total_time": 1.0886e+03, "__timestamp": "2024-10-10 18:21:18.068798"}, {"step": 116000, "num_env_steps": 116000, "scores": {"n": 1, "mean": 3.8665e+03}, "actor_loss": {"n": 1, "mean": -1.8445e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.4273e-01}, "critic_loss": {"n": 1, "mean": 1.9062e+02}, "entropy_coef": {"n": 1, "mean": 1.0115e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8469e-01, "std": 2.5934e-01, "min_value": 1.0344e-03, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0717e+01, "total_time": 1.0993e+03, "__timestamp": "2024-10-10 18:21:28.785485"}, {"step": 117000, "num_env_steps": 117000, "scores": {"n": 1, "mean": 3.7831e+03}, "actor_loss": {"n": 1, "mean": -1.8592e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.5517e-01}, "critic_loss": {"n": 1, "mean": 1.6275e+02}, "entropy_coef": {"n": 1, "mean": 1.0356e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8111e-01, "std": 2.6236e-01, "min_value": 5.5176e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0512e+01, "total_time": 1.1098e+03, "__timestamp": "2024-10-10 18:21:39.296555"}, {"step": 118000, "num_env_steps": 118000, "scores": {"n": 1, "mean": 4.1179e+03}, "actor_loss": {"n": 1, "mean": -1.7854e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1199e+00}, "critic_loss": {"n": 1, "mean": 8.3153e+00}, "entropy_coef": {"n": 1, "mean": 1.0272e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8467e-01, "std": 2.6488e-01, "min_value": 9.2032e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 1.0635e+01, "total_time": 1.1205e+03, "__timestamp": "2024-10-10 18:21:49.932938"}, {"step": 119000, "num_env_steps": 119000, "scores": {"n": 1, "mean": 3.8289e+03}, "actor_loss": {"n": 1, "mean": -1.7875e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.4322e-01}, "critic_loss": {"n": 1, "mean": 1.7658e+02}, "entropy_coef": {"n": 1, "mean": 1.0187e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7356e-01, "std": 2.6509e-01, "min_value": 7.9572e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0635e+01, "total_time": 1.1311e+03, "__timestamp": "2024-10-10 18:22:00.567187"}, {"step": 120000, "num_env_steps": 120000, "scores": {"n": 1, "mean": 4.0630e+03}, "actor_loss": {"n": 1, "mean": -1.8233e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.7796e-02}, "critic_loss": {"n": 1, "mean": 1.4556e+01}, "entropy_coef": {"n": 1, "mean": 1.0380e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8217e-01, "std": 2.6719e-01, "min_value": 6.2215e-04, "max_value": 9.9969e-01}, "num_gradient_steps": 0, "step_time": 1.0573e+01, "total_time": 1.1417e+03, "__timestamp": "2024-10-10 18:22:11.140205"}, {"step": 121000, "num_env_steps": 121000, "scores": {"n": 1, "mean": 4.0205e+03}, "actor_loss": {"n": 1, "mean": -1.8183e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1846e-01}, "critic_loss": {"n": 1, "mean": 1.3983e+01}, "entropy_coef": {"n": 1, "mean": 1.0601e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7810e-01, "std": 2.6478e-01, "min_value": 6.3132e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.0498e+01, "total_time": 1.1522e+03, "__timestamp": "2024-10-10 18:22:21.637885"}, {"step": 122000, "num_env_steps": 122000, "scores": {"n": 1, "mean": 3.9631e+03}, "actor_loss": {"n": 1, "mean": -1.7758e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0856e-01}, "critic_loss": {"n": 1, "mean": 1.0531e+01}, "entropy_coef": {"n": 1, "mean": 1.0687e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8918e-01, "std": 2.5707e-01, "min_value": 2.8934e-04, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 1.0798e+01, "total_time": 1.1630e+03, "__timestamp": "2024-10-10 18:22:32.436290"}, {"step": 123000, "num_env_steps": 123000, "scores": {"n": 1, "mean": 4.1034e+03}, "actor_loss": {"n": 1, "mean": -1.7916e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.9017e-01}, "critic_loss": {"n": 1, "mean": 2.3673e+02}, "entropy_coef": {"n": 1, "mean": 1.0617e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8460e-01, "std": 2.6107e-01, "min_value": 8.3968e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0623e+01, "total_time": 1.1736e+03, "__timestamp": "2024-10-10 18:22:43.059984"}, {"step": 124000, "num_env_steps": 124000, "scores": {"n": 1, "mean": 4.0045e+03}, "actor_loss": {"n": 1, "mean": -1.8652e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.1590e-01}, "critic_loss": {"n": 1, "mean": 1.1816e+01}, "entropy_coef": {"n": 1, "mean": 1.0771e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7956e-01, "std": 2.6417e-01, "min_value": 5.1135e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0685e+01, "total_time": 1.1843e+03, "__timestamp": "2024-10-10 18:22:53.743924"}, {"step": 125000, "num_env_steps": 125000, "scores": {"n": 1, "mean": 3.9884e+03}, "actor_loss": {"n": 1, "mean": -1.8364e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.4600e-02}, "critic_loss": {"n": 1, "mean": 8.9105e+00}, "entropy_coef": {"n": 1, "mean": 1.0743e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7293e-01, "std": 2.6775e-01, "min_value": 7.4953e-06, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0563e+01, "total_time": 1.1948e+03, "__timestamp": "2024-10-10 18:23:04.308161"}, {"step": 126000, "num_env_steps": 126000, "scores": {"n": 1, "mean": 3.8213e+03}, "actor_loss": {"n": 1, "mean": -1.8550e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.7211e-01}, "critic_loss": {"n": 1, "mean": 1.2004e+01}, "entropy_coef": {"n": 1, "mean": 1.0766e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7527e-01, "std": 2.6645e-01, "min_value": 5.1266e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0543e+01, "total_time": 1.2054e+03, "__timestamp": "2024-10-10 18:23:14.850560"}, {"step": 127000, "num_env_steps": 127000, "scores": {"n": 1, "mean": 4.0288e+03}, "actor_loss": {"n": 1, "mean": -1.8323e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2588e-01}, "critic_loss": {"n": 1, "mean": 1.2413e+01}, "entropy_coef": {"n": 1, "mean": 1.0661e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8014e-01, "std": 2.6421e-01, "min_value": 1.3781e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0542e+01, "total_time": 1.2159e+03, "__timestamp": "2024-10-10 18:23:25.392628"}, {"step": 128000, "num_env_steps": 128000, "scores": {"n": 1, "mean": 3.9095e+03}, "actor_loss": {"n": 1, "mean": -1.8570e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.7160e+00}, "critic_loss": {"n": 1, "mean": 9.7527e+00}, "entropy_coef": {"n": 1, "mean": 1.0818e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7198e-01, "std": 2.7058e-01, "min_value": 1.0917e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0473e+01, "total_time": 1.2264e+03, "__timestamp": "2024-10-10 18:23:35.865318"}, {"step": 129000, "num_env_steps": 129000, "scores": {"n": 1, "mean": 4.1069e+03}, "actor_loss": {"n": 1, "mean": -1.9008e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0160e+00}, "critic_loss": {"n": 1, "mean": 2.1300e+02}, "entropy_coef": {"n": 1, "mean": 1.0791e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6990e-01, "std": 2.7178e-01, "min_value": 5.9038e-05, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0698e+01, "total_time": 1.2371e+03, "__timestamp": "2024-10-10 18:23:46.564375"}, {"step": 130000, "num_env_steps": 130000, "scores": {"n": 1, "mean": 4.0913e+03}, "actor_loss": {"n": 1, "mean": -1.8130e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.0379e-01}, "critic_loss": {"n": 1, "mean": 9.5405e+00}, "entropy_coef": {"n": 1, "mean": 1.0870e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7512e-01, "std": 2.6665e-01, "min_value": 7.9513e-05, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 1.1249e+01, "total_time": 1.2483e+03, "__timestamp": "2024-10-10 18:23:57.813042"}, {"step": 131000, "num_env_steps": 131000, "scores": {"n": 1, "mean": 4.1186e+03}, "actor_loss": {"n": 1, "mean": -1.8581e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0738e-01}, "critic_loss": {"n": 1, "mean": 8.6764e+00}, "entropy_coef": {"n": 1, "mean": 1.0853e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6884e-01, "std": 2.7186e-01, "min_value": 3.3054e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.1616e+01, "total_time": 1.2600e+03, "__timestamp": "2024-10-10 18:24:09.428941"}, {"step": 132000, "num_env_steps": 132000, "scores": {"n": 1, "mean": 3.8221e+03}, "actor_loss": {"n": 1, "mean": -1.8158e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.8466e-01}, "critic_loss": {"n": 1, "mean": 1.0699e+01}, "entropy_coef": {"n": 1, "mean": 1.1189e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7426e-01, "std": 2.6757e-01, "min_value": 2.0188e-03, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.1053e+01, "total_time": 1.2710e+03, "__timestamp": "2024-10-10 18:24:20.481924"}, {"step": 133000, "num_env_steps": 133000, "scores": {"n": 1, "mean": 3.9854e+03}, "actor_loss": {"n": 1, "mean": -1.8700e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.4422e-01}, "critic_loss": {"n": 1, "mean": 9.8909e+00}, "entropy_coef": {"n": 1, "mean": 1.1094e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7032e-01, "std": 2.7188e-01, "min_value": 2.4289e-05, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0947e+01, "total_time": 1.2820e+03, "__timestamp": "2024-10-10 18:24:31.428422"}, {"step": 134000, "num_env_steps": 134000, "scores": {"n": 1, "mean": 3.8714e+03}, "actor_loss": {"n": 1, "mean": -1.9345e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.2322e-01}, "critic_loss": {"n": 1, "mean": 8.4675e+00}, "entropy_coef": {"n": 1, "mean": 1.1502e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7918e-01, "std": 2.6488e-01, "min_value": 6.3294e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0872e+01, "total_time": 1.2928e+03, "__timestamp": "2024-10-10 18:24:42.301136"}, {"step": 135000, "num_env_steps": 135000, "scores": {"n": 1, "mean": 4.2131e+03}, "actor_loss": {"n": 1, "mean": -1.8321e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.0990e-03}, "critic_loss": {"n": 1, "mean": 9.5561e+00}, "entropy_coef": {"n": 1, "mean": 1.1594e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7909e-01, "std": 2.6740e-01, "min_value": 9.0953e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.1372e+01, "total_time": 1.3042e+03, "__timestamp": "2024-10-10 18:24:53.673519"}, {"step": 136000, "num_env_steps": 136000, "scores": {"n": 1, "mean": 4.0726e+03}, "actor_loss": {"n": 1, "mean": -1.9237e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.2831e-01}, "critic_loss": {"n": 1, "mean": 8.9542e+00}, "entropy_coef": {"n": 1, "mean": 1.1735e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6598e-01, "std": 2.7491e-01, "min_value": 4.4358e-04, "max_value": 9.9965e-01}, "num_gradient_steps": 0, "step_time": 1.3061e+01, "total_time": 1.3173e+03, "__timestamp": "2024-10-10 18:25:06.733372"}, {"step": 137000, "num_env_steps": 137000, "scores": {"n": 1, "mean": 3.9456e+03}, "actor_loss": {"n": 1, "mean": -1.9530e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.0538e-01}, "critic_loss": {"n": 1, "mean": 1.2082e+01}, "entropy_coef": {"n": 1, "mean": 1.1663e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7450e-01, "std": 2.6847e-01, "min_value": 2.7501e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1072e+01, "total_time": 1.3283e+03, "__timestamp": "2024-10-10 18:25:17.805851"}, {"step": 138000, "num_env_steps": 138000, "scores": {"n": 1, "mean": 4.1518e+03}, "actor_loss": {"n": 1, "mean": -1.9040e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.2683e-01}, "critic_loss": {"n": 1, "mean": 1.2446e+01}, "entropy_coef": {"n": 1, "mean": 1.1306e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7205e-01, "std": 2.6979e-01, "min_value": 3.5059e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.1093e+01, "total_time": 1.3394e+03, "__timestamp": "2024-10-10 18:25:28.900347"}, {"step": 139000, "num_env_steps": 139000, "scores": {"n": 1, "mean": 4.0271e+03}, "actor_loss": {"n": 1, "mean": -1.9414e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5808e+00}, "critic_loss": {"n": 1, "mean": 1.2121e+01}, "entropy_coef": {"n": 1, "mean": 1.1696e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7588e-01, "std": 2.6838e-01, "min_value": 1.2898e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.1004e+01, "total_time": 1.3504e+03, "__timestamp": "2024-10-10 18:25:39.903130"}, {"step": 140000, "num_env_steps": 140000, "scores": {"n": 1, "mean": 4.1594e+03}, "actor_loss": {"n": 1, "mean": -1.9458e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0083e-01}, "critic_loss": {"n": 1, "mean": 1.0148e+01}, "entropy_coef": {"n": 1, "mean": 1.1481e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7074e-01, "std": 2.7155e-01, "min_value": 3.4572e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.1644e+01, "total_time": 1.3621e+03, "__timestamp": "2024-10-10 18:25:51.547040"}, {"step": 141000, "num_env_steps": 141000, "scores": {"n": 1, "mean": 4.0239e+03}, "actor_loss": {"n": 1, "mean": -1.9546e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.4460e-01}, "critic_loss": {"n": 1, "mean": 2.2994e+02}, "entropy_coef": {"n": 1, "mean": 1.1628e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6634e-01, "std": 2.7549e-01, "min_value": 6.2992e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.1171e+01, "total_time": 1.3732e+03, "__timestamp": "2024-10-10 18:26:02.719312"}, {"step": 142000, "num_env_steps": 142000, "scores": {"n": 1, "mean": 4.3003e+03}, "actor_loss": {"n": 1, "mean": -1.9384e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.0278e-01}, "critic_loss": {"n": 1, "mean": 1.7071e+02}, "entropy_coef": {"n": 1, "mean": 1.1490e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6839e-01, "std": 2.7382e-01, "min_value": 2.4962e-04, "max_value": 9.9948e-01}, "num_gradient_steps": 0, "step_time": 1.2885e+01, "total_time": 1.3861e+03, "__timestamp": "2024-10-10 18:26:15.603594"}, {"step": 143000, "num_env_steps": 143000, "scores": {"n": 1, "mean": 4.1486e+03}, "actor_loss": {"n": 1, "mean": -1.9977e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.9092e+00}, "critic_loss": {"n": 1, "mean": 1.0150e+01}, "entropy_coef": {"n": 1, "mean": 1.1812e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7000e-01, "std": 2.7030e-01, "min_value": 4.0111e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0977e+01, "total_time": 1.3971e+03, "__timestamp": "2024-10-10 18:26:26.581867"}, {"step": 144000, "num_env_steps": 144000, "scores": {"n": 1, "mean": 4.0493e+03}, "actor_loss": {"n": 1, "mean": -1.9333e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.1651e-01}, "critic_loss": {"n": 1, "mean": 2.4135e+02}, "entropy_coef": {"n": 1, "mean": 1.1916e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7071e-01, "std": 2.7201e-01, "min_value": 7.4562e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0748e+01, "total_time": 1.4079e+03, "__timestamp": "2024-10-10 18:26:37.330234"}, {"step": 145000, "num_env_steps": 145000, "scores": {"n": 1, "mean": 4.1711e+03}, "actor_loss": {"n": 1, "mean": -1.9577e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.0066e-01}, "critic_loss": {"n": 1, "mean": 1.1345e+01}, "entropy_coef": {"n": 1, "mean": 1.2004e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6927e-01, "std": 2.7002e-01, "min_value": 4.8898e-04, "max_value": 9.9942e-01}, "num_gradient_steps": 0, "step_time": 1.0787e+01, "total_time": 1.4186e+03, "__timestamp": "2024-10-10 18:26:48.116897"}, {"step": 146000, "num_env_steps": 146000, "scores": {"n": 1, "mean": 4.1356e+03}, "actor_loss": {"n": 1, "mean": -1.9109e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.5999e-02}, "critic_loss": {"n": 1, "mean": 1.0195e+01}, "entropy_coef": {"n": 1, "mean": 1.2100e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7265e-01, "std": 2.7183e-01, "min_value": 9.0599e-05, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0848e+01, "total_time": 1.4295e+03, "__timestamp": "2024-10-10 18:26:58.964089"}, {"step": 147000, "num_env_steps": 147000, "scores": {"n": 1, "mean": 4.2268e+03}, "actor_loss": {"n": 1, "mean": -1.9628e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.3361e-02}, "critic_loss": {"n": 1, "mean": 8.5379e+00}, "entropy_coef": {"n": 1, "mean": 1.2027e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6888e-01, "std": 2.7476e-01, "min_value": 7.8735e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.1276e+01, "total_time": 1.4408e+03, "__timestamp": "2024-10-10 18:27:10.239608"}, {"step": 148000, "num_env_steps": 148000, "scores": {"n": 1, "mean": 4.0883e+03}, "actor_loss": {"n": 1, "mean": -1.9876e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4918e-01}, "critic_loss": {"n": 1, "mean": 1.1725e+01}, "entropy_coef": {"n": 1, "mean": 1.1948e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6836e-01, "std": 2.7491e-01, "min_value": 1.4395e-03, "max_value": 9.9947e-01}, "num_gradient_steps": 0, "step_time": 1.2832e+01, "total_time": 1.4536e+03, "__timestamp": "2024-10-10 18:27:23.071188"}, {"step": 149000, "num_env_steps": 149000, "scores": {"n": 1, "mean": 4.1534e+03}, "actor_loss": {"n": 1, "mean": -1.9792e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.8655e-02}, "critic_loss": {"n": 1, "mean": 1.0476e+01}, "entropy_coef": {"n": 1, "mean": 1.1945e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6217e-01, "std": 2.7444e-01, "min_value": 3.9369e-05, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0636e+01, "total_time": 1.4642e+03, "__timestamp": "2024-10-10 18:27:33.708283"}, {"step": 150000, "num_env_steps": 150000, "scores": {"n": 1, "mean": 4.2183e+03}, "actor_loss": {"n": 1, "mean": -1.9548e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.0171e-02}, "critic_loss": {"n": 1, "mean": 1.2245e+01}, "entropy_coef": {"n": 1, "mean": 1.2394e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7302e-01, "std": 2.7382e-01, "min_value": 4.1663e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.1694e+01, "total_time": 1.4759e+03, "__timestamp": "2024-10-10 18:27:45.400918"}, {"step": 151000, "num_env_steps": 151000, "scores": {"n": 1, "mean": 4.2154e+03}, "actor_loss": {"n": 1, "mean": -1.9207e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.3140e-01}, "critic_loss": {"n": 1, "mean": 4.1782e+01}, "entropy_coef": {"n": 1, "mean": 1.2334e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6891e-01, "std": 2.7342e-01, "min_value": 1.3098e-05, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0884e+01, "total_time": 1.4868e+03, "__timestamp": "2024-10-10 18:27:56.285806"}, {"step": 152000, "num_env_steps": 152000, "scores": {"n": 1, "mean": 4.2470e+03}, "actor_loss": {"n": 1, "mean": -1.9021e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.8311e-01}, "critic_loss": {"n": 1, "mean": 9.8521e+00}, "entropy_coef": {"n": 1, "mean": 1.2261e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7009e-01, "std": 2.7183e-01, "min_value": 1.4961e-03, "max_value": 9.9948e-01}, "num_gradient_steps": 0, "step_time": 1.0924e+01, "total_time": 1.4977e+03, "__timestamp": "2024-10-10 18:28:07.209955"}, {"step": 153000, "num_env_steps": 153000, "scores": {"n": 1, "mean": 4.2817e+03}, "actor_loss": {"n": 1, "mean": -1.9985e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0175e+00}, "critic_loss": {"n": 1, "mean": 8.8006e+00}, "entropy_coef": {"n": 1, "mean": 1.2214e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6549e-01, "std": 2.7273e-01, "min_value": 4.6670e-05, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0584e+01, "total_time": 1.5083e+03, "__timestamp": "2024-10-10 18:28:17.792576"}, {"step": 154000, "num_env_steps": 154000, "scores": {"n": 1, "mean": 4.2681e+03}, "actor_loss": {"n": 1, "mean": -2.0349e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2122e-02}, "critic_loss": {"n": 1, "mean": 1.1076e+01}, "entropy_coef": {"n": 1, "mean": 1.2254e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6787e-01, "std": 2.6870e-01, "min_value": 1.1164e-04, "max_value": 9.9951e-01}, "num_gradient_steps": 0, "step_time": 1.0620e+01, "total_time": 1.5189e+03, "__timestamp": "2024-10-10 18:28:28.413860"}, {"step": 155000, "num_env_steps": 155000, "scores": {"n": 1, "mean": 4.0959e+03}, "actor_loss": {"n": 1, "mean": -2.0350e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.9681e-01}, "critic_loss": {"n": 1, "mean": 1.4351e+01}, "entropy_coef": {"n": 1, "mean": 1.2557e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7601e-01, "std": 2.6924e-01, "min_value": 1.6352e-03, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.0489e+01, "total_time": 1.5294e+03, "__timestamp": "2024-10-10 18:28:38.903156"}, {"step": 156000, "num_env_steps": 156000, "scores": {"n": 1, "mean": 4.1817e+03}, "actor_loss": {"n": 1, "mean": -2.1052e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.9110e-01}, "critic_loss": {"n": 1, "mean": 1.8509e+02}, "entropy_coef": {"n": 1, "mean": 1.2755e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6775e-01, "std": 2.7380e-01, "min_value": 1.5453e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.0592e+01, "total_time": 1.5400e+03, "__timestamp": "2024-10-10 18:28:49.493776"}, {"step": 157000, "num_env_steps": 157000, "scores": {"n": 1, "mean": 4.2085e+03}, "actor_loss": {"n": 1, "mean": -2.0243e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.6968e-01}, "critic_loss": {"n": 1, "mean": 1.1315e+01}, "entropy_coef": {"n": 1, "mean": 1.2755e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6526e-01, "std": 2.7845e-01, "min_value": 4.6746e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0501e+01, "total_time": 1.5505e+03, "__timestamp": "2024-10-10 18:28:59.994536"}, {"step": 158000, "num_env_steps": 158000, "scores": {"n": 1, "mean": 4.2534e+03}, "actor_loss": {"n": 1, "mean": -2.0295e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.0162e-01}, "critic_loss": {"n": 1, "mean": 1.2646e+01}, "entropy_coef": {"n": 1, "mean": 1.2528e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6384e-01, "std": 2.7704e-01, "min_value": 6.1253e-04, "max_value": 9.9965e-01}, "num_gradient_steps": 0, "step_time": 1.0597e+01, "total_time": 1.5611e+03, "__timestamp": "2024-10-10 18:29:10.592047"}, {"step": 159000, "num_env_steps": 159000, "scores": {"n": 1, "mean": 4.3573e+03}, "actor_loss": {"n": 1, "mean": -2.0854e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.1267e-01}, "critic_loss": {"n": 1, "mean": 1.0201e+01}, "entropy_coef": {"n": 1, "mean": 1.2596e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6447e-01, "std": 2.7537e-01, "min_value": 1.0424e-03, "max_value": 9.9975e-01}, "num_gradient_steps": 0, "step_time": 1.0540e+01, "total_time": 1.5717e+03, "__timestamp": "2024-10-10 18:29:21.131439"}, {"step": 160000, "num_env_steps": 160000, "scores": {"n": 1, "mean": 4.3736e+03}, "actor_loss": {"n": 1, "mean": -2.0567e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.0525e-01}, "critic_loss": {"n": 1, "mean": 9.9804e+00}, "entropy_coef": {"n": 1, "mean": 1.2803e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6395e-01, "std": 2.7612e-01, "min_value": 1.4399e-04, "max_value": 9.9961e-01}, "num_gradient_steps": 0, "step_time": 1.0522e+01, "total_time": 1.5822e+03, "__timestamp": "2024-10-10 18:29:31.654581"}, {"step": 161000, "num_env_steps": 161000, "scores": {"n": 1, "mean": 4.1338e+03}, "actor_loss": {"n": 1, "mean": -2.0411e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.8329e-01}, "critic_loss": {"n": 1, "mean": 9.9896e+00}, "entropy_coef": {"n": 1, "mean": 1.2649e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5870e-01, "std": 2.7688e-01, "min_value": 6.0052e-05, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0600e+01, "total_time": 1.5928e+03, "__timestamp": "2024-10-10 18:29:42.254331"}, {"step": 162000, "num_env_steps": 162000, "scores": {"n": 1, "mean": 4.1300e+03}, "actor_loss": {"n": 1, "mean": -2.0300e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.8888e-01}, "critic_loss": {"n": 1, "mean": 8.8863e+00}, "entropy_coef": {"n": 1, "mean": 1.2330e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6643e-01, "std": 2.7165e-01, "min_value": 3.8571e-04, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0447e+01, "total_time": 1.6032e+03, "__timestamp": "2024-10-10 18:29:52.701383"}, {"step": 163000, "num_env_steps": 163000, "scores": {"n": 1, "mean": 4.3577e+03}, "actor_loss": {"n": 1, "mean": -2.0468e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.0507e-02}, "critic_loss": {"n": 1, "mean": 1.3539e+01}, "entropy_coef": {"n": 1, "mean": 1.2887e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6779e-01, "std": 2.7470e-01, "min_value": 6.0320e-05, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.0540e+01, "total_time": 1.6138e+03, "__timestamp": "2024-10-10 18:30:03.240210"}, {"step": 164000, "num_env_steps": 164000, "scores": {"n": 1, "mean": 4.2391e+03}, "actor_loss": {"n": 1, "mean": -2.1540e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.3356e-01}, "critic_loss": {"n": 1, "mean": 2.3383e+02}, "entropy_coef": {"n": 1, "mean": 1.2752e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6215e-01, "std": 2.7773e-01, "min_value": 3.6120e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0642e+01, "total_time": 1.6244e+03, "__timestamp": "2024-10-10 18:30:13.882135"}, {"step": 165000, "num_env_steps": 165000, "scores": {"n": 1, "mean": 4.3880e+03}, "actor_loss": {"n": 1, "mean": -2.1058e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.0649e-01}, "critic_loss": {"n": 1, "mean": 9.8872e+01}, "entropy_coef": {"n": 1, "mean": 1.3202e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6496e-01, "std": 2.7676e-01, "min_value": 1.0715e-03, "max_value": 9.9955e-01}, "num_gradient_steps": 0, "step_time": 1.0647e+01, "total_time": 1.6351e+03, "__timestamp": "2024-10-10 18:30:24.529768"}, {"step": 166000, "num_env_steps": 166000, "scores": {"n": 1, "mean": 4.2203e+03}, "actor_loss": {"n": 1, "mean": -1.9984e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.0810e-01}, "critic_loss": {"n": 1, "mean": 9.9088e+00}, "entropy_coef": {"n": 1, "mean": 1.3107e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6453e-01, "std": 2.7421e-01, "min_value": 1.8774e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0532e+01, "total_time": 1.6456e+03, "__timestamp": "2024-10-10 18:30:35.061916"}, {"step": 167000, "num_env_steps": 167000, "scores": {"n": 1, "mean": 4.1835e+03}, "actor_loss": {"n": 1, "mean": -2.0398e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.3089e-01}, "critic_loss": {"n": 1, "mean": 9.9076e+00}, "entropy_coef": {"n": 1, "mean": 1.3128e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6118e-01, "std": 2.7843e-01, "min_value": 2.5448e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0469e+01, "total_time": 1.6561e+03, "__timestamp": "2024-10-10 18:30:45.530422"}, {"step": 168000, "num_env_steps": 168000, "scores": {"n": 1, "mean": 4.0535e+03}, "actor_loss": {"n": 1, "mean": -2.1033e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.2314e-01}, "critic_loss": {"n": 1, "mean": 1.0648e+01}, "entropy_coef": {"n": 1, "mean": 1.3068e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6486e-01, "std": 2.7617e-01, "min_value": 2.2203e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0549e+01, "total_time": 1.6666e+03, "__timestamp": "2024-10-10 18:30:56.078043"}, {"step": 169000, "num_env_steps": 169000, "scores": {"n": 1, "mean": 4.2979e+03}, "actor_loss": {"n": 1, "mean": -2.1460e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.6370e-01}, "critic_loss": {"n": 1, "mean": 1.3628e+01}, "entropy_coef": {"n": 1, "mean": 1.3378e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6925e-01, "std": 2.7474e-01, "min_value": 5.8169e-04, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.0578e+01, "total_time": 1.6772e+03, "__timestamp": "2024-10-10 18:31:06.656780"}, {"step": 170000, "num_env_steps": 170000, "scores": {"n": 1, "mean": 4.3008e+03}, "actor_loss": {"n": 1, "mean": -2.1951e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.6876e-01}, "critic_loss": {"n": 1, "mean": 2.4212e+02}, "entropy_coef": {"n": 1, "mean": 1.2960e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6546e-01, "std": 2.7586e-01, "min_value": 8.7358e-06, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0736e+01, "total_time": 1.6879e+03, "__timestamp": "2024-10-10 18:31:17.392077"}, {"step": 171000, "num_env_steps": 171000, "scores": {"n": 1, "mean": 4.1632e+03}, "actor_loss": {"n": 1, "mean": -2.1532e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.9239e-02}, "critic_loss": {"n": 1, "mean": 1.1947e+01}, "entropy_coef": {"n": 1, "mean": 1.3185e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6224e-01, "std": 2.7854e-01, "min_value": 3.2301e-04, "max_value": 9.9939e-01}, "num_gradient_steps": 0, "step_time": 1.1144e+01, "total_time": 1.6991e+03, "__timestamp": "2024-10-10 18:31:28.537499"}, {"step": 172000, "num_env_steps": 172000, "scores": {"n": 1, "mean": 4.3384e+03}, "actor_loss": {"n": 1, "mean": -2.0864e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.6538e-01}, "critic_loss": {"n": 1, "mean": 1.1088e+01}, "entropy_coef": {"n": 1, "mean": 1.3056e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6742e-01, "std": 2.7557e-01, "min_value": 2.5452e-04, "max_value": 9.9945e-01}, "num_gradient_steps": 0, "step_time": 1.0669e+01, "total_time": 1.7097e+03, "__timestamp": "2024-10-10 18:31:39.205686"}, {"step": 173000, "num_env_steps": 173000, "scores": {"n": 1, "mean": 4.3098e+03}, "actor_loss": {"n": 1, "mean": -2.1019e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.1346e-01}, "critic_loss": {"n": 1, "mean": 1.8804e+02}, "entropy_coef": {"n": 1, "mean": 1.3067e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6287e-01, "std": 2.7771e-01, "min_value": 9.3774e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.0753e+01, "total_time": 1.7205e+03, "__timestamp": "2024-10-10 18:31:49.958212"}, {"step": 174000, "num_env_steps": 174000, "scores": {"n": 1, "mean": 4.3349e+03}, "actor_loss": {"n": 1, "mean": -2.1775e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.6570e-01}, "critic_loss": {"n": 1, "mean": 1.3074e+01}, "entropy_coef": {"n": 1, "mean": 1.3490e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7288e-01, "std": 2.7024e-01, "min_value": 6.3241e-04, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0507e+01, "total_time": 1.7310e+03, "__timestamp": "2024-10-10 18:32:00.465142"}, {"step": 175000, "num_env_steps": 175000, "scores": {"n": 1, "mean": 4.3397e+03}, "actor_loss": {"n": 1, "mean": -2.1908e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.4321e-01}, "critic_loss": {"n": 1, "mean": 1.0419e+01}, "entropy_coef": {"n": 1, "mean": 1.3966e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7029e-01, "std": 2.6949e-01, "min_value": 4.7430e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0655e+01, "total_time": 1.7416e+03, "__timestamp": "2024-10-10 18:32:11.120743"}, {"step": 176000, "num_env_steps": 176000, "scores": {"n": 1, "mean": 4.3086e+03}, "actor_loss": {"n": 1, "mean": -2.1879e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.4805e-01}, "critic_loss": {"n": 1, "mean": 9.8874e+00}, "entropy_coef": {"n": 1, "mean": 1.3590e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6274e-01, "std": 2.7688e-01, "min_value": 1.7768e-04, "max_value": 9.9976e-01}, "num_gradient_steps": 0, "step_time": 1.0542e+01, "total_time": 1.7522e+03, "__timestamp": "2024-10-10 18:32:21.661360"}, {"step": 177000, "num_env_steps": 177000, "scores": {"n": 1, "mean": 4.2732e+03}, "actor_loss": {"n": 1, "mean": -2.1641e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.5125e-01}, "critic_loss": {"n": 1, "mean": 9.7938e+00}, "entropy_coef": {"n": 1, "mean": 1.3626e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6729e-01, "std": 2.7350e-01, "min_value": 6.1512e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0743e+01, "total_time": 1.7629e+03, "__timestamp": "2024-10-10 18:32:32.404669"}, {"step": 178000, "num_env_steps": 178000, "scores": {"n": 1, "mean": 4.4559e+03}, "actor_loss": {"n": 1, "mean": -2.2338e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.7907e-01}, "critic_loss": {"n": 1, "mean": 1.1263e+01}, "entropy_coef": {"n": 1, "mean": 1.3789e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6609e-01, "std": 2.7596e-01, "min_value": 1.4413e-03, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.1101e+01, "total_time": 1.7740e+03, "__timestamp": "2024-10-10 18:32:43.506951"}, {"step": 179000, "num_env_steps": 179000, "scores": {"n": 1, "mean": 4.4942e+03}, "actor_loss": {"n": 1, "mean": -2.1628e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.6523e-01}, "critic_loss": {"n": 1, "mean": 1.0550e+01}, "entropy_coef": {"n": 1, "mean": 1.3916e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7095e-01, "std": 2.7446e-01, "min_value": 2.5922e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.0695e+01, "total_time": 1.7847e+03, "__timestamp": "2024-10-10 18:32:54.202349"}, {"step": 180000, "num_env_steps": 180000, "scores": {"n": 1, "mean": 4.3930e+03}, "actor_loss": {"n": 1, "mean": -2.1316e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.9239e-01}, "critic_loss": {"n": 1, "mean": 2.5826e+02}, "entropy_coef": {"n": 1, "mean": 1.3478e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6391e-01, "std": 2.7060e-01, "min_value": 4.2880e-04, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.0555e+01, "total_time": 1.7953e+03, "__timestamp": "2024-10-10 18:33:04.756624"}, {"step": 181000, "num_env_steps": 181000, "scores": {"n": 1, "mean": 4.3088e+03}, "actor_loss": {"n": 1, "mean": -2.1714e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2649e-01}, "critic_loss": {"n": 1, "mean": 1.1313e+01}, "entropy_coef": {"n": 1, "mean": 1.3600e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6318e-01, "std": 2.7360e-01, "min_value": 8.5650e-04, "max_value": 9.9969e-01}, "num_gradient_steps": 0, "step_time": 1.1556e+01, "total_time": 1.8068e+03, "__timestamp": "2024-10-10 18:33:16.313602"}, {"step": 182000, "num_env_steps": 182000, "scores": {"n": 1, "mean": 4.3457e+03}, "actor_loss": {"n": 1, "mean": -2.1596e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.4561e-01}, "critic_loss": {"n": 1, "mean": 8.9642e+00}, "entropy_coef": {"n": 1, "mean": 1.4071e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6586e-01, "std": 2.7247e-01, "min_value": 3.8898e-04, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.1253e+01, "total_time": 1.8181e+03, "__timestamp": "2024-10-10 18:33:27.566994"}, {"step": 183000, "num_env_steps": 183000, "scores": {"n": 1, "mean": 4.3658e+03}, "actor_loss": {"n": 1, "mean": -2.1983e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.7010e-01}, "critic_loss": {"n": 1, "mean": 2.7430e+02}, "entropy_coef": {"n": 1, "mean": 1.3759e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6461e-01, "std": 2.7266e-01, "min_value": 9.0501e-04, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.1283e+01, "total_time": 1.8294e+03, "__timestamp": "2024-10-10 18:33:38.850046"}, {"step": 184000, "num_env_steps": 184000, "scores": {"n": 1, "mean": 4.2127e+03}, "actor_loss": {"n": 1, "mean": -2.1732e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.5270e-01}, "critic_loss": {"n": 1, "mean": 2.2998e+02}, "entropy_coef": {"n": 1, "mean": 1.3679e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6203e-01, "std": 2.7358e-01, "min_value": 3.6439e-04, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 1.1471e+01, "total_time": 1.8408e+03, "__timestamp": "2024-10-10 18:33:50.320676"}, {"step": 185000, "num_env_steps": 185000, "scores": {"n": 1, "mean": 4.3112e+03}, "actor_loss": {"n": 1, "mean": -2.2280e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.2735e-01}, "critic_loss": {"n": 1, "mean": 1.2112e+01}, "entropy_coef": {"n": 1, "mean": 1.3936e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6461e-01, "std": 2.7226e-01, "min_value": 6.3457e-04, "max_value": 9.9948e-01}, "num_gradient_steps": 0, "step_time": 1.1487e+01, "total_time": 1.8523e+03, "__timestamp": "2024-10-10 18:34:01.807050"}, {"step": 186000, "num_env_steps": 186000, "scores": {"n": 1, "mean": 4.2054e+03}, "actor_loss": {"n": 1, "mean": -2.1817e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.6402e-01}, "critic_loss": {"n": 1, "mean": 9.9555e+00}, "entropy_coef": {"n": 1, "mean": 1.3831e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6527e-01, "std": 2.7232e-01, "min_value": 6.5655e-05, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 1.1487e+01, "total_time": 1.8638e+03, "__timestamp": "2024-10-10 18:34:13.294621"}, {"step": 187000, "num_env_steps": 187000, "scores": {"n": 1, "mean": 4.4594e+03}, "actor_loss": {"n": 1, "mean": -2.2290e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.1511e-01}, "critic_loss": {"n": 1, "mean": 1.1099e+01}, "entropy_coef": {"n": 1, "mean": 1.3686e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6904e-01, "std": 2.7277e-01, "min_value": 6.9201e-05, "max_value": 9.9954e-01}, "num_gradient_steps": 0, "step_time": 1.1159e+01, "total_time": 1.8750e+03, "__timestamp": "2024-10-10 18:34:24.453453"}, {"step": 188000, "num_env_steps": 188000, "scores": {"n": 1, "mean": 4.2218e+03}, "actor_loss": {"n": 1, "mean": -2.2199e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.2144e-01}, "critic_loss": {"n": 1, "mean": 8.7479e+00}, "entropy_coef": {"n": 1, "mean": 1.3842e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6545e-01, "std": 2.7201e-01, "min_value": 4.0224e-04, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 1.1484e+01, "total_time": 1.8865e+03, "__timestamp": "2024-10-10 18:34:35.937112"}]}}