{
    "experiment_id": "2024-09-26_12-29-23_406534~fgHQUf",
    "experiment_tags": [
        "SACDebug",
        "HalfCheetah-v4"
    ],
    "start_time": "2024-09-26 12:29:23.407535",
    "end_time": "2024-09-26 12:32:18.253896",
    "end_exception": "Traceback (most recent call last):\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\experiment_logging\\experiment_logger.py\", line 149, in log_experiment\n    yield experiment_logger\n  File \"C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_14620\\588464061.py\", line 332, in <module>\n    algo.learn(1_000_000)\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\reinforcement_learning\\algorithms\\base\\base_algorithm.py\", line 138, in learn\n    self.callback.on_optimization_done(self, step, info)\n  File \"C:\\Users\\domin\\Git\\pytorch-repository\\src\\reinforcement_learning\\core\\callback.py\", line 42, in on_optimization_done\n    self._on_optimization_done(optim_algo, step, info, scheduler_values)\n  File \"C:\\Users\\domin\\AppData\\Local\\Temp\\ipykernel_14620\\588464061.py\", line 256, in on_optimization_done\n    raise ValueError('Score too low, policy probably fucked :(')\nValueError: Score too low, policy probably fucked :(\n",
    "model_db_reference": null,
    "hyper_parameters": {
        "_type": "SACDebug",
        "_type_fq": "__main__.SACDebug",
        "env": "<RescaleAction<TransformRewardWrapper<AsyncVectorEnv instance>>>",
        "num_envs": 16,
        "policy": "SACPolicy(\n  (actor): DebugActor(\n    (feature_extractor): IdentityExtractor()\n    (network): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (action_selector): PredictedStdActionSelector(\n      (action_net): Linear(in_features=256, out_features=6, bias=True)\n      (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n    )\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (critic_target): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)",
        "policy_parameter_count": 362256,
        "policy_repr": "SACPolicy(\n  (actor): DebugActor(\n    (feature_extractor): IdentityExtractor()\n    (network): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (action_selector): PredictedStdActionSelector(\n      (action_net): Linear(in_features=256, out_features=6, bias=True)\n      (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n    )\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (critic_target): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)",
        "buffer": {
            "_type": "ReplayBuffer",
            "_type_fq": "src.reinforcement_learning.core.buffers.replay.replay_buffer.ReplayBuffer",
            "buffer_size": 50000,
            "num_envs": 16,
            "total_buffer_size": 800000,
            "torch_device": "cuda:0",
            "torch_dtype": "torch.float32",
            "np_dtype": "<class 'numpy.float32'>",
            "optimize_memory_usage": false
        },
        "buffer_step_size": 50000,
        "buffer_total_size": 800000,
        "gamma": 0.99,
        "sde_noise_sample_freq": null,
        "torch_device": "cuda:0",
        "torch_dtype": "torch.float32",
        "tau": 0.005,
        "rollout_steps": 1,
        "gradient_steps": 1,
        "optimization_batch_size": 256,
        "action_noise": null,
        "warmup_steps": 100,
        "actor_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "critic_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "entropy_coef_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object at 0x00007FF81EAB62F0>",
        "weigh_and_reduce_actor_loss": "<function <lambda> at 0x0000018DC4A86660>",
        "weigh_critic_loss": "<function <lambda> at 0x0000018DC4A865C0>",
        "target_update_interval": 1,
        "target_entropy": -6.0,
        "entropy_coef": "dynamic"
    },
    "system_info": {
        "platform": "Windows",
        "platform_release": "10",
        "architecture": "AMD64",
        "processor": {
            "name": "AMD Ryzen 9 3900X 12-Core Processor",
            "cores": 12,
            "logical_cores": 24,
            "speed": "3793 MHz"
        },
        "gpu": [
            {
                "name": "NVIDIA GeForce RTX 3070",
                "video_processor": "NVIDIA GeForce RTX 3070",
                "adapter_ram": "-1 MB",
                "adapter_dac_type": "Integrated RAMDAC",
                "manufacturer": "NVIDIA",
                "memory": "8192 MB",
                "memory_clock": "6800 MHz",
                "compute_capability": "8.6",
            }
        ],
        "ram_speed": "2666 MHz",
        "ram": "64 GB"
    },
    "setup": {
        "sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n",
        "notebook": "from stable_baselines3.common.torch_layers import FlattenExtractor\nfrom dataclasses import dataclass\nfrom typing import Type, Optional, Any, Literal\n\nimport gymnasium\nimport numpy as np\nimport stable_baselines3 as sb\nimport torch\nimport torch.nn.functional as F\nfrom stable_baselines3.common.policies import ContinuousCritic\nfrom torch import optim\n\nfrom src.function_types import TorchTensorFn\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.algorithms.base.base_algorithm import PolicyProvider\nfrom src.reinforcement_learning.algorithms.base.off_policy_algorithm import OffPolicyAlgorithm, ReplayBuf\nfrom src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\nfrom src.reinforcement_learning.core.action_noise import ActionNoise\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import BaseReplayBuffer, ReplayBufferSamples\nfrom src.reinforcement_learning.core.buffers.replay.replay_buffer import ReplayBuffer\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.core.infos import InfoDict, concat_infos\nfrom src.reinforcement_learning.core.logging import LoggingConfig, log_if_enabled\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.core.polyak_update import polyak_update\nfrom src.reinforcement_learning.core.type_aliases import OptimizerProvider, TensorObs, detach_obs\nfrom src.reinforcement_learning.gym.env_analysis import get_single_action_space\nfrom src.torch_device import TorchDevice\nfrom src.torch_functions import identity\n\nSAC_DEFAULT_OPTIMIZER_PROVIDER = lambda params: optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nAUTO_TARGET_ENTROPY = 'auto'\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.policies.components.base_component import BasePolicyComponent\nfrom src.reinforcement_learning.core.policies.components.feature_extractors import FeatureExtractor, IdentityExtractor\nfrom src.reinforcement_learning.core.type_aliases import TensorObs\n\n\nclass DebugActor(BasePolicyComponent):\n\n    action_selector: ActionSelector\n    uses_sde: bool\n\n    def __init__(\n            self,\n            network: nn.Module,\n            action_selector: ActionSelector,\n            # feature_extractor: Optional[FeatureExtractor] = None\n    ):\n        super().__init__(IdentityExtractor())\n        self.network = network\n        self.replace_action_selector(action_selector, copy_action_net_weights=False)\n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return self.update_hps(super().collect_hyper_parameters(), {\n            'network': self.get_hps_or_str(self.network),\n            'action_selector': self.get_hps_or_str(self.action_selector),\n        })\n\n    def forward(self, obs: TensorObs, deterministic: bool = False) -> torch.Tensor:\n        obs = self.feature_extractor(obs)\n        latent_pi = self.network(obs)\n        # different\n        return self.action_selector.update_latent_features(latent_pi).get_actions()\n\n    def action_log_prob(self, obs: TensorObs) -> tuple[torch.Tensor, torch.Tensor]:\n        # different\n        actions = self(obs)\n        # actions = action_selector.get_actions()\n        actions_log_prob = self.action_selector.log_prob(actions)\n        return actions, actions_log_prob\n\n    def replace_action_selector(self, new_action_selector: ActionSelector, copy_action_net_weights: bool) -> None:\n        if copy_action_net_weights:\n            new_action_selector.action_net.load_state_dict(self.action_selector.action_net.state_dict())\n        self.action_selector = new_action_selector\n        self.uses_sde = isinstance(self.action_selector, StateDependentNoiseActionSelector)\n\n    def reset_sde_noise(self, batch_size: int = 1) -> None:\n        if self.uses_sde:\n            self.action_selector: StateDependentNoiseActionSelector\n\n            self.action_selector.sample_exploration_noise(batch_size)\n    \n    def set_training_mode(self, mode: bool):\n        self.set_train_mode(mode)\n\n\n\n@dataclass\nclass SACLoggingConfig(LoggingConfig):\n\n    log_entropy_coef: bool = False\n    entropy_coef_loss: LossLoggingConfig = None\n    actor_loss: LossLoggingConfig = None\n    critic_loss: LossLoggingConfig = None\n\n    def __post_init__(self):\n        if self.actor_loss is None:\n            self.actor_loss = LossLoggingConfig()\n        if self.entropy_coef_loss is None:\n            self.entropy_loss = LossLoggingConfig()\n        if self.critic_loss is None:\n            self.critic_loss = LossLoggingConfig()\n\n        super().__post_init__()\n\n\"\"\"\n\n        Soft Actor-Critic:\n        Off-Policy Maximum Entropy Deep Reinforcement\n        Learning with a Stochastic Actor\n        https://arxiv.org/pdf/1801.01290\n\n\"\"\"\nclass SACDebug(OffPolicyAlgorithm[sb.sac.sac.SACPolicy, ReplayBuf, SACLoggingConfig]):\n    \n    policy: sb.sac.sac.SACPolicy\n    actor: sb.sac.sac.Actor\n    critic: ContinuousCritic\n    buffer: BaseReplayBuffer\n    target_entropy: float\n    log_ent_coef: Optional[torch.Tensor]\n    entropy_coef_optimizer: Optional[optim.Optimizer]\n    entropy_coef_tensor: Optional[torch.Tensor]\n\n    def __init__(\n            self,\n            env: gymnasium.Env,\n            policy: sb.sac.sac.SACPolicy | PolicyProvider[sb.sac.sac.SACPolicy],\n            actor_optimizer_provider: OptimizerProvider = SAC_DEFAULT_OPTIMIZER_PROVIDER,\n            critic_optimizer_provider: OptimizerProvider = SAC_DEFAULT_OPTIMIZER_PROVIDER,\n            weigh_and_reduce_actor_loss: TorchTensorFn = torch.mean,\n            weigh_critic_loss: TorchTensorFn = identity,\n            buffer_type: Type[ReplayBuf] = ReplayBuffer,\n            buffer_size: int = 100_000,\n            buffer_kwargs: dict[str, Any] = None,\n            gamma: float = 0.99,\n            tau: float = 0.005,\n            rollout_steps: int = 100,\n            gradient_steps: int = 1,\n            optimization_batch_size: int = 256,\n            target_update_interval: int = 1,\n            entropy_coef: float = 1.0,\n            target_entropy: float | Literal['auto'] = AUTO_TARGET_ENTROPY,\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider] = None,\n            weigh_and_reduce_entropy_coef_loss: TorchTensorFn = torch.mean,\n            action_noise: Optional[ActionNoise] = None,\n            warmup_steps: int = 100,\n            learning_starts: int = 100,\n            sde_noise_sample_freq: Optional[int] = None,\n            callback: Callback['SAC'] = None,\n            logging_config: SACLoggingConfig = None,\n            torch_device: TorchDevice = 'auto',\n            torch_dtype: torch.dtype = torch.float32,\n    ):\n        super().__init__(\n            env=env,\n            policy=policy,\n            buffer=buffer_type.for_env(env, buffer_size, torch_device, torch_dtype, **(buffer_kwargs or {})),\n            gamma=gamma,\n            tau=tau,\n            rollout_steps=rollout_steps,\n            gradient_steps=gradient_steps,\n            optimization_batch_size=optimization_batch_size,\n            action_noise=action_noise,\n            warmup_steps=warmup_steps,\n            learning_starts=learning_starts,\n            sde_noise_sample_freq=sde_noise_sample_freq,\n            callback=callback or Callback(),\n            logging_config=logging_config or LoggingConfig(),\n            torch_device=torch_device,\n            torch_dtype=torch_dtype,\n        )\n\n        self.actor = self.policy.actor\n        self.critic = self.policy.critic\n        # self.shared_feature_extractor = self.policy.shared_feature_extractor\n\n        self.actor_optimizer = actor_optimizer_provider(\n            # self.chain_parameters(self.actor, self.shared_feature_extractor)\n            self.actor.parameters()\n        )\n        self.critic_optimizer = critic_optimizer_provider(self.critic.parameters())\n\n        self.weigh_and_reduce_entropy_coef_loss = weigh_and_reduce_entropy_coef_loss\n        self.weigh_and_reduce_actor_loss = weigh_and_reduce_actor_loss\n        self.weigh_critic_loss = weigh_critic_loss\n\n        self.target_update_interval = target_update_interval\n        self.gradient_steps_performed = 0\n\n        self._setup_entropy_optimization(entropy_coef, target_entropy, entropy_coef_optimizer_provider)\n\n        # CrossQ doesn't use a target critic\n        if isinstance(self.policy, SACCrossQPolicy):\n            self.tau = 0\n            self.target_update_interval = 0\n\n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return self.update_hps(super().collect_hyper_parameters(), {\n            'actor_optimizer': str(self.actor_optimizer),\n            'critic_optimizer': str(self.critic_optimizer),\n            'entropy_coef_optimizer': str(self.entropy_coef_optimizer),\n            'weigh_and_reduce_entropy_coef_loss': str(self.weigh_and_reduce_entropy_coef_loss),\n            'weigh_and_reduce_actor_loss': str(self.weigh_and_reduce_actor_loss),\n            'weigh_critic_loss': str(self.weigh_critic_loss),\n            'target_update_interval': self.target_update_interval,\n            'target_entropy': self.target_entropy,\n            'entropy_coef': self.entropy_coef_tensor.item() if self.entropy_coef_tensor is not None else 'dynamic',\n        })\n\n    def _setup_entropy_optimization(\n            self,\n            entropy_coef: float,\n            target_entropy: float | Literal['auto'],\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider],\n    ):\n        if target_entropy == 'auto':\n            self.target_entropy = float(-np.prod(get_single_action_space(self.env).shape).astype(np.float32))\n        else:\n            self.target_entropy = float(target_entropy)\n\n        if entropy_coef_optimizer_provider is not None:\n            self.log_ent_coef = torch.log(\n                torch.tensor([entropy_coef], device=self.torch_device, dtype=self.torch_dtype)\n            ).requires_grad_(True)\n            self.entropy_coef_optimizer = entropy_coef_optimizer_provider([self.log_ent_coef])\n            self.entropy_coef_tensor = None\n        else:\n            self.log_ent_coef = None\n            self.entropy_coef_optimizer = None\n            self.entropy_coef_tensor = torch.tensor(entropy_coef, device=self.torch_device, dtype=self.torch_dtype)\n\n    def get_and_optimize_entropy_coef(\n            self,\n            actions_pi_log_prob: torch.Tensor,\n            info: InfoDict\n    ) -> torch.Tensor:\n        if self.entropy_coef_optimizer is not None:\n            entropy_coef = torch.exp(self.log_ent_coef.detach())\n\n            # TODO!\n            # entropy_coef_loss = weigh_and_reduce_loss(\n            #     raw_loss=-self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach(),\n            #     weigh_and_reduce_function=self.weigh_and_reduce_entropy_coef_loss,\n            #     info=info,\n            #     loss_name='entropy_coef_loss',\n            #     logging_config=self.logging_config.entropy_coef_loss\n            # )\n\n            entropy_coef_loss = -(self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach()).mean()\n            info['final_entropy_coef_loss'] = entropy_coef_loss.detach()\n\n            self.entropy_coef_optimizer.zero_grad()\n            entropy_coef_loss.backward()\n            self.entropy_coef_optimizer.step()\n\n            return entropy_coef\n        else:\n            return self.entropy_coef_tensor\n\n    def calculate_critic_loss(\n            self,\n            observation_features: TensorObs,\n            replay_samples: ReplayBufferSamples,\n            entropy_coef: torch.Tensor,\n            info: InfoDict,\n    ):\n        with torch.no_grad():\n                # Select action according to policy\n            next_actions, next_log_prob = self.actor.action_log_prob(replay_samples.next_observations)\n            # Compute the next Q values: min over all critics targets\n            next_q_values = torch.cat(self.policy.critic_target(replay_samples.next_observations, next_actions), dim=1)\n            next_q_values, _ = torch.min(next_q_values, dim=1, keepdim=True)\n            # add entropy term\n            next_q_values = next_q_values - entropy_coef * next_log_prob.reshape(-1, 1)\n            # td error + entropy term\n            target_q_values = replay_samples.rewards + (1 - replay_samples.dones) * self.gamma * next_q_values\n\n        # target_q_values = self.policy.compute_target_values(\n        #     replay_samples=replay_samples,\n        #     entropy_coef=entropy_coef,\n        #     gamma=self.gamma,\n        # )\n        # critic loss should not influence shared feature extractor\n        current_q_values = self.critic(detach_obs(observation_features), replay_samples.actions)\n\n        # noinspection PyTypeChecker\n        critic_loss: torch.Tensor = 0.5 * sum(\n            F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n        )\n        # TODO!\n        # critic_loss = weigh_and_reduce_loss(\n        #     raw_loss=critic_loss,\n        #     weigh_and_reduce_function=self.weigh_critic_loss,\n        #     info=info,\n        #     loss_name='critic_loss',\n        #     logging_config=self.logging_config.critic_loss,\n        # )\n\n        info['final_critic_loss'] = critic_loss.detach()\n        return critic_loss\n\n    def calculate_actor_loss(\n            self,\n            observation_features: TensorObs,\n            actions_pi: torch.Tensor,\n            actions_pi_log_prob: torch.Tensor,\n            entropy_coef: torch.Tensor,\n            info: InfoDict,\n    ) -> torch.Tensor:\n        q_values_pi = torch.cat(self.critic(observation_features, actions_pi), dim=-1)\n        min_q_values_pi, _ = torch.min(q_values_pi, dim=-1, keepdim=True)\n        actor_loss = (entropy_coef * actions_pi_log_prob - min_q_values_pi).mean()  # TODO!\n\n        # TODO!\n        # actor_loss = weigh_and_reduce_loss(\n        #     raw_loss=actor_loss,\n        #     weigh_and_reduce_function=self.weigh_and_reduce_actor_loss,\n        #     info=info,\n        #     loss_name='actor_loss',\n        #     logging_config=self.logging_config.actor_loss,\n        # )\n\n        info['final_actor_loss'] = actor_loss.detach()\n        return actor_loss\n\n    def optimize(self, last_obs: np.ndarray, last_episode_starts: np.ndarray, info: InfoDict) -> None:\n        gradient_step_infos: list[InfoDict] = []\n\n        for gradient_step in range(self.gradient_steps):\n            step_info: InfoDict = {}\n            replay_samples = self.buffer.sample(self.optimization_batch_size)\n\n            # self.actor.reset_sde_noise()  # TODO: set batch size?\n\n            # observation_features = self.shared_feature_extractor(replay_samples.observations)\n            # observation_features = replay_samples.observations\n            actions_pi, actions_pi_log_prob = self.actor.action_log_prob(replay_samples.observations)\n            actions_pi_log_prob = actions_pi_log_prob.reshape(-1, 1)\n\n            entropy_coef = self.get_and_optimize_entropy_coef(actions_pi_log_prob, step_info)\n            log_if_enabled(step_info, 'entropy_coef', entropy_coef, self.logging_config.log_entropy_coef)\n\n            critic_loss = self.calculate_critic_loss(\n                observation_features=replay_samples.observations,\n                replay_samples=replay_samples,\n                entropy_coef=entropy_coef,\n                info=step_info\n            )\n\n            self.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic_optimizer.step()\n\n            actor_loss = self.calculate_actor_loss(\n                observation_features=replay_samples.observations,\n                actions_pi=actions_pi,\n                actions_pi_log_prob=actions_pi_log_prob,\n                entropy_coef=entropy_coef,\n                info=step_info\n            )\n\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            self.gradient_steps_performed += 1\n            if self.target_update_interval > 0 and self.gradient_steps_performed % self.target_update_interval == 0:\n                # self.policy.perform_polyak_update(self.tau)\n                polyak_update(self.critic.parameters(), self.policy.critic_target.parameters(), self.tau)\n            gradient_step_infos.append(step_info)\n        info.update(concat_infos(gradient_step_infos))\n\n\nfrom sac import init_policy, init_action_selector\nfrom stable_baselines3.common.env_util import make_vec_env\nimport stable_baselines3 as sb\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nimport gymnasium\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 16\n\ndef create_env(render_mode: str | None):\n    return gymnasium.make(env_name, render_mode=render_mode, **env_kwargs)\n\nenv = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n\n# env = make_vec_env(\"HalfCheetah-v4\", n_envs=16)\n\nsb_sac = sb.sac.SAC(\"MlpPolicy\", make_vec_env(\"HalfCheetah-v4\", n_envs=16), verbose=1)\n# model.learn(total_timesteps=10_000_000, log_interval=16)\n\ndebug_actor = DebugActor(\n    nn.Sequential(\n        nn.Linear(17, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    ),\n    init_action_selector(256, 6, {})\n)\n\nsb_sac.policy.actor = debug_actor\n\n\nimport inspect\nimport time\n\nfrom gymnasium import Env\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.module_analysis import count_parameters\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import PolicyConstruction\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom typing import Any\nfrom src.reinforcement_learning.core.callback import Callback\n\nimport torch\nfrom torch import optim\nimport gymnasium as gym\nimport numpy as np\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n\\n' + _ih[2] + '\\n\\n\\n' + _ih[3] + '\\n\\n\\n' + _ih[-1],\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    # if parent_policy_id is not None:\n    #     model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n    #     steps_trained = model_entry['model_info']['steps_trained']\n    #     print(f'Loading state dict from policy {parent_policy_id}')\n    # \n    # print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    # return policy_id, policy, optimizer, wrapped_env, steps_trained\n    return policy_id, sb_sac.policy, None, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SACDebug, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    \n    if len(episode_scores) > 0:\n    \n        global best_iteration_score\n        iteration_score = episode_scores.mean()\n        score_moving_average = score_mean_ema.update(iteration_score)\n        # if iteration_score >= best_iteration_score:\n        #     best_iteration_score = iteration_score\n        #     policy_db.save_model_state_dict(\n        #         model_id=policy_id,\n        #         parent_model_id=parent_policy_id,\n        #         model_info={\n        #             'score': iteration_score.item(),\n        #             'steps_trained': steps_trained,\n        #             'wrap_env_source_code': wrap_env_source_code_source,\n        #             'init_policy_source_code': init_policy_source\n        #         },\n        #         model=policy,\n        #         optimizer=optimizer,\n        #     )\n        info['score_moving_average'] = score_moving_average\n    \n    info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SACDebug, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    total_step = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{total_step = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'total_step': total_step,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info['final_entropy_coef_loss']),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'gradient_step': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment()\n    print()\n    \n    if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n        logger.save_experiment()\n        raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\n# def create_env(render_mode: str | None):\n#     return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n# \n# wrap_env_source_code_source = inspect.getsource(wrap_env)\n# init_policy_source = inspect.getsource(init_policy)\n\n# env_name = 'HalfCheetah-v4'\n# # env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# # env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# # env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n# env_kwargs = {}\n# num_envs = 16\n#     \n# # policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n# policy_db = DummyModelDB[MitosisPolicyInfo]()\n# print(f'{policy_db = }')\n# \n# parent_policy_id=None  # '2024-04-28_20.57.23'\n# \n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n\nlogger = ExperimentLogger('experiment_logs/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n    # print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = } \\n\\n')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=50_000,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=100,\n            learning_starts=100,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=[type(algo).__name__, env_name],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(1_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    # policy_db.close()\n    # print('model db closed')\n    \n\nprint('done')"
    },
    "notes": [],
    "logs_by_category": {
        "__default": [
            {
                "step": 1000,
                "total_step": 16000,
                "scores": null,
                "actor_loss": {
                    "n": 1,
                    "mean": -16.264698028564453
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -2.7204525470733643
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 0.9753386974334717
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.7630426287651062
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.520125687122345,
                    "std": 0.28680619597435,
                    "min_value": 2.2314488887786865e-06,
                    "max_value": 0.999971866607666
                },
                "gradient_step": 901,
                "step_time": 10.504863500595093,
                "total_time": 10.489858627319336,
                "__timestamp": "2024-09-26 12:29:33.903395"
            },
            {
                "step": 2000,
                "total_step": 32000,
                "scores": {
                    "n": 16,
                    "mean": -233.20484864195623,
                    "std": 55.08936195638194,
                    "min_value": -328.4260790218541,
                    "max_value": -121.79166738130152
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -24.314010620117188
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -5.621936798095703
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 1.4615695476531982
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.5656549334526062
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5341154932975769,
                    "std": 0.2877594828605652,
                    "min_value": 2.454407513141632e-05,
                    "max_value": 0.9994344115257263
                },
                "gradient_step": 1901,
                "step_time": 11.8663170337677,
                "total_time": 22.356175661087036,
                "__timestamp": "2024-09-26 12:29:45.763710"
            },
            {
                "step": 3000,
                "total_step": 48000,
                "scores": {
                    "n": 16,
                    "mean": -226.8600300778113,
                    "std": 65.10404944095413,
                    "min_value": -354.17803715367336,
                    "max_value": -132.71046399869374
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -29.07868194580078
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -8.598958969116211
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 4.892046928405762
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.4194222688674927
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5356403589248657,
                    "std": 0.28827691078186035,
                    "min_value": 1.5139579772949219e-05,
                    "max_value": 0.9993141293525696
                },
                "gradient_step": 2901,
                "step_time": 10.790411472320557,
                "total_time": 33.14658713340759,
                "__timestamp": "2024-09-26 12:29:56.554123"
            },
            {
                "step": 4000,
                "total_step": 64000,
                "scores": {
                    "n": 16,
                    "mean": -207.67958714863198,
                    "std": 61.79281426200716,
                    "min_value": -301.1656518630334,
                    "max_value": -65.91712216392625
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -31.375314712524414
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -11.459393501281738
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 1.5565519332885742
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.3110429048538208
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5389848947525024,
                    "std": 0.28938308358192444,
                    "min_value": 1.0058283805847168e-06,
                    "max_value": 0.9997962713241577
                },
                "gradient_step": 3901,
                "step_time": 12.613493204116821,
                "total_time": 45.760080337524414,
                "__timestamp": "2024-09-26 12:30:09.167615"
            },
            {
                "step": 5000,
                "total_step": 80000,
                "scores": {
                    "n": 16,
                    "mean": -215.43099503953943,
                    "std": 56.65283888847522,
                    "min_value": -291.6687968184706,
                    "max_value": -87.45322056440637
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -32.5859375
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -14.126855850219727
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 1.6553943157196045
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.23124344646930695
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5435582399368286,
                    "std": 0.2899409830570221,
                    "min_value": 1.3656914234161377e-05,
                    "max_value": 0.9997202157974243
                },
                "gradient_step": 4901,
                "step_time": 12.354151010513306,
                "total_time": 58.11423134803772,
                "__timestamp": "2024-09-26 12:30:21.521766"
            },
            {
                "step": 6000,
                "total_step": 96000,
                "scores": {
                    "n": 16,
                    "mean": -230.63196549021484,
                    "std": 38.18402246517351,
                    "min_value": -301.9775406018307,
                    "max_value": -162.03163358313031
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -32.33973693847656
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -16.24671173095703
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 1.6045355796813965
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.17233996093273163
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5487431287765503,
                    "std": 0.2903248369693756,
                    "min_value": 1.621246337890625e-05,
                    "max_value": 0.9998010396957397
                },
                "gradient_step": 5901,
                "step_time": 11.697853803634644,
                "total_time": 69.81208515167236,
                "__timestamp": "2024-09-26 12:30:33.219620"
            },
            {
                "step": 7000,
                "total_step": 112000,
                "scores": {
                    "n": 16,
                    "mean": -189.39974404773238,
                    "std": 28.077720733865483,
                    "min_value": -233.29859917238355,
                    "max_value": -144.51856900320854
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -31.694229125976562
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -18.089828491210938
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 4.807023048400879
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.12884201109409332
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5545427203178406,
                    "std": 0.2917247414588928,
                    "min_value": 3.874301910400391e-06,
                    "max_value": 0.9999313354492188
                },
                "gradient_step": 6901,
                "step_time": 10.487305641174316,
                "total_time": 80.29939079284668,
                "__timestamp": "2024-09-26 12:30:43.706925"
            },
            {
                "step": 8000,
                "total_step": 128000,
                "scores": {
                    "n": 16,
                    "mean": -173.3247340720029,
                    "std": 40.024167523176686,
                    "min_value": -260.7558105264907,
                    "max_value": -121.4742321758531
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -29.828533172607422
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -19.630138397216797
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 4.013699531555176
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.09670834988355637
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5698160529136658,
                    "std": 0.2922225594520569,
                    "min_value": 2.8371810913085938e-05,
                    "max_value": 0.9998348355293274
                },
                "gradient_step": 7901,
                "step_time": 10.422259092330933,
                "total_time": 90.72164988517761,
                "__timestamp": "2024-09-26 12:30:54.129184"
            },
            {
                "step": 9000,
                "total_step": 144000,
                "scores": {
                    "n": 16,
                    "mean": -120.3927321004021,
                    "std": 28.293744182376095,
                    "min_value": -167.4857034968445,
                    "max_value": -62.11651346052531
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -28.160743713378906
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -20.127588272094727
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 0.968600869178772
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.07287843525409698
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.579048752784729,
                    "std": 0.29406288266181946,
                    "min_value": 1.6808509826660156e-05,
                    "max_value": 0.999539852142334
                },
                "gradient_step": 8901,
                "step_time": 10.467606782913208,
                "total_time": 101.18925666809082,
                "__timestamp": "2024-09-26 12:31:04.596791"
            },
            {
                "step": 10000,
                "total_step": 160000,
                "scores": {
                    "n": 16,
                    "mean": -116.63143018828077,
                    "std": 44.61820806935271,
                    "min_value": -237.92555271391757,
                    "max_value": -69.4090676450287
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -26.89780616760254
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -21.329689025878906
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 1.0176544189453125
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.0549883171916008
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5853468775749207,
                    "std": 0.2945549488067627,
                    "min_value": 2.9802322387695312e-08,
                    "max_value": 0.9997224807739258
                },
                "gradient_step": 9901,
                "step_time": 10.478988409042358,
                "total_time": 111.66824507713318,
                "__timestamp": "2024-09-26 12:31:15.075780"
            },
            {
                "step": 11000,
                "total_step": 176000,
                "scores": {
                    "n": 16,
                    "mean": -81.34873622662508,
                    "std": 30.44415142968282,
                    "min_value": -179.11182903731242,
                    "max_value": -37.30760940871551
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -24.85283088684082
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -23.23830795288086
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 0.9836857914924622
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.04171565920114517
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5985201597213745,
                    "std": 0.29537490010261536,
                    "min_value": 1.2189149856567383e-05,
                    "max_value": 0.9998583793640137
                },
                "gradient_step": 10901,
                "step_time": 10.541826963424683,
                "total_time": 122.21007204055786,
                "__timestamp": "2024-09-26 12:31:25.617606"
            },
            {
                "step": 12000,
                "total_step": 192000,
                "scores": {
                    "n": 16,
                    "mean": 4.0322542870764835,
                    "std": 33.163215214655075,
                    "min_value": -59.548125792032806,
                    "max_value": 92.27545605202613
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -23.6212158203125
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -20.013824462890625
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 1.2995824813842773
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.03194147348403931
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.6220305562019348,
                    "std": 0.2960091829299927,
                    "min_value": 1.4722347259521484e-05,
                    "max_value": 0.9998772144317627
                },
                "gradient_step": 11901,
                "step_time": 10.556037902832031,
                "total_time": 132.7661099433899,
                "__timestamp": "2024-09-26 12:31:36.173644"
            },
            {
                "step": 13000,
                "total_step": 208000,
                "scores": {
                    "n": 16,
                    "mean": 142.6509911935923,
                    "std": 65.98993245292397,
                    "min_value": -83.28649344260339,
                    "max_value": 224.15709371923003
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -21.4552059173584
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -16.85155487060547
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 0.7555778622627258
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.02468331716954708
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.6662716269493103,
                    "std": 0.2930387854576111,
                    "min_value": 9.000301361083984e-06,
                    "max_value": 0.9999313950538635
                },
                "gradient_step": 12901,
                "step_time": 10.57734203338623,
                "total_time": 143.34345197677612,
                "__timestamp": "2024-09-26 12:31:46.750987"
            },
            {
                "step": 14000,
                "total_step": 224000,
                "scores": {
                    "n": 16,
                    "mean": 224.06168114635182,
                    "std": 44.38164410126034,
                    "min_value": 121.80417643976398,
                    "max_value": 298.1438652665238
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -20.531347274780273
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -11.653247833251953
                },
                "critic_loss": {
                    "n": 1,
                    "mean": 0.9917443990707397
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.019614221528172493
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.7138251662254333,
                    "std": 0.28576916456222534,
                    "min_value": 4.410743713378906e-06,
                    "max_value": 0.9999358057975769
                },
                "gradient_step": 13901,
                "step_time": 10.478167295455933,
                "total_time": 153.82161927223206,
                "__timestamp": "2024-09-26 12:31:57.229154"
            },
            {
                "step": 15000,
                "total_step": 240000,
                "scores": {
                    "n": 16,
                    "mean": -288.70244304822245,
                    "std": 19.402702298169547,
                    "min_value": -331.16637422610074,
                    "max_value": -258.3173593012616
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -1.2786773184417643e+23
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -2.929799746405202e+25
                },
                "critic_loss": {
                    "n": 1,
                    "mean": Infinity
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.01762523502111435
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.8889577388763428,
                    "std": 0.22721558809280396,
                    "min_value": 0.00012105703353881836,
                    "max_value": 1.0
                },
                "gradient_step": 14901,
                "step_time": 10.505791425704956,
                "total_time": 164.327410697937,
                "__timestamp": "2024-09-26 12:32:07.734945"
            },
            {
                "step": 16000,
                "total_step": 256000,
                "scores": {
                    "n": 16,
                    "mean": -600.4474636726081,
                    "std": 1.1339551234735648,
                    "min_value": -603.5201342105865,
                    "max_value": -598.6546223759651
                },
                "actor_loss": {
                    "n": 1,
                    "mean": -2.345418841299175e+23
                },
                "entropy_coef_loss": {
                    "n": 1,
                    "mean": -5.37399633376763e+25
                },
                "critic_loss": {
                    "n": 1,
                    "mean": Infinity
                },
                "entropy_coef": {
                    "n": 1,
                    "mean": 0.01762523502111435
                },
                "action_stds": null,
                "action_magnitude": {
                    "n": 96000,
                    "mean": 1.0,
                    "std": 0.0,
                    "min_value": 1.0,
                    "max_value": 1.0
                },
                "gradient_step": 15901,
                "step_time": 10.51427674293518,
                "total_time": 174.8416874408722,
                "__timestamp": "2024-09-26 12:32:18.249222"
            }
        ]
    }
}