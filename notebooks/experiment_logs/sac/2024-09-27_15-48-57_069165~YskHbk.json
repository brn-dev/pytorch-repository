{"experiment_id": "2024-09-27_15-48-57_069165~YskHbk", "experiment_tags": ["SAC", "HalfCheetah-v4"], "start_time": "2024-09-27 15:48:57.070165", "end_time": "2024-09-27 15:52:27.050604", "end_exception": null, "model_db_reference": null, "hyper_parameters": {"_type": "SAC", "_type_fq": "src.reinforcement_learning.algorithms.sac.sac.SAC", "env": "<RescaleAction<TransformRewardWrapper<AsyncVectorEnv instance>>>", "env_specs": [{"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}, {"id": "HalfCheetah-v4", "kwargs": {"render_mode": null}, "max_episode_steps": 1000, "additional_wrappers": []}], "num_envs": 16, "policy": {"_type": "SACPolicy", "_type_fq": "src.reinforcement_learning.algorithms.sac.sac_policy.SACPolicy", "parameter_count": 217870, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "actor": {"_type": "Actor", "_type_fq": "src.reinforcement_learning.core.policies.components.actor.Actor", "parameter_count": 73484, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "network": "Sequential(\n  (0): Linear(in_features=17, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n)", "action_selector": "PredictedStdActionSelector(\n  (action_net): Linear(in_features=256, out_features=6, bias=True)\n  (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n)"}, "critic": {"_type": "QCritic", "_type_fq": "src.reinforcement_learning.core.policies.components.q_critic.QCritic", "parameter_count": 144386, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "n_critics": 2, "q_network": "Sequential(\n  (0): Linear(in_features=23, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=256, out_features=1, bias=True)\n)"}}, "policy_parameter_count": 217870, "policy_repr": "SACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (feature_extractor): IdentityExtractor()\n    (network): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (_action_selector): PredictedStdActionSelector(\n      (action_net): Linear(in_features=256, out_features=6, bias=True)\n      (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n    )\n  )\n  (critic): QCritic(\n    (feature_extractor): IdentityExtractor()\n    (q_networks): ModuleList(\n      (0-1): 2 x Sequential(\n        (0): Linear(in_features=23, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=256, out_features=1, bias=True)\n      )\n    )\n  )\n  (target_critic): QCritic(\n    (feature_extractor): IdentityExtractor()\n    (q_networks): ModuleList(\n      (0-1): 2 x Sequential(\n        (0): Linear(in_features=23, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=256, out_features=1, bias=True)\n      )\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {"_type": "ReplayBuffer", "_type_fq": "src.reinforcement_learning.core.buffers.replay.replay_buffer.ReplayBuffer", "buffer_size": 50000, "num_envs": 16, "total_buffer_size": 800000, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "np_dtype": "<class 'numpy.float32'>", "optimize_memory_usage": false}, "buffer_step_size": 50000, "buffer_total_size": 800000, "gamma": 0.99, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 0.005, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 100, "actor_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)", "critic_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)", "entropy_coef_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)", "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object at 0x00007FFB984362F0>", "weigh_and_reduce_actor_loss": "<function <lambda> at 0x000001D51A307F60>", "weigh_critic_loss": "<function <lambda> at 0x000001D51A307EC0>", "target_update_interval": 1, "target_entropy": -6.0, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 9 3900X 12-Core Processor", "cores": 12, "logical_cores": 24, "speed": "3793 MHz"}, "gpu": [{"name": "NVIDIA GeForce RTX 3070", "video_processor": "NVIDIA GeForce RTX 3070", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA", "memory": "8192 MB", "memory_clock": "6800 MHz", "compute_capability": "8.6"}], "ram_speed": "2666 MHz", "ram": "64 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "import inspect\nimport os\nimport time\nfrom pathlib import Path\n\nimport gymnasium\nfrom gymnasium import Env\nfrom gymnasium.vector import VectorEnv\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.model_db import ModelDB\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.np_functions import inv_symmetric_log\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.model_db.tiny_model_db import TinyModelDB\nfrom src.module_analysis import count_parameters, get_gradients_per_parameter\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\nfrom src.reinforcement_learning.gym.envs.test_env import TestEnv\nfrom src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\nfrom typing import Any, SupportsFloat, Optional\nfrom gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\nfrom src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\nfrom src.reinforcement_learning.core.normalization import NormalizationType\nfrom src.torch_device import set_default_torch_device, optimizer_to_device\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nfrom torch.distributions import Normal, Categorical\n\nimport torch\nfrom torch import optim, nn\nimport torch.distributions as dist\nimport gymnasium as gym\nimport numpy as np\n\nfrom src.torch_functions import antisymmetric_power\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    \n    if len(episode_scores) > 0:\n    \n        global best_iteration_score\n        iteration_score = episode_scores.mean()\n        score_moving_average = score_mean_ema.update(iteration_score)\n        if iteration_score >= best_iteration_score:\n            best_iteration_score = iteration_score\n            policy_db.save_model_state_dict(\n                model_id=policy_id,\n                parent_model_id=parent_policy_id,\n                model_info={\n                    'score': iteration_score.item(),\n                    'steps_trained': steps_trained,\n                    'wrap_env_source_code': wrap_env_source_code_source,\n                    'init_policy_source_code': init_policy_source\n                },\n                model=policy,\n                optimizer=optimizer,\n            )\n        info['score_moving_average'] = score_moving_average\n    \n    info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info['final_entropy_coef_loss']),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n        logger.save_experiment_log()\n        raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 16\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\nenv = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n\nlogger = ExperimentLogger('experiment_logs/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=50_000,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=100,\n            learning_starts=100,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags(),\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "logs_by_category": {"__default": [{"step": 1000, "num_env_steps": 16000, "scores": null, "actor_loss": {"n": 1, "mean": -16.696237564086914}, "entropy_coef_loss": {"n": 1, "mean": -2.7113800048828125}, "critic_loss": {"n": 1, "mean": 0.9724661111831665}, "entropy_coef": {"n": 1, "mean": 0.7631266117095947}, "action_stds": {"n": 96, "mean": 0.9032373428344727, "std": 0.03714798763394356, "min_value": 0.7744014263153076, "max_value": 0.9797609448432922}, "action_magnitude": {"n": 96000, "mean": 0.5220452547073364, "std": 0.28643742203712463, "min_value": 2.034008502960205e-06, "max_value": 0.9997959136962891}, "num_gradient_steps": 901, "step_time": 13.34865665435791, "total_time": 10.547074556350708, "__timestamp": "2024-09-27 15:49:07.616240"}, {"step": 2000, "num_env_steps": 32000, "scores": {"n": 16, "mean": -235.00751743229375, "std": 71.69244009558349, "min_value": -343.1720649646304, "max_value": -68.65615657519083}, "actor_loss": {"n": 1, "mean": -25.024829864501953}, "entropy_coef_loss": {"n": 1, "mean": -5.679919242858887}, "critic_loss": {"n": 1, "mean": 1.6483945846557617}, "entropy_coef": {"n": 1, "mean": 0.5657179951667786}, "action_stds": {"n": 96, "mean": 0.906414270401001, "std": 0.043270986527204514, "min_value": 0.7892752885818481, "max_value": 0.9925869703292847}, "action_magnitude": {"n": 96000, "mean": 0.5340844988822937, "std": 0.2880306839942932, "min_value": 6.094574928283691e-06, "max_value": 0.9997028112411499}, "num_gradient_steps": 1901, "step_time": 10.559587478637695, "total_time": 21.106662034988403, "__timestamp": "2024-09-27 15:49:18.174827"}, {"step": 3000, "num_env_steps": 48000, "scores": {"n": 16, "mean": -236.26184769295378, "std": 82.18325638629841, "min_value": -405.70801285887137, "max_value": -81.34723654470872}, "actor_loss": {"n": 1, "mean": -29.269428253173828}, "entropy_coef_loss": {"n": 1, "mean": -8.653731346130371}, "critic_loss": {"n": 1, "mean": 1.6173598766326904}, "entropy_coef": {"n": 1, "mean": 0.41950523853302}, "action_stds": {"n": 96, "mean": 0.925068199634552, "std": 0.03262803331017494, "min_value": 0.8446764349937439, "max_value": 1.0161559581756592}, "action_magnitude": {"n": 96000, "mean": 0.5357075333595276, "std": 0.2888745069503784, "min_value": 8.102739229798317e-06, "max_value": 0.9995507597923279}, "num_gradient_steps": 2901, "step_time": 10.329689502716064, "total_time": 31.436351537704468, "__timestamp": "2024-09-27 15:49:28.505517"}, {"step": 4000, "num_env_steps": 64000, "scores": {"n": 16, "mean": -205.17778541596135, "std": 59.570719595234415, "min_value": -289.0961029737082, "max_value": -46.654265894554555}, "actor_loss": {"n": 1, "mean": -31.936012268066406}, "entropy_coef_loss": {"n": 1, "mean": -11.51273250579834}, "critic_loss": {"n": 1, "mean": 1.752274990081787}, "entropy_coef": {"n": 1, "mean": 0.3111768364906311}, "action_stds": {"n": 96, "mean": 0.9266293048858643, "std": 0.04330204799771309, "min_value": 0.8228421807289124, "max_value": 1.0484716892242432}, "action_magnitude": {"n": 96000, "mean": 0.5421186685562134, "std": 0.28918930888175964, "min_value": 9.462237358093262e-06, "max_value": 0.9997642636299133}, "num_gradient_steps": 3901, "step_time": 10.311585187911987, "total_time": 41.747936725616455, "__timestamp": "2024-09-27 15:49:38.816102"}, {"step": 5000, "num_env_steps": 80000, "scores": {"n": 16, "mean": -240.41373336396103, "std": 41.67345821100562, "min_value": -309.2538941792718, "max_value": -157.6352016599849}, "actor_loss": {"n": 1, "mean": -32.45893096923828}, "entropy_coef_loss": {"n": 1, "mean": -13.979349136352539}, "critic_loss": {"n": 1, "mean": 5.293946743011475}, "entropy_coef": {"n": 1, "mean": 0.231564000248909}, "action_stds": {"n": 96, "mean": 0.9233242869377136, "std": 0.03926827758550644, "min_value": 0.7606058120727539, "max_value": 1.0064159631729126}, "action_magnitude": {"n": 96000, "mean": 0.5472503304481506, "std": 0.2904391288757324, "min_value": 8.020550012588501e-06, "max_value": 0.9995187520980835}, "num_gradient_steps": 4901, "step_time": 10.300130128860474, "total_time": 52.04806685447693, "__timestamp": "2024-09-27 15:49:49.116232"}, {"step": 6000, "num_env_steps": 96000, "scores": {"n": 16, "mean": -217.64911950030387, "std": 36.49943068012931, "min_value": -314.3052352265804, "max_value": -168.2617217550287}, "actor_loss": {"n": 1, "mean": -32.25477600097656}, "entropy_coef_loss": {"n": 1, "mean": -16.02571678161621}, "critic_loss": {"n": 1, "mean": 1.5851590633392334}, "entropy_coef": {"n": 1, "mean": 0.17296753823757172}, "action_stds": {"n": 96, "mean": 0.898815929889679, "std": 0.05846969410777092, "min_value": 0.7261108756065369, "max_value": 0.9996864795684814}, "action_magnitude": {"n": 96000, "mean": 0.5556703209877014, "std": 0.2914673984050751, "min_value": 2.5406479835510254e-05, "max_value": 0.9998500943183899}, "num_gradient_steps": 5901, "step_time": 10.356951236724854, "total_time": 62.40501809120178, "__timestamp": "2024-09-27 15:49:59.473183"}, {"step": 7000, "num_env_steps": 112000, "scores": {"n": 16, "mean": -186.3997108100882, "std": 32.014086617555236, "min_value": -254.39494384949649, "max_value": -148.51291588167078}, "actor_loss": {"n": 1, "mean": -31.542299270629883}, "entropy_coef_loss": {"n": 1, "mean": -17.819246292114258}, "critic_loss": {"n": 1, "mean": 1.5855367183685303}, "entropy_coef": {"n": 1, "mean": 0.12961220741271973}, "action_stds": {"n": 96, "mean": 0.8808531761169434, "std": 0.07193144410848618, "min_value": 0.6613863110542297, "max_value": 1.054578185081482}, "action_magnitude": {"n": 96000, "mean": 0.561771035194397, "std": 0.2929583787918091, "min_value": 2.5406479835510254e-05, "max_value": 0.9998354911804199}, "num_gradient_steps": 6901, "step_time": 10.338692903518677, "total_time": 72.74371099472046, "__timestamp": "2024-09-27 15:50:09.811876"}, {"step": 8000, "num_env_steps": 128000, "scores": {"n": 16, "mean": -200.43764585094505, "std": 30.632519734992545, "min_value": -262.3014162911859, "max_value": -139.43879337058752}, "actor_loss": {"n": 1, "mean": -30.338186264038086}, "entropy_coef_loss": {"n": 1, "mean": -19.84642219543457}, "critic_loss": {"n": 1, "mean": 1.254146695137024}, "entropy_coef": {"n": 1, "mean": 0.09733682125806808}, "action_stds": {"n": 96, "mean": 0.8882372975349426, "std": 0.0818696916103363, "min_value": 0.6398377418518066, "max_value": 1.0913532972335815}, "action_magnitude": {"n": 96000, "mean": 0.5772273540496826, "std": 0.29452672600746155, "min_value": 1.0251998901367188e-05, "max_value": 0.9998007416725159}, "num_gradient_steps": 7901, "step_time": 10.337799549102783, "total_time": 83.08151054382324, "__timestamp": "2024-09-27 15:50:20.149676"}, {"step": 9000, "num_env_steps": 144000, "scores": {"n": 16, "mean": -128.07363663235333, "std": 42.291916463657266, "min_value": -194.35012204834493, "max_value": -53.870562428379344}, "actor_loss": {"n": 1, "mean": -29.087663650512695}, "entropy_coef_loss": {"n": 1, "mean": -21.020397186279297}, "critic_loss": {"n": 1, "mean": 1.6061570644378662}, "entropy_coef": {"n": 1, "mean": 0.07317440211772919}, "action_stds": {"n": 96, "mean": 0.8639476895332336, "std": 0.07741750031709671, "min_value": 0.7037464380264282, "max_value": 1.025490164756775}, "action_magnitude": {"n": 96000, "mean": 0.579763650894165, "std": 0.29391488432884216, "min_value": 2.205371856689453e-05, "max_value": 0.9997857809066772}, "num_gradient_steps": 8901, "step_time": 10.349064111709595, "total_time": 93.43057465553284, "__timestamp": "2024-09-27 15:50:30.498740"}, {"step": 10000, "num_env_steps": 160000, "scores": {"n": 16, "mean": -85.0310882327417, "std": 28.475147633807268, "min_value": -157.50425746815745, "max_value": -40.118989195209}, "actor_loss": {"n": 1, "mean": -27.19525909423828}, "entropy_coef_loss": {"n": 1, "mean": -20.067562103271484}, "critic_loss": {"n": 1, "mean": 1.3848624229431152}, "entropy_coef": {"n": 1, "mean": 0.05514080077409744}, "action_stds": {"n": 96, "mean": 0.8305092453956604, "std": 0.07213064283132553, "min_value": 0.6646724939346313, "max_value": 0.9877631068229675}, "action_magnitude": {"n": 96000, "mean": 0.5915568470954895, "std": 0.2943524718284607, "min_value": 4.6879053115844727e-05, "max_value": 0.9998577237129211}, "num_gradient_steps": 9901, "step_time": 10.346182823181152, "total_time": 103.77675747871399, "__timestamp": "2024-09-27 15:50:40.844923"}, {"step": 11000, "num_env_steps": 176000, "scores": {"n": 16, "mean": -22.765935300153615, "std": 33.95313951402978, "min_value": -79.40353542682715, "max_value": 40.113973335392075}, "actor_loss": {"n": 1, "mean": -25.83725357055664}, "entropy_coef_loss": {"n": 1, "mean": -20.346843719482422}, "critic_loss": {"n": 1, "mean": 3.192387580871582}, "entropy_coef": {"n": 1, "mean": 0.041978225111961365}, "action_stds": {"n": 96, "mean": 0.7708544135093689, "std": 0.10350576788187027, "min_value": 0.5323797464370728, "max_value": 1.0717670917510986}, "action_magnitude": {"n": 96000, "mean": 0.6155062913894653, "std": 0.29484233260154724, "min_value": 6.191432476043701e-06, "max_value": 0.9999849796295166}, "num_gradient_steps": 10901, "step_time": 10.309396982192993, "total_time": 114.08615446090698, "__timestamp": "2024-09-27 15:50:51.154320"}, {"step": 12000, "num_env_steps": 192000, "scores": {"n": 16, "mean": 59.009920429925785, "std": 38.10814601149828, "min_value": -19.453034798032604, "max_value": 138.65627992671307}, "actor_loss": {"n": 1, "mean": -24.38642120361328}, "entropy_coef_loss": {"n": 1, "mean": -16.091259002685547}, "critic_loss": {"n": 1, "mean": 1.3696130514144897}, "entropy_coef": {"n": 1, "mean": 0.03226007521152496}, "action_stds": {"n": 96, "mean": 0.7178964018821716, "std": 0.10215982049703598, "min_value": 0.47750186920166016, "max_value": 0.9540987610816956}, "action_magnitude": {"n": 96000, "mean": 0.6394320130348206, "std": 0.2941407859325409, "min_value": 1.0266900062561035e-05, "max_value": 0.9999995827674866}, "num_gradient_steps": 11901, "step_time": 10.38811206817627, "total_time": 124.47426652908325, "__timestamp": "2024-09-27 15:51:01.542432"}, {"step": 13000, "num_env_steps": 208000, "scores": {"n": 16, "mean": 82.66136550296272, "std": 57.909088810757844, "min_value": -24.119955846457742, "max_value": 197.85576578986365}, "actor_loss": {"n": 1, "mean": -21.956844329833984}, "entropy_coef_loss": {"n": 1, "mean": -16.647716522216797}, "critic_loss": {"n": 1, "mean": 2.656754970550537}, "entropy_coef": {"n": 1, "mean": 0.025078101083636284}, "action_stds": {"n": 96, "mean": 0.6937458515167236, "std": 0.10724643617868423, "min_value": 0.42872941493988037, "max_value": 0.9556984305381775}, "action_magnitude": {"n": 96000, "mean": 0.6647337079048157, "std": 0.2940253019332886, "min_value": 1.4603137969970703e-06, "max_value": 0.9998524785041809}, "num_gradient_steps": 12901, "step_time": 10.340622186660767, "total_time": 134.81488871574402, "__timestamp": "2024-09-27 15:51:11.883054"}, {"step": 14000, "num_env_steps": 224000, "scores": {"n": 16, "mean": -337.3888262742712, "std": 144.25530059875348, "min_value": -501.07343984080944, "max_value": 166.40431486433954}, "actor_loss": {"n": 1, "mean": -21.53305435180664}, "entropy_coef_loss": {"n": 1, "mean": -17.41573715209961}, "critic_loss": {"n": 1, "mean": 2.3426027297973633}, "entropy_coef": {"n": 1, "mean": 0.019657431170344353}, "action_stds": {"n": 96, "mean": 0.605079710483551, "std": 0.10596024990081787, "min_value": 0.3064635694026947, "max_value": 0.8185648322105408}, "action_magnitude": {"n": 96000, "mean": 0.6244643330574036, "std": 0.29691219329833984, "min_value": 4.887580871582031e-06, "max_value": 0.9999991059303284}, "num_gradient_steps": 13901, "step_time": 10.346683025360107, "total_time": 145.16157174110413, "__timestamp": "2024-09-27 15:51:22.229738"}, {"step": 15000, "num_env_steps": 240000, "scores": {"n": 16, "mean": 204.70558275083567, "std": 77.70575979959672, "min_value": 27.703666789107956, "max_value": 339.834324090858}, "actor_loss": {"n": 1, "mean": -20.146760940551758}, "entropy_coef_loss": {"n": 1, "mean": -9.065696716308594}, "critic_loss": {"n": 1, "mean": 0.9978720545768738}, "entropy_coef": {"n": 1, "mean": 0.01558308769017458}, "action_stds": {"n": 96, "mean": 0.5630826354026794, "std": 0.08997543156147003, "min_value": 0.331564724445343, "max_value": 0.882736086845398}, "action_magnitude": {"n": 96000, "mean": 0.7156068682670593, "std": 0.29061609506607056, "min_value": 3.729667514562607e-05, "max_value": 0.999999463558197}, "num_gradient_steps": 14901, "step_time": 10.338458776473999, "total_time": 155.50003051757812, "__timestamp": "2024-09-27 15:51:32.568196"}, {"step": 16000, "num_env_steps": 256000, "scores": {"n": 16, "mean": 85.87498946025488, "std": 97.64551405425587, "min_value": -25.085703825527162, "max_value": 330.75974998957827}, "actor_loss": {"n": 1, "mean": -19.55574607849121}, "entropy_coef_loss": {"n": 1, "mean": -6.568558692932129}, "critic_loss": {"n": 1, "mean": 1.0677897930145264}, "entropy_coef": {"n": 1, "mean": 0.01285527553409338}, "action_stds": {"n": 96, "mean": 0.45418253540992737, "std": 0.11085014045238495, "min_value": 0.21345756947994232, "max_value": 0.799576461315155}, "action_magnitude": {"n": 96000, "mean": 0.6220921277999878, "std": 0.2996825575828552, "min_value": 2.1111220121383667e-05, "max_value": 0.9999996423721313}, "num_gradient_steps": 15901, "step_time": 10.327394723892212, "total_time": 165.82742524147034, "__timestamp": "2024-09-27 15:51:42.895591"}, {"step": 17000, "num_env_steps": 272000, "scores": {"n": 16, "mean": 68.27127566861486, "std": 26.695360067856186, "min_value": 19.530315247160615, "max_value": 119.7914393931569}, "actor_loss": {"n": 1, "mean": -19.64593505859375}, "entropy_coef_loss": {"n": 1, "mean": -5.440829277038574}, "critic_loss": {"n": 1, "mean": 2.384472370147705}, "entropy_coef": {"n": 1, "mean": 0.010944550856947899}, "action_stds": {"n": 96, "mean": 0.44104811549186707, "std": 0.11542768776416779, "min_value": 0.1939530372619629, "max_value": 0.8323246836662292}, "action_magnitude": {"n": 96000, "mean": 0.5805214643478394, "std": 0.2973034977912903, "min_value": 8.046627044677734e-06, "max_value": 0.9998994469642639}, "num_gradient_steps": 16901, "step_time": 10.374483585357666, "total_time": 176.201908826828, "__timestamp": "2024-09-27 15:51:53.270073"}, {"step": 18000, "num_env_steps": 288000, "scores": {"n": 16, "mean": 192.15907163896398, "std": 50.49282230664417, "min_value": 112.14080662699416, "max_value": 275.2728463863459}, "actor_loss": {"n": 1, "mean": -19.197824478149414}, "entropy_coef_loss": {"n": 1, "mean": -0.4887372553348541}, "critic_loss": {"n": 1, "mean": 2.0650649070739746}, "entropy_coef": {"n": 1, "mean": 0.009761691093444824}, "action_stds": {"n": 96, "mean": 0.4544844627380371, "std": 0.11215107142925262, "min_value": 0.16902092099189758, "max_value": 0.7099472880363464}, "action_magnitude": {"n": 96000, "mean": 0.619327187538147, "std": 0.3022623360157013, "min_value": 5.800207145512104e-05, "max_value": 0.999995768070221}, "num_gradient_steps": 17901, "step_time": 10.355577945709229, "total_time": 186.55748677253723, "__timestamp": "2024-09-27 15:52:03.625652"}, {"step": 19000, "num_env_steps": 304000, "scores": {"n": 16, "mean": 282.9184496501506, "std": 86.24513282013373, "min_value": 112.97632666263962, "max_value": 427.9532358948927}, "actor_loss": {"n": 1, "mean": -18.86212158203125}, "entropy_coef_loss": {"n": 1, "mean": -1.2940798997879028}, "critic_loss": {"n": 1, "mean": 0.9087018370628357}, "entropy_coef": {"n": 1, "mean": 0.009228259325027466}, "action_stds": {"n": 96, "mean": 0.5022813677787781, "std": 0.10931119322776794, "min_value": 0.20152400434017181, "max_value": 0.7852246165275574}, "action_magnitude": {"n": 96000, "mean": 0.7054008841514587, "std": 0.2982121706008911, "min_value": 3.4332275390625e-05, "max_value": 0.9999948143959045}, "num_gradient_steps": 18901, "step_time": 10.390576362609863, "total_time": 196.9480631351471, "__timestamp": "2024-09-27 15:52:14.016228"}, {"step": 20000, "num_env_steps": 320000, "scores": {"n": 16, "mean": 447.4154100387841, "std": 145.87539922100072, "min_value": 287.5861543296487, "max_value": 758.8983299681568}, "actor_loss": {"n": 1, "mean": -19.655445098876953}, "entropy_coef_loss": {"n": 1, "mean": -0.8676983118057251}, "critic_loss": {"n": 1, "mean": 1.2997756004333496}, "entropy_coef": {"n": 1, "mean": 0.009349824860692024}, "action_stds": {"n": 96, "mean": 0.5020280480384827, "std": 0.11823587119579315, "min_value": 0.158477783203125, "max_value": 0.7134683728218079}, "action_magnitude": {"n": 96000, "mean": 0.711763322353363, "std": 0.29583054780960083, "min_value": 9.521842002868652e-06, "max_value": 0.9999953508377075}, "num_gradient_steps": 19901, "step_time": 10.340046644210815, "total_time": 207.2881097793579, "__timestamp": "2024-09-27 15:52:24.356275"}]}}