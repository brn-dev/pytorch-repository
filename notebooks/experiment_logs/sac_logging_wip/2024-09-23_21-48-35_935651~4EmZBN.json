{
    "experiment_id": "2024-09-23_21-48-35_935651~4EmZBN",
    "experiment_tags": [
        "SAC",
        "HalfCheetah-v4"
    ],
    "start_time": "2024-09-23 21:48:35.935651",
    "end_time": "2024-09-23 21:50:17.610216",
    "model_db_reference": null,
    "hyper_parameters": {
        "env": "<RescaleAction<TransformRewardWrapper<AsyncVectorEnv instance>>>",
        "num_envs": 16,
        "policy": {
            "parameter_count": 217870,
            "feature_extractor": "IdentityExtractor()",
            "feature_extractor_parameter_count": 0,
            "actor": {
                "parameter_count": 73484,
                "feature_extractor": "IdentityExtractor()",
                "feature_extractor_parameter_count": 0,
                "network": "Sequential(\n  (0): Linear(in_features=17, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ELU(alpha=1.0)\n)",
                "action_selector": "PredictedStdActionSelector(\n  (action_net): Linear(in_features=256, out_features=6, bias=True)\n  (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n)"
            },
            "critic": {
                "parameter_count": 144386,
                "feature_extractor": "IdentityExtractor()",
                "feature_extractor_parameter_count": 0,
                "n_critics": 2,
                "q_network_architecture": "Sequential(\n  (0): Linear(in_features=23, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=256, out_features=1, bias=True)\n)"
            }
        },
        "policy_parameter_count": 217870,
        "buffer": "<src.reinforcement_learning.core.buffers.replay.replay_buffer.ReplayBuffer object at 0x00000203F93727D0>",
        "buffer_step_size": 0,
        "buffer_total_size": 0,
        "gamma": 0.99,
        "sde_noise_sample_freq": null,
        "torch_device": "cuda:0",
        "torch_dtype": "torch.float32",
        "tau": 0.005,
        "rollout_steps": 2,
        "gradient_steps": 2,
        "optimization_batch_size": 256,
        "action_noise": null,
        "warmup_steps": 500,
        "actor_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.5, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "critic_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.5, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "entropy_coef_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object at 0x00007FF833D262F0>",
        "weigh_and_reduce_actor_loss": "<function <lambda> at 0x00000203F7EA4900>",
        "weigh_critic_loss": "<function <lambda> at 0x00000203F9693B00>",
        "target_update_interval": 1,
        "target_entropy": -6.0,
        "entropy_coef": "Dynamic"
    },
    "setup": {
        "sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ELU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n#             BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n",
        "notebook": "import inspect\nimport os\nimport time\nfrom pathlib import Path\n\nimport gymnasium\nfrom gymnasium import Env\nfrom gymnasium.vector import VectorEnv\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.model_db import ModelDB\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.np_functions import inv_symmetric_log\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.model_db.tiny_model_db import TinyModelDB\nfrom src.module_analysis import count_parameters, get_gradients_per_parameter\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\nfrom src.reinforcement_learning.gym.envs.test_env import TestEnv\nfrom src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\nfrom typing import Any, SupportsFloat, Optional\nfrom gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\nfrom src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\nfrom src.reinforcement_learning.core.normalization import NormalizationType\nfrom src.torch_device import set_default_torch_device, optimizer_to_device\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nfrom torch.distributions import Normal, Categorical\n\nimport torch\nfrom torch import optim, nn\nimport torch.distributions as dist\nimport gymnasium as gym\nimport numpy as np\n\nfrom src.torch_functions import antisymmetric_power\n\n# %load_ext autoreload\n# %autoreload 2\n\nfrom src.summary_statistics import compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    episode_scores = compute_episode_returns(\n        rewards=rewards,\n        episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n        last_episode_starts=info['last_episode_starts'],\n        gamma=1.0,\n        gae_lambda=1.0,\n        normalize_rewards=None,\n        remove_unfinished_episodes=True,\n    )\n    \n    global best_iteration_score\n    iteration_score = episode_scores.mean()\n    score_moving_average = score_mean_ema.update(iteration_score)\n    if iteration_score >= best_iteration_score:\n        best_iteration_score = iteration_score\n        policy_db.save_model_state_dict(\n            model_id=policy_id,\n            parent_model_id=parent_policy_id,\n            model_info={\n                'score': iteration_score.item(),\n                'steps_trained': steps_trained,\n                'wrap_env_source_code': wrap_env_source_code_source,\n                'init_policy_source_code': init_policy_source\n            },\n            model=policy,\n            optimizer=optimizer,\n        )\n    \n    info['episode_scores'] = episode_scores\n    info['score_moving_average'] = score_moving_average\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    \n    time_taken = stopwatch.reset()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info['episode_scores']\n    score_moving_average = info['score_moving_average']\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {time_taken:4.1f} \\n\"\n          )\n    logger.item_start()\n    logger.item_log('step', step)\n    logger.item_log('scores', compute_summary_statistics(episode_scores))\n    logger.item_log('actor_loss', compute_summary_statistics(info['final_actor_loss']))\n    logger.item_log('entropy_coef_loss', compute_summary_statistics(info['final_entropy_coef_loss']))\n    logger.item_log('critic_loss', compute_summary_statistics(info['final_critic_loss']))\n    logger.item_log('entropy_coef', compute_summary_statistics(info['entropy_coef']))\n    logger.item_log('action_stds', compute_summary_statistics(action_stds))\n    logger.item_log('action_magnitude', compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])))\n    logger.item_log('gradient_step', rl.gradient_steps_performed)\n    logger.item_log('time_taken', time_taken)\n    logger.item_end()\n    if step % 1000 == 0:\n        logger.save_experiment()\n    print()\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 16\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\nenv = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n\nlogger = ExperimentLogger('experiment_logs/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = } \\n\\n')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=15_000,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=2,\n            gradient_steps=2,\n            warmup_steps=500,\n            learning_starts=500,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        with log_experiment(\n            logger,\n            experiment_tags=[type(algo).__name__, env_name],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(1_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"
    },
    "notes": [],
    "logs_by_category": {
        "__default": [
            {
                "step": 1000,
                "scores": {
                    "n": 16,
                    "mean": -302.7308044433594,
                    "std": 69.60443115234375,
                    "min_value": -467.9533996582031,
                    "max_value": -183.7851104736328
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -12.680134773254395,
                    "std": 0.23367157578468323,
                    "min_value": -12.845365524291992,
                    "max_value": -12.514904022216797
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -1.494186282157898,
                    "std": 0.003032718552276492,
                    "min_value": -1.496330738067627,
                    "max_value": -1.492041826248169
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 2.338953733444214,
                    "std": 0.2585102319717407,
                    "min_value": 2.1561594009399414,
                    "max_value": 2.5217480659484863
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.8609365820884705,
                    "std": 0.0001818215096136555,
                    "min_value": 0.8608080148696899,
                    "max_value": 0.861065149307251
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.8913924694061279,
                    "std": 0.024477900937199593,
                    "min_value": 0.8247198462486267,
                    "max_value": 0.9427672028541565
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5109028816223145,
                    "std": 0.28812944889068604,
                    "min_value": 4.2663192289182916e-05,
                    "max_value": 0.9999803304672241
                },
                "gradient_step": 502,
                "time_taken": 8.638197422027588,
                "__timestamp": "2024-09-23 21:48:41.743707"
            },
            {
                "step": 2000,
                "scores": {
                    "n": 16,
                    "mean": -220.73654174804688,
                    "std": 55.39399337768555,
                    "min_value": -349.3423156738281,
                    "max_value": -104.80633544921875
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -22.115808486938477,
                    "std": 0.10213428735733032,
                    "min_value": -22.18802833557129,
                    "max_value": -22.043588638305664
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -4.481418609619141,
                    "std": 0.0036859947722405195,
                    "min_value": -4.484025001525879,
                    "max_value": -4.478812217712402
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 6.592729568481445,
                    "std": 2.0659022331237793,
                    "min_value": 5.131916046142578,
                    "max_value": 8.053543090820312
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.6377793550491333,
                    "std": 0.00013457488967105746,
                    "min_value": 0.637684166431427,
                    "max_value": 0.6378744840621948
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.8987669944763184,
                    "std": 0.030157482251524925,
                    "min_value": 0.829744279384613,
                    "max_value": 0.974683403968811
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5289731621742249,
                    "std": 0.2869381308555603,
                    "min_value": 2.5745481252670288e-05,
                    "max_value": 0.9998779892921448
                },
                "gradient_step": 1502,
                "time_taken": 10.114235877990723,
                "__timestamp": "2024-09-23 21:48:51.856942"
            },
            {
                "step": 3000,
                "scores": {
                    "n": 16,
                    "mean": -220.581298828125,
                    "std": 59.98629379272461,
                    "min_value": -308.1903076171875,
                    "max_value": -101.8631591796875
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -27.74069595336914,
                    "std": 0.1236393004655838,
                    "min_value": -27.828121185302734,
                    "max_value": -27.653268814086914
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -7.353653907775879,
                    "std": 0.016707684844732285,
                    "min_value": -7.3654680252075195,
                    "max_value": -7.341839790344238
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 6.721421241760254,
                    "std": 4.221640110015869,
                    "min_value": 3.7362711429595947,
                    "max_value": 9.706571578979492
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.4736887812614441,
                    "std": 9.959300223272294e-05,
                    "min_value": 0.47361835837364197,
                    "max_value": 0.4737592041492462
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9262201189994812,
                    "std": 0.033146508038043976,
                    "min_value": 0.8615589737892151,
                    "max_value": 1.0050098896026611
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5400447249412537,
                    "std": 0.28918027877807617,
                    "min_value": 2.11372971534729e-05,
                    "max_value": 0.9996891021728516
                },
                "gradient_step": 2502,
                "time_taken": 11.228425979614258,
                "__timestamp": "2024-09-23 21:49:03.085369"
            },
            {
                "step": 4000,
                "scores": {
                    "n": 16,
                    "mean": -236.85693359375,
                    "std": 42.00060272216797,
                    "min_value": -304.9359130859375,
                    "max_value": -140.03091430664062
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -30.992277145385742,
                    "std": 0.00918464083224535,
                    "min_value": -30.99877166748047,
                    "max_value": -30.985782623291016
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -10.286172866821289,
                    "std": 0.044781532138586044,
                    "min_value": -10.317838668823242,
                    "max_value": -10.254508018493652
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 6.5699286460876465,
                    "std": 4.491898536682129,
                    "min_value": 3.3936767578125,
                    "max_value": 9.746180534362793
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.35146379470825195,
                    "std": 7.400986942229792e-05,
                    "min_value": 0.35141146183013916,
                    "max_value": 0.35151612758636475
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9302261471748352,
                    "std": 0.03742405027151108,
                    "min_value": 0.8282307386398315,
                    "max_value": 1.0400656461715698
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5446239709854126,
                    "std": 0.2905696928501129,
                    "min_value": 2.384185791015625e-07,
                    "max_value": 0.9996600151062012
                },
                "gradient_step": 3502,
                "time_taken": 10.131020069122314,
                "__timestamp": "2024-09-23 21:49:13.216389"
            },
            {
                "step": 5000,
                "scores": {
                    "n": 16,
                    "mean": -205.84860229492188,
                    "std": 65.66744232177734,
                    "min_value": -315.8603820800781,
                    "max_value": -30.784574508666992
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -32.031646728515625,
                    "std": 0.3255597949028015,
                    "min_value": -32.2618522644043,
                    "max_value": -31.801441192626953
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -13.10222339630127,
                    "std": 0.11033303290605545,
                    "min_value": -13.180240631103516,
                    "max_value": -13.024206161499023
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 8.309819221496582,
                    "std": 5.657541751861572,
                    "min_value": 4.309333324432373,
                    "max_value": 12.31030559539795
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.26095807552337646,
                    "std": 5.460124521050602e-05,
                    "min_value": 0.2609194815158844,
                    "max_value": 0.2609966993331909
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9490005970001221,
                    "std": 0.04391404241323471,
                    "min_value": 0.8285215497016907,
                    "max_value": 1.0866684913635254
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5480221509933472,
                    "std": 0.29062625765800476,
                    "min_value": 1.1418014764785767e-06,
                    "max_value": 0.9995672106742859
                },
                "gradient_step": 4502,
                "time_taken": 10.373321056365967,
                "__timestamp": "2024-09-23 21:49:23.589711"
            },
            {
                "step": 6000,
                "scores": {
                    "n": 16,
                    "mean": -220.03900146484375,
                    "std": 49.33686065673828,
                    "min_value": -303.9928894042969,
                    "max_value": -117.04464721679688
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -31.89027976989746,
                    "std": 0.37978559732437134,
                    "min_value": -32.15882873535156,
                    "max_value": -31.62173080444336
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -15.37837028503418,
                    "std": 0.04532438516616821,
                    "min_value": -15.410419464111328,
                    "max_value": -15.346321105957031
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 6.876460075378418,
                    "std": 5.924942493438721,
                    "min_value": 2.6868929862976074,
                    "max_value": 11.06602668762207
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.19446058571338654,
                    "std": 4.0229166188510135e-05,
                    "min_value": 0.19443213939666748,
                    "max_value": 0.1944890320301056
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9324240684509277,
                    "std": 0.046847037971019745,
                    "min_value": 0.8159171342849731,
                    "max_value": 1.0604801177978516
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5561076402664185,
                    "std": 0.2920200824737549,
                    "min_value": 7.599592208862305e-07,
                    "max_value": 0.9997005462646484
                },
                "gradient_step": 5502,
                "time_taken": 10.495761156082153,
                "__timestamp": "2024-09-23 21:49:34.085471"
            },
            {
                "step": 7000,
                "scores": {
                    "n": 16,
                    "mean": -216.8152313232422,
                    "std": 19.803905487060547,
                    "min_value": -257.66729736328125,
                    "max_value": -180.60800170898438
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -31.400915145874023,
                    "std": 0.3998866081237793,
                    "min_value": -31.683677673339844,
                    "max_value": -31.118152618408203
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -17.30898666381836,
                    "std": 0.018255991861224174,
                    "min_value": -17.321895599365234,
                    "max_value": -17.296077728271484
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 3.5074853897094727,
                    "std": 0.7998676300048828,
                    "min_value": 2.9418935775756836,
                    "max_value": 4.073077201843262
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.14540183544158936,
                    "std": 2.975567440444138e-05,
                    "min_value": 0.14538079500198364,
                    "max_value": 0.14542287588119507
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9193668961524963,
                    "std": 0.0511934794485569,
                    "min_value": 0.7834681272506714,
                    "max_value": 1.0449942350387573
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5627015233039856,
                    "std": 0.2925291359424591,
                    "min_value": 1.9721686840057373e-05,
                    "max_value": 0.999616265296936
                },
                "gradient_step": 6502,
                "time_taken": 10.750276565551758,
                "__timestamp": "2024-09-23 21:49:44.836748"
            },
            {
                "step": 8000,
                "scores": {
                    "n": 16,
                    "mean": -177.8297119140625,
                    "std": 32.79994583129883,
                    "min_value": -247.62098693847656,
                    "max_value": -112.78469848632812
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -29.954120635986328,
                    "std": 0.16997790336608887,
                    "min_value": -30.07431411743164,
                    "max_value": -29.83392906188965
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -18.751317977905273,
                    "std": 0.1229393258690834,
                    "min_value": -18.83824920654297,
                    "max_value": -18.664386749267578
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 3.1222405433654785,
                    "std": 0.27554866671562195,
                    "min_value": 2.927398204803467,
                    "max_value": 3.3170828819274902
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.10915714502334595,
                    "std": 2.219558518845588e-05,
                    "min_value": 0.10914145410060883,
                    "max_value": 0.10917284339666367
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.8991424441337585,
                    "std": 0.07014203071594238,
                    "min_value": 0.6401692628860474,
                    "max_value": 1.0429338216781616
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5716128349304199,
                    "std": 0.29343560338020325,
                    "min_value": 2.6673078536987305e-05,
                    "max_value": 0.9998750686645508
                },
                "gradient_step": 7502,
                "time_taken": 11.269850254058838,
                "__timestamp": "2024-09-23 21:49:56.105598"
            },
            {
                "step": 9000,
                "scores": {
                    "n": 16,
                    "mean": -157.72433471679688,
                    "std": 15.925533294677734,
                    "min_value": -193.32135009765625,
                    "max_value": -133.22569274902344
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -28.44377899169922,
                    "std": 0.08160708844661713,
                    "min_value": -28.501483917236328,
                    "max_value": -28.38607406616211
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -19.840099334716797,
                    "std": 0.10462668538093567,
                    "min_value": -19.914081573486328,
                    "max_value": -19.766117095947266
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 2.7613186836242676,
                    "std": 0.33054104447364807,
                    "min_value": 2.527590751647949,
                    "max_value": 2.995046377182007
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.08206968754529953,
                    "std": 1.656371205172036e-05,
                    "min_value": 0.08205797523260117,
                    "max_value": 0.0820813998579979
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.8796605467796326,
                    "std": 0.08183450251817703,
                    "min_value": 0.6212489604949951,
                    "max_value": 1.0483866930007935
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5817516446113586,
                    "std": 0.29459431767463684,
                    "min_value": 1.9073486328125e-06,
                    "max_value": 0.9999808669090271
                },
                "gradient_step": 8502,
                "time_taken": 10.622543096542358,
                "__timestamp": "2024-09-23 21:50:06.729142"
            }
        ]
    }
}