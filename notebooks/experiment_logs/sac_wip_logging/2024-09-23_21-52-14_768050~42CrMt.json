{
    "experiment_id": "2024-09-23_21-52-14_768050~42CrMt",
    "experiment_tags": [
        "SAC",
        "HalfCheetah-v4"
    ],
    "start_time": "2024-09-23 21:52:14.768050",
    "end_time": "2024-09-23 21:53:12.750212",
    "model_db_reference": null,
    "hyper_parameters": {
        "env": "<RescaleAction<TransformRewardWrapper<AsyncVectorEnv instance>>>",
        "num_envs": 16,
        "policy": {
            "parameter_count": 217870,
            "feature_extractor": "IdentityExtractor()",
            "feature_extractor_parameter_count": 0,
            "actor": {
                "parameter_count": 73484,
                "feature_extractor": "IdentityExtractor()",
                "feature_extractor_parameter_count": 0,
                "network": "Sequential(\n  (0): Linear(in_features=17, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ELU(alpha=1.0)\n)",
                "action_selector": "PredictedStdActionSelector(\n  (action_net): Linear(in_features=256, out_features=6, bias=True)\n  (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n)"
            },
            "critic": {
                "parameter_count": 144386,
                "feature_extractor": "IdentityExtractor()",
                "feature_extractor_parameter_count": 0,
                "n_critics": 2,
                "q_network_architecture": "Sequential(\n  (0): Linear(in_features=23, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=256, out_features=1, bias=True)\n)"
            }
        },
        "policy_parameter_count": 217870,
        "buffer": "<src.reinforcement_learning.core.buffers.replay.replay_buffer.ReplayBuffer object at 0x0000020404DE9910>",
        "buffer_step_size": 0,
        "buffer_total_size": 0,
        "gamma": 0.99,
        "sde_noise_sample_freq": null,
        "torch_device": "cuda:0",
        "torch_dtype": "torch.float32",
        "tau": 0.005,
        "rollout_steps": 2,
        "gradient_steps": 2,
        "optimization_batch_size": 256,
        "action_noise": null,
        "warmup_steps": 500,
        "actor_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.5, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "critic_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.5, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "entropy_coef_optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)",
        "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object at 0x00007FF833D262F0>",
        "weigh_and_reduce_actor_loss": "<function <lambda> at 0x00000203F7DD6020>",
        "weigh_critic_loss": "<function <lambda> at 0x00000203F7DD63E0>",
        "target_update_interval": 1,
        "target_entropy": -6.0,
        "entropy_coef": "Dynamic"
    },
    "setup": {
        "sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ELU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n#             BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n",
        "notebook": "import inspect\nimport os\nimport time\nfrom pathlib import Path\n\nimport gymnasium\nfrom gymnasium import Env\nfrom gymnasium.vector import VectorEnv\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.model_db import ModelDB\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.np_functions import inv_symmetric_log\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.model_db.tiny_model_db import TinyModelDB\nfrom src.module_analysis import count_parameters, get_gradients_per_parameter\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\nfrom src.reinforcement_learning.gym.envs.test_env import TestEnv\nfrom src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\nfrom typing import Any, SupportsFloat, Optional\nfrom gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\nfrom src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\nfrom src.reinforcement_learning.core.normalization import NormalizationType\nfrom src.torch_device import set_default_torch_device, optimizer_to_device\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nfrom torch.distributions import Normal, Categorical\n\nimport torch\nfrom torch import optim, nn\nimport torch.distributions as dist\nimport gymnasium as gym\nimport numpy as np\n\nfrom src.torch_functions import antisymmetric_power\n\n# %load_ext autoreload\n# %autoreload 2\n\nfrom src.summary_statistics import compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    episode_scores = compute_episode_returns(\n        rewards=rewards,\n        episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n        last_episode_starts=info['last_episode_starts'],\n        gamma=1.0,\n        gae_lambda=1.0,\n        normalize_rewards=None,\n        remove_unfinished_episodes=True,\n    )\n    \n    global best_iteration_score\n    iteration_score = episode_scores.mean()\n    score_moving_average = score_mean_ema.update(iteration_score)\n    if iteration_score >= best_iteration_score:\n        best_iteration_score = iteration_score\n        policy_db.save_model_state_dict(\n            model_id=policy_id,\n            parent_model_id=parent_policy_id,\n            model_info={\n                'score': iteration_score.item(),\n                'steps_trained': steps_trained,\n                'wrap_env_source_code': wrap_env_source_code_source,\n                'init_policy_source_code': init_policy_source\n            },\n            model=policy,\n            optimizer=optimizer,\n        )\n    \n    info['episode_scores'] = episode_scores\n    info['score_moving_average'] = score_moving_average\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    \n    time_taken = stopwatch.reset()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info['episode_scores']\n    score_moving_average = info['score_moving_average']\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {time_taken:4.1f} \\n\"\n          )\n    logger.item_start()\n    logger.item_log('step', step)\n    logger.item_log('scores', compute_summary_statistics(episode_scores))\n    logger.item_log('actor_loss', compute_summary_statistics(info['final_actor_loss']))\n    logger.item_log('entropy_coef_loss', compute_summary_statistics(info['final_entropy_coef_loss']))\n    logger.item_log('critic_loss', compute_summary_statistics(info['final_critic_loss']))\n    logger.item_log('entropy_coef', compute_summary_statistics(info['entropy_coef']))\n    logger.item_log('action_stds', compute_summary_statistics(action_stds))\n    logger.item_log('action_magnitude', compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])))\n    logger.item_log('gradient_step', rl.gradient_steps_performed)\n    logger.item_log('time_taken', time_taken)\n    logger.item_end()\n    if step % 1000 == 0:\n        logger.save_experiment()\n    print()\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 16\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\nenv = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n\nlogger = ExperimentLogger('experiment_logs/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = } \\n\\n')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=15_000,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=2,\n            gradient_steps=2,\n            warmup_steps=500,\n            learning_starts=500,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        with log_experiment(\n            logger,\n            experiment_tags=[type(algo).__name__, env_name],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(1_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"
    },
    "notes": [],
    "logs_by_category": {
        "__default": [
            {
                "step": 1000,
                "scores": {
                    "n": 16,
                    "mean": -273.88739013671875,
                    "std": 74.29509735107422,
                    "min_value": -432.6454772949219,
                    "max_value": -205.42306518554688
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -12.865304946899414,
                    "std": 0.20075926184654236,
                    "min_value": -13.00726318359375,
                    "max_value": -12.723346710205078
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -1.500245451927185,
                    "std": 0.006459426134824753,
                    "min_value": -1.5048129558563232,
                    "max_value": -1.4956779479980469
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 3.0310564041137695,
                    "std": 0.0739075317978859,
                    "min_value": 2.9787960052490234,
                    "max_value": 3.0833170413970947
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.8607197999954224,
                    "std": 0.0001823272614274174,
                    "min_value": 0.8605908751487732,
                    "max_value": 0.8608487248420715
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.8895657658576965,
                    "std": 0.027488132938742638,
                    "min_value": 0.8222106695175171,
                    "max_value": 0.9553236365318298
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.510882556438446,
                    "std": 0.2865714430809021,
                    "min_value": 1.6122790839290246e-05,
                    "max_value": 0.9999657273292542
                },
                "gradient_step": 502,
                "time_taken": 8.28929853439331,
                "__timestamp": "2024-09-23 21:52:20.605423"
            },
            {
                "step": 2000,
                "scores": {
                    "n": 16,
                    "mean": -209.76644897460938,
                    "std": 65.3976821899414,
                    "min_value": -386.5463562011719,
                    "max_value": -113.62342834472656
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -22.074474334716797,
                    "std": 0.6475023627281189,
                    "min_value": -22.53232765197754,
                    "max_value": -21.616621017456055
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -4.4692583084106445,
                    "std": 0.017037104815244675,
                    "min_value": -4.4813055992126465,
                    "max_value": -4.457211494445801
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 4.299618244171143,
                    "std": 2.223250389099121,
                    "min_value": 2.7275428771972656,
                    "max_value": 5.8716936111450195
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.6385842561721802,
                    "std": 0.00013486991520039737,
                    "min_value": 0.6384888887405396,
                    "max_value": 0.6386796236038208
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9083384871482849,
                    "std": 0.028262080624699593,
                    "min_value": 0.8415241837501526,
                    "max_value": 0.9923533797264099
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5282952785491943,
                    "std": 0.28727805614471436,
                    "min_value": 3.641098737716675e-05,
                    "max_value": 0.9995664358139038
                },
                "gradient_step": 1502,
                "time_taken": 10.253309965133667,
                "__timestamp": "2024-09-23 21:52:30.859733"
            },
            {
                "step": 3000,
                "scores": {
                    "n": 16,
                    "mean": -251.13613891601562,
                    "std": 51.71752166748047,
                    "min_value": -318.0037536621094,
                    "max_value": -98.70475769042969
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -27.86691665649414,
                    "std": 0.22948385775089264,
                    "min_value": -28.029186248779297,
                    "max_value": -27.704647064208984
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -7.516538619995117,
                    "std": 0.04409201070666313,
                    "min_value": -7.5477166175842285,
                    "max_value": -7.485361099243164
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 3.002546548843384,
                    "std": 0.12972597777843475,
                    "min_value": 2.9108164310455322,
                    "max_value": 3.0942766666412354
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.47307413816452026,
                    "std": 0.00010054130689240992,
                    "min_value": 0.4730030298233032,
                    "max_value": 0.4731452167034149
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9139084219932556,
                    "std": 0.029361030086874962,
                    "min_value": 0.8416472673416138,
                    "max_value": 0.9813246726989746
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5343121886253357,
                    "std": 0.28791913390159607,
                    "min_value": 4.76837158203125e-07,
                    "max_value": 0.9996129274368286
                },
                "gradient_step": 2502,
                "time_taken": 11.084220886230469,
                "__timestamp": "2024-09-23 21:52:41.942954"
            },
            {
                "step": 4000,
                "scores": {
                    "n": 16,
                    "mean": -239.3003692626953,
                    "std": 53.48793029785156,
                    "min_value": -343.6904602050781,
                    "max_value": -172.585205078125
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -30.698015213012695,
                    "std": 0.06232338771224022,
                    "min_value": -30.742084503173828,
                    "max_value": -30.653945922851562
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -10.292963027954102,
                    "std": 0.009159015491604805,
                    "min_value": -10.299439430236816,
                    "max_value": -10.286486625671387
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 10.176583290100098,
                    "std": 0.6079868078231812,
                    "min_value": 9.746671676635742,
                    "max_value": 10.606494903564453
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.35073190927505493,
                    "std": 7.392557017738e-05,
                    "min_value": 0.3506796360015869,
                    "max_value": 0.35078418254852295
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9211909174919128,
                    "std": 0.029066290706396103,
                    "min_value": 0.8512359857559204,
                    "max_value": 1.0162098407745361
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5404735207557678,
                    "std": 0.2880082130432129,
                    "min_value": 7.197260856628418e-06,
                    "max_value": 0.9997683167457581
                },
                "gradient_step": 3502,
                "time_taken": 10.451646089553833,
                "__timestamp": "2024-09-23 21:52:52.394601"
            },
            {
                "step": 5000,
                "scores": {
                    "n": 16,
                    "mean": -252.24911499023438,
                    "std": 50.41291046142578,
                    "min_value": -336.5832824707031,
                    "max_value": -180.2489471435547
                },
                "actor_loss": {
                    "n": [
                        2
                    ],
                    "mean": -32.45295715332031,
                    "std": 0.8584537506103516,
                    "min_value": -33.059974670410156,
                    "max_value": -31.845937728881836
                },
                "entropy_coef_loss": {
                    "n": [
                        2
                    ],
                    "mean": -13.026412963867188,
                    "std": 0.043105099350214005,
                    "min_value": -13.056893348693848,
                    "max_value": -12.995933532714844
                },
                "critic_loss": {
                    "n": [
                        2
                    ],
                    "mean": 7.512304306030273,
                    "std": 4.318911075592041,
                    "min_value": 4.458373069763184,
                    "max_value": 10.566235542297363
                },
                "entropy_coef": {
                    "n": [
                        2
                    ],
                    "mean": 0.2605847120285034,
                    "std": 5.447480725706555e-05,
                    "min_value": 0.26054617762565613,
                    "max_value": 0.2606232166290283
                },
                "action_stds": {
                    "n": [
                        2,
                        16,
                        6
                    ],
                    "mean": 0.9232713580131531,
                    "std": 0.04916307330131531,
                    "min_value": 0.8025926947593689,
                    "max_value": 1.0298649072647095
                },
                "action_magnitude": {
                    "n": 96000,
                    "mean": 0.5440846681594849,
                    "std": 0.29035162925720215,
                    "min_value": 4.3272972106933594e-05,
                    "max_value": 0.9994440078735352
                },
                "gradient_step": 4502,
                "time_taken": 10.285093784332275,
                "__timestamp": "2024-09-23 21:53:02.680696"
            }
        ]
    }
}