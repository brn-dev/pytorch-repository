{"experiment_id": "2024-10-09_22-20-07_292480~PlSFOV", "experiment_tags": ["SACDebug", "HalfCheetah-v4", "Debug"], "start_time": "2024-10-09 22:20:07.292480", "end_time": "2024-10-09 22:53:13.569797", "end_exception": null, "hyper_parameters": {"_type": "SACDebug", "_type_fq": "__main__.SACDebug", "env": "<SingletonVectorEnv instance>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {}, "policy_parameter_count": 362256, "policy_repr": "DebugSACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 9 3900X 12-Core Processor", "cores": 12, "logical_cores": 24, "speed": "3793 MHz"}, "gpu": [{"name": "NVIDIA GeForce RTX 3070", "video_processor": "NVIDIA GeForce RTX 3070", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA", "memory": "8192 MB", "memory_clock": "6800 MHz", "compute_capability": "8.6"}], "ram_speed": "3600 MHz", "ram": "64 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "from sac import init_policy, init_action_selector\nfrom stable_baselines3.common.env_util import make_vec_env\nimport stable_baselines3 as sb\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nimport gymnasium\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n\ndef create_env(render_mode: str | None):\n    return gymnasium.make(env_name, render_mode=render_mode, **env_kwargs)\n\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nsb_sac = sb.SAC(\"MlpPolicy\", env, verbose=10, learning_starts=10000, stats_window_size=1) # , seed=594371)\n\nfrom src.reinforcement_learning.core.polyak_update import polyak_update\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import ReplayBufferSamples\nfrom src.hyper_parameters import HyperParameters\nimport torch\nfrom src.reinforcement_learning.core.type_aliases import TensorObs\nfrom typing import Optional\nfrom src.reinforcement_learning.core.policies.components.feature_extractors import FeatureExtractor\nimport copy\nfrom src.console import print_warning\nfrom src.tags import Tags\nfrom src.reinforcement_learning.core.policies.components.actor import Actor\nfrom src.reinforcement_learning.core.policies.components.q_critic import QCritic\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nimport stable_baselines3 as sb\n\n\nclass DebugSACPolicy(BasePolicy):\n    \n    actor: sb.sac.policies.Actor\n\n    def __init__(\n            self,\n            actor: Actor,\n            critic: QCritic,\n            shared_feature_extractor: Optional[FeatureExtractor] = None\n    ):\n        super().__init__(actor, shared_feature_extractor)\n        self.actor = sb_sac.actor\n        self.critic = sb_sac.critic\n\n        self._build_target()\n\n        self._check_action_selector()\n        \n    @property\n    def uses_sde(self):\n        return False\n        \n    def act(self, obs: TensorObs) -> torch.Tensor:\n        return self.actor(obs, False)\n    \n    def reset_sde_noise(self, batch_size: int) -> None:\n        pass\n        \n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return {}\n\n    def collect_tags(self) -> Tags:\n        return []\n\n    def _check_action_selector(self):\n        # if not isinstance(self.actor.action_selector, (PredictedStdActionSelector, StateDependentNoiseActionSelector)):\n        #     print_warning('SAC not being used with PredictedStdAction Selector or gSDE. LogStds should be clamped!')\n        pass\n\n    def _build_target(self):\n        self.target_critic = copy.deepcopy(self.critic)\n        self.target_critic.set_training_mode(False)\n\n        self.target_shared_feature_extractor = copy.deepcopy(self.shared_feature_extractor)\n        self.target_shared_feature_extractor.set_trainable(False)\n\n    def forward(self):\n        raise NotImplementedError('forward is not used in SACPolicy')\n\n    def compute_target_values(\n            self,\n            replay_samples: ReplayBufferSamples,\n            entropy_coef: torch.Tensor,\n            gamma: float,\n    ):\n        with torch.no_grad():\n            next_observations = replay_samples.next_observations\n\n            next_actions, next_actions_log_prob = self.actor.action_log_prob(\n                self.shared_feature_extractor(next_observations)\n            )\n\n            next_q_values = torch.cat(\n                self.target_critic(self.target_shared_feature_extractor(next_observations), next_actions),\n                dim=-1\n            )\n            next_q_values, _ = torch.min(next_q_values, dim=-1, keepdim=True)\n            next_q_values = next_q_values - entropy_coef * next_actions_log_prob.reshape(-1, 1)\n\n            target_q_values = replay_samples.rewards + (1 - replay_samples.dones) * gamma * next_q_values\n\n            return target_q_values\n\n\n    def perform_polyak_update(self, tau: float):\n        polyak_update(self.critic.parameters(), self.target_critic.parameters(), tau)\n        polyak_update(\n            self.shared_feature_extractor.parameters(),\n            self.target_shared_feature_extractor.parameters(),\n            tau\n        )\n\n    def set_train_mode(self, mode: bool) -> None:\n        self.actor.set_training_mode(mode)\n        self.critic.set_training_mode(mode)\n        # Leaving target_critic on train_mode = False\n\n        self.shared_feature_extractor.set_train_mode(mode)\n        # Leaving target_shared_feature_extractor on train_mode = False\n\n        self.train_mode = mode\n\nfrom src.reinforcement_learning.algorithms.sac.sac import SACLoggingConfig, SAC\nfrom dataclasses import dataclass\nfrom typing import Type, Optional, Any, Literal\n\nimport gymnasium\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom src.function_types import TorchTensorFn\nfrom src.module_analysis import calculate_grad_norm\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.algorithms.base.base_algorithm import PolicyProvider\nfrom src.reinforcement_learning.algorithms.base.off_policy_algorithm import OffPolicyAlgorithm, ReplayBuf\nfrom src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\nfrom src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\nfrom src.reinforcement_learning.core.action_noise import ActionNoise\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import BaseReplayBuffer, ReplayBufferSamples\nfrom src.reinforcement_learning.core.buffers.replay.replay_buffer import ReplayBuffer\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.core.infos import InfoDict, concat_infos\nfrom src.reinforcement_learning.core.logging import LoggingConfig, log_if_enabled\nfrom src.reinforcement_learning.core.loss_config import weigh_and_reduce_loss, LossLoggingConfig\nfrom src.reinforcement_learning.core.type_aliases import OptimizerProvider, TensorObs, detach_obs\nfrom src.reinforcement_learning.gym.env_analysis import get_single_action_space\nfrom src.tags import Tags\nfrom src.torch_device import TorchDevice\nfrom src.torch_functions import identity\nfrom src.repr_utils import func_repr\n\nfrom typing import Literal\n\nSAC_DEFAULT_OPTIMIZER_PROVIDER = lambda params: optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nAUTO_TARGET_ENTROPY = 'auto'\n\n\nclass SACDebug(SAC):\n    \n    @property\n    def replay_buffer(self):\n        return self.buffer\n\n    buffer: BaseReplayBuffer\n    target_entropy: float\n    log_ent_coef: Optional[torch.Tensor]\n    entropy_coef_optimizer: Optional[optim.Optimizer]\n    entropy_coef_tensor: Optional[torch.Tensor]\n    \n    def collect_hyper_parameters(self) -> HyperParameters:\n        print(f'{type(self.policy) = }, {type(self.policy.actor) = }, {type(self.policy.critic) = }, {type(self.policy.target_critic) = }, {type(self.buffer) = }')\n        return super().collect_hyper_parameters()\n    \n    \n    def _setup_entropy_optimization(\n            self,\n            entropy_coef: float,\n            target_entropy: float | Literal['auto'],\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider],\n    ):\n        if target_entropy == 'auto':\n            self.target_entropy = float(-np.prod(get_single_action_space(self.env).shape).astype(np.float32))\n        else:\n            self.target_entropy = float(target_entropy)\n\n        if entropy_coef_optimizer_provider is not None:\n            self.log_ent_coef = torch.log(\n                torch.tensor([entropy_coef], device=self.torch_device, dtype=self.torch_dtype)\n            ).requires_grad_(True)\n            self.entropy_coef_optimizer = entropy_coef_optimizer_provider([self.log_ent_coef])\n            self.entropy_coef_tensor = None\n        else:\n            self.log_ent_coef = None\n            self.entropy_coef_optimizer = None\n            self.entropy_coef_tensor = torch.tensor(entropy_coef, device=self.torch_device, dtype=self.torch_dtype)\n\n    # def get_and_optimize_entropy_coef(\n    #         self,\n    #         actions_pi_log_prob: torch.Tensor,\n    #         info: InfoDict\n    # ) -> torch.Tensor:\n    #     if self.entropy_coef_optimizer is not None:\n    #         entropy_coef = torch.exp(self.log_ent_coef.detach())\n    # \n    #         entropy_coef_loss = weigh_and_reduce_loss(\n    #             raw_loss=-self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach(),\n    #             weigh_and_reduce_function=self.weigh_and_reduce_entropy_coef_loss,\n    #             info=info,\n    #             loss_name='entropy_coef_loss',\n    #             logging_config=self.logging_config.entropy_coef_loss\n    #         )\n    #         self.entropy_coef_optimizer.zero_grad()\n    #         entropy_coef_loss.backward()\n    #         self.entropy_coef_optimizer.step()\n    # \n    #         return entropy_coef\n    #     else:\n    #         return self.entropy_coef_tensor\n    # \n    # def calculate_critic_loss(\n    #         self,\n    #         observation_features: TensorObs,\n    #         replay_samples: ReplayBufferSamples,\n    #         entropy_coef: torch.Tensor,\n    #         info: InfoDict,\n    # ):\n    #     target_q_values = self.policy.compute_target_values(\n    #         replay_samples=replay_samples,\n    #         entropy_coef=entropy_coef,\n    #         gamma=self.gamma,\n    #     )\n    #     # critic loss should not influence shared feature extractor\n    #     current_q_values = self.critic(detach_obs(observation_features), replay_samples.actions)\n    # \n    #     # noinspection PyTypeChecker\n    #     critic_loss: torch.Tensor = 0.5 * sum(\n    #         F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n    #     )\n    #     critic_loss = weigh_and_reduce_loss(\n    #         raw_loss=critic_loss,\n    #         weigh_and_reduce_function=self.weigh_critic_loss,\n    #         info=info,\n    #         loss_name='critic_loss',\n    #         logging_config=self.logging_config.critic_loss,\n    #     )\n    #     return critic_loss\n    # \n    # def calculate_actor_loss(\n    #         self,\n    #         observation_features: TensorObs,\n    #         actions_pi: torch.Tensor,\n    #         actions_pi_log_prob: torch.Tensor,\n    #         entropy_coef: torch.Tensor,\n    #         info: InfoDict,\n    # ) -> torch.Tensor:\n    #     q_values_pi = torch.cat(self.critic(observation_features, actions_pi), dim=-1)\n    #     min_q_values_pi, _ = torch.min(q_values_pi, dim=-1, keepdim=True)\n    #     actor_loss = entropy_coef * actions_pi_log_prob - min_q_values_pi\n    # \n    #     actor_loss = weigh_and_reduce_loss(\n    #         raw_loss=actor_loss,\n    #         weigh_and_reduce_function=self.weigh_and_reduce_actor_loss,\n    #         info=info,\n    #         loss_name='actor_loss',\n    #         logging_config=self.logging_config.actor_loss,\n    #     )\n    # \n    #     return actor_loss\n\n    def optimize(self, last_obs: np.ndarray, last_episode_starts: np.ndarray, info: InfoDict) -> None:\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(self.gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(self.optimization_batch_size, env=None)  # type: ignore[union-attr]\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            # if self.sde_noise_sample_freq:\n            #     self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.entropy_coef_optimizer is not None and self.log_ent_coef is not None:\n                # Important: detach the variable from the graph\n                # so we don't change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = torch.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.entropy_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None and self.entropy_coef_optimizer is not None:\n                self.entropy_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.entropy_coef_optimizer.step()\n\n            with torch.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the next Q values: min over all critics targets\n                next_q_values = torch.cat(self.policy.target_critic(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = torch.min(next_q_values, dim=1, keepdim=True)\n                # add entropy term\n                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n                # td error + entropy term\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            # using action from the replay buffer\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            assert isinstance(critic_loss, torch.Tensor)  # for type checker\n            critic_losses.append(critic_loss.item())  # type: ignore[union-attr]\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = torch.mean(log_prob - qf1_pi)\n            # Min over all critic networks\n            q_values_pi = torch.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n            min_qf_pi, _ = torch.min(q_values_pi, dim=1, keepdim=True)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                polyak_update(self.critic.parameters(), self.policy.target_critic.parameters(), self.tau)\n                # Copy running stats, see GH issue #996\n        \n        info['entropy_coef'] = np.array(ent_coefs)\n        info['final_entropy_coef_loss'] = np.array(ent_coef_losses)\n        info['final_actor_loss'] = np.array(actor_losses)\n        info['final_critic_loss'] = np.array(critic_losses)\n\nimport inspect\nimport time\n\nfrom gymnasium import Env\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.module_analysis import count_parameters\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import PolicyConstruction\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom typing import Any\nfrom src.reinforcement_learning.core.callback import Callback\n\nimport torch\nfrom torch import optim\nimport gymnasium as gym\nimport numpy as np\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -3.4189e+02}, "actor_loss": {"n": 1, "mean": -1.7525e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.9737e+00}, "critic_loss": {"n": 1, "mean": 8.5975e-01}, "entropy_coef": {"n": 1, "mean": 7.4052e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.1847e-01, "std": 2.8996e-01, "min_value": 2.1838e-05, "max_value": 9.9820e-01}, "num_gradient_steps": 0, "step_time": 1.4239e+01, "total_time": 1.4228e+01, "__timestamp": "2024-10-09 22:20:21.521493"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -3.4586e+02}, "actor_loss": {"n": 1, "mean": -2.5175e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9795e+00}, "critic_loss": {"n": 1, "mean": 9.6969e-01}, "entropy_coef": {"n": 1, "mean": 5.4906e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.2651e-01, "std": 2.8837e-01, "min_value": 1.6049e-05, "max_value": 9.9886e-01}, "num_gradient_steps": 0, "step_time": 1.0509e+01, "total_time": 2.4737e+01, "__timestamp": "2024-10-09 22:20:32.030941"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -2.5621e+02}, "actor_loss": {"n": 1, "mean": -2.9583e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.8986e+00}, "critic_loss": {"n": 1, "mean": 1.2367e+00}, "entropy_coef": {"n": 1, "mean": 4.0684e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3752e-01, "std": 2.8739e-01, "min_value": 1.1146e-05, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 1.0369e+01, "total_time": 3.5106e+01, "__timestamp": "2024-10-09 22:20:42.398802"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -2.2235e+02}, "actor_loss": {"n": 1, "mean": -3.1549e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1573e+01}, "critic_loss": {"n": 1, "mean": 1.2897e+00}, "entropy_coef": {"n": 1, "mean": 3.0249e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4106e-01, "std": 2.8902e-01, "min_value": 1.0449e-04, "max_value": 9.9854e-01}, "num_gradient_steps": 0, "step_time": 1.0335e+01, "total_time": 4.5442e+01, "__timestamp": "2024-10-09 22:20:52.733988"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -2.8903e+02}, "actor_loss": {"n": 1, "mean": -3.2522e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3564e+01}, "critic_loss": {"n": 1, "mean": 1.4243e+00}, "entropy_coef": {"n": 1, "mean": 2.2611e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.5840e-01, "std": 2.9153e-01, "min_value": 6.3360e-05, "max_value": 9.9877e-01}, "num_gradient_steps": 0, "step_time": 1.0732e+01, "total_time": 5.6174e+01, "__timestamp": "2024-10-09 22:21:03.467007"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -2.8581e+02}, "actor_loss": {"n": 1, "mean": -3.2839e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5001e+01}, "critic_loss": {"n": 1, "mean": 3.5587e+00}, "entropy_coef": {"n": 1, "mean": 1.6996e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.6590e-01, "std": 2.9050e-01, "min_value": 1.9707e-06, "max_value": 9.9891e-01}, "num_gradient_steps": 0, "step_time": 1.0586e+01, "total_time": 6.6760e+01, "__timestamp": "2024-10-09 22:21:14.053366"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -2.0742e+02}, "actor_loss": {"n": 1, "mean": -3.2165e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6035e+01}, "critic_loss": {"n": 1, "mean": 3.7748e+00}, "entropy_coef": {"n": 1, "mean": 1.2845e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.6955e-01, "std": 2.9464e-01, "min_value": 1.1603e-04, "max_value": 9.9931e-01}, "num_gradient_steps": 0, "step_time": 1.0926e+01, "total_time": 7.7686e+01, "__timestamp": "2024-10-09 22:21:24.979135"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -2.1478e+02}, "actor_loss": {"n": 1, "mean": -3.1520e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6509e+01}, "critic_loss": {"n": 1, "mean": 4.0555e+00}, "entropy_coef": {"n": 1, "mean": 9.7802e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8576e-01, "std": 2.9680e-01, "min_value": 1.5390e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.0901e+01, "total_time": 8.8586e+01, "__timestamp": "2024-10-09 22:21:35.879804"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -1.8667e+02}, "actor_loss": {"n": 1, "mean": -2.9867e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6065e+01}, "critic_loss": {"n": 1, "mean": 2.5819e+00}, "entropy_coef": {"n": 1, "mean": 7.4984e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0761e-01, "std": 2.9777e-01, "min_value": 1.7488e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0567e+01, "total_time": 9.9153e+01, "__timestamp": "2024-10-09 22:21:46.446961"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -1.2780e+02}, "actor_loss": {"n": 1, "mean": -2.9672e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4893e+01}, "critic_loss": {"n": 1, "mean": 1.2198e+00}, "entropy_coef": {"n": 1, "mean": 5.7689e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1065e-01, "std": 2.9808e-01, "min_value": 5.0125e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.0134e+01, "total_time": 1.0929e+02, "__timestamp": "2024-10-09 22:21:56.580138"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -3.3229e+02}, "actor_loss": {"n": 1, "mean": -2.9355e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3799e+01}, "critic_loss": {"n": 1, "mean": 1.4111e+00}, "entropy_coef": {"n": 1, "mean": 4.4479e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.3873e-01, "std": 2.9736e-01, "min_value": 4.7028e-05, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0270e+01, "total_time": 1.1956e+02, "__timestamp": "2024-10-09 22:22:06.849643"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -2.1652e+02}, "actor_loss": {"n": 1, "mean": -2.6823e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3020e+01}, "critic_loss": {"n": 1, "mean": 3.0899e+00}, "entropy_coef": {"n": 1, "mean": 3.4332e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2575e-01, "std": 2.9782e-01, "min_value": 1.4782e-05, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0317e+01, "total_time": 1.2987e+02, "__timestamp": "2024-10-09 22:22:17.167341"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -5.0784e+02}, "actor_loss": {"n": 1, "mean": -2.6010e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3211e+01}, "critic_loss": {"n": 1, "mean": 1.5702e+00}, "entropy_coef": {"n": 1, "mean": 2.6449e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4800e-01, "std": 2.9976e-01, "min_value": 8.6784e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.7175e+00, "total_time": 1.3959e+02, "__timestamp": "2024-10-09 22:22:26.884866"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -1.0157e+02}, "actor_loss": {"n": 1, "mean": -2.4509e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4192e+01}, "critic_loss": {"n": 1, "mean": 1.0777e+00}, "entropy_coef": {"n": 1, "mean": 2.0331e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0925e-01, "std": 3.0199e-01, "min_value": 4.7863e-05, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 9.9931e+00, "total_time": 1.4958e+02, "__timestamp": "2024-10-09 22:22:36.876967"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": -4.4656e+02}, "actor_loss": {"n": 1, "mean": -2.2143e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1234e+01}, "critic_loss": {"n": 1, "mean": 1.3532e+00}, "entropy_coef": {"n": 1, "mean": 1.5808e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8903e-01, "std": 2.9805e-01, "min_value": 1.3351e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 9.5680e+00, "total_time": 1.5915e+02, "__timestamp": "2024-10-09 22:22:46.444979"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": -3.0140e+02}, "actor_loss": {"n": 1, "mean": -1.9850e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.0430e+01}, "critic_loss": {"n": 1, "mean": 1.4818e+00}, "entropy_coef": {"n": 1, "mean": 1.2319e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3981e-01, "std": 2.9325e-01, "min_value": 1.9491e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.3841e+00, "total_time": 1.6854e+02, "__timestamp": "2024-10-09 22:22:55.830037"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": -8.9796e+01}, "actor_loss": {"n": 1, "mean": -1.9365e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.8172e+00}, "critic_loss": {"n": 1, "mean": 1.4759e+00}, "entropy_coef": {"n": 1, "mean": 9.8456e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3262e-01, "std": 2.9859e-01, "min_value": 4.8071e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 9.6334e+00, "total_time": 1.7817e+02, "__timestamp": "2024-10-09 22:23:05.462429"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": -1.6017e+01}, "actor_loss": {"n": 1, "mean": -1.8428e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.5753e+00}, "critic_loss": {"n": 1, "mean": 1.9698e+00}, "entropy_coef": {"n": 1, "mean": 8.0987e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.0367e-01, "std": 2.9162e-01, "min_value": 2.9588e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0226e+01, "total_time": 1.8840e+02, "__timestamp": "2024-10-09 22:23:15.688472"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": 4.7784e+02}, "actor_loss": {"n": 1, "mean": -1.7253e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.2308e+00}, "critic_loss": {"n": 1, "mean": 1.2306e+00}, "entropy_coef": {"n": 1, "mean": 6.9858e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8286e-01, "std": 3.0442e-01, "min_value": 4.7158e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.3166e+00, "total_time": 1.9771e+02, "__timestamp": "2024-10-09 22:23:25.006060"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": 7.5533e+02}, "actor_loss": {"n": 1, "mean": -1.7866e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.1651e+00}, "critic_loss": {"n": 1, "mean": 1.3518e+00}, "entropy_coef": {"n": 1, "mean": 6.5909e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4900e-01, "std": 3.0157e-01, "min_value": 1.1916e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.4795e+00, "total_time": 2.0719e+02, "__timestamp": "2024-10-09 22:23:34.484607"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": 8.8700e+02}, "actor_loss": {"n": 1, "mean": -1.7752e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.4053e-01}, "critic_loss": {"n": 1, "mean": 1.9541e+00}, "entropy_coef": {"n": 1, "mean": 7.0462e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.8534e-01, "std": 3.0247e-01, "min_value": 6.6817e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.7610e+00, "total_time": 2.1695e+02, "__timestamp": "2024-10-09 22:23:44.246647"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": 1.0524e+03}, "actor_loss": {"n": 1, "mean": -1.8893e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.6356e-01}, "critic_loss": {"n": 1, "mean": 1.3556e+00}, "entropy_coef": {"n": 1, "mean": 8.2144e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2082e-01, "std": 2.9717e-01, "min_value": 7.4959e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6191e+00, "total_time": 2.2657e+02, "__timestamp": "2024-10-09 22:23:53.865764"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 1.1432e+03}, "actor_loss": {"n": 1, "mean": -1.8358e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.3300e-01}, "critic_loss": {"n": 1, "mean": 1.8550e+00}, "entropy_coef": {"n": 1, "mean": 9.0332e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2016e-01, "std": 2.9394e-01, "min_value": 2.4617e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.8843e+00, "total_time": 2.3646e+02, "__timestamp": "2024-10-09 22:24:03.750072"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 6.9813e+02}, "actor_loss": {"n": 1, "mean": -1.8892e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.8538e-01}, "critic_loss": {"n": 1, "mean": 1.8266e+00}, "entropy_coef": {"n": 1, "mean": 9.9727e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.9467e-01, "std": 2.9821e-01, "min_value": 4.6229e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.7414e+00, "total_time": 2.4620e+02, "__timestamp": "2024-10-09 22:24:13.491427"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 9.6024e+02}, "actor_loss": {"n": 1, "mean": -2.0230e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.7120e+00}, "critic_loss": {"n": 1, "mean": 2.1567e+00}, "entropy_coef": {"n": 1, "mean": 1.0499e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0030e-01, "std": 3.0178e-01, "min_value": 7.5996e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0012e+01, "total_time": 2.5621e+02, "__timestamp": "2024-10-09 22:24:23.502967"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 1.2375e+03}, "actor_loss": {"n": 1, "mean": -2.1592e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.8514e+00}, "critic_loss": {"n": 1, "mean": 2.2527e+00}, "entropy_coef": {"n": 1, "mean": 1.1132e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0797e-01, "std": 2.9629e-01, "min_value": 7.9370e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.8745e+00, "total_time": 2.6608e+02, "__timestamp": "2024-10-09 22:24:33.376467"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 1.8744e+02}, "actor_loss": {"n": 1, "mean": -2.0348e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.5912e-01}, "critic_loss": {"n": 1, "mean": 2.6678e+00}, "entropy_coef": {"n": 1, "mean": 1.1712e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7429e-01, "std": 2.9952e-01, "min_value": 5.3549e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.7260e+00, "total_time": 2.7581e+02, "__timestamp": "2024-10-09 22:24:43.102474"}, {"step": 38000, "num_env_steps": 38000, "scores": {"n": 1, "mean": 2.3758e+02}, "actor_loss": {"n": 1, "mean": -2.1568e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.9574e-01}, "critic_loss": {"n": 1, "mean": 2.1845e+00}, "entropy_coef": {"n": 1, "mean": 1.2195e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7633e-01, "std": 3.0029e-01, "min_value": 2.1940e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.5688e+00, "total_time": 2.8538e+02, "__timestamp": "2024-10-09 22:24:52.672317"}, {"step": 39000, "num_env_steps": 39000, "scores": {"n": 1, "mean": 1.4704e+03}, "actor_loss": {"n": 1, "mean": -2.2374e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3890e+00}, "critic_loss": {"n": 1, "mean": 2.8435e+00}, "entropy_coef": {"n": 1, "mean": 1.2649e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3363e-01, "std": 2.8759e-01, "min_value": 2.3826e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6796e+00, "total_time": 2.9506e+02, "__timestamp": "2024-10-09 22:25:02.351873"}, {"step": 40000, "num_env_steps": 40000, "scores": {"n": 1, "mean": 1.8192e+03}, "actor_loss": {"n": 1, "mean": -2.3465e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.7844e-02}, "critic_loss": {"n": 1, "mean": 2.2500e+00}, "entropy_coef": {"n": 1, "mean": 1.3919e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3964e-01, "std": 2.9257e-01, "min_value": 1.4483e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6737e+00, "total_time": 3.0473e+02, "__timestamp": "2024-10-09 22:25:12.024575"}, {"step": 41000, "num_env_steps": 41000, "scores": {"n": 1, "mean": 1.9053e+03}, "actor_loss": {"n": 1, "mean": -2.6652e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.0210e+00}, "critic_loss": {"n": 1, "mean": 2.3749e+00}, "entropy_coef": {"n": 1, "mean": 1.5142e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4254e-01, "std": 2.9212e-01, "min_value": 2.9820e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.3468e+00, "total_time": 3.1408e+02, "__timestamp": "2024-10-09 22:25:21.372389"}, {"step": 42000, "num_env_steps": 42000, "scores": {"n": 1, "mean": 1.1130e+03}, "actor_loss": {"n": 1, "mean": -2.6616e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5779e-01}, "critic_loss": {"n": 1, "mean": 4.4163e+00}, "entropy_coef": {"n": 1, "mean": 1.6311e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0718e-01, "std": 2.9517e-01, "min_value": 1.1416e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.3452e+00, "total_time": 3.2342e+02, "__timestamp": "2024-10-09 22:25:30.716624"}, {"step": 43000, "num_env_steps": 43000, "scores": {"n": 1, "mean": 4.4631e+02}, "actor_loss": {"n": 1, "mean": -2.8712e+01}, "entropy_coef_loss": {"n": 1, "mean": 8.0170e-01}, "critic_loss": {"n": 1, "mean": 2.8333e+00}, "entropy_coef": {"n": 1, "mean": 1.7640e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.8418e-01, "std": 3.0103e-01, "min_value": 7.6283e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.7759e+00, "total_time": 3.3320e+02, "__timestamp": "2024-10-09 22:25:40.492487"}, {"step": 44000, "num_env_steps": 44000, "scores": {"n": 1, "mean": 1.7605e+03}, "actor_loss": {"n": 1, "mean": -3.3482e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2949e+00}, "critic_loss": {"n": 1, "mean": 3.0918e+00}, "entropy_coef": {"n": 1, "mean": 1.7799e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5462e-01, "std": 2.8186e-01, "min_value": 1.0817e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.2328e+00, "total_time": 3.4243e+02, "__timestamp": "2024-10-09 22:25:49.725278"}, {"step": 45000, "num_env_steps": 45000, "scores": {"n": 1, "mean": 2.0731e+03}, "actor_loss": {"n": 1, "mean": -3.2132e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2700e+00}, "critic_loss": {"n": 1, "mean": 4.0488e+00}, "entropy_coef": {"n": 1, "mean": 1.9133e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6864e-01, "std": 2.7939e-01, "min_value": 1.0547e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6848e+00, "total_time": 3.5212e+02, "__timestamp": "2024-10-09 22:25:59.410040"}, {"step": 46000, "num_env_steps": 46000, "scores": {"n": 1, "mean": -3.2141e+01}, "actor_loss": {"n": 1, "mean": -3.8465e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.0779e+00}, "critic_loss": {"n": 1, "mean": 3.8898e+00}, "entropy_coef": {"n": 1, "mean": 1.9272e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4463e-01, "std": 3.0106e-01, "min_value": 7.3048e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.5081e+00, "total_time": 3.6163e+02, "__timestamp": "2024-10-09 22:26:08.918175"}, {"step": 47000, "num_env_steps": 47000, "scores": {"n": 1, "mean": 5.3629e+02}, "actor_loss": {"n": 1, "mean": -3.4704e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.8896e-01}, "critic_loss": {"n": 1, "mean": 2.2675e+01}, "entropy_coef": {"n": 1, "mean": 1.9475e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7821e-01, "std": 2.9819e-01, "min_value": 1.1444e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.7895e+00, "total_time": 3.7142e+02, "__timestamp": "2024-10-09 22:26:18.707646"}, {"step": 48000, "num_env_steps": 48000, "scores": {"n": 1, "mean": 5.6526e+02}, "actor_loss": {"n": 1, "mean": -3.3029e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.5736e-01}, "critic_loss": {"n": 1, "mean": 3.9893e+00}, "entropy_coef": {"n": 1, "mean": 1.9925e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.8350e-01, "std": 2.9625e-01, "min_value": 4.1406e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0132e+01, "total_time": 3.8155e+02, "__timestamp": "2024-10-09 22:26:28.839471"}, {"step": 49000, "num_env_steps": 49000, "scores": {"n": 1, "mean": 2.2112e+03}, "actor_loss": {"n": 1, "mean": -4.0106e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.5217e+00}, "critic_loss": {"n": 1, "mean": 3.7737e+00}, "entropy_coef": {"n": 1, "mean": 2.0039e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7807e-01, "std": 2.6888e-01, "min_value": 8.4013e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.3851e+00, "total_time": 3.9093e+02, "__timestamp": "2024-10-09 22:26:38.225566"}, {"step": 50000, "num_env_steps": 50000, "scores": {"n": 1, "mean": 2.3471e+03}, "actor_loss": {"n": 1, "mean": -4.4093e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2008e+00}, "critic_loss": {"n": 1, "mean": 3.8854e+00}, "entropy_coef": {"n": 1, "mean": 2.1462e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1903e-01, "std": 2.4673e-01, "min_value": 2.9974e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.2319e+00, "total_time": 4.0016e+02, "__timestamp": "2024-10-09 22:26:47.457421"}, {"step": 51000, "num_env_steps": 51000, "scores": {"n": 1, "mean": 2.1069e+03}, "actor_loss": {"n": 1, "mean": -4.4750e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.9029e-01}, "critic_loss": {"n": 1, "mean": 5.8843e+00}, "entropy_coef": {"n": 1, "mean": 2.2119e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1218e-01, "std": 2.4723e-01, "min_value": 1.3102e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.7281e+00, "total_time": 4.0989e+02, "__timestamp": "2024-10-09 22:26:57.185493"}, {"step": 52000, "num_env_steps": 52000, "scores": {"n": 1, "mean": 2.3254e+03}, "actor_loss": {"n": 1, "mean": -4.7101e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.6441e+00}, "critic_loss": {"n": 1, "mean": 4.8919e+00}, "entropy_coef": {"n": 1, "mean": 2.3450e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2156e-01, "std": 2.4332e-01, "min_value": 3.9941e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.6782e+00, "total_time": 4.1957e+02, "__timestamp": "2024-10-09 22:27:06.862731"}, {"step": 53000, "num_env_steps": 53000, "scores": {"n": 1, "mean": 2.4350e+03}, "actor_loss": {"n": 1, "mean": -4.2421e+01}, "entropy_coef_loss": {"n": 1, "mean": -6.0043e-01}, "critic_loss": {"n": 1, "mean": 7.6101e+00}, "entropy_coef": {"n": 1, "mean": 2.4193e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1541e-01, "std": 2.4114e-01, "min_value": 2.2467e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.8100e+00, "total_time": 4.2938e+02, "__timestamp": "2024-10-09 22:27:16.673720"}, {"step": 54000, "num_env_steps": 54000, "scores": {"n": 1, "mean": 2.1707e+03}, "actor_loss": {"n": 1, "mean": -4.7072e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.8710e-01}, "critic_loss": {"n": 1, "mean": 3.3771e+00}, "entropy_coef": {"n": 1, "mean": 2.5085e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0635e-01, "std": 2.5145e-01, "min_value": 5.4400e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6365e+00, "total_time": 4.3902e+02, "__timestamp": "2024-10-09 22:27:26.309238"}, {"step": 55000, "num_env_steps": 55000, "scores": {"n": 1, "mean": 2.1739e+03}, "actor_loss": {"n": 1, "mean": -4.7581e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4424e+00}, "critic_loss": {"n": 1, "mean": 4.4904e+00}, "entropy_coef": {"n": 1, "mean": 2.6831e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2038e-01, "std": 2.3905e-01, "min_value": 1.0988e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.6014e+00, "total_time": 4.4862e+02, "__timestamp": "2024-10-09 22:27:35.910604"}, {"step": 56000, "num_env_steps": 56000, "scores": {"n": 1, "mean": 2.2968e+03}, "actor_loss": {"n": 1, "mean": -5.6018e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.0885e+00}, "critic_loss": {"n": 1, "mean": 5.6177e+00}, "entropy_coef": {"n": 1, "mean": 2.8447e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1323e-01, "std": 2.4575e-01, "min_value": 1.3787e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.3492e+00, "total_time": 4.5797e+02, "__timestamp": "2024-10-09 22:27:45.260851"}, {"step": 57000, "num_env_steps": 57000, "scores": {"n": 1, "mean": 2.0310e+03}, "actor_loss": {"n": 1, "mean": -5.6853e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.5007e-01}, "critic_loss": {"n": 1, "mean": 4.6845e+00}, "entropy_coef": {"n": 1, "mean": 2.8532e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8535e-01, "std": 2.6346e-01, "min_value": 4.1845e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.6980e+00, "total_time": 4.6767e+02, "__timestamp": "2024-10-09 22:27:54.957877"}, {"step": 58000, "num_env_steps": 58000, "scores": {"n": 1, "mean": 2.2110e+03}, "actor_loss": {"n": 1, "mean": -5.5312e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.7113e-01}, "critic_loss": {"n": 1, "mean": 4.4840e+00}, "entropy_coef": {"n": 1, "mean": 2.9058e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7923e-01, "std": 2.6861e-01, "min_value": 7.0205e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0066e+01, "total_time": 4.7773e+02, "__timestamp": "2024-10-09 22:28:05.023608"}, {"step": 59000, "num_env_steps": 59000, "scores": {"n": 1, "mean": 2.4703e+03}, "actor_loss": {"n": 1, "mean": -6.2559e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.2868e+00}, "critic_loss": {"n": 1, "mean": 6.0608e+00}, "entropy_coef": {"n": 1, "mean": 3.0207e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9091e-01, "std": 2.5946e-01, "min_value": 1.1864e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 9.4953e+00, "total_time": 4.8723e+02, "__timestamp": "2024-10-09 22:28:14.519869"}, {"step": 60000, "num_env_steps": 60000, "scores": {"n": 1, "mean": 2.3051e+03}, "actor_loss": {"n": 1, "mean": -6.2286e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.1480e+00}, "critic_loss": {"n": 1, "mean": 5.1496e+00}, "entropy_coef": {"n": 1, "mean": 3.1089e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8621e-01, "std": 2.5767e-01, "min_value": 3.5468e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 9.5366e+00, "total_time": 4.9676e+02, "__timestamp": "2024-10-09 22:28:24.055433"}, {"step": 61000, "num_env_steps": 61000, "scores": {"n": 1, "mean": 2.3991e+03}, "actor_loss": {"n": 1, "mean": -6.4371e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.4438e+00}, "critic_loss": {"n": 1, "mean": 6.9388e+00}, "entropy_coef": {"n": 1, "mean": 3.1928e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7958e-01, "std": 2.6341e-01, "min_value": 2.0303e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0061e+01, "total_time": 5.0682e+02, "__timestamp": "2024-10-09 22:28:34.117495"}, {"step": 62000, "num_env_steps": 62000, "scores": {"n": 1, "mean": 2.5138e+03}, "actor_loss": {"n": 1, "mean": -6.8244e+01}, "entropy_coef_loss": {"n": 1, "mean": 6.6303e-01}, "critic_loss": {"n": 1, "mean": 8.0493e+00}, "entropy_coef": {"n": 1, "mean": 3.2461e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9464e-01, "std": 2.5245e-01, "min_value": 1.2839e-03, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.8948e+00, "total_time": 5.1672e+02, "__timestamp": "2024-10-09 22:28:44.012250"}, {"step": 63000, "num_env_steps": 63000, "scores": {"n": 1, "mean": 2.5754e+03}, "actor_loss": {"n": 1, "mean": -7.4450e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6191e+00}, "critic_loss": {"n": 1, "mean": 6.0067e+00}, "entropy_coef": {"n": 1, "mean": 3.4120e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0356e-01, "std": 2.4985e-01, "min_value": 4.6137e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.4653e+00, "total_time": 5.2618e+02, "__timestamp": "2024-10-09 22:28:53.476556"}, {"step": 64000, "num_env_steps": 64000, "scores": {"n": 1, "mean": 2.6922e+03}, "actor_loss": {"n": 1, "mean": -6.6408e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.9928e-02}, "critic_loss": {"n": 1, "mean": 6.5357e+01}, "entropy_coef": {"n": 1, "mean": 3.5288e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1501e-01, "std": 2.4693e-01, "min_value": 6.0439e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 9.8611e+00, "total_time": 5.3605e+02, "__timestamp": "2024-10-09 22:29:03.337656"}, {"step": 65000, "num_env_steps": 65000, "scores": {"n": 1, "mean": 2.7483e+03}, "actor_loss": {"n": 1, "mean": -7.0560e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.6963e-01}, "critic_loss": {"n": 1, "mean": 5.3313e+00}, "entropy_coef": {"n": 1, "mean": 3.5500e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0296e-01, "std": 2.5043e-01, "min_value": 1.5408e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.5819e+00, "total_time": 5.4563e+02, "__timestamp": "2024-10-09 22:29:12.920540"}, {"step": 66000, "num_env_steps": 66000, "scores": {"n": 1, "mean": 3.0899e+03}, "actor_loss": {"n": 1, "mean": -7.8268e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.9150e-01}, "critic_loss": {"n": 1, "mean": 5.2647e+00}, "entropy_coef": {"n": 1, "mean": 3.7176e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0696e-01, "std": 2.5282e-01, "min_value": 4.3809e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.7370e+00, "total_time": 5.5536e+02, "__timestamp": "2024-10-09 22:29:22.657529"}, {"step": 67000, "num_env_steps": 67000, "scores": {"n": 1, "mean": 3.1730e+03}, "actor_loss": {"n": 1, "mean": -8.2848e+01}, "entropy_coef_loss": {"n": 1, "mean": 6.7678e-01}, "critic_loss": {"n": 1, "mean": 6.1368e+00}, "entropy_coef": {"n": 1, "mean": 3.9613e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2583e-01, "std": 2.4019e-01, "min_value": 2.2912e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.7342e+00, "total_time": 5.6510e+02, "__timestamp": "2024-10-09 22:29:32.390680"}, {"step": 68000, "num_env_steps": 68000, "scores": {"n": 1, "mean": 3.1914e+03}, "actor_loss": {"n": 1, "mean": -7.9170e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.2558e-01}, "critic_loss": {"n": 1, "mean": 1.4495e+01}, "entropy_coef": {"n": 1, "mean": 4.1315e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2336e-01, "std": 2.4700e-01, "min_value": 7.0930e-06, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.8732e+00, "total_time": 5.7497e+02, "__timestamp": "2024-10-09 22:29:42.263856"}, {"step": 69000, "num_env_steps": 69000, "scores": {"n": 1, "mean": 3.3718e+03}, "actor_loss": {"n": 1, "mean": -8.6222e+01}, "entropy_coef_loss": {"n": 1, "mean": 7.0326e-01}, "critic_loss": {"n": 1, "mean": 6.9826e+00}, "entropy_coef": {"n": 1, "mean": 4.3302e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4182e-01, "std": 2.2871e-01, "min_value": 2.3821e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.4674e+00, "total_time": 5.8444e+02, "__timestamp": "2024-10-09 22:29:51.731215"}, {"step": 70000, "num_env_steps": 70000, "scores": {"n": 1, "mean": 3.2444e+03}, "actor_loss": {"n": 1, "mean": -9.1294e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9151e-03}, "critic_loss": {"n": 1, "mean": 6.6645e+00}, "entropy_coef": {"n": 1, "mean": 4.4067e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4132e-01, "std": 2.2698e-01, "min_value": 1.1206e-03, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.8644e+00, "total_time": 5.9430e+02, "__timestamp": "2024-10-09 22:30:01.595623"}, {"step": 71000, "num_env_steps": 71000, "scores": {"n": 1, "mean": 3.5457e+03}, "actor_loss": {"n": 1, "mean": -9.0067e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1503e+00}, "critic_loss": {"n": 1, "mean": 6.4450e+00}, "entropy_coef": {"n": 1, "mean": 4.5405e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4369e-01, "std": 2.2880e-01, "min_value": 4.2760e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.5362e+00, "total_time": 6.0384e+02, "__timestamp": "2024-10-09 22:30:11.132835"}, {"step": 72000, "num_env_steps": 72000, "scores": {"n": 1, "mean": 3.1575e+03}, "actor_loss": {"n": 1, "mean": -9.8338e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.8295e-01}, "critic_loss": {"n": 1, "mean": 9.5598e+00}, "entropy_coef": {"n": 1, "mean": 4.7059e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2552e-01, "std": 2.4161e-01, "min_value": 2.1435e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.6507e+00, "total_time": 6.1349e+02, "__timestamp": "2024-10-09 22:30:20.783575"}, {"step": 73000, "num_env_steps": 73000, "scores": {"n": 1, "mean": 3.4219e+03}, "actor_loss": {"n": 1, "mean": -9.2982e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.6022e-01}, "critic_loss": {"n": 1, "mean": 8.2508e+00}, "entropy_coef": {"n": 1, "mean": 4.8965e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3822e-01, "std": 2.3391e-01, "min_value": 7.7979e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 9.4165e+00, "total_time": 6.2291e+02, "__timestamp": "2024-10-09 22:30:30.200054"}, {"step": 74000, "num_env_steps": 74000, "scores": {"n": 1, "mean": 3.4012e+03}, "actor_loss": {"n": 1, "mean": -1.0028e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.7663e-01}, "critic_loss": {"n": 1, "mean": 8.6669e+00}, "entropy_coef": {"n": 1, "mean": 5.1158e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4431e-01, "std": 2.2763e-01, "min_value": 1.6630e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.9676e+00, "total_time": 6.3287e+02, "__timestamp": "2024-10-09 22:30:40.166635"}, {"step": 75000, "num_env_steps": 75000, "scores": {"n": 1, "mean": 3.7130e+03}, "actor_loss": {"n": 1, "mean": -9.8365e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.1343e-01}, "critic_loss": {"n": 1, "mean": 8.1469e+00}, "entropy_coef": {"n": 1, "mean": 5.3182e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5157e-01, "std": 2.2100e-01, "min_value": 1.6345e-03, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 9.3698e+00, "total_time": 6.4224e+02, "__timestamp": "2024-10-09 22:30:49.536483"}, {"step": 76000, "num_env_steps": 76000, "scores": {"n": 1, "mean": 3.7517e+03}, "actor_loss": {"n": 1, "mean": -1.0198e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.0953e-01}, "critic_loss": {"n": 1, "mean": 7.3348e+00}, "entropy_coef": {"n": 1, "mean": 5.4669e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5295e-01, "std": 2.2003e-01, "min_value": 7.7294e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.7382e+00, "total_time": 6.5198e+02, "__timestamp": "2024-10-09 22:30:59.274651"}, {"step": 77000, "num_env_steps": 77000, "scores": {"n": 1, "mean": 3.4561e+03}, "actor_loss": {"n": 1, "mean": -9.7827e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.2262e+00}, "critic_loss": {"n": 1, "mean": 7.6882e+00}, "entropy_coef": {"n": 1, "mean": 5.7106e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4175e-01, "std": 2.2741e-01, "min_value": 5.2199e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.4763e+00, "total_time": 6.6146e+02, "__timestamp": "2024-10-09 22:31:08.750916"}, {"step": 78000, "num_env_steps": 78000, "scores": {"n": 1, "mean": 3.6777e+03}, "actor_loss": {"n": 1, "mean": -1.1633e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.6305e+00}, "critic_loss": {"n": 1, "mean": 8.2112e+00}, "entropy_coef": {"n": 1, "mean": 5.9327e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5700e-01, "std": 2.1521e-01, "min_value": 1.0483e-03, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.7343e+00, "total_time": 6.7119e+02, "__timestamp": "2024-10-09 22:31:18.485172"}, {"step": 79000, "num_env_steps": 79000, "scores": {"n": 1, "mean": 3.6905e+03}, "actor_loss": {"n": 1, "mean": -1.1034e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.0230e-01}, "critic_loss": {"n": 1, "mean": 3.2507e+01}, "entropy_coef": {"n": 1, "mean": 5.9324e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5056e-01, "std": 2.1711e-01, "min_value": 2.5803e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 9.8856e+00, "total_time": 6.8108e+02, "__timestamp": "2024-10-09 22:31:28.370767"}, {"step": 80000, "num_env_steps": 80000, "scores": {"n": 1, "mean": 3.3131e+03}, "actor_loss": {"n": 1, "mean": -1.1890e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1944e+00}, "critic_loss": {"n": 1, "mean": 8.9496e+00}, "entropy_coef": {"n": 1, "mean": 6.1299e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4582e-01, "std": 2.2418e-01, "min_value": 4.3417e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0186e+01, "total_time": 6.9126e+02, "__timestamp": "2024-10-09 22:31:38.556877"}, {"step": 81000, "num_env_steps": 81000, "scores": {"n": 1, "mean": 3.7356e+03}, "actor_loss": {"n": 1, "mean": -1.1543e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.0941e-01}, "critic_loss": {"n": 1, "mean": 8.3782e+00}, "entropy_coef": {"n": 1, "mean": 6.3090e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5108e-01, "std": 2.1750e-01, "min_value": 2.1344e-03, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0008e+01, "total_time": 7.0127e+02, "__timestamp": "2024-10-09 22:31:48.566353"}, {"step": 82000, "num_env_steps": 82000, "scores": {"n": 1, "mean": 3.7519e+03}, "actor_loss": {"n": 1, "mean": -1.2318e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1338e+00}, "critic_loss": {"n": 1, "mean": 8.0462e+00}, "entropy_coef": {"n": 1, "mean": 6.5293e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.6014e-01, "std": 2.0955e-01, "min_value": 3.7460e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 9.4771e+00, "total_time": 7.1075e+02, "__timestamp": "2024-10-09 22:31:58.042440"}, {"step": 83000, "num_env_steps": 83000, "scores": {"n": 1, "mean": 3.4551e+03}, "actor_loss": {"n": 1, "mean": -1.1445e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.7488e-01}, "critic_loss": {"n": 1, "mean": 9.6572e+00}, "entropy_coef": {"n": 1, "mean": 6.5480e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5346e-01, "std": 2.1512e-01, "min_value": 1.5917e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.4159e+00, "total_time": 7.2017e+02, "__timestamp": "2024-10-09 22:32:07.458309"}, {"step": 84000, "num_env_steps": 84000, "scores": {"n": 1, "mean": 3.6749e+03}, "actor_loss": {"n": 1, "mean": -1.2834e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.9522e-01}, "critic_loss": {"n": 1, "mean": 9.8071e+00}, "entropy_coef": {"n": 1, "mean": 6.6753e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4824e-01, "std": 2.1964e-01, "min_value": 2.4664e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 9.6851e+00, "total_time": 7.2985e+02, "__timestamp": "2024-10-09 22:32:17.144362"}, {"step": 85000, "num_env_steps": 85000, "scores": {"n": 1, "mean": 3.5508e+03}, "actor_loss": {"n": 1, "mean": -1.3069e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.7436e-01}, "critic_loss": {"n": 1, "mean": 1.2086e+01}, "entropy_coef": {"n": 1, "mean": 6.8260e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4445e-01, "std": 2.2345e-01, "min_value": 2.0377e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0361e+01, "total_time": 7.4021e+02, "__timestamp": "2024-10-09 22:32:27.504042"}, {"step": 86000, "num_env_steps": 86000, "scores": {"n": 1, "mean": 3.4145e+03}, "actor_loss": {"n": 1, "mean": -1.3283e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.1532e-01}, "critic_loss": {"n": 1, "mean": 8.2704e+00}, "entropy_coef": {"n": 1, "mean": 6.8132e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3363e-01, "std": 2.3296e-01, "min_value": 1.2473e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 9.9693e+00, "total_time": 7.5018e+02, "__timestamp": "2024-10-09 22:32:37.473303"}, {"step": 87000, "num_env_steps": 87000, "scores": {"n": 1, "mean": 3.7299e+03}, "actor_loss": {"n": 1, "mean": -1.3875e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0095e+00}, "critic_loss": {"n": 1, "mean": 9.9255e+00}, "entropy_coef": {"n": 1, "mean": 7.0926e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5722e-01, "std": 2.1303e-01, "min_value": 5.2822e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 9.8385e+00, "total_time": 7.6002e+02, "__timestamp": "2024-10-09 22:32:47.311772"}, {"step": 88000, "num_env_steps": 88000, "scores": {"n": 1, "mean": 3.5720e+03}, "actor_loss": {"n": 1, "mean": -1.3069e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1786e+00}, "critic_loss": {"n": 1, "mean": 1.1791e+01}, "entropy_coef": {"n": 1, "mean": 7.1956e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4389e-01, "std": 2.2438e-01, "min_value": 4.3306e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0012e+01, "total_time": 7.7003e+02, "__timestamp": "2024-10-09 22:32:57.324426"}, {"step": 89000, "num_env_steps": 89000, "scores": {"n": 1, "mean": 3.5938e+03}, "actor_loss": {"n": 1, "mean": -1.3678e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1620e+00}, "critic_loss": {"n": 1, "mean": 7.6479e+00}, "entropy_coef": {"n": 1, "mean": 7.4592e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4822e-01, "std": 2.2026e-01, "min_value": 1.5147e-03, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0271e+01, "total_time": 7.8030e+02, "__timestamp": "2024-10-09 22:33:07.594437"}, {"step": 90000, "num_env_steps": 90000, "scores": {"n": 1, "mean": 3.7013e+03}, "actor_loss": {"n": 1, "mean": -1.2651e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.0896e-01}, "critic_loss": {"n": 1, "mean": 8.3958e+00}, "entropy_coef": {"n": 1, "mean": 7.4713e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4218e-01, "std": 2.2678e-01, "min_value": 3.7819e-05, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 9.7278e+00, "total_time": 7.9003e+02, "__timestamp": "2024-10-09 22:33:17.323215"}, {"step": 91000, "num_env_steps": 91000, "scores": {"n": 1, "mean": 3.8526e+03}, "actor_loss": {"n": 1, "mean": -1.4042e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.7216e-01}, "critic_loss": {"n": 1, "mean": 9.4108e+00}, "entropy_coef": {"n": 1, "mean": 7.5535e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5466e-01, "std": 2.1680e-01, "min_value": 3.2049e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0133e+01, "total_time": 8.0016e+02, "__timestamp": "2024-10-09 22:33:27.455663"}, {"step": 92000, "num_env_steps": 92000, "scores": {"n": 1, "mean": 3.7100e+03}, "actor_loss": {"n": 1, "mean": -1.4244e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.6707e-01}, "critic_loss": {"n": 1, "mean": 1.3322e+01}, "entropy_coef": {"n": 1, "mean": 7.5682e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4494e-01, "std": 2.2378e-01, "min_value": 5.6398e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 9.9828e+00, "total_time": 8.1015e+02, "__timestamp": "2024-10-09 22:33:37.438419"}, {"step": 93000, "num_env_steps": 93000, "scores": {"n": 1, "mean": 3.8199e+03}, "actor_loss": {"n": 1, "mean": -1.4336e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2668e+00}, "critic_loss": {"n": 1, "mean": 7.6253e+00}, "entropy_coef": {"n": 1, "mean": 7.6875e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3896e-01, "std": 2.2812e-01, "min_value": 7.0974e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 9.9131e+00, "total_time": 8.2006e+02, "__timestamp": "2024-10-09 22:33:47.352538"}, {"step": 94000, "num_env_steps": 94000, "scores": {"n": 1, "mean": 3.8967e+03}, "actor_loss": {"n": 1, "mean": -1.3402e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.2153e+00}, "critic_loss": {"n": 1, "mean": 1.1280e+01}, "entropy_coef": {"n": 1, "mean": 7.9929e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4322e-01, "std": 2.2240e-01, "min_value": 2.1249e-03, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 9.2470e+00, "total_time": 8.2931e+02, "__timestamp": "2024-10-09 22:33:56.599575"}, {"step": 95000, "num_env_steps": 95000, "scores": {"n": 1, "mean": 3.9149e+03}, "actor_loss": {"n": 1, "mean": -1.4353e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2046e+00}, "critic_loss": {"n": 1, "mean": 9.4938e+00}, "entropy_coef": {"n": 1, "mean": 7.8797e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4717e-01, "std": 2.1862e-01, "min_value": 1.0605e-03, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 9.7468e+00, "total_time": 8.3905e+02, "__timestamp": "2024-10-09 22:34:06.346346"}, {"step": 96000, "num_env_steps": 96000, "scores": {"n": 1, "mean": 3.9766e+03}, "actor_loss": {"n": 1, "mean": -1.5559e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.5752e-01}, "critic_loss": {"n": 1, "mean": 1.0165e+01}, "entropy_coef": {"n": 1, "mean": 7.9850e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5320e-01, "std": 2.1325e-01, "min_value": 1.1594e-03, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 9.7395e+00, "total_time": 8.4879e+02, "__timestamp": "2024-10-09 22:34:16.086836"}, {"step": 97000, "num_env_steps": 97000, "scores": {"n": 1, "mean": 3.8870e+03}, "actor_loss": {"n": 1, "mean": -1.4490e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0443e+00}, "critic_loss": {"n": 1, "mean": 8.2283e+00}, "entropy_coef": {"n": 1, "mean": 8.0363e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4806e-01, "std": 2.1786e-01, "min_value": 8.8662e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0015e+01, "total_time": 8.5881e+02, "__timestamp": "2024-10-09 22:34:26.099732"}, {"step": 98000, "num_env_steps": 98000, "scores": {"n": 1, "mean": 3.8792e+03}, "actor_loss": {"n": 1, "mean": -1.4887e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.7330e-01}, "critic_loss": {"n": 1, "mean": 1.1069e+01}, "entropy_coef": {"n": 1, "mean": 8.2341e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4875e-01, "std": 2.1740e-01, "min_value": 1.5214e-03, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 9.2739e+00, "total_time": 8.6808e+02, "__timestamp": "2024-10-09 22:34:35.373622"}, {"step": 99000, "num_env_steps": 99000, "scores": {"n": 1, "mean": 3.8432e+03}, "actor_loss": {"n": 1, "mean": -1.4540e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1370e+00}, "critic_loss": {"n": 1, "mean": 1.0603e+01}, "entropy_coef": {"n": 1, "mean": 8.2483e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4334e-01, "std": 2.2348e-01, "min_value": 1.1380e-03, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0153e+01, "total_time": 8.7823e+02, "__timestamp": "2024-10-09 22:34:45.526939"}, {"step": 100000, "num_env_steps": 100000, "scores": {"n": 1, "mean": 4.0909e+03}, "actor_loss": {"n": 1, "mean": -1.5305e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.2970e-01}, "critic_loss": {"n": 1, "mean": 1.0192e+01}, "entropy_coef": {"n": 1, "mean": 8.4040e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4944e-01, "std": 2.1903e-01, "min_value": 1.8649e-03, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0712e+01, "total_time": 8.8895e+02, "__timestamp": "2024-10-09 22:34:56.239500"}, {"step": 101000, "num_env_steps": 101000, "scores": {"n": 1, "mean": 3.9626e+03}, "actor_loss": {"n": 1, "mean": -1.5488e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.8734e-01}, "critic_loss": {"n": 1, "mean": 9.1724e+00}, "entropy_coef": {"n": 1, "mean": 8.6995e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5103e-01, "std": 2.1411e-01, "min_value": 3.1508e-04, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0189e+01, "total_time": 8.9914e+02, "__timestamp": "2024-10-09 22:35:06.428846"}, {"step": 102000, "num_env_steps": 102000, "scores": {"n": 1, "mean": 3.8833e+03}, "actor_loss": {"n": 1, "mean": -1.5865e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.0966e-02}, "critic_loss": {"n": 1, "mean": 1.9323e+02}, "entropy_coef": {"n": 1, "mean": 8.9501e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5255e-01, "std": 2.1387e-01, "min_value": 1.0519e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 9.4688e+00, "total_time": 9.0860e+02, "__timestamp": "2024-10-09 22:35:15.897629"}, {"step": 103000, "num_env_steps": 103000, "scores": {"n": 1, "mean": 3.9109e+03}, "actor_loss": {"n": 1, "mean": -1.5373e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.9653e-01}, "critic_loss": {"n": 1, "mean": 2.4823e+02}, "entropy_coef": {"n": 1, "mean": 9.0560e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4640e-01, "std": 2.2087e-01, "min_value": 4.3184e-05, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 9.3909e+00, "total_time": 9.1800e+02, "__timestamp": "2024-10-09 22:35:25.288525"}, {"step": 104000, "num_env_steps": 104000, "scores": {"n": 1, "mean": 3.8874e+03}, "actor_loss": {"n": 1, "mean": -1.3776e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.0191e+00}, "critic_loss": {"n": 1, "mean": 1.0300e+01}, "entropy_coef": {"n": 1, "mean": 9.3048e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5407e-01, "std": 2.1239e-01, "min_value": 6.9150e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 9.5390e+00, "total_time": 9.2753e+02, "__timestamp": "2024-10-09 22:35:34.827502"}, {"step": 105000, "num_env_steps": 105000, "scores": {"n": 1, "mean": 3.8158e+03}, "actor_loss": {"n": 1, "mean": -1.6388e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.0882e-02}, "critic_loss": {"n": 1, "mean": 1.3772e+01}, "entropy_coef": {"n": 1, "mean": 9.0300e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4612e-01, "std": 2.1594e-01, "min_value": 5.8883e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 9.4307e+00, "total_time": 9.3696e+02, "__timestamp": "2024-10-09 22:35:44.258190"}, {"step": 106000, "num_env_steps": 106000, "scores": {"n": 1, "mean": 3.9066e+03}, "actor_loss": {"n": 1, "mean": -1.6307e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2474e+00}, "critic_loss": {"n": 1, "mean": 1.0721e+01}, "entropy_coef": {"n": 1, "mean": 9.1442e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5117e-01, "std": 2.1156e-01, "min_value": 1.5507e-03, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 9.4010e+00, "total_time": 9.4637e+02, "__timestamp": "2024-10-09 22:35:53.658214"}, {"step": 107000, "num_env_steps": 107000, "scores": {"n": 1, "mean": 3.8713e+03}, "actor_loss": {"n": 1, "mean": -1.5852e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3150e+00}, "critic_loss": {"n": 1, "mean": 1.9774e+02}, "entropy_coef": {"n": 1, "mean": 9.3554e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4289e-01, "std": 2.2268e-01, "min_value": 1.0014e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.4916e+00, "total_time": 9.5586e+02, "__timestamp": "2024-10-09 22:36:03.150799"}, {"step": 108000, "num_env_steps": 108000, "scores": {"n": 1, "mean": 3.8936e+03}, "actor_loss": {"n": 1, "mean": -1.5864e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5364e+00}, "critic_loss": {"n": 1, "mean": 4.3579e+02}, "entropy_coef": {"n": 1, "mean": 9.4268e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4469e-01, "std": 2.2021e-01, "min_value": 1.5557e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 9.7570e+00, "total_time": 9.6561e+02, "__timestamp": "2024-10-09 22:36:12.907791"}, {"step": 109000, "num_env_steps": 109000, "scores": {"n": 1, "mean": 3.8532e+03}, "actor_loss": {"n": 1, "mean": -1.7053e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2886e+00}, "critic_loss": {"n": 1, "mean": 1.6106e+01}, "entropy_coef": {"n": 1, "mean": 9.3731e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4756e-01, "std": 2.1424e-01, "min_value": 2.4955e-03, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 9.6822e+00, "total_time": 9.7530e+02, "__timestamp": "2024-10-09 22:36:22.588969"}, {"step": 110000, "num_env_steps": 110000, "scores": {"n": 1, "mean": 3.4332e+03}, "actor_loss": {"n": 1, "mean": -1.5940e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.3983e-03}, "critic_loss": {"n": 1, "mean": 7.8974e+00}, "entropy_coef": {"n": 1, "mean": 9.6465e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1465e-01, "std": 2.4381e-01, "min_value": 5.3197e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.4228e+00, "total_time": 9.8472e+02, "__timestamp": "2024-10-09 22:36:32.012720"}, {"step": 111000, "num_env_steps": 111000, "scores": {"n": 1, "mean": 4.0095e+03}, "actor_loss": {"n": 1, "mean": -1.6313e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.7204e-01}, "critic_loss": {"n": 1, "mean": 9.7021e+00}, "entropy_coef": {"n": 1, "mean": 9.7262e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4397e-01, "std": 2.2051e-01, "min_value": 1.3137e-03, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 9.6636e+00, "total_time": 9.9438e+02, "__timestamp": "2024-10-09 22:36:41.676362"}, {"step": 112000, "num_env_steps": 112000, "scores": {"n": 1, "mean": 3.8781e+03}, "actor_loss": {"n": 1, "mean": -1.7098e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.6372e-01}, "critic_loss": {"n": 1, "mean": 2.2370e+02}, "entropy_coef": {"n": 1, "mean": 9.8765e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4053e-01, "std": 2.2483e-01, "min_value": 2.5988e-05, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 9.3863e+00, "total_time": 1.0038e+03, "__timestamp": "2024-10-09 22:36:51.062650"}, {"step": 113000, "num_env_steps": 113000, "scores": {"n": 1, "mean": 3.8025e+03}, "actor_loss": {"n": 1, "mean": -1.6920e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.2612e-01}, "critic_loss": {"n": 1, "mean": 1.0194e+01}, "entropy_coef": {"n": 1, "mean": 1.0062e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4790e-01, "std": 2.1299e-01, "min_value": 1.0823e-03, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 9.8047e+00, "total_time": 1.0136e+03, "__timestamp": "2024-10-09 22:37:00.867388"}, {"step": 114000, "num_env_steps": 114000, "scores": {"n": 1, "mean": 3.8439e+03}, "actor_loss": {"n": 1, "mean": -1.7375e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.0665e-01}, "critic_loss": {"n": 1, "mean": 9.7280e+00}, "entropy_coef": {"n": 1, "mean": 1.0032e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4900e-01, "std": 2.1402e-01, "min_value": 2.0354e-03, "max_value": 9.9969e-01}, "num_gradient_steps": 0, "step_time": 9.4906e+00, "total_time": 1.0231e+03, "__timestamp": "2024-10-09 22:37:10.357018"}, {"step": 115000, "num_env_steps": 115000, "scores": {"n": 1, "mean": 3.6733e+03}, "actor_loss": {"n": 1, "mean": -1.6272e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.3535e-01}, "critic_loss": {"n": 1, "mean": 1.0126e+01}, "entropy_coef": {"n": 1, "mean": 1.0244e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3628e-01, "std": 2.2386e-01, "min_value": 4.5128e-04, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 9.4260e+00, "total_time": 1.0325e+03, "__timestamp": "2024-10-09 22:37:19.784001"}, {"step": 116000, "num_env_steps": 116000, "scores": {"n": 1, "mean": 3.7295e+03}, "actor_loss": {"n": 1, "mean": -1.6395e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.6311e-01}, "critic_loss": {"n": 1, "mean": 2.0118e+02}, "entropy_coef": {"n": 1, "mean": 1.0251e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4485e-01, "std": 2.1685e-01, "min_value": 1.4589e-03, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 9.6622e+00, "total_time": 1.0422e+03, "__timestamp": "2024-10-09 22:37:29.445192"}, {"step": 117000, "num_env_steps": 117000, "scores": {"n": 1, "mean": 3.7627e+03}, "actor_loss": {"n": 1, "mean": -1.8322e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.9076e-01}, "critic_loss": {"n": 1, "mean": 1.1022e+01}, "entropy_coef": {"n": 1, "mean": 1.0321e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4034e-01, "std": 2.1931e-01, "min_value": 1.7840e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 9.5922e+00, "total_time": 1.0517e+03, "__timestamp": "2024-10-09 22:37:39.038416"}, {"step": 118000, "num_env_steps": 118000, "scores": {"n": 1, "mean": 3.7407e+03}, "actor_loss": {"n": 1, "mean": -1.6717e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.3364e-01}, "critic_loss": {"n": 1, "mean": 9.2032e+00}, "entropy_coef": {"n": 1, "mean": 1.0466e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4225e-01, "std": 2.1718e-01, "min_value": 4.2160e-04, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 9.9463e+00, "total_time": 1.0617e+03, "__timestamp": "2024-10-09 22:37:48.983679"}, {"step": 119000, "num_env_steps": 119000, "scores": {"n": 1, "mean": 3.9841e+03}, "actor_loss": {"n": 1, "mean": -1.7631e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3848e-01}, "critic_loss": {"n": 1, "mean": 5.9083e+01}, "entropy_coef": {"n": 1, "mean": 1.0456e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5298e-01, "std": 2.1209e-01, "min_value": 2.1504e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 9.8028e+00, "total_time": 1.0715e+03, "__timestamp": "2024-10-09 22:37:58.787503"}, {"step": 120000, "num_env_steps": 120000, "scores": {"n": 1, "mean": 3.8979e+03}, "actor_loss": {"n": 1, "mean": -1.7255e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.8294e-01}, "critic_loss": {"n": 1, "mean": 1.1256e+01}, "entropy_coef": {"n": 1, "mean": 1.0542e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4735e-01, "std": 2.1505e-01, "min_value": 4.1577e-04, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 9.4779e+00, "total_time": 1.0810e+03, "__timestamp": "2024-10-09 22:38:08.264366"}, {"step": 121000, "num_env_steps": 121000, "scores": {"n": 1, "mean": 3.9854e+03}, "actor_loss": {"n": 1, "mean": -1.6722e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.7282e+00}, "critic_loss": {"n": 1, "mean": 7.1826e+01}, "entropy_coef": {"n": 1, "mean": 1.0614e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4664e-01, "std": 2.1718e-01, "min_value": 3.4422e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 9.3315e+00, "total_time": 1.0903e+03, "__timestamp": "2024-10-09 22:38:17.596868"}, {"step": 122000, "num_env_steps": 122000, "scores": {"n": 1, "mean": 3.8892e+03}, "actor_loss": {"n": 1, "mean": -1.7404e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.8042e-01}, "critic_loss": {"n": 1, "mean": 9.0602e+00}, "entropy_coef": {"n": 1, "mean": 1.0797e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4834e-01, "std": 2.1359e-01, "min_value": 2.0631e-04, "max_value": 9.9957e-01}, "num_gradient_steps": 0, "step_time": 9.4658e+00, "total_time": 1.0998e+03, "__timestamp": "2024-10-09 22:38:27.062700"}, {"step": 123000, "num_env_steps": 123000, "scores": {"n": 1, "mean": 3.9163e+03}, "actor_loss": {"n": 1, "mean": -1.7176e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.2884e-01}, "critic_loss": {"n": 1, "mean": 9.3846e+00}, "entropy_coef": {"n": 1, "mean": 1.0920e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4278e-01, "std": 2.1959e-01, "min_value": 2.7078e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 9.4171e+00, "total_time": 1.1092e+03, "__timestamp": "2024-10-09 22:38:36.479832"}, {"step": 124000, "num_env_steps": 124000, "scores": {"n": 1, "mean": 3.9309e+03}, "actor_loss": {"n": 1, "mean": -1.7455e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5800e-02}, "critic_loss": {"n": 1, "mean": 1.0588e+01}, "entropy_coef": {"n": 1, "mean": 1.0778e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4146e-01, "std": 2.2072e-01, "min_value": 6.8952e-04, "max_value": 9.9951e-01}, "num_gradient_steps": 0, "step_time": 9.6843e+00, "total_time": 1.1189e+03, "__timestamp": "2024-10-09 22:38:46.163091"}, {"step": 125000, "num_env_steps": 125000, "scores": {"n": 1, "mean": 4.0351e+03}, "actor_loss": {"n": 1, "mean": -1.6877e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0085e+00}, "critic_loss": {"n": 1, "mean": 8.4411e+00}, "entropy_coef": {"n": 1, "mean": 1.1259e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4949e-01, "std": 2.1643e-01, "min_value": 8.3587e-04, "max_value": 9.9939e-01}, "num_gradient_steps": 0, "step_time": 9.4542e+00, "total_time": 1.1283e+03, "__timestamp": "2024-10-09 22:38:55.617305"}, {"step": 126000, "num_env_steps": 126000, "scores": {"n": 1, "mean": 3.9312e+03}, "actor_loss": {"n": 1, "mean": -1.6569e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.6119e+00}, "critic_loss": {"n": 1, "mean": 6.7513e+00}, "entropy_coef": {"n": 1, "mean": 1.1089e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3728e-01, "std": 2.2335e-01, "min_value": 1.1337e-03, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 9.6633e+00, "total_time": 1.1380e+03, "__timestamp": "2024-10-09 22:39:05.280581"}, {"step": 127000, "num_env_steps": 127000, "scores": {"n": 1, "mean": 3.7297e+03}, "actor_loss": {"n": 1, "mean": -1.7166e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.6823e-01}, "critic_loss": {"n": 1, "mean": 7.5570e+00}, "entropy_coef": {"n": 1, "mean": 1.1194e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3139e-01, "std": 2.2846e-01, "min_value": 3.7130e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 9.5622e+00, "total_time": 1.1476e+03, "__timestamp": "2024-10-09 22:39:14.842741"}, {"step": 128000, "num_env_steps": 128000, "scores": {"n": 1, "mean": 3.9580e+03}, "actor_loss": {"n": 1, "mean": -1.7477e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.5595e-01}, "critic_loss": {"n": 1, "mean": 8.7281e+00}, "entropy_coef": {"n": 1, "mean": 1.1372e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4559e-01, "std": 2.1987e-01, "min_value": 1.0474e-03, "max_value": 9.9942e-01}, "num_gradient_steps": 0, "step_time": 9.5552e+00, "total_time": 1.1571e+03, "__timestamp": "2024-10-09 22:39:24.397961"}, {"step": 129000, "num_env_steps": 129000, "scores": {"n": 1, "mean": 3.7807e+03}, "actor_loss": {"n": 1, "mean": -1.7942e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.5158e-01}, "critic_loss": {"n": 1, "mean": 1.2346e+01}, "entropy_coef": {"n": 1, "mean": 1.1277e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4046e-01, "std": 2.1985e-01, "min_value": 1.3047e-03, "max_value": 9.9976e-01}, "num_gradient_steps": 0, "step_time": 9.7527e+00, "total_time": 1.1669e+03, "__timestamp": "2024-10-09 22:39:34.150656"}, {"step": 130000, "num_env_steps": 130000, "scores": {"n": 1, "mean": 3.9636e+03}, "actor_loss": {"n": 1, "mean": -1.7546e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.4613e-01}, "critic_loss": {"n": 1, "mean": 2.0787e+02}, "entropy_coef": {"n": 1, "mean": 1.1201e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4348e-01, "std": 2.1912e-01, "min_value": 9.9350e-04, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 9.9098e+00, "total_time": 1.1768e+03, "__timestamp": "2024-10-09 22:39:44.060479"}, {"step": 131000, "num_env_steps": 131000, "scores": {"n": 1, "mean": 3.7609e+03}, "actor_loss": {"n": 1, "mean": -1.8827e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.6080e-01}, "critic_loss": {"n": 1, "mean": 2.1390e+02}, "entropy_coef": {"n": 1, "mean": 1.1363e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3880e-01, "std": 2.2278e-01, "min_value": 1.0826e-03, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 9.7267e+00, "total_time": 1.1865e+03, "__timestamp": "2024-10-09 22:39:53.787228"}, {"step": 132000, "num_env_steps": 132000, "scores": {"n": 1, "mean": 4.1157e+03}, "actor_loss": {"n": 1, "mean": -1.8844e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1592e+00}, "critic_loss": {"n": 1, "mean": 9.2599e+00}, "entropy_coef": {"n": 1, "mean": 1.1609e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.5162e-01, "std": 2.0555e-01, "min_value": 1.8219e-03, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 9.3752e+00, "total_time": 1.1959e+03, "__timestamp": "2024-10-09 22:40:03.162447"}, {"step": 133000, "num_env_steps": 133000, "scores": {"n": 1, "mean": 3.9377e+03}, "actor_loss": {"n": 1, "mean": -1.8240e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1784e-01}, "critic_loss": {"n": 1, "mean": 9.1034e+00}, "entropy_coef": {"n": 1, "mean": 1.1392e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4184e-01, "std": 2.1908e-01, "min_value": 5.1534e-04, "max_value": 9.9960e-01}, "num_gradient_steps": 0, "step_time": 9.6027e+00, "total_time": 1.2055e+03, "__timestamp": "2024-10-09 22:40:12.765114"}, {"step": 134000, "num_env_steps": 134000, "scores": {"n": 1, "mean": 3.7516e+03}, "actor_loss": {"n": 1, "mean": -1.8076e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4713e-01}, "critic_loss": {"n": 1, "mean": 1.3832e+01}, "entropy_coef": {"n": 1, "mean": 1.1752e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3305e-01, "std": 2.2908e-01, "min_value": 1.1415e-03, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 9.8307e+00, "total_time": 1.2153e+03, "__timestamp": "2024-10-09 22:40:22.596782"}, {"step": 135000, "num_env_steps": 135000, "scores": {"n": 1, "mean": 3.6611e+03}, "actor_loss": {"n": 1, "mean": -1.7497e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.4091e-01}, "critic_loss": {"n": 1, "mean": 9.9194e+00}, "entropy_coef": {"n": 1, "mean": 1.1871e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4397e-01, "std": 2.1354e-01, "min_value": 2.6921e-03, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 9.6963e+00, "total_time": 1.2250e+03, "__timestamp": "2024-10-09 22:40:32.292046"}, {"step": 136000, "num_env_steps": 136000, "scores": {"n": 1, "mean": 3.8159e+03}, "actor_loss": {"n": 1, "mean": -1.7564e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.2988e-01}, "critic_loss": {"n": 1, "mean": 2.2850e+02}, "entropy_coef": {"n": 1, "mean": 1.1808e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4255e-01, "std": 2.1558e-01, "min_value": 3.3117e-03, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 9.3230e+00, "total_time": 1.2343e+03, "__timestamp": "2024-10-09 22:40:41.616029"}, {"step": 137000, "num_env_steps": 137000, "scores": {"n": 1, "mean": 3.9475e+03}, "actor_loss": {"n": 1, "mean": -1.8053e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.2695e-01}, "critic_loss": {"n": 1, "mean": 1.5077e+02}, "entropy_coef": {"n": 1, "mean": 1.1928e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4465e-01, "std": 2.1281e-01, "min_value": 1.0687e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 9.5989e+00, "total_time": 1.2439e+03, "__timestamp": "2024-10-09 22:40:51.213935"}, {"step": 138000, "num_env_steps": 138000, "scores": {"n": 1, "mean": 3.9444e+03}, "actor_loss": {"n": 1, "mean": -1.8426e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.3095e-01}, "critic_loss": {"n": 1, "mean": 1.0282e+01}, "entropy_coef": {"n": 1, "mean": 1.1751e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3601e-01, "std": 2.2437e-01, "min_value": 1.4427e-03, "max_value": 9.9948e-01}, "num_gradient_steps": 0, "step_time": 9.3885e+00, "total_time": 1.2533e+03, "__timestamp": "2024-10-09 22:41:00.603385"}, {"step": 139000, "num_env_steps": 139000, "scores": {"n": 1, "mean": 3.9257e+03}, "actor_loss": {"n": 1, "mean": -1.8158e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.9906e-01}, "critic_loss": {"n": 1, "mean": 1.0394e+01}, "entropy_coef": {"n": 1, "mean": 1.2196e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3630e-01, "std": 2.2133e-01, "min_value": 9.6351e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 9.3241e+00, "total_time": 1.2626e+03, "__timestamp": "2024-10-09 22:41:09.926509"}, {"step": 140000, "num_env_steps": 140000, "scores": {"n": 1, "mean": 3.9552e+03}, "actor_loss": {"n": 1, "mean": -1.8583e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.5717e-01}, "critic_loss": {"n": 1, "mean": 1.0791e+01}, "entropy_coef": {"n": 1, "mean": 1.1947e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3818e-01, "std": 2.1872e-01, "min_value": 4.0723e-03, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 9.5288e+00, "total_time": 1.2722e+03, "__timestamp": "2024-10-09 22:41:19.456309"}, {"step": 141000, "num_env_steps": 141000, "scores": {"n": 1, "mean": 2.5858e+03}, "actor_loss": {"n": 1, "mean": -1.7883e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.2050e-01}, "critic_loss": {"n": 1, "mean": 3.7073e+02}, "entropy_coef": {"n": 1, "mean": 1.1784e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2894e-01, "std": 2.8734e-01, "min_value": 3.0702e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 9.4851e+00, "total_time": 1.2816e+03, "__timestamp": "2024-10-09 22:41:28.941459"}, {"step": 142000, "num_env_steps": 142000, "scores": {"n": 1, "mean": 3.7345e+03}, "actor_loss": {"n": 1, "mean": -1.8214e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.0299e-01}, "critic_loss": {"n": 1, "mean": 8.6097e+00}, "entropy_coef": {"n": 1, "mean": 1.2100e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3179e-01, "std": 2.2465e-01, "min_value": 1.3355e-03, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 9.3129e+00, "total_time": 1.2910e+03, "__timestamp": "2024-10-09 22:41:38.254341"}, {"step": 143000, "num_env_steps": 143000, "scores": {"n": 1, "mean": 3.8939e+03}, "actor_loss": {"n": 1, "mean": -1.7949e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.6008e-01}, "critic_loss": {"n": 1, "mean": 2.2384e+02}, "entropy_coef": {"n": 1, "mean": 1.2183e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3096e-01, "std": 2.2542e-01, "min_value": 7.9000e-04, "max_value": 9.9954e-01}, "num_gradient_steps": 0, "step_time": 9.4532e+00, "total_time": 1.3004e+03, "__timestamp": "2024-10-09 22:41:47.706508"}, {"step": 144000, "num_env_steps": 144000, "scores": {"n": 1, "mean": 4.0707e+03}, "actor_loss": {"n": 1, "mean": -1.8437e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0947e-01}, "critic_loss": {"n": 1, "mean": 1.1170e+01}, "entropy_coef": {"n": 1, "mean": 1.2083e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3666e-01, "std": 2.1986e-01, "min_value": 4.9937e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 9.2063e+00, "total_time": 1.3096e+03, "__timestamp": "2024-10-09 22:41:56.913817"}, {"step": 145000, "num_env_steps": 145000, "scores": {"n": 1, "mean": 4.0487e+03}, "actor_loss": {"n": 1, "mean": -1.8417e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.6537e-01}, "critic_loss": {"n": 1, "mean": 9.0040e+00}, "entropy_coef": {"n": 1, "mean": 1.2399e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4587e-01, "std": 2.1348e-01, "min_value": 1.1642e-03, "max_value": 9.9950e-01}, "num_gradient_steps": 0, "step_time": 9.2764e+00, "total_time": 1.3189e+03, "__timestamp": "2024-10-09 22:42:06.189186"}, {"step": 146000, "num_env_steps": 146000, "scores": {"n": 1, "mean": 3.8083e+03}, "actor_loss": {"n": 1, "mean": -1.8728e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.5140e-01}, "critic_loss": {"n": 1, "mean": 9.5455e+00}, "entropy_coef": {"n": 1, "mean": 1.2248e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3843e-01, "std": 2.2102e-01, "min_value": 3.0752e-04, "max_value": 9.9972e-01}, "num_gradient_steps": 0, "step_time": 9.6809e+00, "total_time": 1.3286e+03, "__timestamp": "2024-10-09 22:42:15.871073"}, {"step": 147000, "num_env_steps": 147000, "scores": {"n": 1, "mean": 3.7296e+03}, "actor_loss": {"n": 1, "mean": -1.9002e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.7439e-01}, "critic_loss": {"n": 1, "mean": 1.0270e+01}, "entropy_coef": {"n": 1, "mean": 1.2399e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2655e-01, "std": 2.2921e-01, "min_value": 3.5405e-05, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 9.4688e+00, "total_time": 1.3380e+03, "__timestamp": "2024-10-09 22:42:25.339844"}, {"step": 148000, "num_env_steps": 148000, "scores": {"n": 1, "mean": 4.0785e+03}, "actor_loss": {"n": 1, "mean": -1.8653e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1881e+00}, "critic_loss": {"n": 1, "mean": 8.9255e+00}, "entropy_coef": {"n": 1, "mean": 1.2266e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3579e-01, "std": 2.2252e-01, "min_value": 2.9172e-04, "max_value": 9.9957e-01}, "num_gradient_steps": 0, "step_time": 9.5713e+00, "total_time": 1.3476e+03, "__timestamp": "2024-10-09 22:42:34.911105"}, {"step": 149000, "num_env_steps": 149000, "scores": {"n": 1, "mean": 3.9787e+03}, "actor_loss": {"n": 1, "mean": -1.8771e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.8221e-01}, "critic_loss": {"n": 1, "mean": 4.9839e+02}, "entropy_coef": {"n": 1, "mean": 1.2418e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3710e-01, "std": 2.1976e-01, "min_value": 7.0757e-04, "max_value": 9.9943e-01}, "num_gradient_steps": 0, "step_time": 9.5437e+00, "total_time": 1.3572e+03, "__timestamp": "2024-10-09 22:42:44.453802"}, {"step": 150000, "num_env_steps": 150000, "scores": {"n": 1, "mean": 3.8901e+03}, "actor_loss": {"n": 1, "mean": -1.7844e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0341e+00}, "critic_loss": {"n": 1, "mean": 1.2522e+01}, "entropy_coef": {"n": 1, "mean": 1.2819e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3609e-01, "std": 2.2182e-01, "min_value": 2.0148e-03, "max_value": 9.9969e-01}, "num_gradient_steps": 0, "step_time": 1.0237e+01, "total_time": 1.3674e+03, "__timestamp": "2024-10-09 22:42:54.691513"}, {"step": 151000, "num_env_steps": 151000, "scores": {"n": 1, "mean": 3.9960e+03}, "actor_loss": {"n": 1, "mean": -1.8406e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.3625e-01}, "critic_loss": {"n": 1, "mean": 8.3016e+00}, "entropy_coef": {"n": 1, "mean": 1.2621e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3095e-01, "std": 2.2910e-01, "min_value": 8.8196e-04, "max_value": 9.9962e-01}, "num_gradient_steps": 0, "step_time": 1.0429e+01, "total_time": 1.3778e+03, "__timestamp": "2024-10-09 22:43:05.120708"}, {"step": 152000, "num_env_steps": 152000, "scores": {"n": 1, "mean": 3.6968e+03}, "actor_loss": {"n": 1, "mean": -1.7975e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.1844e-01}, "critic_loss": {"n": 1, "mean": 1.0065e+01}, "entropy_coef": {"n": 1, "mean": 1.3134e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2737e-01, "std": 2.2718e-01, "min_value": 2.8786e-04, "max_value": 9.9936e-01}, "num_gradient_steps": 0, "step_time": 1.0674e+01, "total_time": 1.3885e+03, "__timestamp": "2024-10-09 22:43:15.795040"}, {"step": 153000, "num_env_steps": 153000, "scores": {"n": 1, "mean": 4.0854e+03}, "actor_loss": {"n": 1, "mean": -1.8343e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.6838e-01}, "critic_loss": {"n": 1, "mean": 1.3383e+02}, "entropy_coef": {"n": 1, "mean": 1.3071e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.4465e-01, "std": 2.1219e-01, "min_value": 1.6244e-03, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.0461e+01, "total_time": 1.3990e+03, "__timestamp": "2024-10-09 22:43:26.255741"}, {"step": 154000, "num_env_steps": 154000, "scores": {"n": 1, "mean": 3.8426e+03}, "actor_loss": {"n": 1, "mean": -1.8230e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2853e-01}, "critic_loss": {"n": 1, "mean": 9.4350e+00}, "entropy_coef": {"n": 1, "mean": 1.2494e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3009e-01, "std": 2.2391e-01, "min_value": 9.5769e-04, "max_value": 9.9956e-01}, "num_gradient_steps": 0, "step_time": 1.0885e+01, "total_time": 1.4098e+03, "__timestamp": "2024-10-09 22:43:37.140109"}, {"step": 155000, "num_env_steps": 155000, "scores": {"n": 1, "mean": 4.1251e+03}, "actor_loss": {"n": 1, "mean": -1.8474e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.7251e-01}, "critic_loss": {"n": 1, "mean": 9.0150e+00}, "entropy_coef": {"n": 1, "mean": 1.2865e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3553e-01, "std": 2.2468e-01, "min_value": 6.0546e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.0608e+01, "total_time": 1.4205e+03, "__timestamp": "2024-10-09 22:43:47.749164"}, {"step": 156000, "num_env_steps": 156000, "scores": {"n": 1, "mean": 3.8175e+03}, "actor_loss": {"n": 1, "mean": -1.8865e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5056e-01}, "critic_loss": {"n": 1, "mean": 9.9837e+00}, "entropy_coef": {"n": 1, "mean": 1.2728e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2021e-01, "std": 2.3540e-01, "min_value": 1.0653e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0493e+01, "total_time": 1.4309e+03, "__timestamp": "2024-10-09 22:43:58.242335"}, {"step": 157000, "num_env_steps": 157000, "scores": {"n": 1, "mean": 3.6381e+03}, "actor_loss": {"n": 1, "mean": -1.9151e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.9753e-01}, "critic_loss": {"n": 1, "mean": 9.8089e+00}, "entropy_coef": {"n": 1, "mean": 1.3134e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2113e-01, "std": 2.3364e-01, "min_value": 5.9180e-04, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 1.1737e+01, "total_time": 1.4427e+03, "__timestamp": "2024-10-09 22:44:09.978394"}, {"step": 158000, "num_env_steps": 158000, "scores": {"n": 1, "mean": 3.9092e+03}, "actor_loss": {"n": 1, "mean": -1.8193e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.8724e-01}, "critic_loss": {"n": 1, "mean": 1.4135e+01}, "entropy_coef": {"n": 1, "mean": 1.2891e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3084e-01, "std": 2.2221e-01, "min_value": 2.1932e-04, "max_value": 9.9900e-01}, "num_gradient_steps": 0, "step_time": 1.1267e+01, "total_time": 1.4540e+03, "__timestamp": "2024-10-09 22:44:21.246306"}, {"step": 159000, "num_env_steps": 159000, "scores": {"n": 1, "mean": 3.9925e+03}, "actor_loss": {"n": 1, "mean": -1.8916e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.9650e-01}, "critic_loss": {"n": 1, "mean": 2.1720e+01}, "entropy_coef": {"n": 1, "mean": 1.3256e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2981e-01, "std": 2.2589e-01, "min_value": 3.4750e-05, "max_value": 9.9921e-01}, "num_gradient_steps": 0, "step_time": 1.0377e+01, "total_time": 1.4643e+03, "__timestamp": "2024-10-09 22:44:31.623313"}, {"step": 160000, "num_env_steps": 160000, "scores": {"n": 1, "mean": 4.0991e+03}, "actor_loss": {"n": 1, "mean": -1.9515e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.8655e-01}, "critic_loss": {"n": 1, "mean": 1.0300e+01}, "entropy_coef": {"n": 1, "mean": 1.3143e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3432e-01, "std": 2.2239e-01, "min_value": 7.3446e-04, "max_value": 9.9964e-01}, "num_gradient_steps": 0, "step_time": 1.0772e+01, "total_time": 1.4751e+03, "__timestamp": "2024-10-09 22:44:42.394866"}, {"step": 161000, "num_env_steps": 161000, "scores": {"n": 1, "mean": 4.0966e+03}, "actor_loss": {"n": 1, "mean": -1.8874e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.6329e-01}, "critic_loss": {"n": 1, "mean": 1.3740e+01}, "entropy_coef": {"n": 1, "mean": 1.3061e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3198e-01, "std": 2.2427e-01, "min_value": 4.3434e-04, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.0772e+01, "total_time": 1.4859e+03, "__timestamp": "2024-10-09 22:44:53.165961"}, {"step": 162000, "num_env_steps": 162000, "scores": {"n": 1, "mean": 4.0328e+03}, "actor_loss": {"n": 1, "mean": -1.9414e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5103e-01}, "critic_loss": {"n": 1, "mean": 1.0125e+01}, "entropy_coef": {"n": 1, "mean": 1.3250e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3495e-01, "std": 2.2434e-01, "min_value": 5.9263e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0576e+01, "total_time": 1.4964e+03, "__timestamp": "2024-10-09 22:45:03.742208"}, {"step": 163000, "num_env_steps": 163000, "scores": {"n": 1, "mean": 3.9491e+03}, "actor_loss": {"n": 1, "mean": -1.8168e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.3422e+00}, "critic_loss": {"n": 1, "mean": 1.3318e+01}, "entropy_coef": {"n": 1, "mean": 1.3317e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3650e-01, "std": 2.2064e-01, "min_value": 1.3143e-03, "max_value": 9.9927e-01}, "num_gradient_steps": 0, "step_time": 1.0645e+01, "total_time": 1.5071e+03, "__timestamp": "2024-10-09 22:45:14.387761"}, {"step": 164000, "num_env_steps": 164000, "scores": {"n": 1, "mean": 4.0723e+03}, "actor_loss": {"n": 1, "mean": -1.9740e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.9696e-01}, "critic_loss": {"n": 1, "mean": 9.0632e+00}, "entropy_coef": {"n": 1, "mean": 1.3341e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3488e-01, "std": 2.1798e-01, "min_value": 2.0225e-03, "max_value": 9.9928e-01}, "num_gradient_steps": 0, "step_time": 1.1155e+01, "total_time": 1.5182e+03, "__timestamp": "2024-10-09 22:45:25.542762"}, {"step": 165000, "num_env_steps": 165000, "scores": {"n": 1, "mean": 3.9351e+03}, "actor_loss": {"n": 1, "mean": -1.7796e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.6182e+00}, "critic_loss": {"n": 1, "mean": 8.6123e+00}, "entropy_coef": {"n": 1, "mean": 1.3395e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3422e-01, "std": 2.2472e-01, "min_value": 1.3739e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.0706e+01, "total_time": 1.5290e+03, "__timestamp": "2024-10-09 22:45:36.247768"}, {"step": 166000, "num_env_steps": 166000, "scores": {"n": 1, "mean": 4.0442e+03}, "actor_loss": {"n": 1, "mean": -1.9590e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.7803e-01}, "critic_loss": {"n": 1, "mean": 9.2169e+00}, "entropy_coef": {"n": 1, "mean": 1.3196e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3055e-01, "std": 2.2623e-01, "min_value": 3.1900e-04, "max_value": 9.9939e-01}, "num_gradient_steps": 0, "step_time": 1.0710e+01, "total_time": 1.5397e+03, "__timestamp": "2024-10-09 22:45:46.957391"}, {"step": 167000, "num_env_steps": 167000, "scores": {"n": 1, "mean": 3.8474e+03}, "actor_loss": {"n": 1, "mean": -1.8349e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.7582e-01}, "critic_loss": {"n": 1, "mean": 1.5582e+02}, "entropy_coef": {"n": 1, "mean": 1.3500e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2815e-01, "std": 2.2605e-01, "min_value": 2.4128e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0888e+01, "total_time": 1.5506e+03, "__timestamp": "2024-10-09 22:45:57.846545"}, {"step": 168000, "num_env_steps": 168000, "scores": {"n": 1, "mean": 4.0467e+03}, "actor_loss": {"n": 1, "mean": -1.8650e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2845e+00}, "critic_loss": {"n": 1, "mean": 1.0178e+01}, "entropy_coef": {"n": 1, "mean": 1.3746e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3218e-01, "std": 2.2483e-01, "min_value": 1.0678e-03, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.1244e+01, "total_time": 1.5618e+03, "__timestamp": "2024-10-09 22:46:09.089269"}, {"step": 169000, "num_env_steps": 169000, "scores": {"n": 1, "mean": 4.1253e+03}, "actor_loss": {"n": 1, "mean": -1.9373e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.9410e-01}, "critic_loss": {"n": 1, "mean": 8.2838e+00}, "entropy_coef": {"n": 1, "mean": 1.3510e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3625e-01, "std": 2.2122e-01, "min_value": 2.7937e-03, "max_value": 9.9941e-01}, "num_gradient_steps": 0, "step_time": 1.0794e+01, "total_time": 1.5726e+03, "__timestamp": "2024-10-09 22:46:19.884016"}, {"step": 170000, "num_env_steps": 170000, "scores": {"n": 1, "mean": 4.1113e+03}, "actor_loss": {"n": 1, "mean": -1.9064e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.1005e-01}, "critic_loss": {"n": 1, "mean": 8.1705e+00}, "entropy_coef": {"n": 1, "mean": 1.3535e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2977e-01, "std": 2.2882e-01, "min_value": 6.8790e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.1268e+01, "total_time": 1.5839e+03, "__timestamp": "2024-10-09 22:46:31.152217"}, {"step": 171000, "num_env_steps": 171000, "scores": {"n": 1, "mean": 3.6897e+03}, "actor_loss": {"n": 1, "mean": -1.8993e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.9929e-01}, "critic_loss": {"n": 1, "mean": 3.9051e+02}, "entropy_coef": {"n": 1, "mean": 1.3609e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1781e-01, "std": 2.3400e-01, "min_value": 7.8022e-05, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 1.1354e+01, "total_time": 1.5952e+03, "__timestamp": "2024-10-09 22:46:42.505883"}, {"step": 172000, "num_env_steps": 172000, "scores": {"n": 1, "mean": 4.2140e+03}, "actor_loss": {"n": 1, "mean": -1.8967e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0195e+00}, "critic_loss": {"n": 1, "mean": 7.7801e+00}, "entropy_coef": {"n": 1, "mean": 1.3642e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3385e-01, "std": 2.2340e-01, "min_value": 8.9246e-04, "max_value": 9.9917e-01}, "num_gradient_steps": 0, "step_time": 1.1148e+01, "total_time": 1.6064e+03, "__timestamp": "2024-10-09 22:46:53.653671"}, {"step": 173000, "num_env_steps": 173000, "scores": {"n": 1, "mean": 3.9355e+03}, "actor_loss": {"n": 1, "mean": -1.9141e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.1371e-01}, "critic_loss": {"n": 1, "mean": 1.7052e+02}, "entropy_coef": {"n": 1, "mean": 1.3411e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2284e-01, "std": 2.3105e-01, "min_value": 1.8199e-03, "max_value": 9.9936e-01}, "num_gradient_steps": 0, "step_time": 1.1439e+01, "total_time": 1.6178e+03, "__timestamp": "2024-10-09 22:47:05.092841"}, {"step": 174000, "num_env_steps": 174000, "scores": {"n": 1, "mean": 4.2092e+03}, "actor_loss": {"n": 1, "mean": -1.9074e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.4597e-02}, "critic_loss": {"n": 1, "mean": 2.3286e+02}, "entropy_coef": {"n": 1, "mean": 1.3893e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2947e-01, "std": 2.2954e-01, "min_value": 1.8793e-04, "max_value": 9.9958e-01}, "num_gradient_steps": 0, "step_time": 1.0987e+01, "total_time": 1.6288e+03, "__timestamp": "2024-10-09 22:47:16.080201"}, {"step": 175000, "num_env_steps": 175000, "scores": {"n": 1, "mean": 4.0346e+03}, "actor_loss": {"n": 1, "mean": -1.8911e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4529e-01}, "critic_loss": {"n": 1, "mean": 1.3902e+02}, "entropy_coef": {"n": 1, "mean": 1.3791e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2793e-01, "std": 2.2897e-01, "min_value": 2.1231e-03, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 1.0556e+01, "total_time": 1.6393e+03, "__timestamp": "2024-10-09 22:47:26.635052"}, {"step": 176000, "num_env_steps": 176000, "scores": {"n": 1, "mean": 4.1092e+03}, "actor_loss": {"n": 1, "mean": -1.8702e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.5248e-01}, "critic_loss": {"n": 1, "mean": 1.2685e+01}, "entropy_coef": {"n": 1, "mean": 1.4179e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2834e-01, "std": 2.2738e-01, "min_value": 9.6145e-04, "max_value": 9.9947e-01}, "num_gradient_steps": 0, "step_time": 1.1342e+01, "total_time": 1.6507e+03, "__timestamp": "2024-10-09 22:47:37.978432"}, {"step": 177000, "num_env_steps": 177000, "scores": {"n": 1, "mean": 4.2587e+03}, "actor_loss": {"n": 1, "mean": -1.8973e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.4881e-01}, "critic_loss": {"n": 1, "mean": 2.0329e+02}, "entropy_coef": {"n": 1, "mean": 1.3838e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2555e-01, "std": 2.2779e-01, "min_value": 7.2749e-04, "max_value": 9.9948e-01}, "num_gradient_steps": 0, "step_time": 1.0915e+01, "total_time": 1.6616e+03, "__timestamp": "2024-10-09 22:47:48.892578"}, {"step": 178000, "num_env_steps": 178000, "scores": {"n": 1, "mean": 3.9022e+03}, "actor_loss": {"n": 1, "mean": -1.9431e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.7542e-02}, "critic_loss": {"n": 1, "mean": 2.5556e+02}, "entropy_coef": {"n": 1, "mean": 1.3860e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1898e-01, "std": 2.3438e-01, "min_value": 1.7635e-03, "max_value": 9.9933e-01}, "num_gradient_steps": 0, "step_time": 1.1450e+01, "total_time": 1.6731e+03, "__timestamp": "2024-10-09 22:48:00.343618"}, {"step": 179000, "num_env_steps": 179000, "scores": {"n": 1, "mean": 4.1766e+03}, "actor_loss": {"n": 1, "mean": -1.8652e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.6071e-01}, "critic_loss": {"n": 1, "mean": 9.3720e+00}, "entropy_coef": {"n": 1, "mean": 1.4229e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3075e-01, "std": 2.2517e-01, "min_value": 1.9883e-04, "max_value": 9.9956e-01}, "num_gradient_steps": 0, "step_time": 1.0697e+01, "total_time": 1.6837e+03, "__timestamp": "2024-10-09 22:48:11.039129"}, {"step": 180000, "num_env_steps": 180000, "scores": {"n": 1, "mean": 3.9296e+03}, "actor_loss": {"n": 1, "mean": -1.9155e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.6070e-01}, "critic_loss": {"n": 1, "mean": 9.5040e+00}, "entropy_coef": {"n": 1, "mean": 1.3835e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2022e-01, "std": 2.3290e-01, "min_value": 1.1937e-03, "max_value": 9.9941e-01}, "num_gradient_steps": 0, "step_time": 1.1132e+01, "total_time": 1.6949e+03, "__timestamp": "2024-10-09 22:48:22.171504"}, {"step": 181000, "num_env_steps": 181000, "scores": {"n": 1, "mean": 4.1622e+03}, "actor_loss": {"n": 1, "mean": -1.9880e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.7701e-01}, "critic_loss": {"n": 1, "mean": 1.0896e+01}, "entropy_coef": {"n": 1, "mean": 1.4285e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2508e-01, "std": 2.3165e-01, "min_value": 3.9026e-05, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.0796e+01, "total_time": 1.7057e+03, "__timestamp": "2024-10-09 22:48:32.967246"}, {"step": 182000, "num_env_steps": 182000, "scores": {"n": 1, "mean": 4.0367e+03}, "actor_loss": {"n": 1, "mean": -1.9229e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.6332e-02}, "critic_loss": {"n": 1, "mean": 2.1741e+02}, "entropy_coef": {"n": 1, "mean": 1.4404e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2528e-01, "std": 2.3007e-01, "min_value": 9.6184e-04, "max_value": 9.9942e-01}, "num_gradient_steps": 0, "step_time": 1.0794e+01, "total_time": 1.7165e+03, "__timestamp": "2024-10-09 22:48:43.761199"}, {"step": 183000, "num_env_steps": 183000, "scores": {"n": 1, "mean": 4.1259e+03}, "actor_loss": {"n": 1, "mean": -1.9291e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.3653e-02}, "critic_loss": {"n": 1, "mean": 9.1054e+00}, "entropy_coef": {"n": 1, "mean": 1.4197e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2719e-01, "std": 2.2867e-01, "min_value": 3.0891e-04, "max_value": 9.9961e-01}, "num_gradient_steps": 0, "step_time": 1.0601e+01, "total_time": 1.7271e+03, "__timestamp": "2024-10-09 22:48:54.362656"}, {"step": 184000, "num_env_steps": 184000, "scores": {"n": 1, "mean": 4.0178e+03}, "actor_loss": {"n": 1, "mean": -1.8678e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.8262e-02}, "critic_loss": {"n": 1, "mean": 7.7319e+00}, "entropy_coef": {"n": 1, "mean": 1.4096e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2177e-01, "std": 2.3022e-01, "min_value": 1.3921e-03, "max_value": 9.9942e-01}, "num_gradient_steps": 0, "step_time": 1.0763e+01, "total_time": 1.7378e+03, "__timestamp": "2024-10-09 22:49:05.126879"}, {"step": 185000, "num_env_steps": 185000, "scores": {"n": 1, "mean": 4.0270e+03}, "actor_loss": {"n": 1, "mean": -1.9653e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.3411e-01}, "critic_loss": {"n": 1, "mean": 1.0485e+01}, "entropy_coef": {"n": 1, "mean": 1.4351e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1730e-01, "std": 2.3808e-01, "min_value": 3.5062e-04, "max_value": 9.9925e-01}, "num_gradient_steps": 0, "step_time": 1.0717e+01, "total_time": 1.7486e+03, "__timestamp": "2024-10-09 22:49:15.843490"}, {"step": 186000, "num_env_steps": 186000, "scores": {"n": 1, "mean": 4.2010e+03}, "actor_loss": {"n": 1, "mean": -1.9493e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.8537e-01}, "critic_loss": {"n": 1, "mean": 7.9992e+00}, "entropy_coef": {"n": 1, "mean": 1.4273e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2429e-01, "std": 2.2892e-01, "min_value": 3.1683e-04, "max_value": 9.9954e-01}, "num_gradient_steps": 0, "step_time": 1.0423e+01, "total_time": 1.7590e+03, "__timestamp": "2024-10-09 22:49:26.265056"}, {"step": 187000, "num_env_steps": 187000, "scores": {"n": 1, "mean": 4.2428e+03}, "actor_loss": {"n": 1, "mean": -1.8922e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.0698e-01}, "critic_loss": {"n": 1, "mean": 6.8160e+00}, "entropy_coef": {"n": 1, "mean": 1.4094e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.3270e-01, "std": 2.2135e-01, "min_value": 6.2916e-05, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.0531e+01, "total_time": 1.7695e+03, "__timestamp": "2024-10-09 22:49:36.796272"}, {"step": 188000, "num_env_steps": 188000, "scores": {"n": 1, "mean": 4.3249e+03}, "actor_loss": {"n": 1, "mean": -1.9972e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.6545e+00}, "critic_loss": {"n": 1, "mean": 6.4673e+00}, "entropy_coef": {"n": 1, "mean": 1.4498e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2823e-01, "std": 2.2947e-01, "min_value": 4.7356e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.0487e+01, "total_time": 1.7800e+03, "__timestamp": "2024-10-09 22:49:47.283389"}, {"step": 189000, "num_env_steps": 189000, "scores": {"n": 1, "mean": 4.0186e+03}, "actor_loss": {"n": 1, "mean": -1.9723e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.7895e-01}, "critic_loss": {"n": 1, "mean": 1.6445e+02}, "entropy_coef": {"n": 1, "mean": 1.4405e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1596e-01, "std": 2.3671e-01, "min_value": 3.2766e-04, "max_value": 9.9938e-01}, "num_gradient_steps": 0, "step_time": 1.0311e+01, "total_time": 1.7903e+03, "__timestamp": "2024-10-09 22:49:57.595302"}, {"step": 190000, "num_env_steps": 190000, "scores": {"n": 1, "mean": 4.0583e+03}, "actor_loss": {"n": 1, "mean": -1.9737e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.3268e-01}, "critic_loss": {"n": 1, "mean": 7.0255e+00}, "entropy_coef": {"n": 1, "mean": 1.4677e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2504e-01, "std": 2.2690e-01, "min_value": 1.2647e-03, "max_value": 9.9955e-01}, "num_gradient_steps": 0, "step_time": 1.0359e+01, "total_time": 1.8007e+03, "__timestamp": "2024-10-09 22:50:07.953410"}, {"step": 191000, "num_env_steps": 191000, "scores": {"n": 1, "mean": 4.0431e+03}, "actor_loss": {"n": 1, "mean": -1.8860e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.8881e-01}, "critic_loss": {"n": 1, "mean": 8.4485e+00}, "entropy_coef": {"n": 1, "mean": 1.4540e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2393e-01, "std": 2.3260e-01, "min_value": 1.4928e-04, "max_value": 9.9964e-01}, "num_gradient_steps": 0, "step_time": 1.0329e+01, "total_time": 1.8110e+03, "__timestamp": "2024-10-09 22:50:18.282738"}, {"step": 192000, "num_env_steps": 192000, "scores": {"n": 1, "mean": 3.9888e+03}, "actor_loss": {"n": 1, "mean": -1.9260e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2483e+00}, "critic_loss": {"n": 1, "mean": 6.8428e+00}, "entropy_coef": {"n": 1, "mean": 1.4432e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2104e-01, "std": 2.3080e-01, "min_value": 3.9801e-05, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.0381e+01, "total_time": 1.8214e+03, "__timestamp": "2024-10-09 22:50:28.664923"}, {"step": 193000, "num_env_steps": 193000, "scores": {"n": 1, "mean": 4.0698e+03}, "actor_loss": {"n": 1, "mean": -1.8955e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.6326e-01}, "critic_loss": {"n": 1, "mean": 1.1063e+01}, "entropy_coef": {"n": 1, "mean": 1.4278e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2323e-01, "std": 2.3438e-01, "min_value": 3.8275e-04, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 1.0869e+01, "total_time": 1.8322e+03, "__timestamp": "2024-10-09 22:50:39.533982"}, {"step": 194000, "num_env_steps": 194000, "scores": {"n": 1, "mean": 4.2279e+03}, "actor_loss": {"n": 1, "mean": -1.9559e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.0372e-01}, "critic_loss": {"n": 1, "mean": 8.3218e+00}, "entropy_coef": {"n": 1, "mean": 1.4417e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2633e-01, "std": 2.2716e-01, "min_value": 9.5665e-04, "max_value": 9.9945e-01}, "num_gradient_steps": 0, "step_time": 1.0740e+01, "total_time": 1.8430e+03, "__timestamp": "2024-10-09 22:50:50.273632"}, {"step": 195000, "num_env_steps": 195000, "scores": {"n": 1, "mean": 4.0621e+03}, "actor_loss": {"n": 1, "mean": -2.0406e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1096e+00}, "critic_loss": {"n": 1, "mean": 8.0822e+00}, "entropy_coef": {"n": 1, "mean": 1.4583e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1748e-01, "std": 2.3391e-01, "min_value": 9.7819e-04, "max_value": 9.9953e-01}, "num_gradient_steps": 0, "step_time": 1.0546e+01, "total_time": 1.8535e+03, "__timestamp": "2024-10-09 22:51:00.819952"}, {"step": 196000, "num_env_steps": 196000, "scores": {"n": 1, "mean": 4.0773e+03}, "actor_loss": {"n": 1, "mean": -2.0306e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.5840e-01}, "critic_loss": {"n": 1, "mean": 8.2690e+00}, "entropy_coef": {"n": 1, "mean": 1.4433e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2294e-01, "std": 2.2859e-01, "min_value": 1.3539e-03, "max_value": 9.9939e-01}, "num_gradient_steps": 0, "step_time": 1.0852e+01, "total_time": 1.8644e+03, "__timestamp": "2024-10-09 22:51:11.671044"}, {"step": 197000, "num_env_steps": 197000, "scores": {"n": 1, "mean": 4.1367e+03}, "actor_loss": {"n": 1, "mean": -1.9892e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1315e+00}, "critic_loss": {"n": 1, "mean": 1.4394e+01}, "entropy_coef": {"n": 1, "mean": 1.4395e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1879e-01, "std": 2.3112e-01, "min_value": 9.1743e-04, "max_value": 9.9976e-01}, "num_gradient_steps": 0, "step_time": 1.0787e+01, "total_time": 1.8752e+03, "__timestamp": "2024-10-09 22:51:22.458802"}, {"step": 198000, "num_env_steps": 198000, "scores": {"n": 1, "mean": 4.1905e+03}, "actor_loss": {"n": 1, "mean": -2.0000e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.6271e-01}, "critic_loss": {"n": 1, "mean": 9.4059e+00}, "entropy_coef": {"n": 1, "mean": 1.4900e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2433e-01, "std": 2.2903e-01, "min_value": 1.7428e-03, "max_value": 9.9936e-01}, "num_gradient_steps": 0, "step_time": 1.1745e+01, "total_time": 1.8869e+03, "__timestamp": "2024-10-09 22:51:34.203020"}, {"step": 199000, "num_env_steps": 199000, "scores": {"n": 1, "mean": 4.1532e+03}, "actor_loss": {"n": 1, "mean": -1.9360e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.2508e-01}, "critic_loss": {"n": 1, "mean": 7.7447e+00}, "entropy_coef": {"n": 1, "mean": 1.4887e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1387e-01, "std": 2.3813e-01, "min_value": 2.4915e-04, "max_value": 9.9944e-01}, "num_gradient_steps": 0, "step_time": 1.1936e+01, "total_time": 1.8988e+03, "__timestamp": "2024-10-09 22:51:46.138850"}, {"step": 200000, "num_env_steps": 200000, "scores": {"n": 1, "mean": 4.1839e+03}, "actor_loss": {"n": 1, "mean": -1.8977e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.8156e-01}, "critic_loss": {"n": 1, "mean": 8.7167e+00}, "entropy_coef": {"n": 1, "mean": 1.5235e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2216e-01, "std": 2.2902e-01, "min_value": 1.5366e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.1257e+01, "total_time": 1.9101e+03, "__timestamp": "2024-10-09 22:51:57.396483"}, {"step": 201000, "num_env_steps": 201000, "scores": {"n": 1, "mean": 4.0975e+03}, "actor_loss": {"n": 1, "mean": -1.9986e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.7345e-01}, "critic_loss": {"n": 1, "mean": 1.0310e+01}, "entropy_coef": {"n": 1, "mean": 1.4856e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1199e-01, "std": 2.3917e-01, "min_value": 6.4863e-04, "max_value": 9.9924e-01}, "num_gradient_steps": 0, "step_time": 1.2133e+01, "total_time": 1.9222e+03, "__timestamp": "2024-10-09 22:52:09.529334"}, {"step": 202000, "num_env_steps": 202000, "scores": {"n": 1, "mean": 4.1962e+03}, "actor_loss": {"n": 1, "mean": -1.9724e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.6877e-01}, "critic_loss": {"n": 1, "mean": 7.0079e+00}, "entropy_coef": {"n": 1, "mean": 1.4812e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1062e-01, "std": 2.4106e-01, "min_value": 7.2020e-04, "max_value": 9.9947e-01}, "num_gradient_steps": 0, "step_time": 1.1567e+01, "total_time": 1.9338e+03, "__timestamp": "2024-10-09 22:52:21.095330"}, {"step": 203000, "num_env_steps": 203000, "scores": {"n": 1, "mean": 4.1275e+03}, "actor_loss": {"n": 1, "mean": -1.9424e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.5768e-01}, "critic_loss": {"n": 1, "mean": 1.0337e+01}, "entropy_coef": {"n": 1, "mean": 1.4649e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1898e-01, "std": 2.3471e-01, "min_value": 3.8427e-04, "max_value": 9.9939e-01}, "num_gradient_steps": 0, "step_time": 1.2006e+01, "total_time": 1.9458e+03, "__timestamp": "2024-10-09 22:52:33.100990"}, {"step": 204000, "num_env_steps": 204000, "scores": {"n": 1, "mean": 4.1150e+03}, "actor_loss": {"n": 1, "mean": -2.0250e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.0063e-01}, "critic_loss": {"n": 1, "mean": 7.4357e+00}, "entropy_coef": {"n": 1, "mean": 1.4778e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.1826e-01, "std": 2.3424e-01, "min_value": 2.2633e-04, "max_value": 9.9937e-01}, "num_gradient_steps": 0, "step_time": 1.1699e+01, "total_time": 1.9575e+03, "__timestamp": "2024-10-09 22:52:44.801351"}, {"step": 205000, "num_env_steps": 205000, "scores": {"n": 1, "mean": 4.1901e+03}, "actor_loss": {"n": 1, "mean": -1.9338e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.5620e-01}, "critic_loss": {"n": 1, "mean": 8.4079e+00}, "entropy_coef": {"n": 1, "mean": 1.5006e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0979e-01, "std": 2.4317e-01, "min_value": 3.4104e-04, "max_value": 9.9918e-01}, "num_gradient_steps": 0, "step_time": 1.1395e+01, "total_time": 1.9689e+03, "__timestamp": "2024-10-09 22:52:56.196806"}, {"step": 206000, "num_env_steps": 206000, "scores": {"n": 1, "mean": 4.2731e+03}, "actor_loss": {"n": 1, "mean": -1.9804e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.5053e-01}, "critic_loss": {"n": 1, "mean": 1.0202e+01}, "entropy_coef": {"n": 1, "mean": 1.4929e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.2083e-01, "std": 2.3310e-01, "min_value": 1.1319e-04, "max_value": 9.9944e-01}, "num_gradient_steps": 0, "step_time": 1.1839e+01, "total_time": 1.9807e+03, "__timestamp": "2024-10-09 22:53:08.036264"}]}}