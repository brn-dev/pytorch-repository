{"experiment_id": "2024-10-07_14-22-52_749652~k8LGZN", "experiment_tags": ["SAC", "HalfCheetah-v4"], "start_time": "2024-10-07 14:22:52.749652", "end_time": "2024-10-07 14:26:40.800172", "end_exception": null, "hyper_parameters": {"_type": "SAC", "_type_fq": "src.reinforcement_learning.algorithms.sac.sac.SAC", "env": "<SingletonVectorEnv instance>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {"_type": "SACPolicy", "_type_fq": "src.reinforcement_learning.algorithms.sac.sac_policy.SACPolicy", "parameter_count": 217870, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "actor": {"_type": "Actor", "_type_fq": "src.reinforcement_learning.core.policies.components.actor.Actor", "parameter_count": 73484, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "network": {"_type": "Sequential", "_type_fq": "torch.nn.modules.container.Sequential", "num_layers": 4, "layers": [{"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 17, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}, {"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 256, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}], "repr": "Sequential(\n  (0): Linear(in_features=17, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n)"}, "action_selector": {"_type": "PredictedStdActionSelector", "_type_fq": "src.reinforcement_learning.core.action_selectors.predicted_std_action_selector.PredictedStdActionSelector", "latent_dim": 256, "action_dim": 6, "action_net_initialization": null, "base_std": 1.0000e+00, "squash_output": true, "epsilon": 1.0000e-06, "log_std_clamp_range": [-20, 2], "log_std_net_initialization": null}}, "critic": {"_type": "QCritic", "_type_fq": "src.reinforcement_learning.core.policies.components.q_critic.QCritic", "parameter_count": 144386, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "n_critics": 2, "q_network": {"_type": "Sequential", "_type_fq": "torch.nn.modules.container.Sequential", "num_layers": 5, "layers": [{"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 23, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}, {"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 256, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}, {"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 256, "out_features": 1, "bias": true}], "repr": "Sequential(\n  (0): Linear(in_features=23, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=256, out_features=1, bias=True)\n)"}}}, "policy_parameter_count": 217870, "policy_repr": "SACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (feature_extractor): IdentityExtractor()\n    (network): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (_action_selector): PredictedStdActionSelector(\n      (action_net): Linear(in_features=256, out_features=6, bias=True)\n      (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n    )\n  )\n  (critic): QCritic(\n    (feature_extractor): IdentityExtractor()\n    (q_networks): ModuleList(\n      (0-1): 2 x Sequential(\n        (0): Linear(in_features=23, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=256, out_features=1, bias=True)\n      )\n    )\n  )\n  (target_critic): QCritic(\n    (feature_extractor): IdentityExtractor()\n    (q_networks): ModuleList(\n      (0-1): 2 x Sequential(\n        (0): Linear(in_features=23, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=256, out_features=1, bias=True)\n      )\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {"_type": "ReplayBuffer", "_type_fq": "src.reinforcement_learning.core.buffers.replay.replay_buffer.ReplayBuffer", "step_size": 1000000, "num_envs": 1, "total_size": 1000000, "reward_scale": 1.0000e+00, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "np_dtype": "<class 'numpy.float32'>", "optimize_memory_usage": false}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 7 7840HS w/ Radeon 780M Graphics", "cores": 8, "logical_cores": 16, "speed": "3801 MHz"}, "gpu": [{"name": "AMD Radeon(TM) Graphics", "video_processor": "AMD Radeon Graphics Processor (0x15BF)", "adapter_ram": "512 MB", "adapter_dac_type": "Internal DAC(400MHz)", "manufacturer": "Advanced Micro Devices, Inc."}, {"name": "NVIDIA GeForce RTX 4060 Laptop GPU", "video_processor": "NVIDIA GeForce RTX 4060 Laptop GPU", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA"}], "ram_speed": "5600 MHz", "ram": "31 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "import inspect\nimport os\nimport time\nfrom pathlib import Path\n\nimport gymnasium\nfrom gymnasium import Env\nfrom gymnasium.vector import VectorEnv\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.model_db import ModelDB\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.np_functions import inv_symmetric_log\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.model_db.tiny_model_db import TinyModelDB\nfrom src.module_analysis import count_parameters, get_gradients_per_parameter\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\nfrom src.reinforcement_learning.gym.envs.test_env import TestEnv\nfrom src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\nfrom typing import Any, SupportsFloat, Optional\nfrom gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\nfrom src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\nfrom src.reinforcement_learning.core.normalization import NormalizationType\nfrom src.torch_device import set_default_torch_device, optimizer_to_device\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nfrom torch.distributions import Normal, Categorical\n\nimport torch\nfrom torch import optim, nn\nimport torch.distributions as dist\nimport gymnasium as gym\nimport numpy as np\n\nfrom src.torch_functions import antisymmetric_power\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    \n    if len(episode_scores) > 0:\n    \n        global best_iteration_score\n        iteration_score = episode_scores.mean()\n        score_moving_average = score_mean_ema.update(iteration_score)\n        if iteration_score >= best_iteration_score:\n            best_iteration_score = iteration_score\n            policy_db.save_model_state_dict(\n                model_id=policy_id,\n                parent_model_id=parent_policy_id,\n                model_info={\n                    'score': iteration_score.item(),\n                    'steps_trained': steps_trained,\n                    'wrap_env_source_code': wrap_env_source_code_source,\n                    'init_policy_source_code': init_policy_source\n                },\n                model=policy,\n                optimizer=optimizer,\n            )\n        info['score_moving_average'] = score_moving_average\n    \n    info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags(),\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -2.5430e+02}, "actor_loss": {"n": 1, "mean": -1.7595e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.9941e+00}, "critic_loss": {"n": 1, "mean": 9.7165e-01}, "entropy_coef": {"n": 1, "mean": 7.4079e-01}, "action_stds": {"n": 6, "mean": 8.7704e-01, "std": 6.6855e-02, "min_value": 7.4939e-01, "max_value": 9.3638e-01, "data": [[[9.2391e-01, 8.7430e-01, 9.3638e-01, 7.4939e-01, 8.8489e-01, 8.9335e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.1766e-01, "std": 2.8867e-01, "min_value": 2.0397e-04, "max_value": 9.9807e-01}, "num_gradient_steps": 1000, "step_time": 1.1947e+01, "total_time": 1.1336e+01, "__timestamp": "2024-10-07 14:23:04.097212"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -2.5277e+02}, "actor_loss": {"n": 1, "mean": -2.5694e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9544e+00}, "critic_loss": {"n": 1, "mean": 1.4158e+00}, "entropy_coef": {"n": 1, "mean": 5.4958e-01}, "action_stds": {"n": 6, "mean": 8.8817e-01, "std": 5.1690e-02, "min_value": 8.1642e-01, "max_value": 9.3696e-01, "data": [[[8.9668e-01, 8.3302e-01, 9.3696e-01, 8.1642e-01, 9.1084e-01, 9.3510e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.3670e-01, "std": 2.8631e-01, "min_value": 8.7351e-05, "max_value": 9.9871e-01}, "num_gradient_steps": 2000, "step_time": 8.3419e+00, "total_time": 1.9678e+01, "__timestamp": "2024-10-07 14:23:12.426727"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -1.5549e+02}, "actor_loss": {"n": 1, "mean": -2.9696e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.8683e+00}, "critic_loss": {"n": 1, "mean": 3.1319e+00}, "entropy_coef": {"n": 1, "mean": 4.0745e-01}, "action_stds": {"n": 6, "mean": 9.1205e-01, "std": 1.9982e-02, "min_value": 8.9734e-01, "max_value": 9.4459e-01, "data": [[[9.2943e-01, 8.9994e-01, 9.4459e-01, 8.9878e-01, 8.9734e-01, 9.0223e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.3387e-01, "std": 2.8593e-01, "min_value": 2.9925e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 3000, "step_time": 8.1711e+00, "total_time": 2.7849e+01, "__timestamp": "2024-10-07 14:23:20.596780"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -3.0870e+02}, "actor_loss": {"n": 1, "mean": -3.2077e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1619e+01}, "critic_loss": {"n": 1, "mean": 1.7242e+00}, "entropy_coef": {"n": 1, "mean": 3.0256e-01}, "action_stds": {"n": 6, "mean": 9.3612e-01, "std": 1.8089e-02, "min_value": 9.1433e-01, "max_value": 9.5997e-01, "data": [[[9.5997e-01, 9.2698e-01, 9.1433e-01, 9.3655e-01, 9.2383e-01, 9.5504e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.4843e-01, "std": 2.8957e-01, "min_value": 3.0005e-04, "max_value": 9.9833e-01}, "num_gradient_steps": 4000, "step_time": 8.0637e+00, "total_time": 3.5913e+01, "__timestamp": "2024-10-07 14:23:28.661038"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -2.2692e+02}, "actor_loss": {"n": 1, "mean": -3.2932e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3933e+01}, "critic_loss": {"n": 1, "mean": 1.7076e+00}, "entropy_coef": {"n": 1, "mean": 2.2563e-01}, "action_stds": {"n": 6, "mean": 9.0485e-01, "std": 2.8774e-02, "min_value": 8.5573e-01, "max_value": 9.3826e-01, "data": [[[8.5573e-01, 9.1072e-01, 9.3826e-01, 8.9950e-01, 8.9776e-01, 9.2715e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.4688e-01, "std": 2.9011e-01, "min_value": 5.4389e-07, "max_value": 9.9860e-01}, "num_gradient_steps": 5000, "step_time": 8.0384e+00, "total_time": 4.3951e+01, "__timestamp": "2024-10-07 14:23:36.699958"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -2.7765e+02}, "actor_loss": {"n": 1, "mean": -3.3078e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5263e+01}, "critic_loss": {"n": 1, "mean": 1.4738e+00}, "entropy_coef": {"n": 1, "mean": 1.6913e-01}, "action_stds": {"n": 6, "mean": 8.6017e-01, "std": 8.1628e-02, "min_value": 7.1418e-01, "max_value": 9.6511e-01, "data": [[[7.1418e-01, 8.8532e-01, 8.7874e-01, 8.6093e-01, 9.6511e-01, 8.5674e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.5576e-01, "std": 2.9251e-01, "min_value": 3.1471e-05, "max_value": 9.9942e-01}, "num_gradient_steps": 6000, "step_time": 7.9010e+00, "total_time": 5.1852e+01, "__timestamp": "2024-10-07 14:23:44.599940"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -1.6208e+02}, "actor_loss": {"n": 1, "mean": -3.2472e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7205e+01}, "critic_loss": {"n": 1, "mean": 1.3189e+00}, "entropy_coef": {"n": 1, "mean": 1.2749e-01}, "action_stds": {"n": 6, "mean": 8.7767e-01, "std": 3.4182e-02, "min_value": 8.3623e-01, "max_value": 9.2033e-01, "data": [[[8.3623e-01, 8.6968e-01, 9.2033e-01, 8.7295e-01, 9.1622e-01, 8.5061e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.7942e-01, "std": 2.9139e-01, "min_value": 1.5837e-04, "max_value": 9.9848e-01}, "num_gradient_steps": 7000, "step_time": 8.0602e+00, "total_time": 5.9912e+01, "__timestamp": "2024-10-07 14:23:52.660169"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -2.0174e+02}, "actor_loss": {"n": 1, "mean": -3.0923e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7166e+01}, "critic_loss": {"n": 1, "mean": 1.6543e+00}, "entropy_coef": {"n": 1, "mean": 9.6720e-02}, "action_stds": {"n": 6, "mean": 8.1909e-01, "std": 6.7102e-02, "min_value": 7.4370e-01, "max_value": 9.3102e-01, "data": [[[7.5999e-01, 8.4360e-01, 9.3102e-01, 7.4370e-01, 8.0796e-01, 8.2828e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.7760e-01, "std": 2.9786e-01, "min_value": 4.9874e-05, "max_value": 9.9916e-01}, "num_gradient_steps": 8000, "step_time": 8.2983e+00, "total_time": 6.8211e+01, "__timestamp": "2024-10-07 14:24:00.959488"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -1.9141e+02}, "actor_loss": {"n": 1, "mean": -2.9838e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5139e+01}, "critic_loss": {"n": 1, "mean": 1.4105e+00}, "entropy_coef": {"n": 1, "mean": 7.3715e-02}, "action_stds": {"n": 6, "mean": 8.0905e-01, "std": 1.2361e-01, "min_value": 6.9523e-01, "max_value": 1.0318e+00, "data": [[[6.9523e-01, 7.8904e-01, 1.0318e+00, 7.1396e-01, 7.6535e-01, 8.5892e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.9067e-01, "std": 2.9707e-01, "min_value": 4.5365e-04, "max_value": 9.9934e-01}, "num_gradient_steps": 9000, "step_time": 8.0786e+00, "total_time": 7.6289e+01, "__timestamp": "2024-10-07 14:24:09.037092"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -2.3167e+02}, "actor_loss": {"n": 1, "mean": -2.8573e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7024e+01}, "critic_loss": {"n": 1, "mean": 1.5529e+00}, "entropy_coef": {"n": 1, "mean": 5.6508e-02}, "action_stds": {"n": 6, "mean": 6.9221e-01, "std": 2.4665e-02, "min_value": 6.6417e-01, "max_value": 7.2433e-01, "data": [[[6.9929e-01, 6.6975e-01, 6.8048e-01, 6.6417e-01, 7.2433e-01, 7.1526e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1021e-01, "std": 2.9781e-01, "min_value": 1.0073e-05, "max_value": 9.9940e-01}, "num_gradient_steps": 10000, "step_time": 8.1626e+00, "total_time": 8.4452e+01, "__timestamp": "2024-10-07 14:24:17.199677"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -1.7650e+02}, "actor_loss": {"n": 1, "mean": -2.7760e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4744e+01}, "critic_loss": {"n": 1, "mean": 1.4565e+00}, "entropy_coef": {"n": 1, "mean": 4.3621e-02}, "action_stds": {"n": 6, "mean": 7.3502e-01, "std": 6.6849e-02, "min_value": 6.3272e-01, "max_value": 8.3280e-01, "data": [[[6.3272e-01, 7.5514e-01, 8.3280e-01, 7.3941e-01, 6.9592e-01, 7.5411e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.0703e-01, "std": 2.9934e-01, "min_value": 8.4532e-05, "max_value": 9.9979e-01}, "num_gradient_steps": 11000, "step_time": 8.1601e+00, "total_time": 9.2612e+01, "__timestamp": "2024-10-07 14:24:25.359761"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -1.6766e+02}, "actor_loss": {"n": 1, "mean": -2.5940e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3618e+01}, "critic_loss": {"n": 1, "mean": 1.4606e+00}, "entropy_coef": {"n": 1, "mean": 3.3917e-02}, "action_stds": {"n": 6, "mean": 6.9655e-01, "std": 1.1606e-01, "min_value": 5.4596e-01, "max_value": 8.7749e-01, "data": [[[5.4596e-01, 6.8764e-01, 8.7749e-01, 6.9512e-01, 6.1022e-01, 7.6285e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.2218e-01, "std": 3.0029e-01, "min_value": 9.9540e-06, "max_value": 9.9982e-01}, "num_gradient_steps": 12000, "step_time": 8.1331e+00, "total_time": 1.0075e+02, "__timestamp": "2024-10-07 14:24:33.493819"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -1.2602e+02}, "actor_loss": {"n": 1, "mean": -2.4163e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1302e+01}, "critic_loss": {"n": 1, "mean": 2.2760e+00}, "entropy_coef": {"n": 1, "mean": 2.6598e-02}, "action_stds": {"n": 6, "mean": 6.1389e-01, "std": 5.4164e-02, "min_value": 5.7059e-01, "max_value": 6.9449e-01, "data": [[[6.6944e-01, 5.7638e-01, 6.9449e-01, 5.7456e-01, 5.9786e-01, 5.7059e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.3277e-01, "std": 2.9964e-01, "min_value": 2.1684e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 13000, "step_time": 8.3300e+00, "total_time": 1.0908e+02, "__timestamp": "2024-10-07 14:24:41.823307"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -1.1414e+02}, "actor_loss": {"n": 1, "mean": -2.2948e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1179e+01}, "critic_loss": {"n": 1, "mean": 1.1752e+00}, "entropy_coef": {"n": 1, "mean": 2.0922e-02}, "action_stds": {"n": 6, "mean": 5.5736e-01, "std": 3.1158e-02, "min_value": 5.2236e-01, "max_value": 5.9622e-01, "data": [[[5.3396e-01, 5.9622e-01, 5.6501e-01, 5.3644e-01, 5.2236e-01, 5.9015e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.4224e-01, "std": 2.9822e-01, "min_value": 2.3345e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 14000, "step_time": 8.1844e+00, "total_time": 1.1726e+02, "__timestamp": "2024-10-07 14:24:50.008252"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": -1.1205e+02}, "actor_loss": {"n": 1, "mean": -2.0827e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.7468e+00}, "critic_loss": {"n": 1, "mean": 3.8421e+00}, "entropy_coef": {"n": 1, "mean": 1.6649e-02}, "action_stds": {"n": 6, "mean": 4.6266e-01, "std": 4.4956e-02, "min_value": 3.9371e-01, "max_value": 5.0648e-01, "data": [[[5.0648e-01, 4.7446e-01, 4.9109e-01, 3.9371e-01, 4.8953e-01, 4.2068e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.3748e-01, "std": 2.9638e-01, "min_value": 4.8830e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 15000, "step_time": 8.8206e+00, "total_time": 1.2608e+02, "__timestamp": "2024-10-07 14:24:58.827843"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": -7.4378e+01}, "actor_loss": {"n": 1, "mean": -2.1208e+01}, "entropy_coef_loss": {"n": 1, "mean": -6.4396e+00}, "critic_loss": {"n": 1, "mean": 1.4495e+00}, "entropy_coef": {"n": 1, "mean": 1.3467e-02}, "action_stds": {"n": 6, "mean": 4.2657e-01, "std": 3.3706e-02, "min_value": 3.8183e-01, "max_value": 4.5626e-01, "data": [[[4.3778e-01, 3.8627e-01, 4.4260e-01, 4.5469e-01, 4.5626e-01, 3.8183e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.3639e-01, "std": 2.9771e-01, "min_value": 5.9798e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 16000, "step_time": 8.2847e+00, "total_time": 1.3436e+02, "__timestamp": "2024-10-07 14:25:07.114086"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": -1.7355e+02}, "actor_loss": {"n": 1, "mean": -1.9300e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.9661e+00}, "critic_loss": {"n": 1, "mean": 1.1521e+00}, "entropy_coef": {"n": 1, "mean": 1.1033e-02}, "action_stds": {"n": 6, "mean": 4.0538e-01, "std": 6.7219e-02, "min_value": 3.4888e-01, "max_value": 5.2523e-01, "data": [[[3.5887e-01, 5.2523e-01, 3.8554e-01, 3.4888e-01, 3.7172e-01, 4.4206e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.3661e-01, "std": 2.9370e-01, "min_value": 1.2828e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 17000, "step_time": 8.5053e+00, "total_time": 1.4287e+02, "__timestamp": "2024-10-07 14:25:15.617774"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": -3.5608e+01}, "actor_loss": {"n": 1, "mean": -1.8014e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7988e+00}, "critic_loss": {"n": 1, "mean": 1.4130e+00}, "entropy_coef": {"n": 1, "mean": 9.3582e-03}, "action_stds": {"n": 6, "mean": 4.0374e-01, "std": 3.6939e-02, "min_value": 3.4767e-01, "max_value": 4.3511e-01, "data": [[[4.3511e-01, 3.4767e-01, 4.2412e-01, 4.2865e-01, 3.6656e-01, 4.2035e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.4786e-01, "std": 3.0020e-01, "min_value": 5.1332e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 18000, "step_time": 8.3995e+00, "total_time": 1.5127e+02, "__timestamp": "2024-10-07 14:25:24.017225"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": -3.6015e+01}, "actor_loss": {"n": 1, "mean": -1.7306e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.8206e+00}, "critic_loss": {"n": 1, "mean": 1.2211e+00}, "entropy_coef": {"n": 1, "mean": 8.0866e-03}, "action_stds": {"n": 6, "mean": 2.9155e-01, "std": 5.4328e-02, "min_value": 2.2993e-01, "max_value": 3.6021e-01, "data": [[[3.6021e-01, 2.3400e-01, 3.4382e-01, 2.2993e-01, 2.8064e-01, 3.0071e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.3552e-01, "std": 3.0516e-01, "min_value": 5.3234e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 19000, "step_time": 8.3054e+00, "total_time": 1.5957e+02, "__timestamp": "2024-10-07 14:25:32.322641"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": -8.1498e+01}, "actor_loss": {"n": 1, "mean": -1.7031e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.6470e+00}, "critic_loss": {"n": 1, "mean": 1.4435e+00}, "entropy_coef": {"n": 1, "mean": 6.9187e-03}, "action_stds": {"n": 6, "mean": 2.1387e-01, "std": 3.6738e-02, "min_value": 1.4460e-01, "max_value": 2.4001e-01, "data": [[[2.0219e-01, 2.2300e-01, 2.3563e-01, 1.4460e-01, 2.3780e-01, 2.4001e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8994e-01, "std": 3.0362e-01, "min_value": 1.5229e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 20000, "step_time": 8.2707e+00, "total_time": 1.6785e+02, "__timestamp": "2024-10-07 14:25:40.593387"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": 6.0331e+02}, "actor_loss": {"n": 1, "mean": -1.5281e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.4435e+00}, "critic_loss": {"n": 1, "mean": 1.2766e+00}, "entropy_coef": {"n": 1, "mean": 6.3559e-03}, "action_stds": {"n": 6, "mean": 2.3092e-01, "std": 1.0154e-01, "min_value": 1.4742e-01, "max_value": 4.0682e-01, "data": [[[1.4742e-01, 2.0673e-01, 4.0682e-01, 1.7443e-01, 1.5509e-01, 2.9502e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.2074e-01, "std": 3.0915e-01, "min_value": 4.8435e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 21000, "step_time": 8.4686e+00, "total_time": 1.7631e+02, "__timestamp": "2024-10-07 14:25:49.063008"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": 2.6501e+02}, "actor_loss": {"n": 1, "mean": -1.7735e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.5761e+00}, "critic_loss": {"n": 1, "mean": 1.4964e+00}, "entropy_coef": {"n": 1, "mean": 6.1815e-03}, "action_stds": {"n": 6, "mean": 2.2085e-01, "std": 5.8945e-02, "min_value": 1.5719e-01, "max_value": 2.9507e-01, "data": [[[1.7155e-01, 2.8308e-01, 2.9507e-01, 1.5719e-01, 2.3392e-01, 1.8428e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8355e-01, "std": 3.0174e-01, "min_value": 1.4240e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 22000, "step_time": 8.1687e+00, "total_time": 1.8448e+02, "__timestamp": "2024-10-07 14:25:57.231733"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 2.6894e+02}, "actor_loss": {"n": 1, "mean": -1.5800e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.0879e-01}, "critic_loss": {"n": 1, "mean": 1.3492e+00}, "entropy_coef": {"n": 1, "mean": 6.3361e-03}, "action_stds": {"n": 6, "mean": 3.1042e-01, "std": 7.1614e-02, "min_value": 2.4507e-01, "max_value": 4.4270e-01, "data": [[[2.4507e-01, 2.8989e-01, 3.3290e-01, 2.9291e-01, 4.4270e-01, 2.5903e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1609e-01, "std": 3.0395e-01, "min_value": 3.9083e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 23000, "step_time": 7.9793e+00, "total_time": 1.9246e+02, "__timestamp": "2024-10-07 14:26:05.211031"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 5.0924e+02}, "actor_loss": {"n": 1, "mean": -1.6373e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.4252e-01}, "critic_loss": {"n": 1, "mean": 1.5230e+00}, "entropy_coef": {"n": 1, "mean": 6.2573e-03}, "action_stds": {"n": 6, "mean": 2.2823e-01, "std": 3.7922e-02, "min_value": 1.8999e-01, "max_value": 2.8914e-01, "data": [[[1.8999e-01, 2.4708e-01, 2.3927e-01, 2.1083e-01, 2.8914e-01, 1.9305e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.7011e-01, "std": 3.0040e-01, "min_value": 1.5831e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 24000, "step_time": 7.9751e+00, "total_time": 2.0044e+02, "__timestamp": "2024-10-07 14:26:13.186126"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 2.2109e+02}, "actor_loss": {"n": 1, "mean": -1.4319e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.3381e+00}, "critic_loss": {"n": 1, "mean": 1.2466e+00}, "entropy_coef": {"n": 1, "mean": 6.2319e-03}, "action_stds": {"n": 6, "mean": 2.9171e-01, "std": 4.8864e-02, "min_value": 2.2201e-01, "max_value": 3.5579e-01, "data": [[[3.5579e-01, 2.5776e-01, 3.0897e-01, 2.2201e-01, 3.2811e-01, 2.7760e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1143e-01, "std": 3.0069e-01, "min_value": 4.8554e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 25000, "step_time": 8.0517e+00, "total_time": 2.0849e+02, "__timestamp": "2024-10-07 14:26:21.236839"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 1.6112e+02}, "actor_loss": {"n": 1, "mean": -1.3803e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.1467e+00}, "critic_loss": {"n": 1, "mean": 3.4731e+00}, "entropy_coef": {"n": 1, "mean": 6.5431e-03}, "action_stds": {"n": 6, "mean": 2.9345e-01, "std": 4.4551e-02, "min_value": 2.4368e-01, "max_value": 3.6155e-01, "data": [[[2.5974e-01, 2.6960e-01, 3.0016e-01, 2.4368e-01, 3.2598e-01, 3.6155e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.0553e-01, "std": 2.9859e-01, "min_value": 1.1533e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 26000, "step_time": 8.2497e+00, "total_time": 2.1674e+02, "__timestamp": "2024-10-07 14:26:29.487497"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 1.6787e+02}, "actor_loss": {"n": 1, "mean": -1.4086e+01}, "entropy_coef_loss": {"n": 1, "mean": 8.0430e-01}, "critic_loss": {"n": 1, "mean": 1.6947e+00}, "entropy_coef": {"n": 1, "mean": 6.7006e-03}, "action_stds": {"n": 6, "mean": 3.7197e-01, "std": 5.9010e-02, "min_value": 2.9682e-01, "max_value": 4.6914e-01, "data": [[[3.2955e-01, 3.6935e-01, 3.7231e-01, 2.9682e-01, 3.9466e-01, 4.6914e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1172e-01, "std": 3.0109e-01, "min_value": 2.0328e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 27000, "step_time": 8.2910e+00, "total_time": 2.2503e+02, "__timestamp": "2024-10-07 14:26:37.777526"}]}}