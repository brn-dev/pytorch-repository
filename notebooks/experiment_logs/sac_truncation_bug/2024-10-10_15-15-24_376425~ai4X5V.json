{"experiment_id": "2024-10-10_15-15-24_376425~ai4X5V", "experiment_tags": ["SACDebug", "HalfCheetah-v4", "Debug"], "start_time": "2024-10-10 15:15:24.376425", "end_time": "2024-10-10 15:37:05.782640", "end_exception": null, "hyper_parameters": {"_type": "SACDebug", "_type_fq": "__main__.SACDebug", "env": "<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000015B175B7410>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {}, "policy_parameter_count": 362256, "policy_repr": "DebugSACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 9 3900X 12-Core Processor", "cores": 12, "logical_cores": 24, "speed": "3793 MHz"}, "gpu": [{"name": "NVIDIA GeForce RTX 3070", "video_processor": "NVIDIA GeForce RTX 3070", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA", "memory": "8192 MB", "memory_clock": "810 MHz", "compute_capability": "8.6"}], "ram_speed": "3600 MHz", "ram": "64 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "from sac import init_policy, init_action_selector\nfrom stable_baselines3.common.env_util import make_vec_env\nimport stable_baselines3 as sb\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nimport gymnasium\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n\ndef create_env(render_mode: str | None):\n    return gymnasium.make(env_name, render_mode=render_mode, **env_kwargs)\n\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nsb_sac = sb.SAC(\"MlpPolicy\", env, verbose=10, learning_starts=10000, stats_window_size=1) # , seed=594371)\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -2.3778e+02}, "actor_loss": {"n": 1, "mean": -1.7800e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.9906e+00}, "critic_loss": {"n": 1, "mean": 1.8985e+00}, "entropy_coef": {"n": 1, "mean": 7.4094e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.2174e-01, "std": 2.8487e-01, "min_value": 2.1702e-04, "max_value": 9.9788e-01}, "num_gradient_steps": 0, "step_time": 1.4952e+01, "total_time": 1.4941e+01, "__timestamp": "2024-10-10 15:15:39.317260"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -2.4356e+02}, "actor_loss": {"n": 1, "mean": -2.5889e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9845e+00}, "critic_loss": {"n": 1, "mean": 1.4163e+00}, "entropy_coef": {"n": 1, "mean": 5.4949e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3303e-01, "std": 2.8581e-01, "min_value": 1.5420e-04, "max_value": 9.9744e-01}, "num_gradient_steps": 0, "step_time": 1.1005e+01, "total_time": 2.5945e+01, "__timestamp": "2024-10-10 15:15:50.321922"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -1.6806e+02}, "actor_loss": {"n": 1, "mean": -3.0487e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.9065e+00}, "critic_loss": {"n": 1, "mean": 1.2985e+00}, "entropy_coef": {"n": 1, "mean": 4.0733e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4205e-01, "std": 2.8674e-01, "min_value": 3.9534e-04, "max_value": 9.9885e-01}, "num_gradient_steps": 0, "step_time": 1.1801e+01, "total_time": 3.7746e+01, "__timestamp": "2024-10-10 15:16:02.121918"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -3.7181e+02}, "actor_loss": {"n": 1, "mean": -3.2585e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1508e+01}, "critic_loss": {"n": 1, "mean": 1.8412e+00}, "entropy_coef": {"n": 1, "mean": 3.0272e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4665e-01, "std": 2.8664e-01, "min_value": 1.9401e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.1614e+01, "total_time": 4.9361e+01, "__timestamp": "2024-10-10 15:16:13.737032"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -1.7617e+02}, "actor_loss": {"n": 1, "mean": -3.2865e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3731e+01}, "critic_loss": {"n": 1, "mean": 1.7271e+00}, "entropy_coef": {"n": 1, "mean": 2.2587e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.5519e-01, "std": 2.9123e-01, "min_value": 3.3647e-05, "max_value": 9.9859e-01}, "num_gradient_steps": 0, "step_time": 1.0750e+01, "total_time": 6.0111e+01, "__timestamp": "2024-10-10 15:16:24.486278"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -2.1372e+02}, "actor_loss": {"n": 1, "mean": -3.3193e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5485e+01}, "critic_loss": {"n": 1, "mean": 1.8805e+00}, "entropy_coef": {"n": 1, "mean": 1.6957e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.7277e-01, "std": 2.9068e-01, "min_value": 2.0787e-05, "max_value": 9.9945e-01}, "num_gradient_steps": 0, "step_time": 1.0714e+01, "total_time": 7.0824e+01, "__timestamp": "2024-10-10 15:16:35.200886"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -2.4877e+02}, "actor_loss": {"n": 1, "mean": -3.2922e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5982e+01}, "critic_loss": {"n": 1, "mean": 1.8785e+00}, "entropy_coef": {"n": 1, "mean": 1.2812e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8772e-01, "std": 2.9685e-01, "min_value": 2.7984e-05, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 1.0644e+01, "total_time": 8.1469e+01, "__timestamp": "2024-10-10 15:16:45.845244"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -2.5113e+02}, "actor_loss": {"n": 1, "mean": -3.1766e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7333e+01}, "critic_loss": {"n": 1, "mean": 1.7869e+00}, "entropy_coef": {"n": 1, "mean": 9.7287e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9863e-01, "std": 2.9382e-01, "min_value": 4.8637e-05, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.0733e+01, "total_time": 9.2202e+01, "__timestamp": "2024-10-10 15:16:56.577083"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -2.1407e+02}, "actor_loss": {"n": 1, "mean": -3.1433e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5031e+01}, "critic_loss": {"n": 1, "mean": 1.4141e+00}, "entropy_coef": {"n": 1, "mean": 7.4371e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0485e-01, "std": 2.9680e-01, "min_value": 2.4700e-04, "max_value": 9.9972e-01}, "num_gradient_steps": 0, "step_time": 1.0736e+01, "total_time": 1.0294e+02, "__timestamp": "2024-10-10 15:17:07.312603"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -2.4988e+02}, "actor_loss": {"n": 1, "mean": -2.9687e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5825e+01}, "critic_loss": {"n": 1, "mean": 1.4091e+00}, "entropy_coef": {"n": 1, "mean": 5.7138e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2174e-01, "std": 2.9982e-01, "min_value": 2.8713e-04, "max_value": 9.9961e-01}, "num_gradient_steps": 0, "step_time": 1.0669e+01, "total_time": 1.1361e+02, "__timestamp": "2024-10-10 15:17:17.982552"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -2.3261e+02}, "actor_loss": {"n": 1, "mean": -2.7952e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5273e+01}, "critic_loss": {"n": 1, "mean": 1.2961e+00}, "entropy_coef": {"n": 1, "mean": 4.4005e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2378e-01, "std": 2.9861e-01, "min_value": 2.1100e-05, "max_value": 9.9960e-01}, "num_gradient_steps": 0, "step_time": 1.0684e+01, "total_time": 1.2429e+02, "__timestamp": "2024-10-10 15:17:28.665440"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -2.2040e+02}, "actor_loss": {"n": 1, "mean": -2.6147e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3627e+01}, "critic_loss": {"n": 1, "mean": 1.3762e+00}, "entropy_coef": {"n": 1, "mean": 3.4024e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4242e-01, "std": 2.9934e-01, "min_value": 2.0593e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0610e+01, "total_time": 1.3490e+02, "__timestamp": "2024-10-10 15:17:39.275877"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -2.3749e+02}, "actor_loss": {"n": 1, "mean": -2.5191e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3689e+01}, "critic_loss": {"n": 1, "mean": 1.7321e+00}, "entropy_coef": {"n": 1, "mean": 2.6444e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2890e-01, "std": 3.0313e-01, "min_value": 6.6519e-05, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0651e+01, "total_time": 1.4555e+02, "__timestamp": "2024-10-10 15:17:49.928007"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -3.3079e+02}, "actor_loss": {"n": 1, "mean": -2.2226e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.2674e+01}, "critic_loss": {"n": 1, "mean": 4.4678e+00}, "entropy_coef": {"n": 1, "mean": 2.0559e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4352e-01, "std": 2.9803e-01, "min_value": 3.7074e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0757e+01, "total_time": 1.5631e+02, "__timestamp": "2024-10-10 15:18:00.684784"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": -2.3979e+02}, "actor_loss": {"n": 1, "mean": -2.1325e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.0330e+01}, "critic_loss": {"n": 1, "mean": 2.9565e+00}, "entropy_coef": {"n": 1, "mean": 1.6018e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0973e-01, "std": 3.0173e-01, "min_value": 1.5676e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0693e+01, "total_time": 1.6700e+02, "__timestamp": "2024-10-10 15:18:11.377321"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": -1.8890e+02}, "actor_loss": {"n": 1, "mean": -1.9942e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.5221e+00}, "critic_loss": {"n": 1, "mean": 3.1957e+00}, "entropy_coef": {"n": 1, "mean": 1.2682e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8090e-01, "std": 2.9681e-01, "min_value": 9.7968e-05, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0720e+01, "total_time": 1.7772e+02, "__timestamp": "2024-10-10 15:18:22.096096"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": -2.3649e+02}, "actor_loss": {"n": 1, "mean": -1.8109e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.4549e+00}, "critic_loss": {"n": 1, "mean": 1.3830e+00}, "entropy_coef": {"n": 1, "mean": 1.0195e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3144e-01, "std": 2.8704e-01, "min_value": 8.9034e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0662e+01, "total_time": 1.8838e+02, "__timestamp": "2024-10-10 15:18:32.758867"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": 5.1274e+02}, "actor_loss": {"n": 1, "mean": -1.7875e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.9454e-01}, "critic_loss": {"n": 1, "mean": 1.3626e+00}, "entropy_coef": {"n": 1, "mean": 8.6717e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4016e-01, "std": 3.0691e-01, "min_value": 3.3782e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0786e+01, "total_time": 1.9917e+02, "__timestamp": "2024-10-10 15:18:43.543896"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": 3.0093e+02}, "actor_loss": {"n": 1, "mean": -1.7761e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.9956e-01}, "critic_loss": {"n": 1, "mean": 1.5709e+00}, "entropy_coef": {"n": 1, "mean": 8.1791e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7837e-01, "std": 3.0026e-01, "min_value": 2.8305e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0706e+01, "total_time": 2.0987e+02, "__timestamp": "2024-10-10 15:18:54.250291"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": 6.3922e+02}, "actor_loss": {"n": 1, "mean": -1.9260e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.3016e+00}, "critic_loss": {"n": 1, "mean": 1.5757e+00}, "entropy_coef": {"n": 1, "mean": 8.1818e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.9061e-01, "std": 2.9898e-01, "min_value": 4.6924e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0664e+01, "total_time": 2.2054e+02, "__timestamp": "2024-10-10 15:19:04.914413"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": 1.2186e+03}, "actor_loss": {"n": 1, "mean": -1.8560e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5622e-01}, "critic_loss": {"n": 1, "mean": 1.6617e+00}, "entropy_coef": {"n": 1, "mean": 9.2297e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0875e-01, "std": 3.0256e-01, "min_value": 2.7723e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1305e+01, "total_time": 2.3184e+02, "__timestamp": "2024-10-10 15:19:16.219234"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": 1.2727e+03}, "actor_loss": {"n": 1, "mean": -1.8569e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.0831e+00}, "critic_loss": {"n": 1, "mean": 2.2020e+00}, "entropy_coef": {"n": 1, "mean": 1.1167e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4448e-01, "std": 2.9260e-01, "min_value": 8.1003e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1493e+01, "total_time": 2.4334e+02, "__timestamp": "2024-10-10 15:19:27.712826"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 1.7475e+03}, "actor_loss": {"n": 1, "mean": -2.1096e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.9595e-01}, "critic_loss": {"n": 1, "mean": 2.0269e+00}, "entropy_coef": {"n": 1, "mean": 1.3600e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6112e-01, "std": 2.8486e-01, "min_value": 1.3893e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1399e+01, "total_time": 2.5474e+02, "__timestamp": "2024-10-10 15:19:39.111270"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 1.8275e+03}, "actor_loss": {"n": 1, "mean": -2.3801e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.1060e+00}, "critic_loss": {"n": 1, "mean": 2.4975e+00}, "entropy_coef": {"n": 1, "mean": 1.5969e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6248e-01, "std": 2.8927e-01, "min_value": 1.4880e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1556e+01, "total_time": 2.6629e+02, "__timestamp": "2024-10-10 15:19:50.666952"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 1.6604e+03}, "actor_loss": {"n": 1, "mean": -2.6021e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.8229e+00}, "critic_loss": {"n": 1, "mean": 2.7551e+00}, "entropy_coef": {"n": 1, "mean": 1.8622e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5991e-01, "std": 2.8459e-01, "min_value": 6.1363e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0753e+01, "total_time": 2.7704e+02, "__timestamp": "2024-10-10 15:20:01.421345"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 2.2023e+03}, "actor_loss": {"n": 1, "mean": -2.8142e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.7604e-01}, "critic_loss": {"n": 1, "mean": 2.9010e+00}, "entropy_coef": {"n": 1, "mean": 2.1298e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5873e-01, "std": 2.8766e-01, "min_value": 2.6435e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0650e+01, "total_time": 2.8770e+02, "__timestamp": "2024-10-10 15:20:12.070720"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 2.0671e+03}, "actor_loss": {"n": 1, "mean": -3.2158e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.2099e+00}, "critic_loss": {"n": 1, "mean": 2.9875e+00}, "entropy_coef": {"n": 1, "mean": 2.4077e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5866e-01, "std": 2.8242e-01, "min_value": 1.2304e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.1662e+01, "total_time": 2.9936e+02, "__timestamp": "2024-10-10 15:20:23.732857"}, {"step": 38000, "num_env_steps": 38000, "scores": {"n": 1, "mean": 2.4799e+03}, "actor_loss": {"n": 1, "mean": -3.2690e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.8806e+00}, "critic_loss": {"n": 1, "mean": 3.6133e+00}, "entropy_coef": {"n": 1, "mean": 2.6139e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6123e-01, "std": 2.8408e-01, "min_value": 9.0711e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.1306e+01, "total_time": 3.1066e+02, "__timestamp": "2024-10-10 15:20:35.038578"}, {"step": 39000, "num_env_steps": 39000, "scores": {"n": 1, "mean": 2.5010e+03}, "actor_loss": {"n": 1, "mean": -3.6057e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.6010e-01}, "critic_loss": {"n": 1, "mean": 4.3876e+00}, "entropy_coef": {"n": 1, "mean": 2.9210e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6699e-01, "std": 2.7815e-01, "min_value": 5.0761e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1789e+01, "total_time": 3.2245e+02, "__timestamp": "2024-10-10 15:20:46.828788"}, {"step": 40000, "num_env_steps": 40000, "scores": {"n": 1, "mean": 2.6606e+03}, "actor_loss": {"n": 1, "mean": -3.9816e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.1468e+00}, "critic_loss": {"n": 1, "mean": 4.2032e+00}, "entropy_coef": {"n": 1, "mean": 3.2078e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6910e-01, "std": 2.7449e-01, "min_value": 1.4871e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.1537e+01, "total_time": 3.3399e+02, "__timestamp": "2024-10-10 15:20:58.364428"}, {"step": 41000, "num_env_steps": 41000, "scores": {"n": 1, "mean": 2.6096e+03}, "actor_loss": {"n": 1, "mean": -4.8437e+01}, "entropy_coef_loss": {"n": 1, "mean": 7.7516e-02}, "critic_loss": {"n": 1, "mean": 4.5965e+00}, "entropy_coef": {"n": 1, "mean": 3.5883e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7444e-01, "std": 2.7128e-01, "min_value": 5.1588e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.1505e+01, "total_time": 3.4549e+02, "__timestamp": "2024-10-10 15:21:09.869412"}, {"step": 42000, "num_env_steps": 42000, "scores": {"n": 1, "mean": 2.4139e+03}, "actor_loss": {"n": 1, "mean": -4.9294e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.9569e-01}, "critic_loss": {"n": 1, "mean": 6.1981e+00}, "entropy_coef": {"n": 1, "mean": 3.9145e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7578e-01, "std": 2.6796e-01, "min_value": 1.2583e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.1665e+01, "total_time": 3.5716e+02, "__timestamp": "2024-10-10 15:21:21.535339"}, {"step": 43000, "num_env_steps": 43000, "scores": {"n": 1, "mean": 2.3281e+03}, "actor_loss": {"n": 1, "mean": -5.4265e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5357e+00}, "critic_loss": {"n": 1, "mean": 5.1045e+00}, "entropy_coef": {"n": 1, "mean": 4.0145e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7305e-01, "std": 2.6914e-01, "min_value": 1.1396e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1538e+01, "total_time": 3.6870e+02, "__timestamp": "2024-10-10 15:21:33.073163"}, {"step": 44000, "num_env_steps": 44000, "scores": {"n": 1, "mean": 2.5446e+03}, "actor_loss": {"n": 1, "mean": -5.4109e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.6582e-01}, "critic_loss": {"n": 1, "mean": 4.6602e+00}, "entropy_coef": {"n": 1, "mean": 3.9473e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6599e-01, "std": 2.7092e-01, "min_value": 3.0518e-05, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.1717e+01, "total_time": 3.8041e+02, "__timestamp": "2024-10-10 15:21:44.789827"}, {"step": 45000, "num_env_steps": 45000, "scores": {"n": 1, "mean": 2.5083e+03}, "actor_loss": {"n": 1, "mean": -6.1914e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.3376e-02}, "critic_loss": {"n": 1, "mean": 4.5778e+00}, "entropy_coef": {"n": 1, "mean": 4.1271e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6109e-01, "std": 2.7338e-01, "min_value": 4.6691e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.1725e+01, "total_time": 3.9214e+02, "__timestamp": "2024-10-10 15:21:56.514464"}, {"step": 46000, "num_env_steps": 46000, "scores": {"n": 1, "mean": 2.7579e+03}, "actor_loss": {"n": 1, "mean": -6.7056e+01}, "entropy_coef_loss": {"n": 1, "mean": 7.9269e-01}, "critic_loss": {"n": 1, "mean": 5.9735e+00}, "entropy_coef": {"n": 1, "mean": 4.2894e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5687e-01, "std": 2.7323e-01, "min_value": 1.0348e-03, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 1.1624e+01, "total_time": 4.0376e+02, "__timestamp": "2024-10-10 15:22:08.137175"}, {"step": 47000, "num_env_steps": 47000, "scores": {"n": 1, "mean": 2.6345e+03}, "actor_loss": {"n": 1, "mean": -6.9908e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.7678e+00}, "critic_loss": {"n": 1, "mean": 7.3119e+00}, "entropy_coef": {"n": 1, "mean": 4.3444e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5259e-01, "std": 2.7400e-01, "min_value": 1.1680e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.1737e+01, "total_time": 4.1550e+02, "__timestamp": "2024-10-10 15:22:19.875051"}, {"step": 48000, "num_env_steps": 48000, "scores": {"n": 1, "mean": 2.7915e+03}, "actor_loss": {"n": 1, "mean": -7.2015e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.5770e-01}, "critic_loss": {"n": 1, "mean": 6.1860e+00}, "entropy_coef": {"n": 1, "mean": 4.4983e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6163e-01, "std": 2.7244e-01, "min_value": 1.4445e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1674e+01, "total_time": 4.2717e+02, "__timestamp": "2024-10-10 15:22:31.547809"}, {"step": 49000, "num_env_steps": 49000, "scores": {"n": 1, "mean": 2.5815e+03}, "actor_loss": {"n": 1, "mean": -7.0315e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.0573e-01}, "critic_loss": {"n": 1, "mean": 7.1803e+00}, "entropy_coef": {"n": 1, "mean": 4.7218e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6658e-01, "std": 2.6852e-01, "min_value": 1.4740e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.1376e+01, "total_time": 4.3855e+02, "__timestamp": "2024-10-10 15:22:42.924692"}, {"step": 50000, "num_env_steps": 50000, "scores": {"n": 1, "mean": 2.4600e+03}, "actor_loss": {"n": 1, "mean": -7.7747e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5407e-01}, "critic_loss": {"n": 1, "mean": 5.3024e+01}, "entropy_coef": {"n": 1, "mean": 4.7508e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5112e-01, "std": 2.7703e-01, "min_value": 1.4876e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1565e+01, "total_time": 4.5011e+02, "__timestamp": "2024-10-10 15:22:54.489919"}, {"step": 51000, "num_env_steps": 51000, "scores": {"n": 1, "mean": 2.3395e+03}, "actor_loss": {"n": 1, "mean": -8.4100e+01}, "entropy_coef_loss": {"n": 1, "mean": 8.3081e-01}, "critic_loss": {"n": 1, "mean": 8.0827e+00}, "entropy_coef": {"n": 1, "mean": 4.8657e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5218e-01, "std": 2.7899e-01, "min_value": 9.8032e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.2658e+01, "total_time": 4.6277e+02, "__timestamp": "2024-10-10 15:23:07.146584"}, {"step": 52000, "num_env_steps": 52000, "scores": {"n": 1, "mean": 2.4071e+03}, "actor_loss": {"n": 1, "mean": -8.0626e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.9428e-01}, "critic_loss": {"n": 1, "mean": 1.0120e+01}, "entropy_coef": {"n": 1, "mean": 4.8563e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5363e-01, "std": 2.7239e-01, "min_value": 2.7058e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1521e+01, "total_time": 4.7429e+02, "__timestamp": "2024-10-10 15:23:18.668031"}, {"step": 53000, "num_env_steps": 53000, "scores": {"n": 1, "mean": 2.6707e+03}, "actor_loss": {"n": 1, "mean": -7.4675e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3830e+00}, "critic_loss": {"n": 1, "mean": 6.6905e+00}, "entropy_coef": {"n": 1, "mean": 5.0159e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4815e-01, "std": 2.7839e-01, "min_value": 5.4991e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.1432e+01, "total_time": 4.8572e+02, "__timestamp": "2024-10-10 15:23:30.101360"}, {"step": 54000, "num_env_steps": 54000, "scores": {"n": 1, "mean": 2.4630e+03}, "actor_loss": {"n": 1, "mean": -9.2578e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.5397e+00}, "critic_loss": {"n": 1, "mean": 7.2767e+00}, "entropy_coef": {"n": 1, "mean": 4.9824e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5710e-01, "std": 2.7509e-01, "min_value": 3.5763e-06, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.1344e+01, "total_time": 4.9707e+02, "__timestamp": "2024-10-10 15:23:41.444575"}, {"step": 55000, "num_env_steps": 55000, "scores": {"n": 1, "mean": 2.6813e+03}, "actor_loss": {"n": 1, "mean": -8.9633e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.0867e+00}, "critic_loss": {"n": 1, "mean": 6.7631e+00}, "entropy_coef": {"n": 1, "mean": 4.8903e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6063e-01, "std": 2.7091e-01, "min_value": 2.5016e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1165e+01, "total_time": 5.0823e+02, "__timestamp": "2024-10-10 15:23:52.610222"}, {"step": 56000, "num_env_steps": 56000, "scores": {"n": 1, "mean": 2.7312e+03}, "actor_loss": {"n": 1, "mean": -9.4782e+01}, "entropy_coef_loss": {"n": 1, "mean": 8.0210e-01}, "critic_loss": {"n": 1, "mean": 8.8491e+00}, "entropy_coef": {"n": 1, "mean": 5.0110e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5591e-01, "std": 2.7721e-01, "min_value": 1.0659e-03, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.1469e+01, "total_time": 5.1970e+02, "__timestamp": "2024-10-10 15:24:04.079081"}, {"step": 57000, "num_env_steps": 57000, "scores": {"n": 1, "mean": 2.8095e+03}, "actor_loss": {"n": 1, "mean": -9.5373e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7358e-01}, "critic_loss": {"n": 1, "mean": 6.9341e+00}, "entropy_coef": {"n": 1, "mean": 5.2283e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6360e-01, "std": 2.7468e-01, "min_value": 1.5373e-03, "max_value": 9.9988e-01}, "num_gradient_steps": 0, "step_time": 1.1504e+01, "total_time": 5.3121e+02, "__timestamp": "2024-10-10 15:24:15.582333"}, {"step": 58000, "num_env_steps": 58000, "scores": {"n": 1, "mean": 2.8425e+03}, "actor_loss": {"n": 1, "mean": -1.0104e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.6921e-01}, "critic_loss": {"n": 1, "mean": 9.9561e+00}, "entropy_coef": {"n": 1, "mean": 5.2400e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6366e-01, "std": 2.7304e-01, "min_value": 1.2355e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.1378e+01, "total_time": 5.4258e+02, "__timestamp": "2024-10-10 15:24:26.960966"}, {"step": 59000, "num_env_steps": 59000, "scores": {"n": 1, "mean": 2.8582e+03}, "actor_loss": {"n": 1, "mean": -1.0214e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2172e+00}, "critic_loss": {"n": 1, "mean": 7.3686e+00}, "entropy_coef": {"n": 1, "mean": 5.4523e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6629e-01, "std": 2.7369e-01, "min_value": 4.4879e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.1434e+01, "total_time": 5.5402e+02, "__timestamp": "2024-10-10 15:24:38.394635"}, {"step": 60000, "num_env_steps": 60000, "scores": {"n": 1, "mean": 2.9321e+03}, "actor_loss": {"n": 1, "mean": -1.0074e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2522e+00}, "critic_loss": {"n": 1, "mean": 1.0025e+01}, "entropy_coef": {"n": 1, "mean": 5.4784e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6228e-01, "std": 2.7573e-01, "min_value": 6.7141e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.1136e+01, "total_time": 5.6515e+02, "__timestamp": "2024-10-10 15:24:49.530062"}, {"step": 61000, "num_env_steps": 61000, "scores": {"n": 1, "mean": 2.8973e+03}, "actor_loss": {"n": 1, "mean": -1.0475e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.6511e-01}, "critic_loss": {"n": 1, "mean": 1.0285e+02}, "entropy_coef": {"n": 1, "mean": 5.6316e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6214e-01, "std": 2.7420e-01, "min_value": 7.3314e-06, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.3998e+01, "total_time": 5.7915e+02, "__timestamp": "2024-10-10 15:25:03.528886"}, {"step": 62000, "num_env_steps": 62000, "scores": {"n": 1, "mean": 3.0030e+03}, "actor_loss": {"n": 1, "mean": -1.0408e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.1564e+00}, "critic_loss": {"n": 1, "mean": 1.2181e+01}, "entropy_coef": {"n": 1, "mean": 5.6040e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6583e-01, "std": 2.7316e-01, "min_value": 1.0683e-03, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1479e+01, "total_time": 5.9063e+02, "__timestamp": "2024-10-10 15:25:15.007039"}, {"step": 63000, "num_env_steps": 63000, "scores": {"n": 1, "mean": 2.7225e+03}, "actor_loss": {"n": 1, "mean": -1.1499e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2992e+00}, "critic_loss": {"n": 1, "mean": 1.1840e+02}, "entropy_coef": {"n": 1, "mean": 5.6251e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6101e-01, "std": 2.7469e-01, "min_value": 3.7652e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.1167e+01, "total_time": 6.0180e+02, "__timestamp": "2024-10-10 15:25:26.174927"}, {"step": 64000, "num_env_steps": 64000, "scores": {"n": 1, "mean": 2.9926e+03}, "actor_loss": {"n": 1, "mean": -1.1250e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0581e+00}, "critic_loss": {"n": 1, "mean": 9.8097e+00}, "entropy_coef": {"n": 1, "mean": 5.7484e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6612e-01, "std": 2.7170e-01, "min_value": 5.6821e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.1082e+01, "total_time": 6.1288e+02, "__timestamp": "2024-10-10 15:25:37.256007"}, {"step": 65000, "num_env_steps": 65000, "scores": {"n": 1, "mean": 3.1081e+03}, "actor_loss": {"n": 1, "mean": -1.1371e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.9935e-01}, "critic_loss": {"n": 1, "mean": 2.7673e+01}, "entropy_coef": {"n": 1, "mean": 5.7631e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5973e-01, "std": 2.7936e-01, "min_value": 4.8429e-06, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0872e+01, "total_time": 6.2375e+02, "__timestamp": "2024-10-10 15:25:48.128687"}, {"step": 66000, "num_env_steps": 66000, "scores": {"n": 1, "mean": 3.1457e+03}, "actor_loss": {"n": 1, "mean": -1.1870e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.9729e-01}, "critic_loss": {"n": 1, "mean": 7.9877e+00}, "entropy_coef": {"n": 1, "mean": 5.9704e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5719e-01, "std": 2.7763e-01, "min_value": 2.2647e-04, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.0793e+01, "total_time": 6.3455e+02, "__timestamp": "2024-10-10 15:25:58.920429"}, {"step": 67000, "num_env_steps": 67000, "scores": {"n": 1, "mean": 3.0904e+03}, "actor_loss": {"n": 1, "mean": -1.2018e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2473e-01}, "critic_loss": {"n": 1, "mean": 7.8455e+00}, "entropy_coef": {"n": 1, "mean": 5.9981e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5965e-01, "std": 2.7513e-01, "min_value": 9.5733e-05, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.0801e+01, "total_time": 6.4535e+02, "__timestamp": "2024-10-10 15:26:09.721094"}, {"step": 68000, "num_env_steps": 68000, "scores": {"n": 1, "mean": 3.2098e+03}, "actor_loss": {"n": 1, "mean": -1.2220e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.4174e-01}, "critic_loss": {"n": 1, "mean": 8.1092e+00}, "entropy_coef": {"n": 1, "mean": 6.1656e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6149e-01, "std": 2.7258e-01, "min_value": 5.1558e-05, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0964e+01, "total_time": 6.5631e+02, "__timestamp": "2024-10-10 15:26:20.686348"}, {"step": 69000, "num_env_steps": 69000, "scores": {"n": 1, "mean": 3.1435e+03}, "actor_loss": {"n": 1, "mean": -1.1820e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.4338e-01}, "critic_loss": {"n": 1, "mean": 6.4034e+00}, "entropy_coef": {"n": 1, "mean": 6.0922e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5414e-01, "std": 2.7820e-01, "min_value": 1.2454e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.0789e+01, "total_time": 6.6710e+02, "__timestamp": "2024-10-10 15:26:31.475191"}, {"step": 70000, "num_env_steps": 70000, "scores": {"n": 1, "mean": 3.3621e+03}, "actor_loss": {"n": 1, "mean": -1.3082e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2745e-02}, "critic_loss": {"n": 1, "mean": 7.5421e+00}, "entropy_coef": {"n": 1, "mean": 6.2519e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5722e-01, "std": 2.7940e-01, "min_value": 3.4425e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.0755e+01, "total_time": 6.7785e+02, "__timestamp": "2024-10-10 15:26:42.229522"}, {"step": 71000, "num_env_steps": 71000, "scores": {"n": 1, "mean": 3.1506e+03}, "actor_loss": {"n": 1, "mean": -1.2486e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.3213e-01}, "critic_loss": {"n": 1, "mean": 1.2534e+02}, "entropy_coef": {"n": 1, "mean": 6.0706e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4780e-01, "std": 2.8179e-01, "min_value": 9.4399e-06, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.0718e+01, "total_time": 6.8857e+02, "__timestamp": "2024-10-10 15:26:52.947231"}, {"step": 72000, "num_env_steps": 72000, "scores": {"n": 1, "mean": 3.1525e+03}, "actor_loss": {"n": 1, "mean": -1.2567e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.5517e-01}, "critic_loss": {"n": 1, "mean": 8.5417e+00}, "entropy_coef": {"n": 1, "mean": 6.2094e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5643e-01, "std": 2.7527e-01, "min_value": 1.0094e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0658e+01, "total_time": 6.9923e+02, "__timestamp": "2024-10-10 15:27:03.604961"}, {"step": 73000, "num_env_steps": 73000, "scores": {"n": 1, "mean": 3.1244e+03}, "actor_loss": {"n": 1, "mean": -1.3042e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.6344e-01}, "critic_loss": {"n": 1, "mean": 7.3742e+00}, "entropy_coef": {"n": 1, "mean": 6.4852e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5773e-01, "std": 2.7724e-01, "min_value": 2.1937e-04, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.0662e+01, "total_time": 7.0989e+02, "__timestamp": "2024-10-10 15:27:14.267435"}, {"step": 74000, "num_env_steps": 74000, "scores": {"n": 1, "mean": 3.4656e+03}, "actor_loss": {"n": 1, "mean": -1.3183e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.7270e-01}, "critic_loss": {"n": 1, "mean": 1.5624e+02}, "entropy_coef": {"n": 1, "mean": 6.5570e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6984e-01, "std": 2.6947e-01, "min_value": 5.7042e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0650e+01, "total_time": 7.2054e+02, "__timestamp": "2024-10-10 15:27:24.917451"}, {"step": 75000, "num_env_steps": 75000, "scores": {"n": 1, "mean": 3.3381e+03}, "actor_loss": {"n": 1, "mean": -1.3416e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.4667e-01}, "critic_loss": {"n": 1, "mean": 8.7177e+00}, "entropy_coef": {"n": 1, "mean": 6.6992e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5814e-01, "std": 2.7746e-01, "min_value": 1.1700e-04, "max_value": 9.9976e-01}, "num_gradient_steps": 0, "step_time": 1.0691e+01, "total_time": 7.3123e+02, "__timestamp": "2024-10-10 15:27:35.609461"}, {"step": 76000, "num_env_steps": 76000, "scores": {"n": 1, "mean": 3.3106e+03}, "actor_loss": {"n": 1, "mean": -1.4342e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.6673e+00}, "critic_loss": {"n": 1, "mean": 8.4288e+00}, "entropy_coef": {"n": 1, "mean": 6.7046e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6327e-01, "std": 2.7357e-01, "min_value": 1.6869e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.0611e+01, "total_time": 7.4184e+02, "__timestamp": "2024-10-10 15:27:46.220766"}, {"step": 77000, "num_env_steps": 77000, "scores": {"n": 1, "mean": 3.5124e+03}, "actor_loss": {"n": 1, "mean": -1.3710e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.7330e-01}, "critic_loss": {"n": 1, "mean": 9.4051e+00}, "entropy_coef": {"n": 1, "mean": 6.7250e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5577e-01, "std": 2.7840e-01, "min_value": 4.8363e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.0700e+01, "total_time": 7.5254e+02, "__timestamp": "2024-10-10 15:27:56.920889"}, {"step": 78000, "num_env_steps": 78000, "scores": {"n": 1, "mean": 3.7234e+03}, "actor_loss": {"n": 1, "mean": -1.3729e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.7615e+00}, "critic_loss": {"n": 1, "mean": 1.5100e+02}, "entropy_coef": {"n": 1, "mean": 6.7063e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6155e-01, "std": 2.7507e-01, "min_value": 1.7884e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.0629e+01, "total_time": 7.6317e+02, "__timestamp": "2024-10-10 15:28:07.550123"}, {"step": 79000, "num_env_steps": 79000, "scores": {"n": 1, "mean": 3.3773e+03}, "actor_loss": {"n": 1, "mean": -1.4604e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3930e-01}, "critic_loss": {"n": 1, "mean": 8.3200e+00}, "entropy_coef": {"n": 1, "mean": 6.8033e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4330e-01, "std": 2.8410e-01, "min_value": 2.5566e-04, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 1.0720e+01, "total_time": 7.7389e+02, "__timestamp": "2024-10-10 15:28:18.268628"}, {"step": 80000, "num_env_steps": 80000, "scores": {"n": 1, "mean": 3.2468e+03}, "actor_loss": {"n": 1, "mean": -1.3720e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.6138e-01}, "critic_loss": {"n": 1, "mean": 9.5568e+00}, "entropy_coef": {"n": 1, "mean": 6.6262e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5429e-01, "std": 2.7521e-01, "min_value": 1.8534e-03, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0698e+01, "total_time": 7.8459e+02, "__timestamp": "2024-10-10 15:28:28.967620"}, {"step": 81000, "num_env_steps": 81000, "scores": {"n": 1, "mean": 3.6329e+03}, "actor_loss": {"n": 1, "mean": -1.4459e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.6439e-01}, "critic_loss": {"n": 1, "mean": 9.7156e+00}, "entropy_coef": {"n": 1, "mean": 7.0117e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5846e-01, "std": 2.7724e-01, "min_value": 5.3924e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0670e+01, "total_time": 7.9526e+02, "__timestamp": "2024-10-10 15:28:39.637689"}, {"step": 82000, "num_env_steps": 82000, "scores": {"n": 1, "mean": 3.4933e+03}, "actor_loss": {"n": 1, "mean": -1.4970e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.8600e-01}, "critic_loss": {"n": 1, "mean": 1.0158e+01}, "entropy_coef": {"n": 1, "mean": 7.1176e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6459e-01, "std": 2.7298e-01, "min_value": 3.5517e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0744e+01, "total_time": 8.0600e+02, "__timestamp": "2024-10-10 15:28:50.380378"}, {"step": 83000, "num_env_steps": 83000, "scores": {"n": 1, "mean": 3.6959e+03}, "actor_loss": {"n": 1, "mean": -1.4821e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.8563e-01}, "critic_loss": {"n": 1, "mean": 9.9020e+00}, "entropy_coef": {"n": 1, "mean": 7.0445e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5771e-01, "std": 2.7736e-01, "min_value": 1.1735e-03, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0630e+01, "total_time": 8.1664e+02, "__timestamp": "2024-10-10 15:29:01.010691"}, {"step": 84000, "num_env_steps": 84000, "scores": {"n": 1, "mean": 3.5618e+03}, "actor_loss": {"n": 1, "mean": -1.4050e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.5760e-02}, "critic_loss": {"n": 1, "mean": 1.2772e+01}, "entropy_coef": {"n": 1, "mean": 7.0681e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5671e-01, "std": 2.7612e-01, "min_value": 8.9496e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.0607e+01, "total_time": 8.2724e+02, "__timestamp": "2024-10-10 15:29:11.619008"}, {"step": 85000, "num_env_steps": 85000, "scores": {"n": 1, "mean": 2.9166e+03}, "actor_loss": {"n": 1, "mean": -1.5934e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.9933e-01}, "critic_loss": {"n": 1, "mean": 1.2121e+01}, "entropy_coef": {"n": 1, "mean": 7.0957e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3951e-01, "std": 2.8188e-01, "min_value": 2.0650e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0726e+01, "total_time": 8.3797e+02, "__timestamp": "2024-10-10 15:29:22.345231"}, {"step": 86000, "num_env_steps": 86000, "scores": {"n": 1, "mean": 3.6811e+03}, "actor_loss": {"n": 1, "mean": -1.5157e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.8840e-01}, "critic_loss": {"n": 1, "mean": 9.8299e+00}, "entropy_coef": {"n": 1, "mean": 7.1823e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5481e-01, "std": 2.7731e-01, "min_value": 5.4157e-04, "max_value": 9.9952e-01}, "num_gradient_steps": 0, "step_time": 1.0733e+01, "total_time": 8.4870e+02, "__timestamp": "2024-10-10 15:29:33.078038"}, {"step": 87000, "num_env_steps": 87000, "scores": {"n": 1, "mean": 3.6048e+03}, "actor_loss": {"n": 1, "mean": -1.5440e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.4614e-01}, "critic_loss": {"n": 1, "mean": 1.0313e+01}, "entropy_coef": {"n": 1, "mean": 7.0865e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5672e-01, "std": 2.7620e-01, "min_value": 1.8310e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0767e+01, "total_time": 8.5947e+02, "__timestamp": "2024-10-10 15:29:43.843826"}, {"step": 88000, "num_env_steps": 88000, "scores": {"n": 1, "mean": 3.7035e+03}, "actor_loss": {"n": 1, "mean": -1.6032e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.6665e-01}, "critic_loss": {"n": 1, "mean": 1.0536e+01}, "entropy_coef": {"n": 1, "mean": 7.1599e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5883e-01, "std": 2.7916e-01, "min_value": 1.2303e-03, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0637e+01, "total_time": 8.7011e+02, "__timestamp": "2024-10-10 15:29:54.481771"}, {"step": 89000, "num_env_steps": 89000, "scores": {"n": 1, "mean": 3.5082e+03}, "actor_loss": {"n": 1, "mean": -1.5296e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.6192e-01}, "critic_loss": {"n": 1, "mean": 9.1579e+00}, "entropy_coef": {"n": 1, "mean": 7.3576e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5032e-01, "std": 2.8246e-01, "min_value": 2.7514e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.0664e+01, "total_time": 8.8077e+02, "__timestamp": "2024-10-10 15:30:05.145465"}, {"step": 90000, "num_env_steps": 90000, "scores": {"n": 1, "mean": 3.8978e+03}, "actor_loss": {"n": 1, "mean": -1.6618e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.5378e-01}, "critic_loss": {"n": 1, "mean": 1.4720e+02}, "entropy_coef": {"n": 1, "mean": 7.3100e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5613e-01, "std": 2.7606e-01, "min_value": 2.9685e-04, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 1.0668e+01, "total_time": 8.9144e+02, "__timestamp": "2024-10-10 15:30:15.813419"}, {"step": 91000, "num_env_steps": 91000, "scores": {"n": 1, "mean": 3.6491e+03}, "actor_loss": {"n": 1, "mean": -1.6647e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.6106e-01}, "critic_loss": {"n": 1, "mean": 1.1939e+01}, "entropy_coef": {"n": 1, "mean": 7.5203e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5462e-01, "std": 2.7722e-01, "min_value": 1.0282e-05, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0809e+01, "total_time": 9.0225e+02, "__timestamp": "2024-10-10 15:30:26.621516"}, {"step": 92000, "num_env_steps": 92000, "scores": {"n": 1, "mean": 3.9428e+03}, "actor_loss": {"n": 1, "mean": -1.6592e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1106e+00}, "critic_loss": {"n": 1, "mean": 1.7524e+01}, "entropy_coef": {"n": 1, "mean": 7.5531e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5803e-01, "std": 2.7878e-01, "min_value": 1.5470e-03, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0635e+01, "total_time": 9.1288e+02, "__timestamp": "2024-10-10 15:30:37.256697"}, {"step": 93000, "num_env_steps": 93000, "scores": {"n": 1, "mean": 3.7811e+03}, "actor_loss": {"n": 1, "mean": -1.6114e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.3109e-01}, "critic_loss": {"n": 1, "mean": 9.6430e+00}, "entropy_coef": {"n": 1, "mean": 7.7952e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5729e-01, "std": 2.7947e-01, "min_value": 6.6234e-04, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 1.0672e+01, "total_time": 9.2355e+02, "__timestamp": "2024-10-10 15:30:47.928760"}, {"step": 94000, "num_env_steps": 94000, "scores": {"n": 1, "mean": 3.8162e+03}, "actor_loss": {"n": 1, "mean": -1.6775e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0531e+00}, "critic_loss": {"n": 1, "mean": 1.8684e+02}, "entropy_coef": {"n": 1, "mean": 7.8589e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5960e-01, "std": 2.7269e-01, "min_value": 7.4801e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.0738e+01, "total_time": 9.3429e+02, "__timestamp": "2024-10-10 15:30:58.668131"}, {"step": 95000, "num_env_steps": 95000, "scores": {"n": 1, "mean": 3.6923e+03}, "actor_loss": {"n": 1, "mean": -1.7432e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4466e+00}, "critic_loss": {"n": 1, "mean": 1.5298e+01}, "entropy_coef": {"n": 1, "mean": 8.0227e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5643e-01, "std": 2.7631e-01, "min_value": 2.1709e-04, "max_value": 9.9967e-01}, "num_gradient_steps": 0, "step_time": 1.0625e+01, "total_time": 9.4492e+02, "__timestamp": "2024-10-10 15:31:09.291861"}, {"step": 96000, "num_env_steps": 96000, "scores": {"n": 1, "mean": 3.8054e+03}, "actor_loss": {"n": 1, "mean": -1.7404e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.1025e-01}, "critic_loss": {"n": 1, "mean": 9.4197e+00}, "entropy_coef": {"n": 1, "mean": 7.9886e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5279e-01, "std": 2.7892e-01, "min_value": 7.1953e-05, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 1.0600e+01, "total_time": 9.5552e+02, "__timestamp": "2024-10-10 15:31:19.892712"}, {"step": 97000, "num_env_steps": 97000, "scores": {"n": 1, "mean": 3.9792e+03}, "actor_loss": {"n": 1, "mean": -1.6871e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.2083e-01}, "critic_loss": {"n": 1, "mean": 1.0421e+01}, "entropy_coef": {"n": 1, "mean": 8.1658e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6033e-01, "std": 2.7811e-01, "min_value": 2.3626e-03, "max_value": 9.9972e-01}, "num_gradient_steps": 0, "step_time": 1.0679e+01, "total_time": 9.6619e+02, "__timestamp": "2024-10-10 15:31:30.570410"}, {"step": 98000, "num_env_steps": 98000, "scores": {"n": 1, "mean": 3.8782e+03}, "actor_loss": {"n": 1, "mean": -1.7348e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1176e-01}, "critic_loss": {"n": 1, "mean": 2.1810e+02}, "entropy_coef": {"n": 1, "mean": 8.2904e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5558e-01, "std": 2.7547e-01, "min_value": 4.2167e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.0602e+01, "total_time": 9.7680e+02, "__timestamp": "2024-10-10 15:31:41.172616"}, {"step": 99000, "num_env_steps": 99000, "scores": {"n": 1, "mean": 3.8493e+03}, "actor_loss": {"n": 1, "mean": -1.8027e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.6377e-01}, "critic_loss": {"n": 1, "mean": 1.4738e+01}, "entropy_coef": {"n": 1, "mean": 8.5117e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5747e-01, "std": 2.7527e-01, "min_value": 1.2554e-04, "max_value": 9.9976e-01}, "num_gradient_steps": 0, "step_time": 1.0646e+01, "total_time": 9.8744e+02, "__timestamp": "2024-10-10 15:31:51.818712"}, {"step": 100000, "num_env_steps": 100000, "scores": {"n": 1, "mean": 3.7889e+03}, "actor_loss": {"n": 1, "mean": -1.7846e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.4595e-01}, "critic_loss": {"n": 1, "mean": 1.1186e+01}, "entropy_coef": {"n": 1, "mean": 8.4543e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6244e-01, "std": 2.7435e-01, "min_value": 6.2668e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0701e+01, "total_time": 9.9814e+02, "__timestamp": "2024-10-10 15:32:02.520596"}, {"step": 101000, "num_env_steps": 101000, "scores": {"n": 1, "mean": 2.3111e+03}, "actor_loss": {"n": 1, "mean": -1.8124e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2631e-01}, "critic_loss": {"n": 1, "mean": 1.2691e+01}, "entropy_coef": {"n": 1, "mean": 8.4488e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2158e-01, "std": 2.9009e-01, "min_value": 2.6745e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0724e+01, "total_time": 1.0089e+03, "__timestamp": "2024-10-10 15:32:13.244825"}, {"step": 102000, "num_env_steps": 102000, "scores": {"n": 1, "mean": 3.8038e+03}, "actor_loss": {"n": 1, "mean": -1.7396e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.8806e-01}, "critic_loss": {"n": 1, "mean": 1.0838e+01}, "entropy_coef": {"n": 1, "mean": 8.4028e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5975e-01, "std": 2.7139e-01, "min_value": 1.6931e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.0684e+01, "total_time": 1.0196e+03, "__timestamp": "2024-10-10 15:32:23.928451"}, {"step": 103000, "num_env_steps": 103000, "scores": {"n": 1, "mean": 3.7989e+03}, "actor_loss": {"n": 1, "mean": -1.7181e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.1633e-01}, "critic_loss": {"n": 1, "mean": 1.2110e+01}, "entropy_coef": {"n": 1, "mean": 8.5507e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5959e-01, "std": 2.7453e-01, "min_value": 2.9707e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.0675e+01, "total_time": 1.0302e+03, "__timestamp": "2024-10-10 15:32:34.602106"}, {"step": 104000, "num_env_steps": 104000, "scores": {"n": 1, "mean": 4.0415e+03}, "actor_loss": {"n": 1, "mean": -1.8416e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.9062e-01}, "critic_loss": {"n": 1, "mean": 1.8067e+02}, "entropy_coef": {"n": 1, "mean": 8.6872e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6336e-01, "std": 2.7267e-01, "min_value": 1.2369e-03, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.0726e+01, "total_time": 1.0410e+03, "__timestamp": "2024-10-10 15:32:45.328488"}, {"step": 105000, "num_env_steps": 105000, "scores": {"n": 1, "mean": 3.8441e+03}, "actor_loss": {"n": 1, "mean": -1.8356e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.3096e-01}, "critic_loss": {"n": 1, "mean": 1.2098e+01}, "entropy_coef": {"n": 1, "mean": 8.7734e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6577e-01, "std": 2.7631e-01, "min_value": 6.4513e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0675e+01, "total_time": 1.0516e+03, "__timestamp": "2024-10-10 15:32:56.003874"}, {"step": 106000, "num_env_steps": 106000, "scores": {"n": 1, "mean": 3.8276e+03}, "actor_loss": {"n": 1, "mean": -1.8259e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.5201e-01}, "critic_loss": {"n": 1, "mean": 1.4997e+01}, "entropy_coef": {"n": 1, "mean": 8.7263e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5749e-01, "std": 2.7723e-01, "min_value": 8.2457e-04, "max_value": 9.9959e-01}, "num_gradient_steps": 0, "step_time": 1.0751e+01, "total_time": 1.0624e+03, "__timestamp": "2024-10-10 15:33:06.755527"}, {"step": 107000, "num_env_steps": 107000, "scores": {"n": 1, "mean": 4.0681e+03}, "actor_loss": {"n": 1, "mean": -1.8223e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2213e-01}, "critic_loss": {"n": 1, "mean": 1.4417e+01}, "entropy_coef": {"n": 1, "mean": 8.8386e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5885e-01, "std": 2.7492e-01, "min_value": 2.5052e-04, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.0756e+01, "total_time": 1.0731e+03, "__timestamp": "2024-10-10 15:33:17.511362"}, {"step": 108000, "num_env_steps": 108000, "scores": {"n": 1, "mean": 3.8472e+03}, "actor_loss": {"n": 1, "mean": -1.7827e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.7227e-01}, "critic_loss": {"n": 1, "mean": 1.1707e+01}, "entropy_coef": {"n": 1, "mean": 8.8064e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5492e-01, "std": 2.7609e-01, "min_value": 6.2905e-05, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0703e+01, "total_time": 1.0838e+03, "__timestamp": "2024-10-10 15:33:28.214559"}, {"step": 109000, "num_env_steps": 109000, "scores": {"n": 1, "mean": 3.9442e+03}, "actor_loss": {"n": 1, "mean": -1.8572e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.6555e-01}, "critic_loss": {"n": 1, "mean": 1.0626e+01}, "entropy_coef": {"n": 1, "mean": 8.9912e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5225e-01, "std": 2.8007e-01, "min_value": 6.7592e-05, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.0744e+01, "total_time": 1.0946e+03, "__timestamp": "2024-10-10 15:33:38.958538"}, {"step": 110000, "num_env_steps": 110000, "scores": {"n": 1, "mean": 3.7825e+03}, "actor_loss": {"n": 1, "mean": -1.9135e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.0053e-01}, "critic_loss": {"n": 1, "mean": 1.1147e+01}, "entropy_coef": {"n": 1, "mean": 9.2219e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5655e-01, "std": 2.7775e-01, "min_value": 1.6928e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0728e+01, "total_time": 1.1053e+03, "__timestamp": "2024-10-10 15:33:49.686639"}, {"step": 111000, "num_env_steps": 111000, "scores": {"n": 1, "mean": 3.7824e+03}, "actor_loss": {"n": 1, "mean": -1.8832e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.0381e-01}, "critic_loss": {"n": 1, "mean": 1.0249e+01}, "entropy_coef": {"n": 1, "mean": 9.0375e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5489e-01, "std": 2.7751e-01, "min_value": 6.4212e-04, "max_value": 9.9929e-01}, "num_gradient_steps": 0, "step_time": 1.0694e+01, "total_time": 1.1160e+03, "__timestamp": "2024-10-10 15:34:00.380490"}, {"step": 112000, "num_env_steps": 112000, "scores": {"n": 1, "mean": 4.1205e+03}, "actor_loss": {"n": 1, "mean": -1.8341e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.9199e-01}, "critic_loss": {"n": 1, "mean": 1.1481e+01}, "entropy_coef": {"n": 1, "mean": 9.2450e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5732e-01, "std": 2.7818e-01, "min_value": 1.4126e-05, "max_value": 9.9963e-01}, "num_gradient_steps": 0, "step_time": 1.0993e+01, "total_time": 1.1270e+03, "__timestamp": "2024-10-10 15:34:11.373700"}, {"step": 113000, "num_env_steps": 113000, "scores": {"n": 1, "mean": 4.1070e+03}, "actor_loss": {"n": 1, "mean": -1.8652e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.5633e-01}, "critic_loss": {"n": 1, "mean": 1.1693e+01}, "entropy_coef": {"n": 1, "mean": 8.9615e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4715e-01, "std": 2.8196e-01, "min_value": 1.5072e-04, "max_value": 9.9978e-01}, "num_gradient_steps": 0, "step_time": 1.2162e+01, "total_time": 1.1392e+03, "__timestamp": "2024-10-10 15:34:23.534211"}, {"step": 114000, "num_env_steps": 114000, "scores": {"n": 1, "mean": 3.8063e+03}, "actor_loss": {"n": 1, "mean": -1.9091e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3687e-01}, "critic_loss": {"n": 1, "mean": 1.8693e+02}, "entropy_coef": {"n": 1, "mean": 9.0511e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4560e-01, "std": 2.8261e-01, "min_value": 6.3549e-04, "max_value": 9.9946e-01}, "num_gradient_steps": 0, "step_time": 1.1817e+01, "total_time": 1.1510e+03, "__timestamp": "2024-10-10 15:34:35.352168"}, {"step": 115000, "num_env_steps": 115000, "scores": {"n": 1, "mean": 4.0456e+03}, "actor_loss": {"n": 1, "mean": -1.9603e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.1285e-01}, "critic_loss": {"n": 1, "mean": 1.1563e+01}, "entropy_coef": {"n": 1, "mean": 9.2886e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4662e-01, "std": 2.8433e-01, "min_value": 7.2122e-06, "max_value": 9.9979e-01}, "num_gradient_steps": 0, "step_time": 1.1730e+01, "total_time": 1.1627e+03, "__timestamp": "2024-10-10 15:34:47.082104"}, {"step": 116000, "num_env_steps": 116000, "scores": {"n": 1, "mean": 4.0392e+03}, "actor_loss": {"n": 1, "mean": -1.9983e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0372e+00}, "critic_loss": {"n": 1, "mean": 1.2027e+01}, "entropy_coef": {"n": 1, "mean": 9.5134e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4985e-01, "std": 2.8100e-01, "min_value": 7.2241e-05, "max_value": 9.9980e-01}, "num_gradient_steps": 0, "step_time": 1.1267e+01, "total_time": 1.1740e+03, "__timestamp": "2024-10-10 15:34:58.348715"}, {"step": 117000, "num_env_steps": 117000, "scores": {"n": 1, "mean": 4.1620e+03}, "actor_loss": {"n": 1, "mean": -1.9967e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3122e+00}, "critic_loss": {"n": 1, "mean": 1.2281e+01}, "entropy_coef": {"n": 1, "mean": 9.5805e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4927e-01, "std": 2.8185e-01, "min_value": 2.9057e-05, "max_value": 9.9973e-01}, "num_gradient_steps": 0, "step_time": 1.1389e+01, "total_time": 1.1854e+03, "__timestamp": "2024-10-10 15:35:09.737841"}, {"step": 118000, "num_env_steps": 118000, "scores": {"n": 1, "mean": 4.0238e+03}, "actor_loss": {"n": 1, "mean": -1.9476e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.3416e-01}, "critic_loss": {"n": 1, "mean": 1.3400e+02}, "entropy_coef": {"n": 1, "mean": 9.5121e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4768e-01, "std": 2.8066e-01, "min_value": 3.2138e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.2222e+01, "total_time": 1.1976e+03, "__timestamp": "2024-10-10 15:35:21.959426"}, {"step": 119000, "num_env_steps": 119000, "scores": {"n": 1, "mean": 4.0760e+03}, "actor_loss": {"n": 1, "mean": -1.9560e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.9587e-01}, "critic_loss": {"n": 1, "mean": 1.7033e+01}, "entropy_coef": {"n": 1, "mean": 9.5222e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4963e-01, "std": 2.7819e-01, "min_value": 1.6019e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.1754e+01, "total_time": 1.2093e+03, "__timestamp": "2024-10-10 15:35:33.713880"}, {"step": 120000, "num_env_steps": 120000, "scores": {"n": 1, "mean": 4.1012e+03}, "actor_loss": {"n": 1, "mean": -1.9516e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.7433e-01}, "critic_loss": {"n": 1, "mean": 1.8619e+01}, "entropy_coef": {"n": 1, "mean": 9.6179e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4922e-01, "std": 2.7957e-01, "min_value": 2.5374e-04, "max_value": 9.9952e-01}, "num_gradient_steps": 0, "step_time": 1.1847e+01, "total_time": 1.2212e+03, "__timestamp": "2024-10-10 15:35:45.559898"}, {"step": 121000, "num_env_steps": 121000, "scores": {"n": 1, "mean": 4.0385e+03}, "actor_loss": {"n": 1, "mean": -2.0766e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0979e+00}, "critic_loss": {"n": 1, "mean": 1.1747e+01}, "entropy_coef": {"n": 1, "mean": 9.7085e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4800e-01, "std": 2.8058e-01, "min_value": 3.7312e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.1881e+01, "total_time": 1.2331e+03, "__timestamp": "2024-10-10 15:35:57.441428"}, {"step": 122000, "num_env_steps": 122000, "scores": {"n": 1, "mean": 4.1424e+03}, "actor_loss": {"n": 1, "mean": -1.9780e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.5293e-01}, "critic_loss": {"n": 1, "mean": 1.0967e+01}, "entropy_coef": {"n": 1, "mean": 9.7402e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5276e-01, "std": 2.7979e-01, "min_value": 1.0717e-04, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.1415e+01, "total_time": 1.2445e+03, "__timestamp": "2024-10-10 15:36:08.855526"}, {"step": 123000, "num_env_steps": 123000, "scores": {"n": 1, "mean": 4.1900e+03}, "actor_loss": {"n": 1, "mean": -2.0279e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.1141e-01}, "critic_loss": {"n": 1, "mean": 1.2751e+01}, "entropy_coef": {"n": 1, "mean": 9.8284e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4764e-01, "std": 2.8163e-01, "min_value": 5.6275e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.2756e+01, "total_time": 1.2572e+03, "__timestamp": "2024-10-10 15:36:21.612671"}, {"step": 124000, "num_env_steps": 124000, "scores": {"n": 1, "mean": 4.0469e+03}, "actor_loss": {"n": 1, "mean": -2.0230e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.2234e-01}, "critic_loss": {"n": 1, "mean": 1.2953e+01}, "entropy_coef": {"n": 1, "mean": 9.9296e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4906e-01, "std": 2.8116e-01, "min_value": 7.6067e-05, "max_value": 9.9960e-01}, "num_gradient_steps": 0, "step_time": 1.1840e+01, "total_time": 1.2691e+03, "__timestamp": "2024-10-10 15:36:33.451744"}, {"step": 125000, "num_env_steps": 125000, "scores": {"n": 1, "mean": 3.9917e+03}, "actor_loss": {"n": 1, "mean": -2.0852e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.4992e-01}, "critic_loss": {"n": 1, "mean": 1.5325e+01}, "entropy_coef": {"n": 1, "mean": 1.0063e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4836e-01, "std": 2.8197e-01, "min_value": 1.3253e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.1541e+01, "total_time": 1.2806e+03, "__timestamp": "2024-10-10 15:36:44.993204"}, {"step": 126000, "num_env_steps": 126000, "scores": {"n": 1, "mean": 4.2651e+03}, "actor_loss": {"n": 1, "mean": -1.9963e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.9715e-02}, "critic_loss": {"n": 1, "mean": 2.7303e+02}, "entropy_coef": {"n": 1, "mean": 9.9298e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4774e-01, "std": 2.8019e-01, "min_value": 3.1024e-05, "max_value": 9.9952e-01}, "num_gradient_steps": 0, "step_time": 1.2096e+01, "total_time": 1.2927e+03, "__timestamp": "2024-10-10 15:36:57.090463"}]}}