{"experiment_id": "2024-10-09_10-55-11_912229~Fvj8xc", "experiment_tags": ["SAC", "HalfCheetah-v4", "Debug"], "start_time": "2024-10-09 10:55:11.912229", "end_time": "2024-10-09 13:09:07.013222", "end_exception": null, "hyper_parameters": {"_type": "SAC", "_type_fq": "src.reinforcement_learning.algorithms.sac.sac.SAC", "env": "<SingletonVectorEnv instance>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {}, "policy_parameter_count": 362256, "policy_repr": "DebugSACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 9 3900X 12-Core Processor", "cores": 12, "logical_cores": 24, "speed": "3793 MHz"}, "gpu": [{"name": "NVIDIA GeForce RTX 3070", "video_processor": "NVIDIA GeForce RTX 3070", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA", "memory": "8192 MB", "memory_clock": "405 MHz", "compute_capability": "8.6"}], "ram_speed": "3600 MHz", "ram": "64 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\nfrom stable_baselines3.common.torch_layers import FlattenExtractor\nfrom dataclasses import dataclass\nfrom typing import Type, Optional, Any, Literal\n\nimport gymnasium\nimport numpy as np\nimport stable_baselines3 as sb\nimport torch\nimport torch.nn.functional as F\nfrom stable_baselines3.common.policies import ContinuousCritic\nfrom torch import optim\n\nfrom src.function_types import TorchTensorFn\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.algorithms.base.base_algorithm import PolicyProvider\nfrom src.reinforcement_learning.algorithms.base.off_policy_algorithm import OffPolicyAlgorithm, ReplayBuf\nfrom src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\nfrom src.reinforcement_learning.core.action_noise import ActionNoise\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import BaseReplayBuffer, ReplayBufferSamples\nfrom src.reinforcement_learning.core.buffers.replay.replay_buffer import ReplayBuffer\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.core.infos import InfoDict, concat_infos\nfrom src.reinforcement_learning.core.logging import LoggingConfig, log_if_enabled\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.core.polyak_update import polyak_update\nfrom src.reinforcement_learning.core.type_aliases import OptimizerProvider, TensorObs, detach_obs\nfrom src.reinforcement_learning.gym.env_analysis import get_single_action_space\nfrom src.torch_device import TorchDevice\nfrom src.torch_functions import identity\n\nSAC_DEFAULT_OPTIMIZER_PROVIDER = lambda params: optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nAUTO_TARGET_ENTROPY = 'auto'\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn\n\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.policies.components.base_component import BasePolicyComponent\nfrom src.reinforcement_learning.core.policies.components.feature_extractors import FeatureExtractor, IdentityExtractor\nfrom src.reinforcement_learning.core.type_aliases import TensorObs\n\n\nclass DebugActor(BasePolicyComponent):\n\n    action_selector: ActionSelector\n    uses_sde: bool\n\n    def __init__(\n            self,\n            network: nn.Module,\n            action_selector: ActionSelector,\n            # feature_extractor: Optional[FeatureExtractor] = None\n    ):\n        assert isinstance(action_selector, PredictedStdActionSelector)\n        super().__init__(IdentityExtractor())\n        self.network = network\n        self.replace_action_selector(action_selector, copy_action_net_weights=False)\n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return self.update_hps(super().collect_hyper_parameters(), {\n            'network': self.get_hps_or_str(self.network),\n            'action_selector': self.get_hps_or_str(self.action_selector),\n        })\n\n    def forward(self, obs: TensorObs, deterministic: bool = False) -> torch.Tensor:\n        obs = self.feature_extractor(obs)\n        latent_pi = self.network(obs)\n        return self.action_selector.update_latent_features(latent_pi).get_actions(deterministic=debug_actor)\n\n    def get_actions_with_log_probs(\n            self,\n            obs: TensorObs,\n            deterministic: bool = False\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        obs = self.feature_extractor(obs)\n        latent_pi = self.network(obs)\n        return self.action_selector.get_actions_with_log_probs(latent_pi, deterministic=deterministic)\n\n    def replace_action_selector(self, new_action_selector: ActionSelector, copy_action_net_weights: bool) -> None:\n        if copy_action_net_weights:\n            new_action_selector.action_net.load_state_dict(self.action_selector.action_net.state_dict())\n        self.action_selector = new_action_selector\n        self.uses_sde = isinstance(self.action_selector, StateDependentNoiseActionSelector)\n\n    def reset_sde_noise(self, batch_size: int = 1) -> None:\n        if self.uses_sde:\n            raise NotImplemented\n    \n    def set_training_mode(self, mode: bool):\n        self.set_train_mode(mode)\n\n\n\n@dataclass\nclass SACLoggingConfig(LoggingConfig):\n\n    log_entropy_coef: bool = False\n    entropy_coef_loss: LossLoggingConfig = None\n    actor_loss: LossLoggingConfig = None\n    critic_loss: LossLoggingConfig = None\n\n    def __post_init__(self):\n        if self.actor_loss is None:\n            self.actor_loss = LossLoggingConfig()\n        if self.entropy_coef_loss is None:\n            self.entropy_loss = LossLoggingConfig()\n        if self.critic_loss is None:\n            self.critic_loss = LossLoggingConfig()\n\n        super().__post_init__()\n\n\"\"\"\n\n        Soft Actor-Critic:\n        Off-Policy Maximum Entropy Deep Reinforcement\n        Learning with a Stochastic Actor\n        https://arxiv.org/pdf/1801.01290\n\n\"\"\"\nclass SACDebug(OffPolicyAlgorithm[sb.sac.sac.SACPolicy, ReplayBuf, SACLoggingConfig]):\n    \n    policy: sb.sac.sac.SACPolicy\n    actor: sb.sac.sac.Actor\n    critic: ContinuousCritic\n    buffer: BaseReplayBuffer\n    target_entropy: float\n    log_ent_coef: Optional[torch.Tensor]\n    entropy_coef_optimizer: Optional[optim.Optimizer]\n    entropy_coef_tensor: Optional[torch.Tensor]\n\n    def __init__(\n            self,\n            env: gymnasium.Env,\n            policy: sb.sac.sac.SACPolicy | PolicyProvider[sb.sac.sac.SACPolicy],\n            actor_optimizer_provider: OptimizerProvider = SAC_DEFAULT_OPTIMIZER_PROVIDER,\n            critic_optimizer_provider: OptimizerProvider = SAC_DEFAULT_OPTIMIZER_PROVIDER,\n            weigh_and_reduce_actor_loss: TorchTensorFn = torch.mean,\n            weigh_critic_loss: TorchTensorFn = identity,\n            buffer_type: Type[ReplayBuf] = ReplayBuffer,\n            buffer_size: int = 100_000,\n            buffer_kwargs: dict[str, Any] = None,\n            gamma: float = 0.99,\n            tau: float = 0.005,\n            rollout_steps: int = 100,\n            gradient_steps: int = 1,\n            optimization_batch_size: int = 256,\n            target_update_interval: int = 1,\n            entropy_coef: float = 1.0,\n            target_entropy: float | Literal['auto'] = AUTO_TARGET_ENTROPY,\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider] = None,\n            weigh_and_reduce_entropy_coef_loss: TorchTensorFn = torch.mean,\n            action_noise: Optional[ActionNoise] = None,\n            warmup_steps: int = 100,\n            learning_starts: int = 100,\n            sde_noise_sample_freq: Optional[int] = None,\n            callback: Callback['SAC'] = None,\n            logging_config: SACLoggingConfig = None,\n            torch_device: TorchDevice = 'auto',\n            torch_dtype: torch.dtype = torch.float32,\n    ):\n        super().__init__(\n            env=env,\n            policy=policy,\n            buffer=buffer_type.for_env(env, buffer_size, torch_device, torch_dtype, **(buffer_kwargs or {})),\n            gamma=gamma,\n            tau=tau,\n            rollout_steps=rollout_steps,\n            gradient_steps=gradient_steps,\n            optimization_batch_size=optimization_batch_size,\n            action_noise=action_noise,\n            warmup_steps=warmup_steps,\n            learning_starts=learning_starts,\n            sde_noise_sample_freq=sde_noise_sample_freq,\n            callback=callback or Callback(),\n            logging_config=logging_config or LoggingConfig(),\n            torch_device=torch_device,\n            torch_dtype=torch_dtype,\n        )\n\n        self.actor = self.policy.actor\n        self.critic = self.policy.critic\n        # self.shared_feature_extractor = self.policy.shared_feature_extractor\n\n        self.actor_optimizer = actor_optimizer_provider(\n            # self.chain_parameters(self.actor, self.shared_feature_extractor)\n            self.actor.parameters()\n        )\n        self.critic_optimizer = critic_optimizer_provider(self.critic.parameters())\n\n        self.weigh_and_reduce_entropy_coef_loss = weigh_and_reduce_entropy_coef_loss\n        self.weigh_and_reduce_actor_loss = weigh_and_reduce_actor_loss\n        self.weigh_critic_loss = weigh_critic_loss\n\n        self.target_update_interval = target_update_interval\n        self.gradient_steps_performed = 0\n\n        self._setup_entropy_optimization(entropy_coef, target_entropy, entropy_coef_optimizer_provider)\n\n        # CrossQ doesn't use a target critic\n        if isinstance(self.policy, SACCrossQPolicy):\n            self.tau = 0\n            self.target_update_interval = 0\n\n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return self.update_hps(super().collect_hyper_parameters(), {\n            'actor_optimizer': str(self.actor_optimizer),\n            'critic_optimizer': str(self.critic_optimizer),\n            'entropy_coef_optimizer': str(self.entropy_coef_optimizer),\n            'weigh_and_reduce_entropy_coef_loss': str(self.weigh_and_reduce_entropy_coef_loss),\n            'weigh_and_reduce_actor_loss': str(self.weigh_and_reduce_actor_loss),\n            'weigh_critic_loss': str(self.weigh_critic_loss),\n            'target_update_interval': self.target_update_interval,\n            'target_entropy': self.target_entropy,\n            'entropy_coef': self.entropy_coef_tensor.item() if self.entropy_coef_tensor is not None else 'dynamic',\n        })\n\n    def _setup_entropy_optimization(\n            self,\n            entropy_coef: float,\n            target_entropy: float | Literal['auto'],\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider],\n    ):\n        if target_entropy == 'auto':\n            self.target_entropy = float(-np.prod(get_single_action_space(self.env).shape).astype(np.float32))\n        else:\n            self.target_entropy = float(target_entropy)\n\n        if entropy_coef_optimizer_provider is not None:\n            self.log_ent_coef = torch.log(\n                torch.tensor([entropy_coef], device=self.torch_device, dtype=self.torch_dtype)\n            ).requires_grad_(True)\n            self.entropy_coef_optimizer = entropy_coef_optimizer_provider([self.log_ent_coef])\n            self.entropy_coef_tensor = None\n        else:\n            self.log_ent_coef = None\n            self.entropy_coef_optimizer = None\n            self.entropy_coef_tensor = torch.tensor(entropy_coef, device=self.torch_device, dtype=self.torch_dtype)\n\n    def get_and_optimize_entropy_coef(\n            self,\n            actions_pi_log_prob: torch.Tensor,\n            info: InfoDict\n    ) -> torch.Tensor:\n        if self.entropy_coef_optimizer is not None:\n            entropy_coef = torch.exp(self.log_ent_coef.detach())\n\n            # TODO!\n            # entropy_coef_loss = weigh_and_reduce_loss(\n            #     raw_loss=-self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach(),\n            #     weigh_and_reduce_function=self.weigh_and_reduce_entropy_coef_loss,\n            #     info=info,\n            #     loss_name='entropy_coef_loss',\n            #     logging_config=self.logging_config.entropy_coef_loss\n            # )\n\n            entropy_coef_loss = -(self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach()).mean()\n            info['final_entropy_coef_loss'] = entropy_coef_loss.detach()\n\n            self.entropy_coef_optimizer.zero_grad()\n            entropy_coef_loss.backward()\n            self.entropy_coef_optimizer.step()\n\n            return entropy_coef\n        else:\n            return self.entropy_coef_tensor\n\n    def calculate_critic_loss(\n            self,\n            observation_features: TensorObs,\n            replay_samples: ReplayBufferSamples,\n            entropy_coef: torch.Tensor,\n            info: InfoDict,\n    ):\n        with torch.no_grad():\n                # Select action according to policy\n            next_actions, next_log_prob = self.actor.get_actions_with_log_probs(replay_samples.next_observations)\n            # Compute the next Q values: min over all critics targets\n            next_q_values = torch.cat(self.policy.critic_target(replay_samples.next_observations, next_actions), dim=1)\n            next_q_values, _ = torch.min(next_q_values, dim=1, keepdim=True)\n            # add entropy term\n            next_q_values = next_q_values - entropy_coef * next_log_prob.reshape(-1, 1)\n            # td error + entropy term\n            target_q_values = replay_samples.rewards + (1 - replay_samples.dones) * self.gamma * next_q_values\n\n        # target_q_values = self.policy.compute_target_values(\n        #     replay_samples=replay_samples,\n        #     entropy_coef=entropy_coef,\n        #     gamma=self.gamma,\n        # )\n        # critic loss should not influence shared feature extractor\n        current_q_values = self.critic(detach_obs(observation_features), replay_samples.actions)\n\n        # noinspection PyTypeChecker\n        critic_loss: torch.Tensor = 0.5 * sum(\n            F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n        )\n        # TODO!\n        # critic_loss = weigh_and_reduce_loss(\n        #     raw_loss=critic_loss,\n        #     weigh_and_reduce_function=self.weigh_critic_loss,\n        #     info=info,\n        #     loss_name='critic_loss',\n        #     logging_config=self.logging_config.critic_loss,\n        # )\n\n        info['final_critic_loss'] = critic_loss.detach()\n        return critic_loss\n\n    def calculate_actor_loss(\n            self,\n            observation_features: TensorObs,\n            actions_pi: torch.Tensor,\n            actions_pi_log_prob: torch.Tensor,\n            entropy_coef: torch.Tensor,\n            info: InfoDict,\n    ) -> torch.Tensor:\n        q_values_pi = torch.cat(self.critic(observation_features, actions_pi), dim=-1)\n        min_q_values_pi, _ = torch.min(q_values_pi, dim=-1, keepdim=True)\n        actor_loss = (entropy_coef * actions_pi_log_prob - min_q_values_pi).mean()  # TODO!\n\n        # TODO!\n        # actor_loss = weigh_and_reduce_loss(\n        #     raw_loss=actor_loss,\n        #     weigh_and_reduce_function=self.weigh_and_reduce_actor_loss,\n        #     info=info,\n        #     loss_name='actor_loss',\n        #     logging_config=self.logging_config.actor_loss,\n        # )\n\n        info['final_actor_loss'] = actor_loss.detach()\n        return actor_loss\n\n    def optimize(self, last_obs: np.ndarray, last_episode_starts: np.ndarray, info: InfoDict) -> None:\n        gradient_step_infos: list[InfoDict] = []\n\n        for gradient_step in range(self.gradient_steps):\n            step_info: InfoDict = {}\n            replay_samples = self.buffer.sample(self.optimization_batch_size)\n\n            # self.actor.reset_sde_noise()  # TODO: set batch size?\n\n            # observation_features = self.shared_feature_extractor(replay_samples.observations)\n            # observation_features = replay_samples.observations\n            actions_pi, actions_pi_log_prob = self.actor.get_actions_with_log_probs(replay_samples.observations)\n            actions_pi_log_prob = actions_pi_log_prob.reshape(-1, 1)\n\n            entropy_coef = self.get_and_optimize_entropy_coef(actions_pi_log_prob, step_info)\n            log_if_enabled(step_info, 'entropy_coef', entropy_coef, self.logging_config.log_entropy_coef)\n\n            critic_loss = self.calculate_critic_loss(\n                observation_features=replay_samples.observations,\n                replay_samples=replay_samples,\n                entropy_coef=entropy_coef,\n                info=step_info\n            )\n\n            self.critic_optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic_optimizer.step()\n\n            actor_loss = self.calculate_actor_loss(\n                observation_features=replay_samples.observations,\n                actions_pi=actions_pi,\n                actions_pi_log_prob=actions_pi_log_prob,\n                entropy_coef=entropy_coef,\n                info=step_info\n            )\n\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            self.gradient_steps_performed += 1\n            if self.target_update_interval > 0 and self.gradient_steps_performed % self.target_update_interval == 0:\n                # self.policy.perform_polyak_update(self.tau)\n                polyak_update(self.critic.parameters(), self.policy.critic_target.parameters(), self.tau)\n            gradient_step_infos.append(step_info)\n        info.update(concat_infos(gradient_step_infos))\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')\n\nimport copy\nfrom src.console import print_warning\nfrom src.tags import Tags\nfrom src.reinforcement_learning.core.policies.components.actor import Actor\nfrom src.reinforcement_learning.core.policies.components.q_critic import QCritic\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nimport stable_baselines3 as sb\n\n\nclass DebugSACPolicy(BasePolicy):\n    \n    actor: sb.sac.policies.Actor\n\n    def __init__(\n            self,\n            actor: Actor,\n            critic: QCritic,\n            shared_feature_extractor: Optional[FeatureExtractor] = None\n    ):\n        super().__init__(actor, shared_feature_extractor)\n        self.actor = sb_sac.actor\n        self.critic = sb_sac.critic\n\n        self._build_target()\n\n        self._check_action_selector()\n        \n    @property\n    def uses_sde(self):\n        return False\n        \n    def act(self, obs: TensorObs) -> torch.Tensor:\n        return self.actor(obs, False)\n    \n    def reset_sde_noise(self, batch_size: int) -> None:\n        pass\n        \n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return {}\n\n    def collect_tags(self) -> Tags:\n        return []\n\n    def _check_action_selector(self):\n        # if not isinstance(self.actor.action_selector, (PredictedStdActionSelector, StateDependentNoiseActionSelector)):\n        #     print_warning('SAC not being used with PredictedStdAction Selector or gSDE. LogStds should be clamped!')\n        pass\n\n    def _build_target(self):\n        self.target_critic = copy.deepcopy(self.critic)\n        self.target_critic.set_training_mode(False)\n\n        self.target_shared_feature_extractor = copy.deepcopy(self.shared_feature_extractor)\n        self.target_shared_feature_extractor.set_trainable(False)\n\n    def forward(self):\n        raise NotImplementedError('forward is not used in SACPolicy')\n\n    def compute_target_values(\n            self,\n            replay_samples: ReplayBufferSamples,\n            entropy_coef: torch.Tensor,\n            gamma: float,\n    ):\n        with torch.no_grad():\n            next_observations = replay_samples.next_observations\n\n            next_actions, next_actions_log_prob = self.actor.action_log_prob(\n                self.shared_feature_extractor(next_observations)\n            )\n\n            next_q_values = torch.cat(\n                self.target_critic(self.target_shared_feature_extractor(next_observations), next_actions),\n                dim=-1\n            )\n            next_q_values, _ = torch.min(next_q_values, dim=-1, keepdim=True)\n            next_q_values = next_q_values - entropy_coef * next_actions_log_prob.reshape(-1, 1)\n\n            target_q_values = replay_samples.rewards + (1 - replay_samples.dones) * gamma * next_q_values\n\n            return target_q_values\n\n\n    def perform_polyak_update(self, tau: float):\n        polyak_update(self.critic.parameters(), self.target_critic.parameters(), tau)\n        polyak_update(\n            self.shared_feature_extractor.parameters(),\n            self.target_shared_feature_extractor.parameters(),\n            tau\n        )\n\n    def set_train_mode(self, mode: bool) -> None:\n        self.actor.set_training_mode(mode)\n        self.critic.set_training_mode(mode)\n        # Leaving target_critic on train_mode = False\n\n        self.shared_feature_extractor.set_train_mode(mode)\n        # Leaving target_shared_feature_extractor on train_mode = False\n\n        self.train_mode = mode\n\nimport inspect\nimport time\n\nfrom gymnasium import Env\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.module_analysis import count_parameters\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import PolicyConstruction\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom typing import Any\nfrom src.reinforcement_learning.core.callback import Callback\n\nimport torch\nfrom torch import optim\nimport gymnasium as gym\nimport numpy as np\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -2.7230e+02}, "actor_loss": {"n": 1, "mean": -1.7896e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.9936e+00}, "critic_loss": {"n": 1, "mean": 2.7580e+00}, "entropy_coef": {"n": 1, "mean": 7.4071e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.2427e-01, "std": 2.8655e-01, "min_value": 1.2048e-05, "max_value": 9.9941e-01}, "num_gradient_steps": 1000, "step_time": 1.3183e+01, "total_time": 1.3171e+01, "__timestamp": "2024-10-09 10:55:25.087449"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -1.1362e+02}, "actor_loss": {"n": 1, "mean": -2.5696e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9450e+00}, "critic_loss": {"n": 1, "mean": 1.4395e+00}, "entropy_coef": {"n": 1, "mean": 5.4955e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3852e-01, "std": 2.8748e-01, "min_value": 5.1096e-05, "max_value": 9.9770e-01}, "num_gradient_steps": 2000, "step_time": 9.3641e+00, "total_time": 2.2535e+01, "__timestamp": "2024-10-09 10:55:34.447517"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -3.2436e+02}, "actor_loss": {"n": 1, "mean": -3.0175e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.8498e+00}, "critic_loss": {"n": 1, "mean": 1.5296e+00}, "entropy_coef": {"n": 1, "mean": 4.0728e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4612e-01, "std": 2.8692e-01, "min_value": 2.0189e-04, "max_value": 9.9777e-01}, "num_gradient_steps": 3000, "step_time": 9.3552e+00, "total_time": 3.1890e+01, "__timestamp": "2024-10-09 10:55:43.802693"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -1.2842e+02}, "actor_loss": {"n": 1, "mean": -3.2466e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1557e+01}, "critic_loss": {"n": 1, "mean": 1.6527e+00}, "entropy_coef": {"n": 1, "mean": 3.0270e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3963e-01, "std": 2.9066e-01, "min_value": 1.1724e-04, "max_value": 9.9888e-01}, "num_gradient_steps": 4000, "step_time": 9.3529e+00, "total_time": 4.1243e+01, "__timestamp": "2024-10-09 10:55:53.155611"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -2.2566e+02}, "actor_loss": {"n": 1, "mean": -3.3293e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3851e+01}, "critic_loss": {"n": 1, "mean": 1.4855e+00}, "entropy_coef": {"n": 1, "mean": 2.2581e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.5701e-01, "std": 2.9111e-01, "min_value": 1.4332e-04, "max_value": 9.9781e-01}, "num_gradient_steps": 5000, "step_time": 9.3933e+00, "total_time": 5.0637e+01, "__timestamp": "2024-10-09 10:56:02.548909"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -2.0536e+02}, "actor_loss": {"n": 1, "mean": -3.3115e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5697e+01}, "critic_loss": {"n": 1, "mean": 1.9691e+00}, "entropy_coef": {"n": 1, "mean": 1.6929e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.6100e-01, "std": 2.9155e-01, "min_value": 2.3144e-04, "max_value": 9.9906e-01}, "num_gradient_steps": 6000, "step_time": 9.3436e+00, "total_time": 5.9980e+01, "__timestamp": "2024-10-09 10:56:11.892524"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -2.0479e+02}, "actor_loss": {"n": 1, "mean": -3.2944e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6426e+01}, "critic_loss": {"n": 1, "mean": 2.1966e+00}, "entropy_coef": {"n": 1, "mean": 1.2783e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.7316e-01, "std": 2.9433e-01, "min_value": 1.8173e-04, "max_value": 9.9918e-01}, "num_gradient_steps": 7000, "step_time": 9.3313e+00, "total_time": 6.9312e+01, "__timestamp": "2024-10-09 10:56:21.223840"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -1.9957e+02}, "actor_loss": {"n": 1, "mean": -3.1519e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6372e+01}, "critic_loss": {"n": 1, "mean": 1.7861e+00}, "entropy_coef": {"n": 1, "mean": 9.7263e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8358e-01, "std": 2.9299e-01, "min_value": 2.4959e-04, "max_value": 9.9895e-01}, "num_gradient_steps": 8000, "step_time": 9.3767e+00, "total_time": 7.8688e+01, "__timestamp": "2024-10-09 10:56:30.600560"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -1.9477e+02}, "actor_loss": {"n": 1, "mean": -3.0600e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5976e+01}, "critic_loss": {"n": 1, "mean": 1.5486e+00}, "entropy_coef": {"n": 1, "mean": 7.4352e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9929e-01, "std": 2.9293e-01, "min_value": 1.6266e-04, "max_value": 9.9972e-01}, "num_gradient_steps": 9000, "step_time": 9.5703e+00, "total_time": 8.8259e+01, "__timestamp": "2024-10-09 10:56:40.170838"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -1.5222e+02}, "actor_loss": {"n": 1, "mean": -2.9958e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6403e+01}, "critic_loss": {"n": 1, "mean": 1.5767e+00}, "entropy_coef": {"n": 1, "mean": 5.7131e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9674e-01, "std": 2.9658e-01, "min_value": 3.0383e-05, "max_value": 9.9994e-01}, "num_gradient_steps": 10000, "step_time": 9.5003e+00, "total_time": 9.7759e+01, "__timestamp": "2024-10-09 10:56:49.671152"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -2.2209e+02}, "actor_loss": {"n": 1, "mean": -2.9708e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4268e+01}, "critic_loss": {"n": 1, "mean": 1.5899e+00}, "entropy_coef": {"n": 1, "mean": 4.4116e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0519e-01, "std": 2.9707e-01, "min_value": 4.4608e-04, "max_value": 9.9975e-01}, "num_gradient_steps": 11000, "step_time": 9.3586e+00, "total_time": 1.0712e+02, "__timestamp": "2024-10-09 10:56:59.029768"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -1.6343e+02}, "actor_loss": {"n": 1, "mean": -2.8311e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3573e+01}, "critic_loss": {"n": 1, "mean": 1.3915e+00}, "entropy_coef": {"n": 1, "mean": 3.4267e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1496e-01, "std": 2.9800e-01, "min_value": 8.2433e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 12000, "step_time": 9.4036e+00, "total_time": 1.1652e+02, "__timestamp": "2024-10-09 10:57:08.433333"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -2.3660e+02}, "actor_loss": {"n": 1, "mean": -2.6284e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3478e+01}, "critic_loss": {"n": 1, "mean": 1.5212e+00}, "entropy_coef": {"n": 1, "mean": 2.6748e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2441e-01, "std": 2.9961e-01, "min_value": 1.4417e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 13000, "step_time": 9.3606e+00, "total_time": 1.2588e+02, "__timestamp": "2024-10-09 10:57:17.793953"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -1.6319e+02}, "actor_loss": {"n": 1, "mean": -2.5636e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.0928e+00}, "critic_loss": {"n": 1, "mean": 1.3933e+00}, "entropy_coef": {"n": 1, "mean": 2.0998e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1945e-01, "std": 2.9720e-01, "min_value": 3.3897e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 14000, "step_time": 9.3721e+00, "total_time": 1.3525e+02, "__timestamp": "2024-10-09 10:57:27.166086"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": -1.8133e+02}, "actor_loss": {"n": 1, "mean": -2.4861e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.2243e+00}, "critic_loss": {"n": 1, "mean": 1.8390e+00}, "entropy_coef": {"n": 1, "mean": 1.6664e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.3742e-01, "std": 2.9487e-01, "min_value": 3.1200e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 15000, "step_time": 9.3967e+00, "total_time": 1.4465e+02, "__timestamp": "2024-10-09 10:57:36.562805"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": -1.3398e+02}, "actor_loss": {"n": 1, "mean": -2.3585e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.2838e+00}, "critic_loss": {"n": 1, "mean": 1.6079e+00}, "entropy_coef": {"n": 1, "mean": 1.3403e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.3376e-01, "std": 2.9750e-01, "min_value": 5.5743e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 16000, "step_time": 9.2946e+00, "total_time": 1.5395e+02, "__timestamp": "2024-10-09 10:57:45.856397"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": -1.3892e+02}, "actor_loss": {"n": 1, "mean": -2.2827e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.3921e+00}, "critic_loss": {"n": 1, "mean": 1.4975e+00}, "entropy_coef": {"n": 1, "mean": 1.0870e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.6472e-01, "std": 2.9831e-01, "min_value": 2.0973e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 17000, "step_time": 9.3543e+00, "total_time": 1.6330e+02, "__timestamp": "2024-10-09 10:57:55.211711"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": 3.0085e+01}, "actor_loss": {"n": 1, "mean": -2.0422e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.6412e+00}, "critic_loss": {"n": 1, "mean": 1.3409e+00}, "entropy_coef": {"n": 1, "mean": 8.9758e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1088e-01, "std": 2.9826e-01, "min_value": 1.2189e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 18000, "step_time": 9.3335e+00, "total_time": 1.7263e+02, "__timestamp": "2024-10-09 10:58:04.545197"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": 3.0729e+02}, "actor_loss": {"n": 1, "mean": -2.2337e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.1229e-01}, "critic_loss": {"n": 1, "mean": 1.7215e+00}, "entropy_coef": {"n": 1, "mean": 7.5753e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9684e-01, "std": 3.0254e-01, "min_value": 6.4880e-05, "max_value": 9.9982e-01}, "num_gradient_steps": 19000, "step_time": 9.3798e+00, "total_time": 1.8201e+02, "__timestamp": "2024-10-09 10:58:13.924996"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": 2.4504e+02}, "actor_loss": {"n": 1, "mean": -1.8749e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.0171e+00}, "critic_loss": {"n": 1, "mean": 1.3144e+00}, "entropy_coef": {"n": 1, "mean": 7.0171e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8145e-01, "std": 2.9544e-01, "min_value": 1.8205e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 20000, "step_time": 9.3963e+00, "total_time": 1.9141e+02, "__timestamp": "2024-10-09 10:58:23.321270"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": 7.2100e+02}, "actor_loss": {"n": 1, "mean": -2.1749e+01}, "entropy_coef_loss": {"n": 1, "mean": 4.9226e-01}, "critic_loss": {"n": 1, "mean": 1.5015e+00}, "entropy_coef": {"n": 1, "mean": 7.0639e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9894e-01, "std": 3.0072e-01, "min_value": 9.4277e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 21000, "step_time": 9.3911e+00, "total_time": 2.0080e+02, "__timestamp": "2024-10-09 10:58:32.712342"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": 4.5344e+02}, "actor_loss": {"n": 1, "mean": -2.2916e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2392e+00}, "critic_loss": {"n": 1, "mean": 1.6040e+00}, "entropy_coef": {"n": 1, "mean": 7.4401e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0173e-01, "std": 2.9965e-01, "min_value": 9.0949e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 22000, "step_time": 9.3387e+00, "total_time": 2.1014e+02, "__timestamp": "2024-10-09 10:58:42.051056"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 9.4867e+02}, "actor_loss": {"n": 1, "mean": -2.0165e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.5259e+00}, "critic_loss": {"n": 1, "mean": 1.5554e+00}, "entropy_coef": {"n": 1, "mean": 7.6585e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1501e-01, "std": 2.9841e-01, "min_value": 2.6172e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 23000, "step_time": 9.3877e+00, "total_time": 2.1953e+02, "__timestamp": "2024-10-09 10:58:51.437716"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 7.9073e+02}, "actor_loss": {"n": 1, "mean": -2.2185e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.2853e-01}, "critic_loss": {"n": 1, "mean": 1.5565e+00}, "entropy_coef": {"n": 1, "mean": 7.8649e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0232e-01, "std": 3.0266e-01, "min_value": 5.9523e-05, "max_value": 9.9988e-01}, "num_gradient_steps": 24000, "step_time": 9.3774e+00, "total_time": 2.2890e+02, "__timestamp": "2024-10-09 10:59:00.816161"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 8.5074e+02}, "actor_loss": {"n": 1, "mean": -2.1903e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.6881e-01}, "critic_loss": {"n": 1, "mean": 1.8565e+00}, "entropy_coef": {"n": 1, "mean": 8.1099e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0684e-01, "std": 3.0266e-01, "min_value": 1.9372e-05, "max_value": 9.9993e-01}, "num_gradient_steps": 25000, "step_time": 9.3826e+00, "total_time": 2.3829e+02, "__timestamp": "2024-10-09 10:59:10.198781"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 1.0343e+03}, "actor_loss": {"n": 1, "mean": -2.3429e+01}, "entropy_coef_loss": {"n": 1, "mean": 7.3972e-01}, "critic_loss": {"n": 1, "mean": 1.6655e+00}, "entropy_coef": {"n": 1, "mean": 8.4807e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1755e-01, "std": 3.0158e-01, "min_value": 4.9676e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 26000, "step_time": 9.4553e+00, "total_time": 2.4774e+02, "__timestamp": "2024-10-09 10:59:19.654088"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 1.2284e+03}, "actor_loss": {"n": 1, "mean": -2.1355e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.8429e+00}, "critic_loss": {"n": 1, "mean": 1.5998e+00}, "entropy_coef": {"n": 1, "mean": 9.0274e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2520e-01, "std": 2.9967e-01, "min_value": 2.6181e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 27000, "step_time": 9.5071e+00, "total_time": 2.5725e+02, "__timestamp": "2024-10-09 10:59:29.161174"}, {"step": 38000, "num_env_steps": 38000, "scores": {"n": 1, "mean": 1.3802e+03}, "actor_loss": {"n": 1, "mean": -2.2572e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.9646e+00}, "critic_loss": {"n": 1, "mean": 1.6344e+00}, "entropy_coef": {"n": 1, "mean": 1.0494e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.3580e-01, "std": 3.0184e-01, "min_value": 5.8959e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 28000, "step_time": 9.2648e+00, "total_time": 2.6651e+02, "__timestamp": "2024-10-09 10:59:38.425952"}, {"step": 39000, "num_env_steps": 39000, "scores": {"n": 1, "mean": 1.5061e+03}, "actor_loss": {"n": 1, "mean": -2.7714e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.3910e+00}, "critic_loss": {"n": 1, "mean": 1.1042e+01}, "entropy_coef": {"n": 1, "mean": 1.2192e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7178e-01, "std": 3.0223e-01, "min_value": 2.5983e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 29000, "step_time": 1.0187e+01, "total_time": 2.7670e+02, "__timestamp": "2024-10-09 10:59:48.613187"}, {"step": 40000, "num_env_steps": 40000, "scores": {"n": 1, "mean": 1.6948e+03}, "actor_loss": {"n": 1, "mean": -2.5737e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.5286e+00}, "critic_loss": {"n": 1, "mean": 1.8735e+00}, "entropy_coef": {"n": 1, "mean": 1.4313e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0135e-01, "std": 2.9867e-01, "min_value": 3.6170e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 30000, "step_time": 1.0055e+01, "total_time": 2.8676e+02, "__timestamp": "2024-10-09 10:59:58.667814"}, {"step": 41000, "num_env_steps": 41000, "scores": {"n": 1, "mean": 1.6584e+03}, "actor_loss": {"n": 1, "mean": -2.8287e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1067e+00}, "critic_loss": {"n": 1, "mean": 2.1785e+00}, "entropy_coef": {"n": 1, "mean": 1.5551e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1338e-01, "std": 2.9875e-01, "min_value": 9.5800e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 31000, "step_time": 9.4729e+00, "total_time": 2.9623e+02, "__timestamp": "2024-10-09 11:00:08.140749"}, {"step": 42000, "num_env_steps": 42000, "scores": {"n": 1, "mean": 1.7751e+03}, "actor_loss": {"n": 1, "mean": -2.9831e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.5095e+00}, "critic_loss": {"n": 1, "mean": 2.7575e+00}, "entropy_coef": {"n": 1, "mean": 1.7753e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1391e-01, "std": 2.9646e-01, "min_value": 4.5492e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 32000, "step_time": 9.4336e+00, "total_time": 3.0566e+02, "__timestamp": "2024-10-09 11:00:17.574329"}, {"step": 43000, "num_env_steps": 43000, "scores": {"n": 1, "mean": 1.7822e+03}, "actor_loss": {"n": 1, "mean": -3.4293e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.7675e+00}, "critic_loss": {"n": 1, "mean": 2.7269e+01}, "entropy_coef": {"n": 1, "mean": 1.8989e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3321e-01, "std": 2.8893e-01, "min_value": 7.0555e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 33000, "step_time": 9.4277e+00, "total_time": 3.1509e+02, "__timestamp": "2024-10-09 11:00:27.001053"}, {"step": 44000, "num_env_steps": 44000, "scores": {"n": 1, "mean": 1.9612e+03}, "actor_loss": {"n": 1, "mean": -3.6733e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.1732e-01}, "critic_loss": {"n": 1, "mean": 3.6191e+00}, "entropy_coef": {"n": 1, "mean": 2.0761e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3381e-01, "std": 2.8877e-01, "min_value": 6.7535e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 34000, "step_time": 9.4553e+00, "total_time": 3.2455e+02, "__timestamp": "2024-10-09 11:00:36.457384"}, {"step": 45000, "num_env_steps": 45000, "scores": {"n": 1, "mean": 1.9712e+03}, "actor_loss": {"n": 1, "mean": -3.7735e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.4404e-01}, "critic_loss": {"n": 1, "mean": 2.2054e+00}, "entropy_coef": {"n": 1, "mean": 2.1642e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2179e-01, "std": 2.9244e-01, "min_value": 3.9369e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 35000, "step_time": 9.4163e+00, "total_time": 3.3396e+02, "__timestamp": "2024-10-09 11:00:45.873733"}, {"step": 46000, "num_env_steps": 46000, "scores": {"n": 1, "mean": 2.2075e+03}, "actor_loss": {"n": 1, "mean": -4.5014e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.9999e-01}, "critic_loss": {"n": 1, "mean": 2.8847e+00}, "entropy_coef": {"n": 1, "mean": 2.3665e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3408e-01, "std": 2.9276e-01, "min_value": 4.6849e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 36000, "step_time": 9.4541e+00, "total_time": 3.4342e+02, "__timestamp": "2024-10-09 11:00:55.327790"}, {"step": 47000, "num_env_steps": 47000, "scores": {"n": 1, "mean": 2.0067e+03}, "actor_loss": {"n": 1, "mean": -4.5738e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.1496e+00}, "critic_loss": {"n": 1, "mean": 4.0097e+00}, "entropy_coef": {"n": 1, "mean": 2.4462e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2663e-01, "std": 2.9410e-01, "min_value": 3.3724e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 37000, "step_time": 9.4437e+00, "total_time": 3.5286e+02, "__timestamp": "2024-10-09 11:01:04.770490"}, {"step": 48000, "num_env_steps": 48000, "scores": {"n": 1, "mean": 2.1015e+03}, "actor_loss": {"n": 1, "mean": -4.7006e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.4606e-01}, "critic_loss": {"n": 1, "mean": 4.2629e+00}, "entropy_coef": {"n": 1, "mean": 2.4793e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2008e-01, "std": 2.9637e-01, "min_value": 1.3456e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 38000, "step_time": 9.4929e+00, "total_time": 3.6235e+02, "__timestamp": "2024-10-09 11:01:14.264394"}, {"step": 49000, "num_env_steps": 49000, "scores": {"n": 1, "mean": 2.3094e+03}, "actor_loss": {"n": 1, "mean": -4.9409e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2409e+00}, "critic_loss": {"n": 1, "mean": 3.6190e+00}, "entropy_coef": {"n": 1, "mean": 2.6155e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.2166e-01, "std": 2.9310e-01, "min_value": 2.9290e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 39000, "step_time": 9.6503e+00, "total_time": 3.7200e+02, "__timestamp": "2024-10-09 11:01:23.913739"}, {"step": 50000, "num_env_steps": 50000, "scores": {"n": 1, "mean": 2.2796e+03}, "actor_loss": {"n": 1, "mean": -5.5331e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.6459e-01}, "critic_loss": {"n": 1, "mean": 4.2622e+00}, "entropy_coef": {"n": 1, "mean": 2.7931e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3638e-01, "std": 2.9253e-01, "min_value": 7.1385e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 40000, "step_time": 9.7312e+00, "total_time": 3.8173e+02, "__timestamp": "2024-10-09 11:01:33.644968"}, {"step": 51000, "num_env_steps": 51000, "scores": {"n": 1, "mean": 2.2076e+03}, "actor_loss": {"n": 1, "mean": -5.9245e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.0045e+00}, "critic_loss": {"n": 1, "mean": 4.5645e+00}, "entropy_coef": {"n": 1, "mean": 2.8987e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6246e-01, "std": 2.7974e-01, "min_value": 7.8826e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 41000, "step_time": 9.1802e+00, "total_time": 3.9091e+02, "__timestamp": "2024-10-09 11:01:42.825215"}, {"step": 52000, "num_env_steps": 52000, "scores": {"n": 1, "mean": 2.3782e+03}, "actor_loss": {"n": 1, "mean": -5.6969e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.7752e+00}, "critic_loss": {"n": 1, "mean": 5.0414e+00}, "entropy_coef": {"n": 1, "mean": 2.9769e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5172e-01, "std": 2.8230e-01, "min_value": 9.8451e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 42000, "step_time": 9.1328e+00, "total_time": 4.0005e+02, "__timestamp": "2024-10-09 11:01:51.958992"}, {"step": 53000, "num_env_steps": 53000, "scores": {"n": 1, "mean": 2.4936e+03}, "actor_loss": {"n": 1, "mean": -5.9870e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3236e+00}, "critic_loss": {"n": 1, "mean": 1.7521e+01}, "entropy_coef": {"n": 1, "mean": 3.0060e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4485e-01, "std": 2.8473e-01, "min_value": 1.5067e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 43000, "step_time": 9.1895e+00, "total_time": 4.0924e+02, "__timestamp": "2024-10-09 11:02:01.148538"}, {"step": 54000, "num_env_steps": 54000, "scores": {"n": 1, "mean": 2.6934e+03}, "actor_loss": {"n": 1, "mean": -6.0360e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.8333e-01}, "critic_loss": {"n": 1, "mean": 5.2214e+00}, "entropy_coef": {"n": 1, "mean": 3.1465e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5621e-01, "std": 2.8002e-01, "min_value": 4.3914e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 44000, "step_time": 1.1143e+01, "total_time": 4.2038e+02, "__timestamp": "2024-10-09 11:02:12.291512"}, {"step": 55000, "num_env_steps": 55000, "scores": {"n": 1, "mean": 2.5020e+03}, "actor_loss": {"n": 1, "mean": -6.3691e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.1066e-01}, "critic_loss": {"n": 1, "mean": 5.3153e+00}, "entropy_coef": {"n": 1, "mean": 3.3126e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6138e-01, "std": 2.7965e-01, "min_value": 3.4712e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 45000, "step_time": 9.9477e+00, "total_time": 4.3033e+02, "__timestamp": "2024-10-09 11:02:22.239248"}, {"step": 56000, "num_env_steps": 56000, "scores": {"n": 1, "mean": 2.5705e+03}, "actor_loss": {"n": 1, "mean": -6.8191e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.3009e+00}, "critic_loss": {"n": 1, "mean": 4.6703e+00}, "entropy_coef": {"n": 1, "mean": 3.4618e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6346e-01, "std": 2.7604e-01, "min_value": 3.0886e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 46000, "step_time": 1.0059e+01, "total_time": 4.4039e+02, "__timestamp": "2024-10-09 11:02:32.298073"}, {"step": 57000, "num_env_steps": 57000, "scores": {"n": 1, "mean": 2.7976e+03}, "actor_loss": {"n": 1, "mean": -7.4262e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.3002e+00}, "critic_loss": {"n": 1, "mean": 5.6004e+00}, "entropy_coef": {"n": 1, "mean": 3.6264e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6830e-01, "std": 2.7963e-01, "min_value": 8.2569e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 47000, "step_time": 1.0123e+01, "total_time": 4.5051e+02, "__timestamp": "2024-10-09 11:02:42.419962"}, {"step": 58000, "num_env_steps": 58000, "scores": {"n": 1, "mean": 3.1139e+03}, "actor_loss": {"n": 1, "mean": -7.3001e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3432e+00}, "critic_loss": {"n": 1, "mean": 6.4539e+01}, "entropy_coef": {"n": 1, "mean": 3.8280e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7523e-01, "std": 2.7306e-01, "min_value": 2.5320e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 48000, "step_time": 9.5407e+00, "total_time": 4.6005e+02, "__timestamp": "2024-10-09 11:02:51.960649"}, {"step": 59000, "num_env_steps": 59000, "scores": {"n": 1, "mean": 2.7622e+03}, "actor_loss": {"n": 1, "mean": -7.5884e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.1264e+00}, "critic_loss": {"n": 1, "mean": 4.7672e+00}, "entropy_coef": {"n": 1, "mean": 3.8285e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7162e-01, "std": 2.7501e-01, "min_value": 5.4912e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 49000, "step_time": 9.0620e+00, "total_time": 4.6911e+02, "__timestamp": "2024-10-09 11:03:01.022606"}, {"step": 60000, "num_env_steps": 60000, "scores": {"n": 1, "mean": 3.0554e+03}, "actor_loss": {"n": 1, "mean": -8.0898e+01}, "entropy_coef_loss": {"n": 1, "mean": 5.6014e-01}, "critic_loss": {"n": 1, "mean": 6.6258e+00}, "entropy_coef": {"n": 1, "mean": 3.9062e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6887e-01, "std": 2.7912e-01, "min_value": 6.2853e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 50000, "step_time": 9.2207e+00, "total_time": 4.7833e+02, "__timestamp": "2024-10-09 11:03:10.244324"}, {"step": 61000, "num_env_steps": 61000, "scores": {"n": 1, "mean": 3.3288e+03}, "actor_loss": {"n": 1, "mean": -8.2975e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.3970e+00}, "critic_loss": {"n": 1, "mean": 6.3658e+00}, "entropy_coef": {"n": 1, "mean": 4.0790e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7945e-01, "std": 2.7601e-01, "min_value": 1.2010e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 51000, "step_time": 9.0996e+00, "total_time": 4.8743e+02, "__timestamp": "2024-10-09 11:03:19.343883"}, {"step": 62000, "num_env_steps": 62000, "scores": {"n": 1, "mean": 3.2066e+03}, "actor_loss": {"n": 1, "mean": -8.9659e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.0329e+00}, "critic_loss": {"n": 1, "mean": 6.4863e+00}, "entropy_coef": {"n": 1, "mean": 4.1941e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8957e-01, "std": 2.6802e-01, "min_value": 4.2818e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 52000, "step_time": 1.0062e+01, "total_time": 4.9749e+02, "__timestamp": "2024-10-09 11:03:29.405613"}, {"step": 63000, "num_env_steps": 63000, "scores": {"n": 1, "mean": 3.0012e+03}, "actor_loss": {"n": 1, "mean": -8.7595e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.8323e-01}, "critic_loss": {"n": 1, "mean": 6.7776e+00}, "entropy_coef": {"n": 1, "mean": 4.3380e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7849e-01, "std": 2.7306e-01, "min_value": 1.1095e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 53000, "step_time": 9.4862e+00, "total_time": 5.0698e+02, "__timestamp": "2024-10-09 11:03:38.891768"}, {"step": 64000, "num_env_steps": 64000, "scores": {"n": 1, "mean": 3.1578e+03}, "actor_loss": {"n": 1, "mean": -9.2840e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.2210e-02}, "critic_loss": {"n": 1, "mean": 5.4187e+00}, "entropy_coef": {"n": 1, "mean": 4.5257e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8072e-01, "std": 2.7433e-01, "min_value": 2.2190e-03, "max_value": 9.9991e-01}, "num_gradient_steps": 54000, "step_time": 9.4309e+00, "total_time": 5.1641e+02, "__timestamp": "2024-10-09 11:03:48.322627"}, {"step": 65000, "num_env_steps": 65000, "scores": {"n": 1, "mean": 3.0168e+03}, "actor_loss": {"n": 1, "mean": -9.5785e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.8542e-01}, "critic_loss": {"n": 1, "mean": 5.8421e+00}, "entropy_coef": {"n": 1, "mean": 4.6604e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7695e-01, "std": 2.7133e-01, "min_value": 1.5589e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 55000, "step_time": 9.2262e+00, "total_time": 5.2564e+02, "__timestamp": "2024-10-09 11:03:57.548851"}, {"step": 66000, "num_env_steps": 66000, "scores": {"n": 1, "mean": 3.0274e+03}, "actor_loss": {"n": 1, "mean": -1.0244e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.4512e-01}, "critic_loss": {"n": 1, "mean": 6.2267e+01}, "entropy_coef": {"n": 1, "mean": 4.8394e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7546e-01, "std": 2.7158e-01, "min_value": 3.0604e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 56000, "step_time": 9.3155e+00, "total_time": 5.3495e+02, "__timestamp": "2024-10-09 11:04:06.863344"}, {"step": 67000, "num_env_steps": 67000, "scores": {"n": 1, "mean": 3.2254e+03}, "actor_loss": {"n": 1, "mean": -9.6348e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.2291e-02}, "critic_loss": {"n": 1, "mean": 8.1771e+00}, "entropy_coef": {"n": 1, "mean": 4.8052e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8354e-01, "std": 2.6897e-01, "min_value": 3.6633e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 57000, "step_time": 9.2039e+00, "total_time": 5.4416e+02, "__timestamp": "2024-10-09 11:04:16.068210"}, {"step": 68000, "num_env_steps": 68000, "scores": {"n": 1, "mean": 3.3580e+03}, "actor_loss": {"n": 1, "mean": -1.0186e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.6708e-01}, "critic_loss": {"n": 1, "mean": 4.1150e+01}, "entropy_coef": {"n": 1, "mean": 5.0074e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8628e-01, "std": 2.6827e-01, "min_value": 7.1996e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 58000, "step_time": 9.1928e+00, "total_time": 5.5335e+02, "__timestamp": "2024-10-09 11:04:25.261005"}, {"step": 69000, "num_env_steps": 69000, "scores": {"n": 1, "mean": 2.9932e+03}, "actor_loss": {"n": 1, "mean": -1.0714e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.4760e+00}, "critic_loss": {"n": 1, "mean": 5.1509e+00}, "entropy_coef": {"n": 1, "mean": 5.1734e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7362e-01, "std": 2.7110e-01, "min_value": 1.4575e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 59000, "step_time": 1.0101e+01, "total_time": 5.6345e+02, "__timestamp": "2024-10-09 11:04:35.360751"}, {"step": 70000, "num_env_steps": 70000, "scores": {"n": 1, "mean": 3.4157e+03}, "actor_loss": {"n": 1, "mean": -1.0679e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.3041e+00}, "critic_loss": {"n": 1, "mean": 7.5871e+00}, "entropy_coef": {"n": 1, "mean": 5.4270e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8237e-01, "std": 2.7159e-01, "min_value": 3.4453e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 60000, "step_time": 9.3371e+00, "total_time": 5.7279e+02, "__timestamp": "2024-10-09 11:04:44.698834"}, {"step": 71000, "num_env_steps": 71000, "scores": {"n": 1, "mean": 3.1924e+03}, "actor_loss": {"n": 1, "mean": -1.0715e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4164e+00}, "critic_loss": {"n": 1, "mean": 5.9666e+00}, "entropy_coef": {"n": 1, "mean": 5.3532e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7277e-01, "std": 2.7236e-01, "min_value": 1.1539e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 61000, "step_time": 9.2790e+00, "total_time": 5.8207e+02, "__timestamp": "2024-10-09 11:04:53.976806"}, {"step": 72000, "num_env_steps": 72000, "scores": {"n": 1, "mean": 3.1917e+03}, "actor_loss": {"n": 1, "mean": -1.1541e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0282e+00}, "critic_loss": {"n": 1, "mean": 9.5124e+00}, "entropy_coef": {"n": 1, "mean": 5.4213e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7071e-01, "std": 2.7308e-01, "min_value": 1.2229e-03, "max_value": 9.9992e-01}, "num_gradient_steps": 62000, "step_time": 9.4896e+00, "total_time": 5.9156e+02, "__timestamp": "2024-10-09 11:05:03.467445"}, {"step": 73000, "num_env_steps": 73000, "scores": {"n": 1, "mean": 3.3044e+03}, "actor_loss": {"n": 1, "mean": -1.2268e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.6374e-01}, "critic_loss": {"n": 1, "mean": 9.7618e+00}, "entropy_coef": {"n": 1, "mean": 5.4237e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6190e-01, "std": 2.7563e-01, "min_value": 1.9476e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 63000, "step_time": 9.1601e+00, "total_time": 6.0072e+02, "__timestamp": "2024-10-09 11:05:12.626544"}, {"step": 74000, "num_env_steps": 74000, "scores": {"n": 1, "mean": 3.2358e+03}, "actor_loss": {"n": 1, "mean": -1.2261e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.8087e-01}, "critic_loss": {"n": 1, "mean": 8.6623e+00}, "entropy_coef": {"n": 1, "mean": 5.5042e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7203e-01, "std": 2.7231e-01, "min_value": 7.9840e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 64000, "step_time": 9.4811e+00, "total_time": 6.1020e+02, "__timestamp": "2024-10-09 11:05:22.107622"}, {"step": 75000, "num_env_steps": 75000, "scores": {"n": 1, "mean": 3.2821e+03}, "actor_loss": {"n": 1, "mean": -1.2185e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0740e+00}, "critic_loss": {"n": 1, "mean": 7.9889e+00}, "entropy_coef": {"n": 1, "mean": 5.6359e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7695e-01, "std": 2.6685e-01, "min_value": 4.2915e-05, "max_value": 9.9980e-01}, "num_gradient_steps": 65000, "step_time": 9.8179e+00, "total_time": 6.2001e+02, "__timestamp": "2024-10-09 11:05:31.926570"}, {"step": 76000, "num_env_steps": 76000, "scores": {"n": 1, "mean": 3.1821e+03}, "actor_loss": {"n": 1, "mean": -1.1903e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.3562e-01}, "critic_loss": {"n": 1, "mean": 7.6916e+00}, "entropy_coef": {"n": 1, "mean": 5.8995e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7130e-01, "std": 2.7209e-01, "min_value": 2.1327e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 66000, "step_time": 9.8042e+00, "total_time": 6.2982e+02, "__timestamp": "2024-10-09 11:05:41.730755"}, {"step": 77000, "num_env_steps": 77000, "scores": {"n": 1, "mean": 3.4406e+03}, "actor_loss": {"n": 1, "mean": -1.3081e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.9593e-01}, "critic_loss": {"n": 1, "mean": 1.1440e+01}, "entropy_coef": {"n": 1, "mean": 6.0498e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6896e-01, "std": 2.6944e-01, "min_value": 5.8013e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 67000, "step_time": 9.7563e+00, "total_time": 6.3957e+02, "__timestamp": "2024-10-09 11:05:51.487068"}, {"step": 78000, "num_env_steps": 78000, "scores": {"n": 1, "mean": 3.5205e+03}, "actor_loss": {"n": 1, "mean": -1.3033e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.4432e-01}, "critic_loss": {"n": 1, "mean": 1.0152e+02}, "entropy_coef": {"n": 1, "mean": 6.0451e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6777e-01, "std": 2.7305e-01, "min_value": 8.6546e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 68000, "step_time": 9.7436e+00, "total_time": 6.4932e+02, "__timestamp": "2024-10-09 11:06:01.230701"}, {"step": 79000, "num_env_steps": 79000, "scores": {"n": 1, "mean": 3.3113e+03}, "actor_loss": {"n": 1, "mean": -1.3293e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.9865e-01}, "critic_loss": {"n": 1, "mean": 7.1351e+00}, "entropy_coef": {"n": 1, "mean": 6.1037e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6352e-01, "std": 2.7673e-01, "min_value": 2.0772e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 69000, "step_time": 9.7609e+00, "total_time": 6.5908e+02, "__timestamp": "2024-10-09 11:06:10.991619"}, {"step": 80000, "num_env_steps": 80000, "scores": {"n": 1, "mean": 3.1637e+03}, "actor_loss": {"n": 1, "mean": -1.3673e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1322e+00}, "critic_loss": {"n": 1, "mean": 8.6873e+00}, "entropy_coef": {"n": 1, "mean": 6.2817e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6820e-01, "std": 2.7071e-01, "min_value": 2.3247e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 70000, "step_time": 9.9675e+00, "total_time": 6.6905e+02, "__timestamp": "2024-10-09 11:06:20.958089"}, {"step": 81000, "num_env_steps": 81000, "scores": {"n": 1, "mean": 3.2751e+03}, "actor_loss": {"n": 1, "mean": -1.3866e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2751e-01}, "critic_loss": {"n": 1, "mean": 8.3170e+00}, "entropy_coef": {"n": 1, "mean": 6.2843e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6537e-01, "std": 2.7285e-01, "min_value": 2.7920e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 71000, "step_time": 9.6481e+00, "total_time": 6.7869e+02, "__timestamp": "2024-10-09 11:06:30.607148"}, {"step": 82000, "num_env_steps": 82000, "scores": {"n": 1, "mean": 3.1632e+03}, "actor_loss": {"n": 1, "mean": -1.3443e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.4824e-02}, "critic_loss": {"n": 1, "mean": 1.4480e+02}, "entropy_coef": {"n": 1, "mean": 6.3865e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6239e-01, "std": 2.7664e-01, "min_value": 2.0957e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 72000, "step_time": 9.1292e+00, "total_time": 6.8782e+02, "__timestamp": "2024-10-09 11:06:39.735347"}, {"step": 83000, "num_env_steps": 83000, "scores": {"n": 1, "mean": 3.3573e+03}, "actor_loss": {"n": 1, "mean": -1.4024e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0814e-01}, "critic_loss": {"n": 1, "mean": 8.5096e+00}, "entropy_coef": {"n": 1, "mean": 6.4781e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6650e-01, "std": 2.7141e-01, "min_value": 1.2133e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 73000, "step_time": 9.2618e+00, "total_time": 6.9709e+02, "__timestamp": "2024-10-09 11:06:48.998163"}, {"step": 84000, "num_env_steps": 84000, "scores": {"n": 1, "mean": 2.2979e+02}, "actor_loss": {"n": 1, "mean": -1.4205e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.4052e-01}, "critic_loss": {"n": 1, "mean": 8.8743e+00}, "entropy_coef": {"n": 1, "mean": 6.4823e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.9956e-01, "std": 2.9597e-01, "min_value": 2.2376e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 74000, "step_time": 9.1099e+00, "total_time": 7.0620e+02, "__timestamp": "2024-10-09 11:06:58.108069"}, {"step": 85000, "num_env_steps": 85000, "scores": {"n": 1, "mean": 3.2680e+03}, "actor_loss": {"n": 1, "mean": -1.4499e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0918e+00}, "critic_loss": {"n": 1, "mean": 8.8526e+00}, "entropy_coef": {"n": 1, "mean": 6.4215e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6042e-01, "std": 2.7635e-01, "min_value": 3.1769e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 75000, "step_time": 9.3940e+00, "total_time": 7.1559e+02, "__timestamp": "2024-10-09 11:07:07.501040"}, {"step": 86000, "num_env_steps": 86000, "scores": {"n": 1, "mean": 3.4372e+03}, "actor_loss": {"n": 1, "mean": -1.4786e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.5916e-01}, "critic_loss": {"n": 1, "mean": 8.9718e+00}, "entropy_coef": {"n": 1, "mean": 6.5231e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7271e-01, "std": 2.7275e-01, "min_value": 5.3987e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 76000, "step_time": 9.5256e+00, "total_time": 7.2512e+02, "__timestamp": "2024-10-09 11:07:17.026625"}, {"step": 87000, "num_env_steps": 87000, "scores": {"n": 1, "mean": 3.2619e+03}, "actor_loss": {"n": 1, "mean": -1.4846e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0377e+00}, "critic_loss": {"n": 1, "mean": 9.5828e+00}, "entropy_coef": {"n": 1, "mean": 6.5003e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6994e-01, "std": 2.7096e-01, "min_value": 3.0398e-05, "max_value": 9.9979e-01}, "num_gradient_steps": 77000, "step_time": 9.4117e+00, "total_time": 7.3453e+02, "__timestamp": "2024-10-09 11:07:26.439340"}, {"step": 88000, "num_env_steps": 88000, "scores": {"n": 1, "mean": 3.3931e+03}, "actor_loss": {"n": 1, "mean": -1.4760e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.2184e-01}, "critic_loss": {"n": 1, "mean": 1.0258e+01}, "entropy_coef": {"n": 1, "mean": 6.4898e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7623e-01, "std": 2.6730e-01, "min_value": 4.1842e-05, "max_value": 9.9994e-01}, "num_gradient_steps": 78000, "step_time": 9.9492e+00, "total_time": 7.4448e+02, "__timestamp": "2024-10-09 11:07:36.387542"}, {"step": 89000, "num_env_steps": 89000, "scores": {"n": 1, "mean": 3.3946e+03}, "actor_loss": {"n": 1, "mean": -1.4251e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.0846e-02}, "critic_loss": {"n": 1, "mean": 1.6399e+02}, "entropy_coef": {"n": 1, "mean": 6.6393e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7502e-01, "std": 2.6888e-01, "min_value": 4.5782e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 79000, "step_time": 1.0162e+01, "total_time": 7.5464e+02, "__timestamp": "2024-10-09 11:07:46.550607"}, {"step": 90000, "num_env_steps": 90000, "scores": {"n": 1, "mean": 3.4810e+03}, "actor_loss": {"n": 1, "mean": -1.5313e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.1361e+00}, "critic_loss": {"n": 1, "mean": 1.2560e+01}, "entropy_coef": {"n": 1, "mean": 6.5991e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6535e-01, "std": 2.7561e-01, "min_value": 1.5270e-03, "max_value": 9.9983e-01}, "num_gradient_steps": 80000, "step_time": 9.8222e+00, "total_time": 7.6446e+02, "__timestamp": "2024-10-09 11:07:56.371802"}, {"step": 91000, "num_env_steps": 91000, "scores": {"n": 1, "mean": 3.6104e+03}, "actor_loss": {"n": 1, "mean": -1.5923e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3209e+00}, "critic_loss": {"n": 1, "mean": 9.6650e+00}, "entropy_coef": {"n": 1, "mean": 6.5854e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7048e-01, "std": 2.7283e-01, "min_value": 2.3237e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 81000, "step_time": 9.4767e+00, "total_time": 7.7394e+02, "__timestamp": "2024-10-09 11:08:05.848516"}, {"step": 92000, "num_env_steps": 92000, "scores": {"n": 1, "mean": 3.3375e+03}, "actor_loss": {"n": 1, "mean": -1.4601e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.3917e-02}, "critic_loss": {"n": 1, "mean": 9.4217e+00}, "entropy_coef": {"n": 1, "mean": 6.8119e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6006e-01, "std": 2.7598e-01, "min_value": 3.6460e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 82000, "step_time": 9.6974e+00, "total_time": 7.8363e+02, "__timestamp": "2024-10-09 11:08:15.546928"}, {"step": 93000, "num_env_steps": 93000, "scores": {"n": 1, "mean": 3.6828e+03}, "actor_loss": {"n": 1, "mean": -1.6060e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.7678e+00}, "critic_loss": {"n": 1, "mean": 8.7278e+00}, "entropy_coef": {"n": 1, "mean": 6.8837e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7913e-01, "std": 2.6983e-01, "min_value": 5.1898e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 83000, "step_time": 9.7592e+00, "total_time": 7.9339e+02, "__timestamp": "2024-10-09 11:08:25.306083"}, {"step": 94000, "num_env_steps": 94000, "scores": {"n": 1, "mean": 3.4005e+03}, "actor_loss": {"n": 1, "mean": -1.5558e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.0080e-01}, "critic_loss": {"n": 1, "mean": 1.1057e+01}, "entropy_coef": {"n": 1, "mean": 6.8235e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6987e-01, "std": 2.7356e-01, "min_value": 2.6941e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 84000, "step_time": 9.8075e+00, "total_time": 8.0320e+02, "__timestamp": "2024-10-09 11:08:35.113563"}, {"step": 95000, "num_env_steps": 95000, "scores": {"n": 1, "mean": 3.6766e+03}, "actor_loss": {"n": 1, "mean": -1.5905e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.1241e-02}, "critic_loss": {"n": 1, "mean": 8.9175e+00}, "entropy_coef": {"n": 1, "mean": 6.8658e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7220e-01, "std": 2.7104e-01, "min_value": 1.3779e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 85000, "step_time": 9.9523e+00, "total_time": 8.1315e+02, "__timestamp": "2024-10-09 11:08:45.065900"}, {"step": 96000, "num_env_steps": 96000, "scores": {"n": 1, "mean": 3.3734e+03}, "actor_loss": {"n": 1, "mean": -1.6649e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1551e-01}, "critic_loss": {"n": 1, "mean": 8.4768e+01}, "entropy_coef": {"n": 1, "mean": 7.1059e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.5559e-01, "std": 2.8235e-01, "min_value": 9.5814e-05, "max_value": 9.9991e-01}, "num_gradient_steps": 86000, "step_time": 9.6784e+00, "total_time": 8.2283e+02, "__timestamp": "2024-10-09 11:08:54.744336"}, {"step": 97000, "num_env_steps": 97000, "scores": {"n": 1, "mean": 3.5541e+03}, "actor_loss": {"n": 1, "mean": -1.6153e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.2310e+00}, "critic_loss": {"n": 1, "mean": 1.0929e+01}, "entropy_coef": {"n": 1, "mean": 7.1281e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7765e-01, "std": 2.7066e-01, "min_value": 4.3774e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 87000, "step_time": 9.1194e+00, "total_time": 8.3195e+02, "__timestamp": "2024-10-09 11:09:03.862748"}, {"step": 98000, "num_env_steps": 98000, "scores": {"n": 1, "mean": 3.4082e+03}, "actor_loss": {"n": 1, "mean": -1.5574e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.5501e-01}, "critic_loss": {"n": 1, "mean": 1.5291e+02}, "entropy_coef": {"n": 1, "mean": 7.3351e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6648e-01, "std": 2.7311e-01, "min_value": 1.7899e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 88000, "step_time": 9.0641e+00, "total_time": 8.4102e+02, "__timestamp": "2024-10-09 11:09:12.927821"}, {"step": 99000, "num_env_steps": 99000, "scores": {"n": 1, "mean": 3.5895e+03}, "actor_loss": {"n": 1, "mean": -1.6867e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.5222e-02}, "critic_loss": {"n": 1, "mean": 1.1438e+01}, "entropy_coef": {"n": 1, "mean": 7.3543e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6829e-01, "std": 2.7301e-01, "min_value": 2.9498e-04, "max_value": 9.9967e-01}, "num_gradient_steps": 89000, "step_time": 9.0693e+00, "total_time": 8.5008e+02, "__timestamp": "2024-10-09 11:09:21.996098"}, {"step": 100000, "num_env_steps": 100000, "scores": {"n": 1, "mean": 3.6586e+03}, "actor_loss": {"n": 1, "mean": -1.6191e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.7287e-01}, "critic_loss": {"n": 1, "mean": 1.3720e+01}, "entropy_coef": {"n": 1, "mean": 7.3300e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6963e-01, "std": 2.7309e-01, "min_value": 1.7490e-04, "max_value": 9.9973e-01}, "num_gradient_steps": 90000, "step_time": 9.0930e+00, "total_time": 8.5918e+02, "__timestamp": "2024-10-09 11:09:31.090084"}, {"step": 101000, "num_env_steps": 101000, "scores": {"n": 1, "mean": 3.6968e+03}, "actor_loss": {"n": 1, "mean": -1.5522e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.0315e-03}, "critic_loss": {"n": 1, "mean": 1.0247e+01}, "entropy_coef": {"n": 1, "mean": 7.5610e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7456e-01, "std": 2.7118e-01, "min_value": 9.6595e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 91000, "step_time": 9.0662e+00, "total_time": 8.6824e+02, "__timestamp": "2024-10-09 11:09:40.156251"}, {"step": 102000, "num_env_steps": 102000, "scores": {"n": 1, "mean": 3.6704e+03}, "actor_loss": {"n": 1, "mean": -1.6437e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.8353e-01}, "critic_loss": {"n": 1, "mean": 9.2275e+00}, "entropy_coef": {"n": 1, "mean": 7.5794e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7171e-01, "std": 2.7409e-01, "min_value": 3.0547e-05, "max_value": 9.9972e-01}, "num_gradient_steps": 92000, "step_time": 9.0859e+00, "total_time": 8.7733e+02, "__timestamp": "2024-10-09 11:09:49.242190"}, {"step": 103000, "num_env_steps": 103000, "scores": {"n": 1, "mean": 3.6019e+03}, "actor_loss": {"n": 1, "mean": -1.6008e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5057e+00}, "critic_loss": {"n": 1, "mean": 1.1609e+01}, "entropy_coef": {"n": 1, "mean": 7.6043e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7272e-01, "std": 2.7259e-01, "min_value": 7.9504e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 93000, "step_time": 9.1124e+00, "total_time": 8.8644e+02, "__timestamp": "2024-10-09 11:09:58.353573"}, {"step": 104000, "num_env_steps": 104000, "scores": {"n": 1, "mean": 3.6072e+03}, "actor_loss": {"n": 1, "mean": -1.7068e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.0944e-02}, "critic_loss": {"n": 1, "mean": 1.2580e+01}, "entropy_coef": {"n": 1, "mean": 7.5615e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6759e-01, "std": 2.7528e-01, "min_value": 6.7345e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 94000, "step_time": 9.0680e+00, "total_time": 8.9551e+02, "__timestamp": "2024-10-09 11:10:07.422618"}, {"step": 105000, "num_env_steps": 105000, "scores": {"n": 1, "mean": 3.6184e+03}, "actor_loss": {"n": 1, "mean": -1.6091e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.0232e-01}, "critic_loss": {"n": 1, "mean": 1.2323e+01}, "entropy_coef": {"n": 1, "mean": 7.7094e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7099e-01, "std": 2.7141e-01, "min_value": 1.0057e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 95000, "step_time": 9.1622e+00, "total_time": 9.0467e+02, "__timestamp": "2024-10-09 11:10:16.583816"}, {"step": 106000, "num_env_steps": 106000, "scores": {"n": 1, "mean": 3.3570e+03}, "actor_loss": {"n": 1, "mean": -1.6420e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.0263e-01}, "critic_loss": {"n": 1, "mean": 1.6130e+02}, "entropy_coef": {"n": 1, "mean": 8.0122e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6058e-01, "std": 2.7684e-01, "min_value": 7.0184e-05, "max_value": 9.9993e-01}, "num_gradient_steps": 96000, "step_time": 9.0073e+00, "total_time": 9.1368e+02, "__timestamp": "2024-10-09 11:10:25.592137"}, {"step": 107000, "num_env_steps": 107000, "scores": {"n": 1, "mean": 3.7250e+03}, "actor_loss": {"n": 1, "mean": -1.6911e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.7327e-01}, "critic_loss": {"n": 1, "mean": 9.0051e+00}, "entropy_coef": {"n": 1, "mean": 7.8920e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8369e-01, "std": 2.6454e-01, "min_value": 2.3567e-03, "max_value": 9.9982e-01}, "num_gradient_steps": 97000, "step_time": 9.1562e+00, "total_time": 9.2284e+02, "__timestamp": "2024-10-09 11:10:34.748290"}, {"step": 108000, "num_env_steps": 108000, "scores": {"n": 1, "mean": 3.7336e+03}, "actor_loss": {"n": 1, "mean": -1.6681e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.4828e-01}, "critic_loss": {"n": 1, "mean": 1.2470e+01}, "entropy_coef": {"n": 1, "mean": 7.7814e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7324e-01, "std": 2.7237e-01, "min_value": 9.0039e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 98000, "step_time": 9.1563e+00, "total_time": 9.3199e+02, "__timestamp": "2024-10-09 11:10:43.904583"}, {"step": 109000, "num_env_steps": 109000, "scores": {"n": 1, "mean": 3.7049e+03}, "actor_loss": {"n": 1, "mean": -1.6113e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.9606e-01}, "critic_loss": {"n": 1, "mean": 1.3621e+01}, "entropy_coef": {"n": 1, "mean": 7.6954e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7097e-01, "std": 2.7157e-01, "min_value": 4.5601e-04, "max_value": 9.9981e-01}, "num_gradient_steps": 99000, "step_time": 9.0487e+00, "total_time": 9.4104e+02, "__timestamp": "2024-10-09 11:10:52.953284"}, {"step": 110000, "num_env_steps": 110000, "scores": {"n": 1, "mean": 3.8561e+03}, "actor_loss": {"n": 1, "mean": -1.7519e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.7439e-01}, "critic_loss": {"n": 1, "mean": 2.1164e+02}, "entropy_coef": {"n": 1, "mean": 8.0205e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7708e-01, "std": 2.7250e-01, "min_value": 4.1337e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 100000, "step_time": 9.1637e+00, "total_time": 9.5020e+02, "__timestamp": "2024-10-09 11:11:02.115996"}, {"step": 111000, "num_env_steps": 111000, "scores": {"n": 1, "mean": 3.6362e+03}, "actor_loss": {"n": 1, "mean": -1.7210e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.3572e+00}, "critic_loss": {"n": 1, "mean": 1.0776e+01}, "entropy_coef": {"n": 1, "mean": 8.2351e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7194e-01, "std": 2.7346e-01, "min_value": 7.6443e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 101000, "step_time": 9.0467e+00, "total_time": 9.5925e+02, "__timestamp": "2024-10-09 11:11:11.162740"}, {"step": 112000, "num_env_steps": 112000, "scores": {"n": 1, "mean": 3.8575e+03}, "actor_loss": {"n": 1, "mean": -1.7455e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.3041e-01}, "critic_loss": {"n": 1, "mean": 1.4888e+01}, "entropy_coef": {"n": 1, "mean": 8.0753e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7919e-01, "std": 2.6810e-01, "min_value": 3.5369e-04, "max_value": 9.9950e-01}, "num_gradient_steps": 102000, "step_time": 9.0608e+00, "total_time": 9.6831e+02, "__timestamp": "2024-10-09 11:11:20.224579"}, {"step": 113000, "num_env_steps": 113000, "scores": {"n": 1, "mean": 3.7852e+03}, "actor_loss": {"n": 1, "mean": -1.7483e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.4817e-01}, "critic_loss": {"n": 1, "mean": 1.3070e+01}, "entropy_coef": {"n": 1, "mean": 8.2748e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7447e-01, "std": 2.7765e-01, "min_value": 8.5638e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 103000, "step_time": 9.1692e+00, "total_time": 9.7748e+02, "__timestamp": "2024-10-09 11:11:29.393778"}, {"step": 114000, "num_env_steps": 114000, "scores": {"n": 1, "mean": 3.9247e+03}, "actor_loss": {"n": 1, "mean": -1.7256e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0626e+00}, "critic_loss": {"n": 1, "mean": 9.8909e+00}, "entropy_coef": {"n": 1, "mean": 8.2041e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7100e-01, "std": 2.7834e-01, "min_value": 6.3610e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 104000, "step_time": 9.0269e+00, "total_time": 9.8651e+02, "__timestamp": "2024-10-09 11:11:38.420685"}, {"step": 115000, "num_env_steps": 115000, "scores": {"n": 1, "mean": 3.5545e+03}, "actor_loss": {"n": 1, "mean": -1.7336e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.3000e-01}, "critic_loss": {"n": 1, "mean": 1.0850e+01}, "entropy_coef": {"n": 1, "mean": 8.3015e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6570e-01, "std": 2.7663e-01, "min_value": 1.3133e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 105000, "step_time": 9.1158e+00, "total_time": 9.9562e+02, "__timestamp": "2024-10-09 11:11:47.536516"}, {"step": 116000, "num_env_steps": 116000, "scores": {"n": 1, "mean": 3.6715e+03}, "actor_loss": {"n": 1, "mean": -1.6720e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2020e+00}, "critic_loss": {"n": 1, "mean": 5.6736e+01}, "entropy_coef": {"n": 1, "mean": 8.5284e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7164e-01, "std": 2.7121e-01, "min_value": 3.5238e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 106000, "step_time": 9.2000e+00, "total_time": 1.0048e+03, "__timestamp": "2024-10-09 11:11:56.736499"}, {"step": 117000, "num_env_steps": 117000, "scores": {"n": 1, "mean": 3.7991e+03}, "actor_loss": {"n": 1, "mean": -1.7522e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.8760e-01}, "critic_loss": {"n": 1, "mean": 9.0568e+01}, "entropy_coef": {"n": 1, "mean": 8.5925e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7755e-01, "std": 2.7103e-01, "min_value": 1.5616e-04, "max_value": 9.9979e-01}, "num_gradient_steps": 107000, "step_time": 9.6445e+00, "total_time": 1.0145e+03, "__timestamp": "2024-10-09 11:12:06.381014"}, {"step": 118000, "num_env_steps": 118000, "scores": {"n": 1, "mean": 3.9911e+03}, "actor_loss": {"n": 1, "mean": -1.7558e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.4649e-01}, "critic_loss": {"n": 1, "mean": 1.7363e+02}, "entropy_coef": {"n": 1, "mean": 8.8756e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8339e-01, "std": 2.7035e-01, "min_value": 1.3003e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 108000, "step_time": 9.2118e+00, "total_time": 1.0237e+03, "__timestamp": "2024-10-09 11:12:15.592801"}, {"step": 119000, "num_env_steps": 119000, "scores": {"n": 1, "mean": 3.9113e+03}, "actor_loss": {"n": 1, "mean": -1.7831e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.0239e-01}, "critic_loss": {"n": 1, "mean": 1.0527e+01}, "entropy_coef": {"n": 1, "mean": 8.8764e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7162e-01, "std": 2.7862e-01, "min_value": 1.0222e-05, "max_value": 9.9987e-01}, "num_gradient_steps": 109000, "step_time": 9.3956e+00, "total_time": 1.0331e+03, "__timestamp": "2024-10-09 11:12:24.988404"}, {"step": 120000, "num_env_steps": 120000, "scores": {"n": 1, "mean": 3.6757e+03}, "actor_loss": {"n": 1, "mean": -1.7697e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.3311e-01}, "critic_loss": {"n": 1, "mean": 9.7967e+00}, "entropy_coef": {"n": 1, "mean": 8.9326e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6193e-01, "std": 2.7698e-01, "min_value": 4.8448e-04, "max_value": 9.9976e-01}, "num_gradient_steps": 110000, "step_time": 9.6328e+00, "total_time": 1.0427e+03, "__timestamp": "2024-10-09 11:12:34.621234"}, {"step": 121000, "num_env_steps": 121000, "scores": {"n": 1, "mean": 3.7373e+03}, "actor_loss": {"n": 1, "mean": -1.8548e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0322e+00}, "critic_loss": {"n": 1, "mean": 1.3547e+01}, "entropy_coef": {"n": 1, "mean": 8.8145e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8063e-01, "std": 2.6650e-01, "min_value": 1.7752e-04, "max_value": 9.9971e-01}, "num_gradient_steps": 111000, "step_time": 9.6392e+00, "total_time": 1.0523e+03, "__timestamp": "2024-10-09 11:12:44.259440"}, {"step": 122000, "num_env_steps": 122000, "scores": {"n": 1, "mean": 3.8250e+03}, "actor_loss": {"n": 1, "mean": -1.8080e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.6245e-01}, "critic_loss": {"n": 1, "mean": 2.1124e+02}, "entropy_coef": {"n": 1, "mean": 9.0951e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7700e-01, "std": 2.7224e-01, "min_value": 2.4617e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 112000, "step_time": 9.7212e+00, "total_time": 1.0621e+03, "__timestamp": "2024-10-09 11:12:53.980614"}, {"step": 123000, "num_env_steps": 123000, "scores": {"n": 1, "mean": 3.7596e+03}, "actor_loss": {"n": 1, "mean": -1.8405e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.0480e-01}, "critic_loss": {"n": 1, "mean": 1.0716e+01}, "entropy_coef": {"n": 1, "mean": 9.1120e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6565e-01, "std": 2.7649e-01, "min_value": 5.9705e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 113000, "step_time": 1.0015e+01, "total_time": 1.0721e+03, "__timestamp": "2024-10-09 11:13:03.996237"}, {"step": 124000, "num_env_steps": 124000, "scores": {"n": 1, "mean": 3.8190e+03}, "actor_loss": {"n": 1, "mean": -1.8902e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2215e+00}, "critic_loss": {"n": 1, "mean": 1.0456e+01}, "entropy_coef": {"n": 1, "mean": 9.2604e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7404e-01, "std": 2.7294e-01, "min_value": 4.4253e-04, "max_value": 9.9976e-01}, "num_gradient_steps": 114000, "step_time": 9.5326e+00, "total_time": 1.0816e+03, "__timestamp": "2024-10-09 11:13:13.528864"}, {"step": 125000, "num_env_steps": 125000, "scores": {"n": 1, "mean": 3.9142e+03}, "actor_loss": {"n": 1, "mean": -1.8683e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.5057e-01}, "critic_loss": {"n": 1, "mean": 1.3388e+01}, "entropy_coef": {"n": 1, "mean": 9.2983e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7257e-01, "std": 2.7592e-01, "min_value": 2.8036e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 115000, "step_time": 9.8752e+00, "total_time": 1.0915e+03, "__timestamp": "2024-10-09 11:13:23.403031"}, {"step": 126000, "num_env_steps": 126000, "scores": {"n": 1, "mean": 3.8546e+03}, "actor_loss": {"n": 1, "mean": -1.8700e+02}, "entropy_coef_loss": {"n": 1, "mean": -7.0876e-01}, "critic_loss": {"n": 1, "mean": 1.3305e+01}, "entropy_coef": {"n": 1, "mean": 9.3655e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7607e-01, "std": 2.7076e-01, "min_value": 1.2580e-03, "max_value": 9.9973e-01}, "num_gradient_steps": 116000, "step_time": 9.9556e+00, "total_time": 1.1014e+03, "__timestamp": "2024-10-09 11:13:33.358613"}, {"step": 127000, "num_env_steps": 127000, "scores": {"n": 1, "mean": 3.9690e+03}, "actor_loss": {"n": 1, "mean": -1.8456e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.9769e-01}, "critic_loss": {"n": 1, "mean": 1.1728e+01}, "entropy_coef": {"n": 1, "mean": 9.5486e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8195e-01, "std": 2.6649e-01, "min_value": 2.0527e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 117000, "step_time": 9.0680e+00, "total_time": 1.1105e+03, "__timestamp": "2024-10-09 11:13:42.427584"}, {"step": 128000, "num_env_steps": 128000, "scores": {"n": 1, "mean": 3.8297e+03}, "actor_loss": {"n": 1, "mean": -1.8785e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.0176e-01}, "critic_loss": {"n": 1, "mean": 1.4109e+01}, "entropy_coef": {"n": 1, "mean": 9.5847e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7714e-01, "std": 2.7080e-01, "min_value": 4.5870e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 118000, "step_time": 9.1135e+00, "total_time": 1.1196e+03, "__timestamp": "2024-10-09 11:13:51.540097"}, {"step": 129000, "num_env_steps": 129000, "scores": {"n": 1, "mean": 3.8199e+03}, "actor_loss": {"n": 1, "mean": -1.8372e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.1210e-01}, "critic_loss": {"n": 1, "mean": 9.9072e+00}, "entropy_coef": {"n": 1, "mean": 9.4247e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7437e-01, "std": 2.7087e-01, "min_value": 4.0498e-04, "max_value": 9.9988e-01}, "num_gradient_steps": 119000, "step_time": 9.1118e+00, "total_time": 1.1287e+03, "__timestamp": "2024-10-09 11:14:00.651901"}, {"step": 130000, "num_env_steps": 130000, "scores": {"n": 1, "mean": 3.6572e+03}, "actor_loss": {"n": 1, "mean": -1.9299e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1796e+00}, "critic_loss": {"n": 1, "mean": 1.2479e+01}, "entropy_coef": {"n": 1, "mean": 9.5804e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6427e-01, "std": 2.7706e-01, "min_value": 2.8878e-05, "max_value": 9.9991e-01}, "num_gradient_steps": 120000, "step_time": 9.1139e+00, "total_time": 1.1379e+03, "__timestamp": "2024-10-09 11:14:09.765820"}, {"step": 131000, "num_env_steps": 131000, "scores": {"n": 1, "mean": 3.8684e+03}, "actor_loss": {"n": 1, "mean": -1.8497e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.5100e-01}, "critic_loss": {"n": 1, "mean": 1.2950e+01}, "entropy_coef": {"n": 1, "mean": 9.4400e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6387e-01, "std": 2.7522e-01, "min_value": 2.8086e-04, "max_value": 9.9984e-01}, "num_gradient_steps": 121000, "step_time": 1.0353e+01, "total_time": 1.1482e+03, "__timestamp": "2024-10-09 11:14:20.119545"}, {"step": 132000, "num_env_steps": 132000, "scores": {"n": 1, "mean": 3.9225e+03}, "actor_loss": {"n": 1, "mean": -1.8842e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.6685e-01}, "critic_loss": {"n": 1, "mean": 1.7998e+02}, "entropy_coef": {"n": 1, "mean": 9.6686e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7848e-01, "std": 2.6912e-01, "min_value": 3.9726e-05, "max_value": 9.9964e-01}, "num_gradient_steps": 122000, "step_time": 6.8400e+03, "total_time": 7.9882e+03, "__timestamp": "2024-10-09 13:08:20.130737"}, {"step": 133000, "num_env_steps": 133000, "scores": {"n": 1, "mean": 3.7560e+03}, "actor_loss": {"n": 1, "mean": -1.8471e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1790e+00}, "critic_loss": {"n": 1, "mean": 1.4612e+02}, "entropy_coef": {"n": 1, "mean": 9.6175e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7586e-01, "std": 2.6766e-01, "min_value": 2.5093e-04, "max_value": 9.9989e-01}, "num_gradient_steps": 123000, "step_time": 9.3975e+00, "total_time": 7.9976e+03, "__timestamp": "2024-10-09 13:08:29.527229"}, {"step": 134000, "num_env_steps": 134000, "scores": {"n": 1, "mean": 3.9729e+03}, "actor_loss": {"n": 1, "mean": -1.9289e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.8812e-01}, "critic_loss": {"n": 1, "mean": 1.3854e+01}, "entropy_coef": {"n": 1, "mean": 9.7822e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7037e-01, "std": 2.7389e-01, "min_value": 3.1091e-05, "max_value": 9.9964e-01}, "num_gradient_steps": 124000, "step_time": 9.7652e+00, "total_time": 8.0074e+03, "__timestamp": "2024-10-09 13:08:39.293454"}, {"step": 135000, "num_env_steps": 135000, "scores": {"n": 1, "mean": 3.9905e+03}, "actor_loss": {"n": 1, "mean": -1.8449e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.7621e-01}, "critic_loss": {"n": 1, "mean": 1.6500e+01}, "entropy_coef": {"n": 1, "mean": 1.0066e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7238e-01, "std": 2.7720e-01, "min_value": 5.1141e-05, "max_value": 9.9963e-01}, "num_gradient_steps": 125000, "step_time": 9.7551e+00, "total_time": 8.0171e+03, "__timestamp": "2024-10-09 13:08:49.047536"}, {"step": 136000, "num_env_steps": 136000, "scores": {"n": 1, "mean": 4.0143e+03}, "actor_loss": {"n": 1, "mean": -1.9640e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.0926e-01}, "critic_loss": {"n": 1, "mean": 1.9345e+02}, "entropy_coef": {"n": 1, "mean": 9.8949e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7608e-01, "std": 2.6864e-01, "min_value": 7.5435e-04, "max_value": 9.9954e-01}, "num_gradient_steps": 126000, "step_time": 1.0052e+01, "total_time": 8.0272e+03, "__timestamp": "2024-10-09 13:08:59.100048"}]}}