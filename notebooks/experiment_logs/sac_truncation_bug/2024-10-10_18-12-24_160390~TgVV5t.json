{"experiment_id": "2024-10-10_18-12-24_160390~TgVV5t", "experiment_tags": ["SACDebug", "HalfCheetah-v4", "Debug"], "start_time": "2024-10-10 18:12:24.160390", "end_time": "2024-10-10 18:34:48.972594", "end_exception": null, "hyper_parameters": {"_type": "SACDebug", "_type_fq": "__main__.SACDebug", "env": "<SingletonVectorEnv instance>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {}, "policy_parameter_count": 362256, "policy_repr": "DebugSACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (latent_pi): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (mu): Linear(in_features=256, out_features=6, bias=True)\n    (log_std): Linear(in_features=256, out_features=6, bias=True)\n  )\n  (critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_critic): ContinuousCritic(\n    (features_extractor): FlattenExtractor(\n      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (qf0): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n    (qf1): Sequential(\n      (0): Linear(in_features=23, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 9 3900X 12-Core Processor", "cores": 12, "logical_cores": 24, "speed": "3793 MHz"}, "gpu": [{"name": "NVIDIA GeForce RTX 3070", "video_processor": "NVIDIA GeForce RTX 3070", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA", "memory": "8192 MB", "memory_clock": "6800 MHz", "compute_capability": "8.6"}], "ram_speed": "3600 MHz", "ram": "64 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "from sac import init_policy, init_action_selector\nfrom stable_baselines3.common.env_util import make_vec_env\nimport stable_baselines3 as sb\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nimport gymnasium\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n\ndef create_env(render_mode: str | None):\n    return gymnasium.make(env_name, render_mode=render_mode, **env_kwargs)\n\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nsb_sac = sb.SAC(\"MlpPolicy\", env, verbose=10, learning_starts=10000, stats_window_size=1) # , seed=594371)\n\nfrom src.reinforcement_learning.core.polyak_update import polyak_update\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import ReplayBufferSamples\nfrom src.hyper_parameters import HyperParameters\nimport torch\nfrom src.reinforcement_learning.core.type_aliases import TensorObs\nfrom typing import Optional\nfrom src.reinforcement_learning.core.policies.components.feature_extractors import FeatureExtractor\nimport copy\nfrom src.console import print_warning\nfrom src.tags import Tags\nfrom src.reinforcement_learning.core.policies.components.actor import Actor\nfrom src.reinforcement_learning.core.policies.components.q_critic import QCritic\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nimport stable_baselines3 as sb\n\n\nclass DebugSACPolicy(BasePolicy):\n    \n    actor: sb.sac.policies.Actor\n\n    def __init__(\n            self,\n            actor: Actor,\n            critic: QCritic,\n            shared_feature_extractor: Optional[FeatureExtractor] = None\n    ):\n        super().__init__(actor, shared_feature_extractor)\n        self.actor = sb_sac.actor\n        self.critic = sb_sac.critic\n\n        self._build_target()\n\n        self._check_action_selector()\n        \n    @property\n    def uses_sde(self):\n        return False\n        \n    def act(self, obs: TensorObs) -> torch.Tensor:\n        return self.actor(obs, False)\n    \n    def reset_sde_noise(self, batch_size: int) -> None:\n        pass\n        \n\n    def collect_hyper_parameters(self) -> HyperParameters:\n        return {}\n\n    def collect_tags(self) -> Tags:\n        return []\n\n    def _check_action_selector(self):\n        # if not isinstance(self.actor.action_selector, (PredictedStdActionSelector, StateDependentNoiseActionSelector)):\n        #     print_warning('SAC not being used with PredictedStdAction Selector or gSDE. LogStds should be clamped!')\n        pass\n\n    def _build_target(self):\n        self.target_critic = copy.deepcopy(self.critic)\n        self.target_critic.set_training_mode(False)\n\n        self.target_shared_feature_extractor = copy.deepcopy(self.shared_feature_extractor)\n        self.target_shared_feature_extractor.set_trainable(False)\n\n    def forward(self):\n        raise NotImplementedError('forward is not used in SACPolicy')\n\n    def compute_target_values(\n            self,\n            replay_samples: ReplayBufferSamples,\n            entropy_coef: torch.Tensor,\n            gamma: float,\n    ):\n        with torch.no_grad():\n            next_observations = replay_samples.next_observations\n\n            next_actions, next_actions_log_prob = self.actor.action_log_prob(\n                self.shared_feature_extractor(next_observations)\n            )\n\n            next_q_values = torch.cat(\n                self.target_critic(self.target_shared_feature_extractor(next_observations), next_actions),\n                dim=-1\n            )\n            next_q_values, _ = torch.min(next_q_values, dim=-1, keepdim=True)\n            next_q_values = next_q_values - entropy_coef * next_actions_log_prob.reshape(-1, 1)\n\n            target_q_values = replay_samples.rewards + (1 - replay_samples.dones) * gamma * next_q_values\n\n            return target_q_values\n\n\n    def perform_polyak_update(self, tau: float):\n        polyak_update(self.critic.parameters(), self.target_critic.parameters(), tau)\n        polyak_update(\n            self.shared_feature_extractor.parameters(),\n            self.target_shared_feature_extractor.parameters(),\n            tau\n        )\n\n    def set_train_mode(self, mode: bool) -> None:\n        self.actor.set_training_mode(mode)\n        self.critic.set_training_mode(mode)\n        # Leaving target_critic on train_mode = False\n\n        self.shared_feature_extractor.set_train_mode(mode)\n        # Leaving target_shared_feature_extractor on train_mode = False\n\n        self.train_mode = mode\n\nfrom src.reinforcement_learning.algorithms.sac.sac import SACLoggingConfig, SAC\nfrom dataclasses import dataclass\nfrom typing import Type, Optional, Any, Literal\n\nimport gymnasium\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom src.function_types import TorchTensorFn\nfrom src.module_analysis import calculate_grad_norm\nfrom src.hyper_parameters import HyperParameters\nfrom src.reinforcement_learning.algorithms.base.base_algorithm import PolicyProvider\nfrom src.reinforcement_learning.algorithms.base.off_policy_algorithm import OffPolicyAlgorithm, ReplayBuf\nfrom src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\nfrom src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\nfrom src.reinforcement_learning.core.action_noise import ActionNoise\nfrom src.reinforcement_learning.core.buffers.replay.base_replay_buffer import BaseReplayBuffer, ReplayBufferSamples\nfrom src.reinforcement_learning.core.buffers.replay.replay_buffer import ReplayBuffer\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.core.infos import InfoDict, concat_infos\nfrom src.reinforcement_learning.core.logging import LoggingConfig, log_if_enabled\nfrom src.reinforcement_learning.core.loss_config import weigh_and_reduce_loss, LossLoggingConfig\nfrom src.reinforcement_learning.core.type_aliases import OptimizerProvider, TensorObs, detach_obs\nfrom src.reinforcement_learning.gym.env_analysis import get_single_action_space\nfrom src.tags import Tags\nfrom src.torch_device import TorchDevice\nfrom src.torch_functions import identity\nfrom src.repr_utils import func_repr\n\nfrom typing import Literal\n\nSAC_DEFAULT_OPTIMIZER_PROVIDER = lambda params: optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nAUTO_TARGET_ENTROPY = 'auto'\n\n\nclass SACDebug(SAC):\n    \n    @property\n    def replay_buffer(self):\n        return self.buffer\n\n    buffer: BaseReplayBuffer\n    target_entropy: float\n    log_ent_coef: Optional[torch.Tensor]\n    entropy_coef_optimizer: Optional[optim.Optimizer]\n    entropy_coef_tensor: Optional[torch.Tensor]\n    \n    def collect_hyper_parameters(self) -> HyperParameters:\n        print(f'{type(self.policy) = }, {type(self.policy.actor) = }, {type(self.policy.critic) = }, {type(self.policy.target_critic) = }, {type(self.buffer) = }')\n        return super().collect_hyper_parameters()\n    \n    \n    def _setup_entropy_optimization(\n            self,\n            entropy_coef: float,\n            target_entropy: float | Literal['auto'],\n            entropy_coef_optimizer_provider: Optional[OptimizerProvider],\n    ):\n        if target_entropy == 'auto':\n            self.target_entropy = float(-np.prod(get_single_action_space(self.env).shape).astype(np.float32))\n        else:\n            self.target_entropy = float(target_entropy)\n\n        if entropy_coef_optimizer_provider is not None:\n            self.log_ent_coef = torch.log(\n                torch.tensor([entropy_coef], device=self.torch_device, dtype=self.torch_dtype)\n            ).requires_grad_(True)\n            self.entropy_coef_optimizer = entropy_coef_optimizer_provider([self.log_ent_coef])\n            self.entropy_coef_tensor = None\n        else:\n            self.log_ent_coef = None\n            self.entropy_coef_optimizer = None\n            self.entropy_coef_tensor = torch.tensor(entropy_coef, device=self.torch_device, dtype=self.torch_dtype)\n\n    # def get_and_optimize_entropy_coef(\n    #         self,\n    #         actions_pi_log_prob: torch.Tensor,\n    #         info: InfoDict\n    # ) -> torch.Tensor:\n    #     if self.entropy_coef_optimizer is not None:\n    #         entropy_coef = torch.exp(self.log_ent_coef.detach())\n    # \n    #         entropy_coef_loss = weigh_and_reduce_loss(\n    #             raw_loss=-self.log_ent_coef * (actions_pi_log_prob + self.target_entropy).detach(),\n    #             weigh_and_reduce_function=self.weigh_and_reduce_entropy_coef_loss,\n    #             info=info,\n    #             loss_name='entropy_coef_loss',\n    #             logging_config=self.logging_config.entropy_coef_loss\n    #         )\n    #         self.entropy_coef_optimizer.zero_grad()\n    #         entropy_coef_loss.backward()\n    #         self.entropy_coef_optimizer.step()\n    # \n    #         return entropy_coef\n    #     else:\n    #         return self.entropy_coef_tensor\n    # \n    # def calculate_critic_loss(\n    #         self,\n    #         observation_features: TensorObs,\n    #         replay_samples: ReplayBufferSamples,\n    #         entropy_coef: torch.Tensor,\n    #         info: InfoDict,\n    # ):\n    #     target_q_values = self.policy.compute_target_values(\n    #         replay_samples=replay_samples,\n    #         entropy_coef=entropy_coef,\n    #         gamma=self.gamma,\n    #     )\n    #     # critic loss should not influence shared feature extractor\n    #     current_q_values = self.critic(detach_obs(observation_features), replay_samples.actions)\n    # \n    #     # noinspection PyTypeChecker\n    #     critic_loss: torch.Tensor = 0.5 * sum(\n    #         F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n    #     )\n    #     critic_loss = weigh_and_reduce_loss(\n    #         raw_loss=critic_loss,\n    #         weigh_and_reduce_function=self.weigh_critic_loss,\n    #         info=info,\n    #         loss_name='critic_loss',\n    #         logging_config=self.logging_config.critic_loss,\n    #     )\n    #     return critic_loss\n    # \n    # def calculate_actor_loss(\n    #         self,\n    #         observation_features: TensorObs,\n    #         actions_pi: torch.Tensor,\n    #         actions_pi_log_prob: torch.Tensor,\n    #         entropy_coef: torch.Tensor,\n    #         info: InfoDict,\n    # ) -> torch.Tensor:\n    #     q_values_pi = torch.cat(self.critic(observation_features, actions_pi), dim=-1)\n    #     min_q_values_pi, _ = torch.min(q_values_pi, dim=-1, keepdim=True)\n    #     actor_loss = entropy_coef * actions_pi_log_prob - min_q_values_pi\n    # \n    #     actor_loss = weigh_and_reduce_loss(\n    #         raw_loss=actor_loss,\n    #         weigh_and_reduce_function=self.weigh_and_reduce_actor_loss,\n    #         info=info,\n    #         loss_name='actor_loss',\n    #         logging_config=self.logging_config.actor_loss,\n    #     )\n    # \n    #     return actor_loss\n\n    def optimize(self, last_obs: np.ndarray, last_episode_starts: np.ndarray, info: InfoDict) -> None:\n        ent_coef_losses, ent_coefs = [], []\n        actor_losses, critic_losses = [], []\n\n        for gradient_step in range(self.gradient_steps):\n            # Sample replay buffer\n            replay_data = self.replay_buffer.sample(self.optimization_batch_size, env=None)  # type: ignore[union-attr]\n\n            # We need to sample because `log_std` may have changed between two gradient steps\n            # if self.sde_noise_sample_freq:\n            #     self.actor.reset_noise()\n\n            # Action by the current actor for the sampled state\n            actions_pi, log_prob = self.actor.action_log_prob(replay_data.observations)\n            log_prob = log_prob.reshape(-1, 1)\n\n            ent_coef_loss = None\n            if self.entropy_coef_optimizer is not None and self.log_ent_coef is not None:\n                # Important: detach the variable from the graph\n                # so we don't change it with other losses\n                # see https://github.com/rail-berkeley/softlearning/issues/60\n                ent_coef = torch.exp(self.log_ent_coef.detach())\n                ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n                ent_coef_losses.append(ent_coef_loss.item())\n            else:\n                ent_coef = self.entropy_coef_tensor\n\n            ent_coefs.append(ent_coef.item())\n\n            # Optimize entropy coefficient, also called\n            # entropy temperature or alpha in the paper\n            if ent_coef_loss is not None and self.entropy_coef_optimizer is not None:\n                self.entropy_coef_optimizer.zero_grad()\n                ent_coef_loss.backward()\n                self.entropy_coef_optimizer.step()\n\n            with torch.no_grad():\n                # Select action according to policy\n                next_actions, next_log_prob = self.actor.action_log_prob(replay_data.next_observations)\n                # Compute the next Q values: min over all critics targets\n                next_q_values = torch.cat(self.policy.target_critic(replay_data.next_observations, next_actions), dim=1)\n                next_q_values, _ = torch.min(next_q_values, dim=1, keepdim=True)\n                # add entropy term\n                next_q_values = next_q_values - ent_coef * next_log_prob.reshape(-1, 1)\n                # td error + entropy term\n                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n            # Get current Q-values estimates for each critic network\n            # using action from the replay buffer\n            current_q_values = self.critic(replay_data.observations, replay_data.actions)\n\n            # Compute critic loss\n            critic_loss = 0.5 * sum(F.mse_loss(current_q, target_q_values) for current_q in current_q_values)\n            assert isinstance(critic_loss, torch.Tensor)  # for type checker\n            critic_losses.append(critic_loss.item())  # type: ignore[union-attr]\n\n            # Optimize the critic\n            self.critic.optimizer.zero_grad()\n            critic_loss.backward()\n            self.critic.optimizer.step()\n\n            # Compute actor loss\n            # Alternative: actor_loss = torch.mean(log_prob - qf1_pi)\n            # Min over all critic networks\n            q_values_pi = torch.cat(self.critic(replay_data.observations, actions_pi), dim=1)\n            min_qf_pi, _ = torch.min(q_values_pi, dim=1, keepdim=True)\n            actor_loss = (ent_coef * log_prob - min_qf_pi).mean()\n            actor_losses.append(actor_loss.item())\n\n            # Optimize the actor\n            self.actor.optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor.optimizer.step()\n\n            # Update target networks\n            if gradient_step % self.target_update_interval == 0:\n                polyak_update(self.critic.parameters(), self.policy.target_critic.parameters(), self.tau)\n                # Copy running stats, see GH issue #996\n        \n        info['entropy_coef'] = np.array(ent_coefs)\n        info['final_entropy_coef_loss'] = np.array(ent_coef_losses)\n        info['final_actor_loss'] = np.array(actor_losses)\n        info['final_critic_loss'] = np.array(critic_losses)\n\nimport inspect\nimport time\n\nfrom gymnasium import Env\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.module_analysis import count_parameters\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import PolicyConstruction\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom typing import Any\nfrom src.reinforcement_learning.core.callback import Callback\n\nimport torch\nfrom torch import optim\nimport gymnasium as gym\nimport numpy as np\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-4] + '\\n\\n' + _ih[-3] + '\\n\\n' + _ih[-2] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    # episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    # \n    # if len(episode_scores) > 0:\n    # \n    #     global best_iteration_score\n    #     iteration_score = episode_scores.mean()\n    #     score_moving_average = score_mean_ema.update(iteration_score)\n    #     if iteration_score >= best_iteration_score:\n    #         best_iteration_score = iteration_score\n    #         policy_db.save_model_state_dict(\n    #             model_id=policy_id,\n    #             parent_model_id=parent_policy_id,\n    #             model_info={\n    #                 'score': iteration_score.item(),\n    #                 'steps_trained': steps_trained,\n    #                 'wrap_env_source_code': wrap_env_source_code_source,\n    #                 'init_policy_source_code': init_policy_source\n    #             },\n    #             model=policy,\n    #             optimizer=optimizer,\n    #         )\n    #     info['score_moving_average'] = score_moving_average\n    # \n    # info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    # TODO!!\n    # tail_indices = rl.buffer.tail_indices(1000)\n    \n    # episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    tail_indices = np.arange(rl.buffer.pos - 1000, rl.buffer.pos)\n    episode_scores = rl.buffer.rewards[tail_indices].sum(axis=0)\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          f\"{critic_loss = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SACDebug(\n            env=wrapped_env,\n            policy=DebugSACPolicy(policy.actor, policy.critic),\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        \n        # Todo!\n        algo.buffer = sb_sac.replay_buffer\n        algo.buffer.to_torch = lambda arr: torch.tensor(arr, device='cuda', dtype=torch.float32)\n        \n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags() + ['Debug'],\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -1.1407e+02}, "actor_loss": {"n": 1, "mean": -1.7977e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.9702e+00}, "critic_loss": {"n": 1, "mean": 9.7906e-01}, "entropy_coef": {"n": 1, "mean": 7.4129e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.2618e-01, "std": 2.8712e-01, "min_value": 9.5144e-05, "max_value": 9.9942e-01}, "num_gradient_steps": 0, "step_time": 1.4474e+01, "total_time": 1.4450e+01, "__timestamp": "2024-10-10 18:12:38.609109"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -2.5749e+02}, "actor_loss": {"n": 1, "mean": -2.5659e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9809e+00}, "critic_loss": {"n": 1, "mean": 1.2805e+00}, "entropy_coef": {"n": 1, "mean": 5.4903e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3807e-01, "std": 2.8686e-01, "min_value": 7.3537e-06, "max_value": 9.9905e-01}, "num_gradient_steps": 0, "step_time": 1.0466e+01, "total_time": 2.4915e+01, "__timestamp": "2024-10-10 18:12:49.074826"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -1.8226e+02}, "actor_loss": {"n": 1, "mean": -3.0954e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.9047e+00}, "critic_loss": {"n": 1, "mean": 1.3603e+00}, "entropy_coef": {"n": 1, "mean": 4.0694e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.3921e-01, "std": 2.9083e-01, "min_value": 4.7523e-04, "max_value": 9.9971e-01}, "num_gradient_steps": 0, "step_time": 1.0662e+01, "total_time": 3.5577e+01, "__timestamp": "2024-10-10 18:12:59.737847"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -1.9428e+02}, "actor_loss": {"n": 1, "mean": -3.2880e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1663e+01}, "critic_loss": {"n": 1, "mean": 1.6665e+00}, "entropy_coef": {"n": 1, "mean": 3.0257e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.4246e-01, "std": 2.8956e-01, "min_value": 9.0003e-05, "max_value": 9.9934e-01}, "num_gradient_steps": 0, "step_time": 1.0578e+01, "total_time": 4.6155e+01, "__timestamp": "2024-10-10 18:13:10.314551"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -1.7838e+02}, "actor_loss": {"n": 1, "mean": -3.3887e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3403e+01}, "critic_loss": {"n": 1, "mean": 1.8690e+00}, "entropy_coef": {"n": 1, "mean": 2.2599e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.5403e-01, "std": 2.9293e-01, "min_value": 8.8811e-06, "max_value": 9.9941e-01}, "num_gradient_steps": 0, "step_time": 1.0558e+01, "total_time": 5.6713e+01, "__timestamp": "2024-10-10 18:13:20.872083"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -1.9594e+02}, "actor_loss": {"n": 1, "mean": -3.4174e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5322e+01}, "critic_loss": {"n": 1, "mean": 1.8619e+00}, "entropy_coef": {"n": 1, "mean": 1.6965e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.5800e-01, "std": 2.8975e-01, "min_value": 1.2340e-04, "max_value": 9.9874e-01}, "num_gradient_steps": 0, "step_time": 1.0608e+01, "total_time": 6.7320e+01, "__timestamp": "2024-10-10 18:13:31.480857"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -1.7072e+02}, "actor_loss": {"n": 1, "mean": -3.3333e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6187e+01}, "critic_loss": {"n": 1, "mean": 5.7923e+00}, "entropy_coef": {"n": 1, "mean": 1.2817e-01}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.7491e-01, "std": 2.9426e-01, "min_value": 1.9111e-04, "max_value": 9.9970e-01}, "num_gradient_steps": 0, "step_time": 1.0661e+01, "total_time": 7.7982e+01, "__timestamp": "2024-10-10 18:13:42.141247"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -2.3963e+02}, "actor_loss": {"n": 1, "mean": -3.3001e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6309e+01}, "critic_loss": {"n": 1, "mean": 1.9390e+00}, "entropy_coef": {"n": 1, "mean": 9.7364e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.8913e-01, "std": 2.9450e-01, "min_value": 2.4052e-04, "max_value": 9.9937e-01}, "num_gradient_steps": 0, "step_time": 1.0870e+01, "total_time": 8.8852e+01, "__timestamp": "2024-10-10 18:13:53.011563"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -2.4092e+02}, "actor_loss": {"n": 1, "mean": -3.2078e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6807e+01}, "critic_loss": {"n": 1, "mean": 1.6293e+00}, "entropy_coef": {"n": 1, "mean": 7.4209e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0175e-01, "std": 2.9667e-01, "min_value": 1.7405e-05, "max_value": 9.9966e-01}, "num_gradient_steps": 0, "step_time": 1.0494e+01, "total_time": 9.9346e+01, "__timestamp": "2024-10-10 18:14:03.505668"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -1.7510e+02}, "actor_loss": {"n": 1, "mean": -3.0116e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5107e+01}, "critic_loss": {"n": 1, "mean": 1.6950e+00}, "entropy_coef": {"n": 1, "mean": 5.6744e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9904e-01, "std": 2.9982e-01, "min_value": 1.5411e-04, "max_value": 9.9930e-01}, "num_gradient_steps": 0, "step_time": 1.0575e+01, "total_time": 1.0992e+02, "__timestamp": "2024-10-10 18:14:14.080742"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -3.0570e+02}, "actor_loss": {"n": 1, "mean": -2.8662e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6250e+01}, "critic_loss": {"n": 1, "mean": 1.6190e+00}, "entropy_coef": {"n": 1, "mean": 4.3578e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1695e-01, "std": 2.9491e-01, "min_value": 2.6447e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.2061e+01, "total_time": 1.2198e+02, "__timestamp": "2024-10-10 18:14:26.141419"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -1.5564e+02}, "actor_loss": {"n": 1, "mean": -2.7694e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5185e+01}, "critic_loss": {"n": 1, "mean": 1.5672e+00}, "entropy_coef": {"n": 1, "mean": 3.3596e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1956e-01, "std": 2.9897e-01, "min_value": 1.2101e-04, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.1309e+01, "total_time": 1.3329e+02, "__timestamp": "2024-10-10 18:14:37.451341"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -7.0223e+01}, "actor_loss": {"n": 1, "mean": -2.6557e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1742e+01}, "critic_loss": {"n": 1, "mean": 1.4392e+00}, "entropy_coef": {"n": 1, "mean": 2.6120e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4622e-01, "std": 2.9807e-01, "min_value": 2.4080e-05, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.1575e+01, "total_time": 1.4487e+02, "__timestamp": "2024-10-10 18:14:49.026059"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -1.8296e+02}, "actor_loss": {"n": 1, "mean": -2.5719e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.8638e+00}, "critic_loss": {"n": 1, "mean": 1.8838e+00}, "entropy_coef": {"n": 1, "mean": 2.0377e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.2615e-01, "std": 3.0174e-01, "min_value": 1.6436e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1674e+01, "total_time": 1.5654e+02, "__timestamp": "2024-10-10 18:15:00.698789"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": -2.0939e+02}, "actor_loss": {"n": 1, "mean": -2.4540e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.5471e+00}, "critic_loss": {"n": 1, "mean": 1.6197e+00}, "entropy_coef": {"n": 1, "mean": 1.5997e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 5.9296e-01, "std": 2.9723e-01, "min_value": 4.5486e-06, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1415e+01, "total_time": 1.6795e+02, "__timestamp": "2024-10-10 18:15:12.115169"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": 2.7737e+02}, "actor_loss": {"n": 1, "mean": -2.4205e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.1213e+00}, "critic_loss": {"n": 1, "mean": 1.7264e+00}, "entropy_coef": {"n": 1, "mean": 1.2871e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1932e-01, "std": 2.9949e-01, "min_value": 3.0991e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0915e+01, "total_time": 1.7887e+02, "__timestamp": "2024-10-10 18:15:23.029617"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": 3.8239e+02}, "actor_loss": {"n": 1, "mean": -2.2360e+01}, "entropy_coef_loss": {"n": 1, "mean": -4.5987e+00}, "critic_loss": {"n": 1, "mean": 1.8251e+00}, "entropy_coef": {"n": 1, "mean": 1.0762e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.0850e-01, "std": 3.0561e-01, "min_value": 2.8348e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1312e+01, "total_time": 1.9018e+02, "__timestamp": "2024-10-10 18:15:34.342863"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": 6.5353e+02}, "actor_loss": {"n": 1, "mean": -2.3413e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.3138e+00}, "critic_loss": {"n": 1, "mean": 1.5152e+00}, "entropy_coef": {"n": 1, "mean": 9.4527e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.4561e-01, "std": 2.9976e-01, "min_value": 2.1869e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1390e+01, "total_time": 2.0157e+02, "__timestamp": "2024-10-10 18:15:45.732952"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": 3.4301e+02}, "actor_loss": {"n": 1, "mean": -2.2963e+01}, "entropy_coef_loss": {"n": 1, "mean": 5.3065e+00}, "critic_loss": {"n": 1, "mean": 4.4572e+00}, "entropy_coef": {"n": 1, "mean": 9.0442e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.3222e-01, "std": 3.0359e-01, "min_value": 4.9690e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.1323e+01, "total_time": 2.1290e+02, "__timestamp": "2024-10-10 18:15:57.055147"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": 7.6147e+02}, "actor_loss": {"n": 1, "mean": -2.3677e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.9803e+00}, "critic_loss": {"n": 1, "mean": 1.9374e+00}, "entropy_coef": {"n": 1, "mean": 9.0300e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.3742e-01, "std": 3.0369e-01, "min_value": 4.9450e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1051e+01, "total_time": 2.2395e+02, "__timestamp": "2024-10-10 18:16:08.106401"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": 1.0320e+03}, "actor_loss": {"n": 1, "mean": -2.2580e+01}, "entropy_coef_loss": {"n": 1, "mean": 7.5872e-01}, "critic_loss": {"n": 1, "mean": 1.5493e+00}, "entropy_coef": {"n": 1, "mean": 9.3414e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.6478e-01, "std": 2.9909e-01, "min_value": 6.4506e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.2512e+01, "total_time": 2.3646e+02, "__timestamp": "2024-10-10 18:16:20.618780"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": 4.3369e+02}, "actor_loss": {"n": 1, "mean": -2.3185e+01}, "entropy_coef_loss": {"n": 1, "mean": -2.6464e+00}, "critic_loss": {"n": 1, "mean": 1.7646e+00}, "entropy_coef": {"n": 1, "mean": 9.7853e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.8951e-01, "std": 2.9759e-01, "min_value": 1.8299e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1054e+01, "total_time": 2.4751e+02, "__timestamp": "2024-10-10 18:16:31.674143"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 1.1183e+03}, "actor_loss": {"n": 1, "mean": -2.2462e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6622e+00}, "critic_loss": {"n": 1, "mean": 2.1048e+00}, "entropy_coef": {"n": 1, "mean": 9.7555e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.7097e-01, "std": 2.9949e-01, "min_value": 3.3851e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0841e+01, "total_time": 2.5835e+02, "__timestamp": "2024-10-10 18:16:42.514662"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 1.9230e+02}, "actor_loss": {"n": 1, "mean": -2.3655e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.1994e-01}, "critic_loss": {"n": 1, "mean": 1.7735e+00}, "entropy_coef": {"n": 1, "mean": 9.7546e-03}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.6027e-01, "std": 3.0052e-01, "min_value": 8.5451e-05, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0514e+01, "total_time": 2.6887e+02, "__timestamp": "2024-10-10 18:16:53.027376"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 1.5930e+03}, "actor_loss": {"n": 1, "mean": -2.5945e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5987e+00}, "critic_loss": {"n": 1, "mean": 2.4870e+00}, "entropy_coef": {"n": 1, "mean": 1.0312e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0017e-01, "std": 2.9803e-01, "min_value": 1.0729e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0551e+01, "total_time": 2.7942e+02, "__timestamp": "2024-10-10 18:17:03.577992"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 2.1349e+03}, "actor_loss": {"n": 1, "mean": -2.6625e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.8732e+00}, "critic_loss": {"n": 1, "mean": 2.5801e+00}, "entropy_coef": {"n": 1, "mean": 1.1404e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.3831e-01, "std": 2.8850e-01, "min_value": 1.2696e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0993e+01, "total_time": 2.9041e+02, "__timestamp": "2024-10-10 18:17:14.572415"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 1.4879e+03}, "actor_loss": {"n": 1, "mean": -2.7269e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6079e+00}, "critic_loss": {"n": 1, "mean": 2.5579e+00}, "entropy_coef": {"n": 1, "mean": 1.2916e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1461e-01, "std": 2.9704e-01, "min_value": 2.1318e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.1064e+01, "total_time": 3.0148e+02, "__timestamp": "2024-10-10 18:17:25.635563"}, {"step": 38000, "num_env_steps": 38000, "scores": {"n": 1, "mean": 2.3835e+03}, "actor_loss": {"n": 1, "mean": -3.0828e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.9957e+00}, "critic_loss": {"n": 1, "mean": 2.7671e+00}, "entropy_coef": {"n": 1, "mean": 1.4694e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6641e-01, "std": 2.8039e-01, "min_value": 2.4266e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0952e+01, "total_time": 3.1243e+02, "__timestamp": "2024-10-10 18:17:36.587518"}, {"step": 39000, "num_env_steps": 39000, "scores": {"n": 1, "mean": 2.4142e+03}, "actor_loss": {"n": 1, "mean": -3.3492e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.8234e+00}, "critic_loss": {"n": 1, "mean": 3.3451e+00}, "entropy_coef": {"n": 1, "mean": 1.6798e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7373e-01, "std": 2.7640e-01, "min_value": 4.0621e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0723e+01, "total_time": 3.2315e+02, "__timestamp": "2024-10-10 18:17:47.310640"}, {"step": 40000, "num_env_steps": 40000, "scores": {"n": 1, "mean": 6.4226e+02}, "actor_loss": {"n": 1, "mean": -3.6829e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.3474e+00}, "critic_loss": {"n": 1, "mean": 3.0399e+00}, "entropy_coef": {"n": 1, "mean": 1.8811e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0134e-01, "std": 2.9579e-01, "min_value": 7.2604e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0678e+01, "total_time": 3.3383e+02, "__timestamp": "2024-10-10 18:17:57.989868"}, {"step": 41000, "num_env_steps": 41000, "scores": {"n": 1, "mean": 9.8974e+02}, "actor_loss": {"n": 1, "mean": -3.4920e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.0837e-01}, "critic_loss": {"n": 1, "mean": 3.8975e+00}, "entropy_coef": {"n": 1, "mean": 1.9753e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.0272e-01, "std": 2.9843e-01, "min_value": 4.0199e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0616e+01, "total_time": 3.4445e+02, "__timestamp": "2024-10-10 18:18:08.605296"}, {"step": 42000, "num_env_steps": 42000, "scores": {"n": 1, "mean": 2.6647e+03}, "actor_loss": {"n": 1, "mean": -3.9214e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.3030e+00}, "critic_loss": {"n": 1, "mean": 4.7052e+00}, "entropy_coef": {"n": 1, "mean": 2.1702e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8705e-01, "std": 2.7007e-01, "min_value": 3.1953e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0738e+01, "total_time": 3.5518e+02, "__timestamp": "2024-10-10 18:18:19.343972"}, {"step": 43000, "num_env_steps": 43000, "scores": {"n": 1, "mean": 2.7568e+03}, "actor_loss": {"n": 1, "mean": -4.2347e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.6931e+00}, "critic_loss": {"n": 1, "mean": 4.3048e+00}, "entropy_coef": {"n": 1, "mean": 2.3367e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8514e-01, "std": 2.6941e-01, "min_value": 4.2626e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0925e+01, "total_time": 3.6611e+02, "__timestamp": "2024-10-10 18:18:30.267701"}, {"step": 44000, "num_env_steps": 44000, "scores": {"n": 1, "mean": 2.2032e+03}, "actor_loss": {"n": 1, "mean": -4.4976e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.6375e-02}, "critic_loss": {"n": 1, "mean": 4.4337e+00}, "entropy_coef": {"n": 1, "mean": 2.5368e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6376e-01, "std": 2.8199e-01, "min_value": 1.3374e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1002e+01, "total_time": 3.7711e+02, "__timestamp": "2024-10-10 18:18:41.270780"}, {"step": 45000, "num_env_steps": 45000, "scores": {"n": 1, "mean": 2.5548e+03}, "actor_loss": {"n": 1, "mean": -5.3417e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.2015e+00}, "critic_loss": {"n": 1, "mean": 5.4704e+00}, "entropy_coef": {"n": 1, "mean": 2.7423e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7314e-01, "std": 2.7174e-01, "min_value": 1.1012e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1273e+01, "total_time": 3.8838e+02, "__timestamp": "2024-10-10 18:18:52.543918"}, {"step": 46000, "num_env_steps": 46000, "scores": {"n": 1, "mean": 2.4606e+03}, "actor_loss": {"n": 1, "mean": -5.8821e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.4023e+00}, "critic_loss": {"n": 1, "mean": 5.5536e+00}, "entropy_coef": {"n": 1, "mean": 2.8919e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6736e-01, "std": 2.7789e-01, "min_value": 8.9586e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0782e+01, "total_time": 3.9917e+02, "__timestamp": "2024-10-10 18:19:03.326146"}, {"step": 47000, "num_env_steps": 47000, "scores": {"n": 1, "mean": 2.7175e+03}, "actor_loss": {"n": 1, "mean": -5.8296e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.4153e-01}, "critic_loss": {"n": 1, "mean": 6.1686e+00}, "entropy_coef": {"n": 1, "mean": 3.1580e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7482e-01, "std": 2.7411e-01, "min_value": 2.6584e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0500e+01, "total_time": 4.0967e+02, "__timestamp": "2024-10-10 18:19:13.825468"}, {"step": 48000, "num_env_steps": 48000, "scores": {"n": 1, "mean": 2.6787e+03}, "actor_loss": {"n": 1, "mean": -5.7271e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.3900e-01}, "critic_loss": {"n": 1, "mean": 6.4098e+00}, "entropy_coef": {"n": 1, "mean": 3.3353e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6720e-01, "std": 2.7643e-01, "min_value": 7.0846e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0574e+01, "total_time": 4.2024e+02, "__timestamp": "2024-10-10 18:19:24.400844"}, {"step": 49000, "num_env_steps": 49000, "scores": {"n": 1, "mean": 2.7943e+03}, "actor_loss": {"n": 1, "mean": -6.8852e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.2014e+00}, "critic_loss": {"n": 1, "mean": 7.0893e+00}, "entropy_coef": {"n": 1, "mean": 3.4686e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6921e-01, "std": 2.7116e-01, "min_value": 1.0128e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0789e+01, "total_time": 4.3103e+02, "__timestamp": "2024-10-10 18:19:35.190181"}, {"step": 50000, "num_env_steps": 50000, "scores": {"n": 1, "mean": 2.5303e+03}, "actor_loss": {"n": 1, "mean": -6.4868e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.4684e-01}, "critic_loss": {"n": 1, "mean": 6.9999e+00}, "entropy_coef": {"n": 1, "mean": 3.5430e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6530e-01, "std": 2.7434e-01, "min_value": 6.0013e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0578e+01, "total_time": 4.4161e+02, "__timestamp": "2024-10-10 18:19:45.768126"}, {"step": 51000, "num_env_steps": 51000, "scores": {"n": 1, "mean": 2.9589e+03}, "actor_loss": {"n": 1, "mean": -7.0571e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.2327e+00}, "critic_loss": {"n": 1, "mean": 6.6695e+00}, "entropy_coef": {"n": 1, "mean": 3.5961e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6950e-01, "std": 2.7475e-01, "min_value": 2.7439e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0756e+01, "total_time": 4.5236e+02, "__timestamp": "2024-10-10 18:19:56.522665"}, {"step": 52000, "num_env_steps": 52000, "scores": {"n": 1, "mean": 2.6233e+03}, "actor_loss": {"n": 1, "mean": -7.9195e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1618e+00}, "critic_loss": {"n": 1, "mean": 6.8221e+01}, "entropy_coef": {"n": 1, "mean": 3.8023e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4791e-01, "std": 2.8507e-01, "min_value": 1.6640e-03, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1595e+01, "total_time": 4.6396e+02, "__timestamp": "2024-10-10 18:20:08.118460"}, {"step": 53000, "num_env_steps": 53000, "scores": {"n": 1, "mean": -4.1005e+01}, "actor_loss": {"n": 1, "mean": -7.7444e+01}, "entropy_coef_loss": {"n": 1, "mean": 8.2227e-01}, "critic_loss": {"n": 1, "mean": 7.2503e+00}, "entropy_coef": {"n": 1, "mean": 3.7878e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.1918e-01, "std": 2.9518e-01, "min_value": 4.5093e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0796e+01, "total_time": 4.7475e+02, "__timestamp": "2024-10-10 18:20:18.914855"}, {"step": 54000, "num_env_steps": 54000, "scores": {"n": 1, "mean": 3.0282e+03}, "actor_loss": {"n": 1, "mean": -7.5825e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4102e+00}, "critic_loss": {"n": 1, "mean": 6.3978e+00}, "entropy_coef": {"n": 1, "mean": 3.7262e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6688e-01, "std": 2.7745e-01, "min_value": 2.8167e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0474e+01, "total_time": 4.8523e+02, "__timestamp": "2024-10-10 18:20:29.387607"}, {"step": 55000, "num_env_steps": 55000, "scores": {"n": 1, "mean": 3.0509e+03}, "actor_loss": {"n": 1, "mean": -8.2611e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3096e+00}, "critic_loss": {"n": 1, "mean": 7.3831e+00}, "entropy_coef": {"n": 1, "mean": 3.7616e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6934e-01, "std": 2.7469e-01, "min_value": 4.0665e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0665e+01, "total_time": 4.9589e+02, "__timestamp": "2024-10-10 18:20:40.052848"}, {"step": 56000, "num_env_steps": 56000, "scores": {"n": 1, "mean": 2.8192e+03}, "actor_loss": {"n": 1, "mean": -8.8713e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.5887e-01}, "critic_loss": {"n": 1, "mean": 8.7729e+00}, "entropy_coef": {"n": 1, "mean": 3.9841e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7378e-01, "std": 2.7276e-01, "min_value": 1.0496e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0593e+01, "total_time": 5.0649e+02, "__timestamp": "2024-10-10 18:20:50.646684"}, {"step": 57000, "num_env_steps": 57000, "scores": {"n": 1, "mean": 2.9993e+03}, "actor_loss": {"n": 1, "mean": -8.7090e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.0973e-01}, "critic_loss": {"n": 1, "mean": 6.8584e+00}, "entropy_coef": {"n": 1, "mean": 4.1284e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8609e-01, "std": 2.6408e-01, "min_value": 9.4524e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0971e+01, "total_time": 5.1746e+02, "__timestamp": "2024-10-10 18:21:01.617067"}, {"step": 58000, "num_env_steps": 58000, "scores": {"n": 1, "mean": 3.0530e+03}, "actor_loss": {"n": 1, "mean": -7.9771e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.0060e+00}, "critic_loss": {"n": 1, "mean": 6.2106e+00}, "entropy_coef": {"n": 1, "mean": 4.1983e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8904e-01, "std": 2.6115e-01, "min_value": 1.8503e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0515e+01, "total_time": 5.2797e+02, "__timestamp": "2024-10-10 18:21:12.131888"}, {"step": 59000, "num_env_steps": 59000, "scores": {"n": 1, "mean": 3.3200e+03}, "actor_loss": {"n": 1, "mean": -9.8978e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.1054e+00}, "critic_loss": {"n": 1, "mean": 8.8172e+00}, "entropy_coef": {"n": 1, "mean": 4.3478e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9533e-01, "std": 2.6288e-01, "min_value": 1.3675e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0675e+01, "total_time": 5.3865e+02, "__timestamp": "2024-10-10 18:21:22.807098"}, {"step": 60000, "num_env_steps": 60000, "scores": {"n": 1, "mean": 2.9823e+03}, "actor_loss": {"n": 1, "mean": -8.7500e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3448e-01}, "critic_loss": {"n": 1, "mean": 1.0214e+01}, "entropy_coef": {"n": 1, "mean": 4.4797e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9349e-01, "std": 2.6259e-01, "min_value": 3.1620e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0567e+01, "total_time": 5.4921e+02, "__timestamp": "2024-10-10 18:21:33.375329"}, {"step": 61000, "num_env_steps": 61000, "scores": {"n": 1, "mean": 3.0061e+03}, "actor_loss": {"n": 1, "mean": -9.6482e+01}, "entropy_coef_loss": {"n": 1, "mean": 1.8157e-01}, "critic_loss": {"n": 1, "mean": 7.9188e+00}, "entropy_coef": {"n": 1, "mean": 4.6132e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9239e-01, "std": 2.6212e-01, "min_value": 2.6269e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0504e+01, "total_time": 5.5972e+02, "__timestamp": "2024-10-10 18:21:43.878616"}, {"step": 62000, "num_env_steps": 62000, "scores": {"n": 1, "mean": 3.0155e+03}, "actor_loss": {"n": 1, "mean": -9.3816e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.8636e-02}, "critic_loss": {"n": 1, "mean": 6.8838e+00}, "entropy_coef": {"n": 1, "mean": 4.7530e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9120e-01, "std": 2.6475e-01, "min_value": 3.6028e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0672e+01, "total_time": 5.7039e+02, "__timestamp": "2024-10-10 18:21:54.552034"}, {"step": 63000, "num_env_steps": 63000, "scores": {"n": 1, "mean": 3.2159e+03}, "actor_loss": {"n": 1, "mean": -1.0472e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.0511e-01}, "critic_loss": {"n": 1, "mean": 7.5541e+00}, "entropy_coef": {"n": 1, "mean": 4.8274e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 8.0347e-01, "std": 2.5740e-01, "min_value": 2.8275e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0542e+01, "total_time": 5.8093e+02, "__timestamp": "2024-10-10 18:22:05.093533"}, {"step": 64000, "num_env_steps": 64000, "scores": {"n": 1, "mean": 3.1429e+03}, "actor_loss": {"n": 1, "mean": -1.0242e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.1278e-01}, "critic_loss": {"n": 1, "mean": 1.2688e+01}, "entropy_coef": {"n": 1, "mean": 4.8914e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9010e-01, "std": 2.6005e-01, "min_value": 2.2831e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0597e+01, "total_time": 5.9153e+02, "__timestamp": "2024-10-10 18:22:15.691409"}, {"step": 65000, "num_env_steps": 65000, "scores": {"n": 1, "mean": 3.3052e+03}, "actor_loss": {"n": 1, "mean": -1.0249e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.7826e-01}, "critic_loss": {"n": 1, "mean": 9.7409e+01}, "entropy_coef": {"n": 1, "mean": 5.0242e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9948e-01, "std": 2.5705e-01, "min_value": 9.4843e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0633e+01, "total_time": 6.0216e+02, "__timestamp": "2024-10-10 18:22:26.324305"}, {"step": 66000, "num_env_steps": 66000, "scores": {"n": 1, "mean": 3.2564e+03}, "actor_loss": {"n": 1, "mean": -1.0818e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.6138e-01}, "critic_loss": {"n": 1, "mean": 1.4032e+01}, "entropy_coef": {"n": 1, "mean": 5.2120e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9415e-01, "std": 2.6268e-01, "min_value": 6.6310e-05, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0587e+01, "total_time": 6.1275e+02, "__timestamp": "2024-10-10 18:22:36.911849"}, {"step": 67000, "num_env_steps": 67000, "scores": {"n": 1, "mean": 3.2218e+03}, "actor_loss": {"n": 1, "mean": -1.1578e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.7130e-01}, "critic_loss": {"n": 1, "mean": 1.1501e+01}, "entropy_coef": {"n": 1, "mean": 5.3503e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8725e-01, "std": 2.6804e-01, "min_value": 3.7571e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0722e+01, "total_time": 6.2347e+02, "__timestamp": "2024-10-10 18:22:47.632367"}, {"step": 68000, "num_env_steps": 68000, "scores": {"n": 1, "mean": 3.3095e+03}, "actor_loss": {"n": 1, "mean": -1.1537e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0748e+00}, "critic_loss": {"n": 1, "mean": 8.8213e+00}, "entropy_coef": {"n": 1, "mean": 5.6361e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9407e-01, "std": 2.6443e-01, "min_value": 1.4778e-03, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0518e+01, "total_time": 6.3399e+02, "__timestamp": "2024-10-10 18:22:58.150195"}, {"step": 69000, "num_env_steps": 69000, "scores": {"n": 1, "mean": 3.3617e+03}, "actor_loss": {"n": 1, "mean": -1.1576e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.0005e-01}, "critic_loss": {"n": 1, "mean": 1.1893e+01}, "entropy_coef": {"n": 1, "mean": 5.7348e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9221e-01, "std": 2.6381e-01, "min_value": 1.0316e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0583e+01, "total_time": 6.4457e+02, "__timestamp": "2024-10-10 18:23:08.733830"}, {"step": 70000, "num_env_steps": 70000, "scores": {"n": 1, "mean": 3.1243e+03}, "actor_loss": {"n": 1, "mean": -1.1581e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.8858e-01}, "critic_loss": {"n": 1, "mean": 1.0910e+01}, "entropy_coef": {"n": 1, "mean": 5.8119e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.9288e-01, "std": 2.6288e-01, "min_value": 1.0936e-03, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.0520e+01, "total_time": 6.5509e+02, "__timestamp": "2024-10-10 18:23:19.253737"}, {"step": 71000, "num_env_steps": 71000, "scores": {"n": 1, "mean": 3.2859e+03}, "actor_loss": {"n": 1, "mean": -1.1787e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2004e+00}, "critic_loss": {"n": 1, "mean": 1.0082e+01}, "entropy_coef": {"n": 1, "mean": 5.8696e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8415e-01, "std": 2.6796e-01, "min_value": 5.5754e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0531e+01, "total_time": 6.6562e+02, "__timestamp": "2024-10-10 18:23:29.785149"}, {"step": 72000, "num_env_steps": 72000, "scores": {"n": 1, "mean": 3.2847e+03}, "actor_loss": {"n": 1, "mean": -1.1937e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.0041e-03}, "critic_loss": {"n": 1, "mean": 7.7122e+00}, "entropy_coef": {"n": 1, "mean": 5.9780e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8279e-01, "std": 2.6573e-01, "min_value": 5.4219e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0477e+01, "total_time": 6.7610e+02, "__timestamp": "2024-10-10 18:23:40.261262"}, {"step": 73000, "num_env_steps": 73000, "scores": {"n": 1, "mean": 3.1993e+03}, "actor_loss": {"n": 1, "mean": -1.2098e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5592e+00}, "critic_loss": {"n": 1, "mean": 8.9509e+00}, "entropy_coef": {"n": 1, "mean": 6.0756e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7978e-01, "std": 2.6987e-01, "min_value": 1.1802e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0838e+01, "total_time": 6.8694e+02, "__timestamp": "2024-10-10 18:23:51.100619"}, {"step": 74000, "num_env_steps": 74000, "scores": {"n": 1, "mean": 3.2438e+03}, "actor_loss": {"n": 1, "mean": -1.1993e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.7008e-01}, "critic_loss": {"n": 1, "mean": 7.6601e+00}, "entropy_coef": {"n": 1, "mean": 6.1647e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8496e-01, "std": 2.6520e-01, "min_value": 1.3754e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1315e+01, "total_time": 6.9826e+02, "__timestamp": "2024-10-10 18:24:02.415541"}, {"step": 75000, "num_env_steps": 75000, "scores": {"n": 1, "mean": 3.2498e+03}, "actor_loss": {"n": 1, "mean": -1.3222e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.0293e-01}, "critic_loss": {"n": 1, "mean": 1.3782e+02}, "entropy_coef": {"n": 1, "mean": 6.3242e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8616e-01, "std": 2.6574e-01, "min_value": 2.3389e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.1569e+01, "total_time": 7.0982e+02, "__timestamp": "2024-10-10 18:24:13.984244"}, {"step": 76000, "num_env_steps": 76000, "scores": {"n": 1, "mean": 3.1753e+03}, "actor_loss": {"n": 1, "mean": -1.2843e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1007e+00}, "critic_loss": {"n": 1, "mean": 8.3290e+00}, "entropy_coef": {"n": 1, "mean": 6.3537e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7146e-01, "std": 2.7254e-01, "min_value": 4.0531e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0918e+01, "total_time": 7.2074e+02, "__timestamp": "2024-10-10 18:24:24.902237"}, {"step": 77000, "num_env_steps": 77000, "scores": {"n": 1, "mean": 3.5030e+03}, "actor_loss": {"n": 1, "mean": -1.2664e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5586e+00}, "critic_loss": {"n": 1, "mean": 6.5197e+01}, "entropy_coef": {"n": 1, "mean": 6.4814e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7926e-01, "std": 2.6752e-01, "min_value": 1.9741e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0864e+01, "total_time": 7.3161e+02, "__timestamp": "2024-10-10 18:24:35.766198"}, {"step": 78000, "num_env_steps": 78000, "scores": {"n": 1, "mean": 3.2972e+03}, "actor_loss": {"n": 1, "mean": -1.3130e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.3804e-01}, "critic_loss": {"n": 1, "mean": 7.9909e+00}, "entropy_coef": {"n": 1, "mean": 6.3822e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7181e-01, "std": 2.6997e-01, "min_value": 5.9605e-08, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.1048e+01, "total_time": 7.4266e+02, "__timestamp": "2024-10-10 18:24:46.814398"}, {"step": 79000, "num_env_steps": 79000, "scores": {"n": 1, "mean": 3.2980e+03}, "actor_loss": {"n": 1, "mean": -1.4228e+02}, "entropy_coef_loss": {"n": 1, "mean": 7.6723e-01}, "critic_loss": {"n": 1, "mean": 8.0677e+00}, "entropy_coef": {"n": 1, "mean": 6.5692e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7425e-01, "std": 2.6721e-01, "min_value": 2.2388e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.2021e+01, "total_time": 7.5468e+02, "__timestamp": "2024-10-10 18:24:58.836060"}, {"step": 80000, "num_env_steps": 80000, "scores": {"n": 1, "mean": 3.3579e+03}, "actor_loss": {"n": 1, "mean": -1.4072e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.1269e-01}, "critic_loss": {"n": 1, "mean": 8.0382e+00}, "entropy_coef": {"n": 1, "mean": 6.7917e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8015e-01, "std": 2.6972e-01, "min_value": 2.7329e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.2330e+01, "total_time": 7.6701e+02, "__timestamp": "2024-10-10 18:25:11.165199"}, {"step": 81000, "num_env_steps": 81000, "scores": {"n": 1, "mean": 3.3570e+03}, "actor_loss": {"n": 1, "mean": -1.3914e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0685e+00}, "critic_loss": {"n": 1, "mean": 1.2754e+02}, "entropy_coef": {"n": 1, "mean": 6.7188e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8224e-01, "std": 2.6217e-01, "min_value": 2.2569e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.1089e+01, "total_time": 7.7809e+02, "__timestamp": "2024-10-10 18:25:22.254008"}, {"step": 82000, "num_env_steps": 82000, "scores": {"n": 1, "mean": 3.4280e+03}, "actor_loss": {"n": 1, "mean": -1.3405e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.5108e-01}, "critic_loss": {"n": 1, "mean": 1.0935e+02}, "entropy_coef": {"n": 1, "mean": 6.8225e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7666e-01, "std": 2.6781e-01, "min_value": 6.1039e-04, "max_value": 9.9999e-01}, "num_gradient_steps": 0, "step_time": 1.1054e+01, "total_time": 7.8915e+02, "__timestamp": "2024-10-10 18:25:33.308582"}, {"step": 83000, "num_env_steps": 83000, "scores": {"n": 1, "mean": 3.3122e+03}, "actor_loss": {"n": 1, "mean": -1.4960e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.9092e-01}, "critic_loss": {"n": 1, "mean": 7.8608e+00}, "entropy_coef": {"n": 1, "mean": 7.0201e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7641e-01, "std": 2.6810e-01, "min_value": 9.9117e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0932e+01, "total_time": 8.0008e+02, "__timestamp": "2024-10-10 18:25:44.240565"}, {"step": 84000, "num_env_steps": 84000, "scores": {"n": 1, "mean": 3.5073e+03}, "actor_loss": {"n": 1, "mean": -1.3852e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.3237e+00}, "critic_loss": {"n": 1, "mean": 8.8502e+00}, "entropy_coef": {"n": 1, "mean": 7.0958e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8272e-01, "std": 2.6519e-01, "min_value": 3.5933e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.1843e+01, "total_time": 8.1192e+02, "__timestamp": "2024-10-10 18:25:56.083996"}, {"step": 85000, "num_env_steps": 85000, "scores": {"n": 1, "mean": 3.2052e+03}, "actor_loss": {"n": 1, "mean": -1.3586e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0307e+00}, "critic_loss": {"n": 1, "mean": 1.3594e+01}, "entropy_coef": {"n": 1, "mean": 7.1281e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6965e-01, "std": 2.7044e-01, "min_value": 2.5555e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.1560e+01, "total_time": 8.2348e+02, "__timestamp": "2024-10-10 18:26:07.643813"}, {"step": 86000, "num_env_steps": 86000, "scores": {"n": 1, "mean": 3.5358e+03}, "actor_loss": {"n": 1, "mean": -1.4658e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.1828e-01}, "critic_loss": {"n": 1, "mean": 9.8919e+00}, "entropy_coef": {"n": 1, "mean": 7.3026e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7930e-01, "std": 2.6599e-01, "min_value": 8.9735e-05, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.2204e+01, "total_time": 8.3569e+02, "__timestamp": "2024-10-10 18:26:19.847488"}, {"step": 87000, "num_env_steps": 87000, "scores": {"n": 1, "mean": 3.4956e+03}, "actor_loss": {"n": 1, "mean": -1.5489e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.2042e-01}, "critic_loss": {"n": 1, "mean": 1.1259e+01}, "entropy_coef": {"n": 1, "mean": 7.1232e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8010e-01, "std": 2.6705e-01, "min_value": 2.1052e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 0, "step_time": 1.0892e+01, "total_time": 8.4658e+02, "__timestamp": "2024-10-10 18:26:30.739539"}, {"step": 88000, "num_env_steps": 88000, "scores": {"n": 1, "mean": 3.4518e+03}, "actor_loss": {"n": 1, "mean": -1.4477e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.3533e-01}, "critic_loss": {"n": 1, "mean": 9.5408e+00}, "entropy_coef": {"n": 1, "mean": 7.1960e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7285e-01, "std": 2.7070e-01, "min_value": 2.6032e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0600e+01, "total_time": 8.5718e+02, "__timestamp": "2024-10-10 18:26:41.338620"}, {"step": 89000, "num_env_steps": 89000, "scores": {"n": 1, "mean": 3.4067e+03}, "actor_loss": {"n": 1, "mean": -1.4621e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.0975e-01}, "critic_loss": {"n": 1, "mean": 7.0325e+00}, "entropy_coef": {"n": 1, "mean": 7.3223e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8534e-01, "std": 2.6250e-01, "min_value": 9.9498e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0975e+01, "total_time": 8.6815e+02, "__timestamp": "2024-10-10 18:26:52.313167"}, {"step": 90000, "num_env_steps": 90000, "scores": {"n": 1, "mean": 3.5642e+03}, "actor_loss": {"n": 1, "mean": -1.5268e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.3996e-01}, "critic_loss": {"n": 1, "mean": 8.1274e+00}, "entropy_coef": {"n": 1, "mean": 7.4082e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7408e-01, "std": 2.6877e-01, "min_value": 2.8218e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0658e+01, "total_time": 8.7881e+02, "__timestamp": "2024-10-10 18:27:02.972431"}, {"step": 91000, "num_env_steps": 91000, "scores": {"n": 1, "mean": 3.6073e+03}, "actor_loss": {"n": 1, "mean": -1.5472e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1466e+00}, "critic_loss": {"n": 1, "mean": 1.5138e+01}, "entropy_coef": {"n": 1, "mean": 7.5113e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7831e-01, "std": 2.6532e-01, "min_value": 1.0079e-03, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.2072e+01, "total_time": 8.9088e+02, "__timestamp": "2024-10-10 18:27:15.043727"}, {"step": 92000, "num_env_steps": 92000, "scores": {"n": 1, "mean": 3.5902e+03}, "actor_loss": {"n": 1, "mean": -1.5689e+02}, "entropy_coef_loss": {"n": 1, "mean": 9.8620e-02}, "critic_loss": {"n": 1, "mean": 7.3397e+00}, "entropy_coef": {"n": 1, "mean": 7.4729e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7932e-01, "std": 2.6579e-01, "min_value": 7.0250e-04, "max_value": 9.9990e-01}, "num_gradient_steps": 0, "step_time": 1.1959e+01, "total_time": 9.0284e+02, "__timestamp": "2024-10-10 18:27:27.002375"}, {"step": 93000, "num_env_steps": 93000, "scores": {"n": 1, "mean": 2.8978e+03}, "actor_loss": {"n": 1, "mean": -1.5278e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2672e-02}, "critic_loss": {"n": 1, "mean": 8.3581e+00}, "entropy_coef": {"n": 1, "mean": 7.6000e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.4492e-01, "std": 2.8070e-01, "min_value": 7.3460e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0857e+01, "total_time": 9.1370e+02, "__timestamp": "2024-10-10 18:27:37.859091"}, {"step": 94000, "num_env_steps": 94000, "scores": {"n": 1, "mean": 3.2056e+03}, "actor_loss": {"n": 1, "mean": -1.5489e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.4308e-01}, "critic_loss": {"n": 1, "mean": 9.1591e+00}, "entropy_coef": {"n": 1, "mean": 7.7482e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7244e-01, "std": 2.7028e-01, "min_value": 6.6286e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.1452e+01, "total_time": 9.2515e+02, "__timestamp": "2024-10-10 18:27:49.310802"}, {"step": 95000, "num_env_steps": 95000, "scores": {"n": 1, "mean": -2.3088e+02}, "actor_loss": {"n": 1, "mean": -1.4623e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.1653e+00}, "critic_loss": {"n": 1, "mean": 1.0108e+01}, "entropy_coef": {"n": 1, "mean": 7.8044e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 6.1376e-01, "std": 2.9645e-01, "min_value": 2.9123e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 0, "step_time": 1.0936e+01, "total_time": 9.3609e+02, "__timestamp": "2024-10-10 18:28:00.246889"}, {"step": 96000, "num_env_steps": 96000, "scores": {"n": 1, "mean": 3.5948e+03}, "actor_loss": {"n": 1, "mean": -1.5263e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.3696e+00}, "critic_loss": {"n": 1, "mean": 1.0940e+01}, "entropy_coef": {"n": 1, "mean": 7.7748e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7098e-01, "std": 2.7339e-01, "min_value": 1.4228e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0779e+01, "total_time": 9.4687e+02, "__timestamp": "2024-10-10 18:28:11.025988"}, {"step": 97000, "num_env_steps": 97000, "scores": {"n": 1, "mean": 3.5993e+03}, "actor_loss": {"n": 1, "mean": -1.5130e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.9110e-02}, "critic_loss": {"n": 1, "mean": 9.0892e+00}, "entropy_coef": {"n": 1, "mean": 7.5980e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7911e-01, "std": 2.6597e-01, "min_value": 8.8194e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0565e+01, "total_time": 9.5743e+02, "__timestamp": "2024-10-10 18:28:21.591526"}, {"step": 98000, "num_env_steps": 98000, "scores": {"n": 1, "mean": 3.7877e+03}, "actor_loss": {"n": 1, "mean": -1.5089e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.9560e+00}, "critic_loss": {"n": 1, "mean": 8.8734e+00}, "entropy_coef": {"n": 1, "mean": 7.8612e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8527e-01, "std": 2.6326e-01, "min_value": 1.5550e-03, "max_value": 9.9981e-01}, "num_gradient_steps": 0, "step_time": 1.0619e+01, "total_time": 9.6805e+02, "__timestamp": "2024-10-10 18:28:32.210980"}, {"step": 99000, "num_env_steps": 99000, "scores": {"n": 1, "mean": 3.7613e+03}, "actor_loss": {"n": 1, "mean": -1.5454e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.5299e+00}, "critic_loss": {"n": 1, "mean": 9.9402e+00}, "entropy_coef": {"n": 1, "mean": 7.8734e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8213e-01, "std": 2.6395e-01, "min_value": 1.6838e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 0, "step_time": 1.0473e+01, "total_time": 9.7852e+02, "__timestamp": "2024-10-10 18:28:42.682680"}, {"step": 100000, "num_env_steps": 100000, "scores": {"n": 1, "mean": 3.5950e+03}, "actor_loss": {"n": 1, "mean": -1.6607e+02}, "entropy_coef_loss": {"n": 1, "mean": -5.0092e-02}, "critic_loss": {"n": 1, "mean": 1.4192e+01}, "entropy_coef": {"n": 1, "mean": 7.8779e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7491e-01, "std": 2.6897e-01, "min_value": 5.3063e-05, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0526e+01, "total_time": 9.8905e+02, "__timestamp": "2024-10-10 18:28:53.208350"}, {"step": 101000, "num_env_steps": 101000, "scores": {"n": 1, "mean": 3.5415e+03}, "actor_loss": {"n": 1, "mean": -1.6185e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.9059e-01}, "critic_loss": {"n": 1, "mean": 7.8058e+00}, "entropy_coef": {"n": 1, "mean": 7.7384e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7723e-01, "std": 2.6592e-01, "min_value": 7.4996e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0592e+01, "total_time": 9.9964e+02, "__timestamp": "2024-10-10 18:29:03.799954"}, {"step": 102000, "num_env_steps": 102000, "scores": {"n": 1, "mean": 3.5105e+03}, "actor_loss": {"n": 1, "mean": -1.6776e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.2282e-01}, "critic_loss": {"n": 1, "mean": 2.0432e+02}, "entropy_coef": {"n": 1, "mean": 7.8494e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7088e-01, "std": 2.7177e-01, "min_value": 5.3990e-04, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 1.0574e+01, "total_time": 1.0102e+03, "__timestamp": "2024-10-10 18:29:14.374312"}, {"step": 103000, "num_env_steps": 103000, "scores": {"n": 1, "mean": 3.3297e+03}, "actor_loss": {"n": 1, "mean": -1.5671e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4214e+00}, "critic_loss": {"n": 1, "mean": 1.2747e+01}, "entropy_coef": {"n": 1, "mean": 7.9460e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7112e-01, "std": 2.6976e-01, "min_value": 2.7715e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0428e+01, "total_time": 1.0206e+03, "__timestamp": "2024-10-10 18:29:24.802121"}, {"step": 104000, "num_env_steps": 104000, "scores": {"n": 1, "mean": 3.7270e+03}, "actor_loss": {"n": 1, "mean": -1.6531e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.3850e-01}, "critic_loss": {"n": 1, "mean": 1.0104e+01}, "entropy_coef": {"n": 1, "mean": 7.9689e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8197e-01, "std": 2.6718e-01, "min_value": 7.2177e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0577e+01, "total_time": 1.0312e+03, "__timestamp": "2024-10-10 18:29:35.380162"}, {"step": 105000, "num_env_steps": 105000, "scores": {"n": 1, "mean": 3.3959e+03}, "actor_loss": {"n": 1, "mean": -1.5443e+02}, "entropy_coef_loss": {"n": 1, "mean": -8.4371e-01}, "critic_loss": {"n": 1, "mean": 6.6973e+00}, "entropy_coef": {"n": 1, "mean": 7.8888e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7126e-01, "std": 2.6949e-01, "min_value": 2.0064e-04, "max_value": 9.9972e-01}, "num_gradient_steps": 0, "step_time": 1.0514e+01, "total_time": 1.0417e+03, "__timestamp": "2024-10-10 18:29:45.892961"}, {"step": 106000, "num_env_steps": 106000, "scores": {"n": 1, "mean": 3.6035e+03}, "actor_loss": {"n": 1, "mean": -1.7538e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.6732e-01}, "critic_loss": {"n": 1, "mean": 1.2721e+01}, "entropy_coef": {"n": 1, "mean": 7.8677e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7248e-01, "std": 2.6787e-01, "min_value": 4.6790e-06, "max_value": 9.9968e-01}, "num_gradient_steps": 0, "step_time": 1.0471e+01, "total_time": 1.0522e+03, "__timestamp": "2024-10-10 18:29:56.365203"}, {"step": 107000, "num_env_steps": 107000, "scores": {"n": 1, "mean": 3.6880e+03}, "actor_loss": {"n": 1, "mean": -1.6808e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.9168e-01}, "critic_loss": {"n": 1, "mean": 1.0370e+01}, "entropy_coef": {"n": 1, "mean": 8.0244e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7059e-01, "std": 2.7410e-01, "min_value": 5.8849e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.0568e+01, "total_time": 1.0628e+03, "__timestamp": "2024-10-10 18:30:06.932050"}, {"step": 108000, "num_env_steps": 108000, "scores": {"n": 1, "mean": 3.4160e+03}, "actor_loss": {"n": 1, "mean": -1.6618e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.8857e-01}, "critic_loss": {"n": 1, "mean": 1.0918e+01}, "entropy_coef": {"n": 1, "mean": 8.0664e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7007e-01, "std": 2.7178e-01, "min_value": 1.5378e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0623e+01, "total_time": 1.0734e+03, "__timestamp": "2024-10-10 18:30:17.554798"}, {"step": 109000, "num_env_steps": 109000, "scores": {"n": 1, "mean": 3.4995e+03}, "actor_loss": {"n": 1, "mean": -1.7153e+02}, "entropy_coef_loss": {"n": 1, "mean": -4.3926e-01}, "critic_loss": {"n": 1, "mean": 1.1689e+01}, "entropy_coef": {"n": 1, "mean": 8.0303e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7626e-01, "std": 2.6859e-01, "min_value": 7.0773e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0589e+01, "total_time": 1.0840e+03, "__timestamp": "2024-10-10 18:30:28.144781"}, {"step": 110000, "num_env_steps": 110000, "scores": {"n": 1, "mean": 3.8578e+03}, "actor_loss": {"n": 1, "mean": -1.7562e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.6135e-01}, "critic_loss": {"n": 1, "mean": 1.4460e+01}, "entropy_coef": {"n": 1, "mean": 8.2538e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7686e-01, "std": 2.7077e-01, "min_value": 1.7382e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.0488e+01, "total_time": 1.0945e+03, "__timestamp": "2024-10-10 18:30:38.631525"}, {"step": 111000, "num_env_steps": 111000, "scores": {"n": 1, "mean": 3.7281e+03}, "actor_loss": {"n": 1, "mean": -1.8184e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.6670e+00}, "critic_loss": {"n": 1, "mean": 1.3573e+01}, "entropy_coef": {"n": 1, "mean": 8.3210e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.8007e-01, "std": 2.6824e-01, "min_value": 1.2043e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.0500e+01, "total_time": 1.1050e+03, "__timestamp": "2024-10-10 18:30:49.132900"}, {"step": 112000, "num_env_steps": 112000, "scores": {"n": 1, "mean": 3.6834e+03}, "actor_loss": {"n": 1, "mean": -1.7284e+02}, "entropy_coef_loss": {"n": 1, "mean": 5.2211e-01}, "critic_loss": {"n": 1, "mean": 1.3282e+01}, "entropy_coef": {"n": 1, "mean": 8.5484e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7615e-01, "std": 2.6719e-01, "min_value": 5.9009e-06, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.0506e+01, "total_time": 1.1155e+03, "__timestamp": "2024-10-10 18:30:59.638237"}, {"step": 113000, "num_env_steps": 113000, "scores": {"n": 1, "mean": 3.8195e+03}, "actor_loss": {"n": 1, "mean": -1.7745e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.6751e-01}, "critic_loss": {"n": 1, "mean": 1.2512e+01}, "entropy_coef": {"n": 1, "mean": 8.5420e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7799e-01, "std": 2.6677e-01, "min_value": 3.0100e-04, "max_value": 9.9972e-01}, "num_gradient_steps": 0, "step_time": 1.0711e+01, "total_time": 1.1262e+03, "__timestamp": "2024-10-10 18:31:10.348940"}, {"step": 114000, "num_env_steps": 114000, "scores": {"n": 1, "mean": 3.5409e+03}, "actor_loss": {"n": 1, "mean": -1.7708e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.1438e+00}, "critic_loss": {"n": 1, "mean": 1.3356e+01}, "entropy_coef": {"n": 1, "mean": 8.4918e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6426e-01, "std": 2.7359e-01, "min_value": 1.5065e-03, "max_value": 9.9984e-01}, "num_gradient_steps": 0, "step_time": 1.0769e+01, "total_time": 1.1370e+03, "__timestamp": "2024-10-10 18:31:21.118909"}, {"step": 115000, "num_env_steps": 115000, "scores": {"n": 1, "mean": 3.8879e+03}, "actor_loss": {"n": 1, "mean": -1.7819e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.3559e-01}, "critic_loss": {"n": 1, "mean": 1.8967e+02}, "entropy_coef": {"n": 1, "mean": 8.6253e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7014e-01, "std": 2.7173e-01, "min_value": 6.0606e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 0, "step_time": 1.1082e+01, "total_time": 1.1480e+03, "__timestamp": "2024-10-10 18:31:32.199541"}, {"step": 116000, "num_env_steps": 116000, "scores": {"n": 1, "mean": 3.7588e+03}, "actor_loss": {"n": 1, "mean": -1.7195e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.0230e+00}, "critic_loss": {"n": 1, "mean": 1.5011e+01}, "entropy_coef": {"n": 1, "mean": 8.7382e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7494e-01, "std": 2.6842e-01, "min_value": 9.9838e-06, "max_value": 9.9982e-01}, "num_gradient_steps": 0, "step_time": 1.0610e+01, "total_time": 1.1587e+03, "__timestamp": "2024-10-10 18:31:42.809839"}, {"step": 117000, "num_env_steps": 117000, "scores": {"n": 1, "mean": 3.5891e+03}, "actor_loss": {"n": 1, "mean": -1.7872e+02}, "entropy_coef_loss": {"n": 1, "mean": 3.0676e-01}, "critic_loss": {"n": 1, "mean": 8.5648e+00}, "entropy_coef": {"n": 1, "mean": 8.6504e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7190e-01, "std": 2.6847e-01, "min_value": 9.9182e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 0, "step_time": 1.0661e+01, "total_time": 1.1693e+03, "__timestamp": "2024-10-10 18:31:53.470538"}, {"step": 118000, "num_env_steps": 118000, "scores": {"n": 1, "mean": 3.8224e+03}, "actor_loss": {"n": 1, "mean": -1.7622e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.0251e+00}, "critic_loss": {"n": 1, "mean": 1.2265e+01}, "entropy_coef": {"n": 1, "mean": 8.7080e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6746e-01, "std": 2.7622e-01, "min_value": 2.7120e-04, "max_value": 9.9974e-01}, "num_gradient_steps": 0, "step_time": 1.0540e+01, "total_time": 1.1799e+03, "__timestamp": "2024-10-10 18:32:04.010700"}, {"step": 119000, "num_env_steps": 119000, "scores": {"n": 1, "mean": 3.6943e+03}, "actor_loss": {"n": 1, "mean": -1.7546e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.6773e-01}, "critic_loss": {"n": 1, "mean": 1.2322e+01}, "entropy_coef": {"n": 1, "mean": 8.7792e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7060e-01, "std": 2.7567e-01, "min_value": 1.8057e-04, "max_value": 9.9986e-01}, "num_gradient_steps": 0, "step_time": 1.0566e+01, "total_time": 1.1904e+03, "__timestamp": "2024-10-10 18:32:14.578191"}, {"step": 120000, "num_env_steps": 120000, "scores": {"n": 1, "mean": 3.9371e+03}, "actor_loss": {"n": 1, "mean": -1.8532e+02}, "entropy_coef_loss": {"n": 1, "mean": 4.1711e-01}, "critic_loss": {"n": 1, "mean": 9.7866e+00}, "entropy_coef": {"n": 1, "mean": 8.8455e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7094e-01, "std": 2.7144e-01, "min_value": 8.1435e-04, "max_value": 9.9985e-01}, "num_gradient_steps": 0, "step_time": 1.0561e+01, "total_time": 1.2010e+03, "__timestamp": "2024-10-10 18:32:25.139226"}, {"step": 121000, "num_env_steps": 121000, "scores": {"n": 1, "mean": 3.7948e+03}, "actor_loss": {"n": 1, "mean": -1.7050e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.2200e+00}, "critic_loss": {"n": 1, "mean": 1.1676e+01}, "entropy_coef": {"n": 1, "mean": 9.1208e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6349e-01, "std": 2.7451e-01, "min_value": 1.3248e-03, "max_value": 9.9993e-01}, "num_gradient_steps": 0, "step_time": 1.0905e+01, "total_time": 1.2119e+03, "__timestamp": "2024-10-10 18:32:36.044476"}, {"step": 122000, "num_env_steps": 122000, "scores": {"n": 1, "mean": 3.5701e+03}, "actor_loss": {"n": 1, "mean": -1.8044e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.4632e-01}, "critic_loss": {"n": 1, "mean": 7.6557e+00}, "entropy_coef": {"n": 1, "mean": 8.9470e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6358e-01, "std": 2.7178e-01, "min_value": 6.1464e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.0920e+01, "total_time": 1.2228e+03, "__timestamp": "2024-10-10 18:32:46.963203"}, {"step": 123000, "num_env_steps": 123000, "scores": {"n": 1, "mean": 3.5967e+03}, "actor_loss": {"n": 1, "mean": -1.8438e+02}, "entropy_coef_loss": {"n": 1, "mean": -6.4495e-02}, "critic_loss": {"n": 1, "mean": 9.6726e+00}, "entropy_coef": {"n": 1, "mean": 9.0304e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6177e-01, "std": 2.7518e-01, "min_value": 9.1434e-05, "max_value": 9.9989e-01}, "num_gradient_steps": 0, "step_time": 1.0704e+01, "total_time": 1.2335e+03, "__timestamp": "2024-10-10 18:32:57.668320"}, {"step": 124000, "num_env_steps": 124000, "scores": {"n": 1, "mean": 3.6912e+03}, "actor_loss": {"n": 1, "mean": -1.8738e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.3256e-01}, "critic_loss": {"n": 1, "mean": 9.2734e+00}, "entropy_coef": {"n": 1, "mean": 9.1143e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7389e-01, "std": 2.6916e-01, "min_value": 4.9286e-04, "max_value": 9.9983e-01}, "num_gradient_steps": 0, "step_time": 1.0565e+01, "total_time": 1.2441e+03, "__timestamp": "2024-10-10 18:33:08.233654"}, {"step": 125000, "num_env_steps": 125000, "scores": {"n": 1, "mean": 3.4841e+03}, "actor_loss": {"n": 1, "mean": -1.8066e+02}, "entropy_coef_loss": {"n": 1, "mean": 8.8439e-01}, "critic_loss": {"n": 1, "mean": 8.8672e+00}, "entropy_coef": {"n": 1, "mean": 8.9602e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6118e-01, "std": 2.7538e-01, "min_value": 2.5362e-04, "max_value": 9.9987e-01}, "num_gradient_steps": 0, "step_time": 1.1666e+01, "total_time": 1.2557e+03, "__timestamp": "2024-10-10 18:33:19.898502"}, {"step": 126000, "num_env_steps": 126000, "scores": {"n": 1, "mean": 3.5604e+03}, "actor_loss": {"n": 1, "mean": -1.8429e+02}, "entropy_coef_loss": {"n": 1, "mean": 1.0115e-01}, "critic_loss": {"n": 1, "mean": 1.1939e+01}, "entropy_coef": {"n": 1, "mean": 9.1545e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6772e-01, "std": 2.7154e-01, "min_value": 1.2532e-05, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1288e+01, "total_time": 1.2670e+03, "__timestamp": "2024-10-10 18:33:31.186839"}, {"step": 127000, "num_env_steps": 127000, "scores": {"n": 1, "mean": 3.6394e+03}, "actor_loss": {"n": 1, "mean": -1.8559e+02}, "entropy_coef_loss": {"n": 1, "mean": -1.6681e+00}, "critic_loss": {"n": 1, "mean": 1.0435e+01}, "entropy_coef": {"n": 1, "mean": 9.1955e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6634e-01, "std": 2.7160e-01, "min_value": 1.4806e-04, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.1114e+01, "total_time": 1.2781e+03, "__timestamp": "2024-10-10 18:33:42.300823"}, {"step": 128000, "num_env_steps": 128000, "scores": {"n": 1, "mean": 3.7308e+03}, "actor_loss": {"n": 1, "mean": -1.8391e+02}, "entropy_coef_loss": {"n": 1, "mean": 2.2690e-01}, "critic_loss": {"n": 1, "mean": 2.1211e+02}, "entropy_coef": {"n": 1, "mean": 9.5348e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6823e-01, "std": 2.7269e-01, "min_value": 1.5179e-03, "max_value": 9.9995e-01}, "num_gradient_steps": 0, "step_time": 1.1653e+01, "total_time": 1.2898e+03, "__timestamp": "2024-10-10 18:33:53.955128"}, {"step": 129000, "num_env_steps": 129000, "scores": {"n": 1, "mean": 3.9117e+03}, "actor_loss": {"n": 1, "mean": -1.8682e+02}, "entropy_coef_loss": {"n": 1, "mean": -9.1982e-01}, "critic_loss": {"n": 1, "mean": 1.7144e+01}, "entropy_coef": {"n": 1, "mean": 9.4375e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7498e-01, "std": 2.6845e-01, "min_value": 6.5625e-05, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.1308e+01, "total_time": 1.3011e+03, "__timestamp": "2024-10-10 18:34:05.262736"}, {"step": 130000, "num_env_steps": 130000, "scores": {"n": 1, "mean": 3.5917e+03}, "actor_loss": {"n": 1, "mean": -1.8957e+02}, "entropy_coef_loss": {"n": 1, "mean": 6.1444e-01}, "critic_loss": {"n": 1, "mean": 1.3627e+01}, "entropy_coef": {"n": 1, "mean": 9.4159e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.6499e-01, "std": 2.7412e-01, "min_value": 9.9838e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 0, "step_time": 1.1330e+01, "total_time": 1.3124e+03, "__timestamp": "2024-10-10 18:34:16.591540"}, {"step": 131000, "num_env_steps": 131000, "scores": {"n": 1, "mean": 3.7838e+03}, "actor_loss": {"n": 1, "mean": -1.8980e+02}, "entropy_coef_loss": {"n": 1, "mean": -2.7599e-01}, "critic_loss": {"n": 1, "mean": 1.0952e+01}, "entropy_coef": {"n": 1, "mean": 9.4253e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7340e-01, "std": 2.6853e-01, "min_value": 1.0234e-03, "max_value": 9.9977e-01}, "num_gradient_steps": 0, "step_time": 1.1471e+01, "total_time": 1.3239e+03, "__timestamp": "2024-10-10 18:34:28.063852"}, {"step": 132000, "num_env_steps": 132000, "scores": {"n": 1, "mean": 3.9156e+03}, "actor_loss": {"n": 1, "mean": -1.9086e+02}, "entropy_coef_loss": {"n": 1, "mean": -3.4156e-01}, "critic_loss": {"n": 1, "mean": 1.0274e+01}, "entropy_coef": {"n": 1, "mean": 9.4719e-02}, "action_stds": null, "action_magnitude": {"n": 6000, "mean": 7.7205e-01, "std": 2.6778e-01, "min_value": 6.6406e-04, "max_value": 9.9991e-01}, "num_gradient_steps": 0, "step_time": 1.1368e+01, "total_time": 1.3353e+03, "__timestamp": "2024-10-10 18:34:39.431960"}]}}