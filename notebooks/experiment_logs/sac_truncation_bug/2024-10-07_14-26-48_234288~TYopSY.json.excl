{"experiment_id": "2024-10-07_14-26-48_234288~TYopSY", "experiment_tags": ["SAC", "HalfCheetah-v4"], "start_time": "2024-10-07 14:26:48.235287", "end_time": "2024-10-07 14:31:28.478378", "end_exception": null, "hyper_parameters": {"_type": "SAC", "_type_fq": "src.reinforcement_learning.algorithms.sac.sac.SAC", "env": "<SingletonVectorEnv instance>", "num_envs": 1, "env_specs": [{"_count": 1, "id": "HalfCheetah-v4", "entry_point": "gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv", "reward_threshold": 4.8000e+03, "nondeterministic": false, "max_episode_steps": 1000, "order_enforce": true, "autoreset": false, "disable_env_checker": false, "apply_api_compatibility": false, "kwargs": {"render_mode": null}, "additional_wrappers": [], "vector_entry_point": null, "namespace": null, "name": "HalfCheetah", "version": 4}], "policy": {"_type": "SACPolicy", "_type_fq": "src.reinforcement_learning.algorithms.sac.sac_policy.SACPolicy", "parameter_count": 217870, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "actor": {"_type": "Actor", "_type_fq": "src.reinforcement_learning.core.policies.components.actor.Actor", "parameter_count": 73484, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "network": {"_type": "Sequential", "_type_fq": "torch.nn.modules.container.Sequential", "num_layers": 4, "layers": [{"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 17, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}, {"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 256, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}], "repr": "Sequential(\n  (0): Linear(in_features=17, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n)"}, "action_selector": {"_type": "PredictedStdActionSelector", "_type_fq": "src.reinforcement_learning.core.action_selectors.predicted_std_action_selector.PredictedStdActionSelector", "latent_dim": 256, "action_dim": 6, "action_net_initialization": null, "base_std": 1.0000e+00, "squash_output": true, "epsilon": 1.0000e-06, "log_std_clamp_range": [-20, 2], "log_std_net_initialization": null}}, "critic": {"_type": "QCritic", "_type_fq": "src.reinforcement_learning.core.policies.components.q_critic.QCritic", "parameter_count": 144386, "feature_extractor": {"_type": "IdentityExtractor", "_type_fq": "src.reinforcement_learning.core.policies.components.feature_extractors.IdentityExtractor", "parameter_count": 0}, "n_critics": 2, "q_network": {"_type": "Sequential", "_type_fq": "torch.nn.modules.container.Sequential", "num_layers": 5, "layers": [{"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 23, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}, {"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 256, "out_features": 256, "bias": true}, {"_type": "ReLU", "_type_fq": "torch.nn.modules.activation.ReLU", "repr": "ReLU()"}, {"_type": "Linear", "_type_fq": "torch.nn.modules.linear.Linear", "in_features": 256, "out_features": 1, "bias": true}], "repr": "Sequential(\n  (0): Linear(in_features=23, out_features=256, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=256, out_features=256, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=256, out_features=1, bias=True)\n)"}}}, "policy_parameter_count": 217870, "policy_repr": "SACPolicy(\n  (feature_extractor): IdentityExtractor()\n  (actor): Actor(\n    (feature_extractor): IdentityExtractor()\n    (network): Sequential(\n      (0): Linear(in_features=17, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=256, out_features=256, bias=True)\n      (3): ReLU()\n    )\n    (_action_selector): PredictedStdActionSelector(\n      (action_net): Linear(in_features=256, out_features=6, bias=True)\n      (log_std_net): Linear(in_features=256, out_features=6, bias=True)\n    )\n  )\n  (critic): QCritic(\n    (feature_extractor): IdentityExtractor()\n    (q_networks): ModuleList(\n      (0-1): 2 x Sequential(\n        (0): Linear(in_features=23, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=256, out_features=1, bias=True)\n      )\n    )\n  )\n  (target_critic): QCritic(\n    (feature_extractor): IdentityExtractor()\n    (q_networks): ModuleList(\n      (0-1): 2 x Sequential(\n        (0): Linear(in_features=23, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=256, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=256, out_features=1, bias=True)\n      )\n    )\n  )\n  (target_shared_feature_extractor): IdentityExtractor()\n)", "buffer": {"_type": "ReplayBuffer", "_type_fq": "src.reinforcement_learning.core.buffers.replay.replay_buffer.ReplayBuffer", "step_size": 1000000, "num_envs": 1, "total_size": 1000000, "reward_scale": 1.0000e+00, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "np_dtype": "<class 'numpy.float32'>", "optimize_memory_usage": false}, "gamma": 9.9000e-01, "sde_noise_sample_freq": null, "torch_device": "cuda:0", "torch_dtype": "torch.float32", "tau": 5.0000e-03, "rollout_steps": 1, "gradient_steps": 1, "optimization_batch_size": 256, "action_noise": null, "warmup_steps": 10000, "actor_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "critic_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "entropy_coef_optimizer": {"_type": "Adam", "_type_fq": "torch.optim.adam.Adam", "lr": 3.0000e-04, "weight_decay": 0, "maximize": false, "foreach": null, "differentiable": false, "fused": null, "betas": [9.0000e-01, 9.9900e-01], "eps": 1.0000e-08, "amsgrad": false, "capturable": false, "repr": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0003\n    maximize: False\n    weight_decay: 0\n)"}, "weigh_and_reduce_entropy_coef_loss": "<built-in method mean of type object, module=torch>", "weigh_and_reduce_actor_loss": "<built-in method mean of type object, module=torch>", "weigh_critic_loss": "<function identity, module=src.torch_functions>", "target_update_interval": 1, "target_entropy": -6.0000e+00, "entropy_coef": "dynamic"}, "system_info": {"platform": "Windows", "platform_release": "10", "architecture": "AMD64", "processor": {"name": "AMD Ryzen 7 7840HS w/ Radeon 780M Graphics", "cores": 8, "logical_cores": 16, "speed": "3801 MHz"}, "gpu": [{"name": "AMD Radeon(TM) Graphics", "video_processor": "AMD Radeon Graphics Processor (0x15BF)", "adapter_ram": "512 MB", "adapter_dac_type": "Internal DAC(400MHz)", "manufacturer": "Advanced Micro Devices, Inc."}, {"name": "NVIDIA GeForce RTX 4060 Laptop GPU", "video_processor": "NVIDIA GeForce RTX 4060 Laptop GPU", "adapter_ram": "-1 MB", "adapter_dac_type": "Integrated RAMDAC", "manufacturer": "NVIDIA"}], "ram_speed": "5600 MHz", "ram": "31 GB"}, "setup": {"sac.py": "from typing import Any\n\nimport gymnasium\nimport torch\n\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction\n\npolicy_construction_hyper_parameter = {}\n\ndef init_action_selector(latent_dim: int, action_dim: int, hyper_parameters: dict[str, 'Any']) -> 'ActionSelector':\n    from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector \\\n        import PredictedStdActionSelector\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.weight_initialization import orthogonal_initialization\n\n    return PredictedStdActionSelector(\n        latent_dim=latent_dim,\n        action_dim=action_dim,\n        base_std=1.0,\n        squash_output=True,\n        # action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n        # log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n    )\n    # return PredictedStdActionSelector(\n    #     latent_dim=latent_dim,\n    #     action_dim=action_dim,\n    #     std=1.0,\n    #     std_learnable=True,\n    #     action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n    # )\n\n\n\ndef init_policy(\n        init_action_selector_: 'InitActionSelectorFunction',\n        hyper_parameters: dict[str, 'Any']\n) -> 'BasePolicy':\n    import torch\n    from torch import nn\n    import numpy as np\n\n    from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n    from src.reinforcement_learning.algorithms.sac.sac_crossq_policy import SACCrossQPolicy\n    from src.networks.core.seq_net import SeqNet\n    from src.networks.core.net import Net\n    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n    from src.weight_initialization import orthogonal_initialization\n    from src.reinforcement_learning.core.policies.components.actor import Actor\n    from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n    from src.networks.normalization.batch_renorm import BatchRenorm\n\n    # in_size = 376\n    # action_size = 17\n    # actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    # critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    in_size = 17\n    action_size = 6\n\n    actor_layers = 3\n    actor_features = 96\n\n    critic_layers = 2\n    critic_features = 96\n\n    hidden_activation_function = nn.ELU\n\n    # actor_net = nn.Sequential(\n    #     nn.Linear(in_size, actor_features),\n    #     hidden_activation_function(),\n    #     SeqNet.from_layer_provider(\n    #         layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #         AdditiveSkipConnection(Net.seq_as_net(\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             hidden_activation_function(),\n    #             orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #             nn.Tanh() if is_last_layer else hidden_activation_function(),\n    #         )),\n    #         num_features=actor_features,\n    #         num_layers=actor_layers,\n    #     )\n    # )\n    #\n    # critic = QCritic(\n    #     n_critics=2,\n    #     create_q_network=lambda: nn.Sequential(\n    #         nn.Linear(in_size + action_size, critic_features),\n    #         hidden_activation_function(),\n    #         SeqNet.from_layer_provider(\n    #             layer_provider=lambda layer_nr, is_last_layer, in_features, out_features:\n    #             AdditiveSkipConnection(Net.seq_as_net(\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #                 orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n    #                 hidden_activation_function(),\n    #             )),\n    #             num_features=critic_features,\n    #             num_layers=critic_layers,\n    #         ),\n    #         nn.Linear(critic_features, 1)\n    #     )\n    # )\n\n    actor_net = nn.Sequential(\n        nn.Linear(in_size, 256),\n        nn.ReLU(),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n    )\n\n    critic = QCritic(\n        n_critics=2,\n        create_q_network=lambda: nn.Sequential(\n            nn.Linear(in_size + action_size, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            # BatchRenorm(256),\n            nn.Linear(256, 1)\n        )\n    )\n\n    return SACPolicy(\n        actor=Actor(actor_net, init_action_selector_(\n            latent_dim=256,\n            action_dim=action_size,\n            hyper_parameters=hyper_parameters,\n        )),\n        critic=critic\n    )\n\ndef init_optimizer(pol: 'BasePolicy', hyper_parameters: dict[str, 'Any']) -> 'torch.optim.Optimizer':\n    import torch.optim\n    return torch.optim.AdamW(pol.parameters(), lr=3e-4, weight_decay=1e-4)\n\ndef wrap_env(env_: 'gymnasium.vector.VectorEnv', hyper_parameters: dict[str, 'Any']) -> 'gymnasium.Env':\n    from src.reinforcement_learning.gym.wrappers.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n    from src.np_functions import symmetric_log\n\n\n    # env_ = TransformRewardWrapper(env_, lambda reward_: 1 * reward_)\n    # env_ = RescaleAction(env_, min_action=-1.0, max_action=1.0)\n    return env_\n", "notebook": "import inspect\nimport os\nimport time\nfrom pathlib import Path\n\nimport gymnasium\nfrom gymnasium import Env\nfrom gymnasium.vector import VectorEnv\n\nfrom sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\nfrom src.datetime import get_current_timestamp\nfrom src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\nfrom src.model_db.model_db import ModelDB\nfrom src.model_db.dummy_model_db import DummyModelDB\nfrom src.np_functions import inv_symmetric_log\nfrom src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\nfrom src.model_db.tiny_model_db import TinyModelDB\nfrom src.module_analysis import count_parameters, get_gradients_per_parameter\nfrom src.moving_averages import ExponentialMovingAverage\nfrom src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\nfrom src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\nfrom src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n    StateDependentNoiseActionSelector\nfrom src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\nfrom src.reinforcement_learning.core.policies.base_policy import BasePolicy\nfrom src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\nfrom src.reinforcement_learning.gym.envs.test_env import TestEnv\nfrom src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\nfrom src.stopwatch import Stopwatch\nfrom src.summary_statistics import format_summary_statics\nfrom src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\nfrom typing import Any, SupportsFloat, Optional\nfrom gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\nfrom src.reinforcement_learning.core.callback import Callback\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\nfrom src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\nfrom src.reinforcement_learning.core.normalization import NormalizationType\nfrom src.torch_device import set_default_torch_device, optimizer_to_device\nfrom src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\nfrom torch.distributions import Normal, Categorical\n\nimport torch\nfrom torch import optim, nn\nimport torch.distributions as dist\nimport gymnasium as gym\nimport numpy as np\n\nfrom src.torch_functions import antisymmetric_power\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nfrom src.summary_statistics import maybe_compute_summary_statistics\nfrom src.reinforcement_learning.core.loss_config import LossLoggingConfig\nfrom src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\ndef get_setup() -> dict[str, str]:\n    import inspect\n    import sac\n    return {\n        'sac.py': inspect.getsource(sac),\n        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n    }\n\npolicy_id: str\npolicy: BasePolicy\noptimizer: optim.Optimizer\nwrapped_env: Env\nsteps_trained: int\ndef get_policy(create_new_if_exists: bool):\n    \n    global policy_id, policy, optimizer, wrapped_env, steps_trained\n    \n    policy_in_ram = 'policy' in globals()\n    if not policy_in_ram or create_new_if_exists:\n        if not policy_in_ram:\n            print('No policy in RAM, creating a new one')\n        \n        policy_id = get_current_timestamp()\n        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n            env=env,\n            info=PolicyConstruction.create_policy_initialization_info(\n                init_action_selector=init_action_selector,\n                init_policy=init_policy,\n                init_optimizer=init_optimizer,\n                wrap_env=wrap_env,\n                hyper_parameters=policy_construction_hyper_parameter,\n            ),\n        )\n        steps_trained = 0\n        print(f'New policy {policy_id} created')\n    \n    if parent_policy_id is not None:\n        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n        steps_trained = model_entry['model_info']['steps_trained']\n        print(f'Loading state dict from policy {parent_policy_id}')\n    \n    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n    return policy_id, policy, optimizer, wrapped_env, steps_trained\n\nscore_mean_ema = ExponentialMovingAverage(alpha=0.25)\nstep_stopwatch = Stopwatch()\ntotal_stopwatch = Stopwatch()\nbest_iteration_score = -1e6\n\ndef on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    \n    if step % 1000 != 0:\n        return\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    # rewards = rl.buffer.rewards[tail_indices]\n    # if 'raw_rewards' in info['rollout']:\n    #     rewards = info['rollout']['raw_rewards']\n    \n    # episode_scores = compute_episode_returns(\n    #     rewards=rewards,\n    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n    #     last_episode_starts=info['last_episode_starts'],\n    #     gamma=1.0,\n    #     gae_lambda=1.0,\n    #     normalize_rewards=None,\n    #     remove_unfinished_episodes=True,\n    # )\n    \n    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n    \n    if len(episode_scores) > 0:\n    \n        global best_iteration_score\n        iteration_score = episode_scores.mean()\n        score_moving_average = score_mean_ema.update(iteration_score)\n        if iteration_score >= best_iteration_score:\n            best_iteration_score = iteration_score\n            policy_db.save_model_state_dict(\n                model_id=policy_id,\n                parent_model_id=parent_policy_id,\n                model_info={\n                    'score': iteration_score.item(),\n                    'steps_trained': steps_trained,\n                    'wrap_env_source_code': wrap_env_source_code_source,\n                    'init_policy_source_code': init_policy_source\n                },\n                model=policy,\n                optimizer=optimizer,\n            )\n        info['score_moving_average'] = score_moving_average\n    \n    info['episode_scores'] = episode_scores\n        \ndef on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n    # global steps_trained\n    # steps_trained += rl.buffer.pos\n    \n    if step % 1000 != 0:\n        return\n    num_env_steps = step * rl.num_envs\n    \n    step_time = step_stopwatch.reset()\n    total_time = total_stopwatch.time_passed()\n    \n    tail_indices = rl.buffer.tail_indices(1000)\n    \n    episode_scores = info.get('episode_scores')\n    score_moving_average = info.get('score_moving_average') or 0.0\n    \n    scores = format_summary_statics(\n        episode_scores, \n        mean_format=' 6.3f',\n        std_format='4.3f',\n        min_value_format=' 6.3f',\n        max_value_format='5.3f',\n        n_format='>2'\n    )\n    # scores2 = format_summary_statics(\n    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n    #     mean_format=' 6.3f',\n    #     std_format='4.3f',\n    #     min_value_format=' 6.3f',\n    #     max_value_format='5.3f',\n    #     n_format='>2'\n    # )\n    # advantages = format_summary_statics(\n    #     rl.buffer.advantages, \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    actor_loss = format_summary_statics(\n        info['final_actor_loss'],  \n        mean_format=' 5.3f',\n        # std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # actor_loss_raw = format_summary_statics(\n    #     info['raw_actor_loss'],  \n    #     mean_format=' 5.3f',\n    #     std_format='5.3f',\n    #     min_value_format=None,\n    #     max_value_format=None,\n    # )\n    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n        info['final_entropy_coef_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    critic_loss = format_summary_statics(\n        info['final_critic_loss'], \n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    entropy_coef = format_summary_statics(\n        info['entropy_coef'],\n        mean_format='5.3f',\n#         std_format='5.3f',\n        std_format=None,\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # resets = format_summary_statics(\n    #     rl.buffer.dones.astype(int).sum(axis=0), \n    #     mean_format='.2f',\n    #     std_format=None,\n    #     min_value_format='1d',\n    #     max_value_format=None,\n    # )\n    # kl_div = info['actor_kl_divergence'][-1]\n    # grad_norm = format_summary_statics(\n    #     info['grad_norm'], \n    #     mean_format=' 6.3f',\n    #     std_format='.1f',\n    #     min_value_format=' 7.3f',\n    #     max_value_format='6.3f',\n    # )\n    action_stds = info['rollout'].get('action_stds')\n    if action_stds is not None:\n        rollout_action_stds = format_summary_statics(\n            action_stds,\n            mean_format='5.3f',\n            std_format='5.3f',\n            min_value_format=None,\n            max_value_format=None,\n        )\n    else:\n        rollout_action_stds = 'N/A'\n    action_magnitude = format_summary_statics(\n        np.abs(rl.buffer.actions[tail_indices]),\n        mean_format='5.3f',\n        std_format='5.3f',\n        min_value_format=None,\n        max_value_format=None,\n    )\n    # ppo_epochs = info['nr_ppo_epochs']\n    # ppo_updates = info['nr_ppo_updates']\n    # expl_var = rl.buffer.compute_critic_explained_variance()\n    print(f\"{step = : >7}, \"\n          f\"{num_env_steps = : >7}, \"\n          f\"{scores = :s}, \"\n          # f\"{scores2 = :s}, \"\n          f'score_ema = {score_moving_average: 6.3f}, '\n          # f\"{advantages = :s}, \"\n          f\"{actor_loss = :s}, \"\n          # f\"{actor_loss_raw = :s}, \"\n          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n          f\"{critic_loss = :s}, \"\n          f\"{entropy_coef = :s}, \"\n          f\"rollout_stds = {rollout_action_stds:s}, \"\n          f\"{action_magnitude = :s}, \"\n          # f\"{expl_var = :.3f}, \"\n          # f\"{kl_div = :.4f}, \"\n          # f\"{ppo_epochs = }, \"\n          # f\"{ppo_updates = }, \"\n          # f\"{grad_norm = :s}, \"\n          f\"n_updates = {rl.gradient_steps_performed}, \"\n          # f\"{resets = :s}, \"\n          f\"time = {step_time:4.1f}, \"\n          f\"total_time = {total_time:4.1f} \\n\"\n          )\n    logger.add_item({\n        'step': step,\n        'num_env_steps': num_env_steps,\n        'scores': maybe_compute_summary_statistics(episode_scores),\n        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n        'action_stds': maybe_compute_summary_statistics(action_stds),\n        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n        'num_gradient_steps': rl.gradient_steps_performed,\n        'step_time': step_time,\n        'total_time': total_time\n    })\n    if step % 10000 == 0:\n        logger.save_experiment_log()\n        print()\n    print()\n    \n    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n    #     logger.save_experiment_log()\n    #     raise ValueError('Score too low, policy probably fucked :(')\n\ndevice = torch.device(\"cuda:0\") if True else torch.device('cpu')\nprint(f'using device {device}')\n\ndef create_env(render_mode: str | None):\n    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n\nwrap_env_source_code_source = inspect.getsource(wrap_env)\ninit_policy_source = inspect.getsource(init_policy)\n\nenv_name = 'HalfCheetah-v4'\n# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\nenv_kwargs = {}\nnum_envs = 1\n    \n# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\npolicy_db = DummyModelDB[MitosisPolicyInfo]()\nprint(f'{policy_db = }')\n\nparent_policy_id=None  # '2024-04-28_20.57.23'\n\n# TODO\n# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\nenv = create_env(render_mode=None)\n\nlogger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n\ntry:\n    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n    print(f'{count_parameters(policy) = }')\n    print(f'{env = }, {num_envs = }')\n        \n    with ((torch.autograd.set_detect_anomaly(False))):\n        algo = SAC(\n            env=wrapped_env,\n            policy=policy,\n            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n            # weigh_critic_loss=lambda l: 1 * l,\n            buffer_size=1_000_000,\n            reward_scale=1,\n            gamma=0.99,\n            tau=0.005,\n            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n            entropy_coef=1.0,\n            rollout_steps=1,\n            gradient_steps=1,\n            warmup_steps=10_000,\n            optimization_batch_size=256,\n            target_update_interval=1,\n            # sde_noise_sample_freq=50,\n            callback=Callback(\n                on_rollout_done=on_rollout_done,\n                rollout_schedulers={},\n                on_optimization_done=on_optimization_done,\n                optimization_schedulers={},\n            ),\n            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n                                            log_last_obs=True, log_entropy_coef=True,\n                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n                                            critic_loss=LossLoggingConfig(log_final=True)),\n            torch_device=device,\n        )\n        total_stopwatch.reset()\n        with log_experiment(\n            logger,\n            experiment_tags=algo.collect_tags(),\n            hyper_parameters=algo.collect_hyper_parameters(),\n            setup=get_setup(),\n        ) as x:\n            logger.save_experiment_log()\n            print('\\nStarting Training\\n\\n')\n            # import cProfile\n            # pr = cProfile.Profile()\n            # pr.enable()\n            algo.learn(5_000_000)\n            # pr.disable()  \n            # pr.dump_stats('profile_stats.pstat')\nexcept KeyboardInterrupt:\n    print('keyboard interrupt')\nfinally:\n    print('closing envs')\n    time.sleep(0.5)\n    env.close()\n    print('envs closed')\n    policy_db.close()\n    print('model db closed')\n    \n\nprint('done')"}, "notes": [], "model_db_references": [], "logs_by_category": {"__default": [{"step": 11000, "num_env_steps": 11000, "scores": {"n": 1, "mean": -2.0333e+02}, "actor_loss": {"n": 1, "mean": -1.7793e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.0145e+00}, "critic_loss": {"n": 1, "mean": 1.0721e+00}, "entropy_coef": {"n": 1, "mean": 7.4091e-01}, "action_stds": {"n": 6, "mean": 9.2100e-01, "std": 3.2026e-02, "min_value": 8.6891e-01, "max_value": 9.4841e-01, "data": [[[9.4339e-01, 9.4841e-01, 8.9451e-01, 9.4023e-01, 9.3054e-01, 8.6891e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.2364e-01, "std": 2.8793e-01, "min_value": 3.6213e-04, "max_value": 9.9629e-01}, "num_gradient_steps": 1000, "step_time": 1.1398e+01, "total_time": 1.0813e+01, "__timestamp": "2024-10-07 14:26:59.048447"}, {"step": 12000, "num_env_steps": 12000, "scores": {"n": 1, "mean": -3.3190e+02}, "actor_loss": {"n": 1, "mean": -2.4921e+01}, "entropy_coef_loss": {"n": 1, "mean": -5.9068e+00}, "critic_loss": {"n": 1, "mean": 1.1802e+00}, "entropy_coef": {"n": 1, "mean": 5.4928e-01}, "action_stds": {"n": 6, "mean": 8.8744e-01, "std": 1.9560e-02, "min_value": 8.5734e-01, "max_value": 9.1495e-01, "data": [[[8.9679e-01, 8.8085e-01, 8.5734e-01, 8.7964e-01, 9.1495e-01, 8.9508e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.2846e-01, "std": 2.9043e-01, "min_value": 2.7976e-04, "max_value": 9.9869e-01}, "num_gradient_steps": 2000, "step_time": 8.1255e+00, "total_time": 1.8939e+01, "__timestamp": "2024-10-07 14:27:07.173991"}, {"step": 13000, "num_env_steps": 13000, "scores": {"n": 1, "mean": -3.3784e+02}, "actor_loss": {"n": 1, "mean": -3.0196e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.6831e+00}, "critic_loss": {"n": 1, "mean": 1.7573e+00}, "entropy_coef": {"n": 1, "mean": 4.0778e-01}, "action_stds": {"n": 6, "mean": 9.3266e-01, "std": 2.2685e-02, "min_value": 9.0805e-01, "max_value": 9.6997e-01, "data": [[[9.0805e-01, 9.2066e-01, 9.4841e-01, 9.2933e-01, 9.1954e-01, 9.6997e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.4373e-01, "std": 2.8986e-01, "min_value": 2.3082e-05, "max_value": 9.9956e-01}, "num_gradient_steps": 3000, "step_time": 8.1216e+00, "total_time": 2.7060e+01, "__timestamp": "2024-10-07 14:27:15.294599"}, {"step": 14000, "num_env_steps": 14000, "scores": {"n": 1, "mean": -1.4167e+02}, "actor_loss": {"n": 1, "mean": -3.2358e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.1382e+01}, "critic_loss": {"n": 1, "mean": 1.8523e+00}, "entropy_coef": {"n": 1, "mean": 3.0330e-01}, "action_stds": {"n": 6, "mean": 9.0675e-01, "std": 4.0120e-02, "min_value": 8.5683e-01, "max_value": 9.7716e-01, "data": [[[8.9922e-01, 8.5683e-01, 8.9220e-01, 9.2039e-01, 8.9471e-01, 9.7716e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.4390e-01, "std": 2.8930e-01, "min_value": 2.0027e-05, "max_value": 9.9935e-01}, "num_gradient_steps": 4000, "step_time": 8.1323e+00, "total_time": 3.5193e+01, "__timestamp": "2024-10-07 14:27:23.426856"}, {"step": 15000, "num_env_steps": 15000, "scores": {"n": 1, "mean": -2.6966e+02}, "actor_loss": {"n": 1, "mean": -3.3305e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3898e+01}, "critic_loss": {"n": 1, "mean": 1.7816e+00}, "entropy_coef": {"n": 1, "mean": 2.2663e-01}, "action_stds": {"n": 6, "mean": 8.6522e-01, "std": 9.3556e-02, "min_value": 6.9818e-01, "max_value": 9.4136e-01, "data": [[[9.3880e-01, 9.2585e-01, 9.4136e-01, 6.9818e-01, 8.3928e-01, 8.4783e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.6037e-01, "std": 2.9133e-01, "min_value": 1.8924e-04, "max_value": 9.9908e-01}, "num_gradient_steps": 5000, "step_time": 8.0058e+00, "total_time": 4.3198e+01, "__timestamp": "2024-10-07 14:27:31.432650"}, {"step": 16000, "num_env_steps": 16000, "scores": {"n": 1, "mean": -3.1023e+02}, "actor_loss": {"n": 1, "mean": -3.2469e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5159e+01}, "critic_loss": {"n": 1, "mean": 2.2018e+00}, "entropy_coef": {"n": 1, "mean": 1.7007e-01}, "action_stds": {"n": 6, "mean": 7.5216e-01, "std": 4.6546e-02, "min_value": 6.7113e-01, "max_value": 7.8971e-01, "data": [[[7.8971e-01, 7.8645e-01, 7.8797e-01, 6.7113e-01, 7.3415e-01, 7.4358e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.7128e-01, "std": 2.9295e-01, "min_value": 8.2510e-05, "max_value": 9.9975e-01}, "num_gradient_steps": 6000, "step_time": 7.9365e+00, "total_time": 5.1135e+01, "__timestamp": "2024-10-07 14:27:39.369155"}, {"step": 17000, "num_env_steps": 17000, "scores": {"n": 1, "mean": -3.2084e+02}, "actor_loss": {"n": 1, "mean": -3.2424e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6818e+01}, "critic_loss": {"n": 1, "mean": 2.0100e+00}, "entropy_coef": {"n": 1, "mean": 1.2845e-01}, "action_stds": {"n": 6, "mean": 8.5245e-01, "std": 5.7083e-02, "min_value": 7.6160e-01, "max_value": 9.2320e-01, "data": [[[7.6160e-01, 8.1991e-01, 8.4858e-01, 8.9417e-01, 9.2320e-01, 8.6725e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8078e-01, "std": 2.9655e-01, "min_value": 2.0292e-04, "max_value": 9.9943e-01}, "num_gradient_steps": 7000, "step_time": 7.9392e+00, "total_time": 5.9074e+01, "__timestamp": "2024-10-07 14:27:47.308337"}, {"step": 18000, "num_env_steps": 18000, "scores": {"n": 1, "mean": -3.3021e+02}, "actor_loss": {"n": 1, "mean": -3.0943e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6977e+01}, "critic_loss": {"n": 1, "mean": 1.9047e+00}, "entropy_coef": {"n": 1, "mean": 9.7280e-02}, "action_stds": {"n": 6, "mean": 7.9884e-01, "std": 8.3333e-02, "min_value": 6.9174e-01, "max_value": 8.9756e-01, "data": [[[7.3935e-01, 8.9756e-01, 8.4598e-01, 7.4785e-01, 6.9174e-01, 8.7057e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8587e-01, "std": 2.9917e-01, "min_value": 2.2888e-04, "max_value": 9.9930e-01}, "num_gradient_steps": 8000, "step_time": 7.9817e+00, "total_time": 6.7056e+01, "__timestamp": "2024-10-07 14:27:55.290999"}, {"step": 19000, "num_env_steps": 19000, "scores": {"n": 1, "mean": -2.4129e+02}, "actor_loss": {"n": 1, "mean": -2.9805e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7010e+01}, "critic_loss": {"n": 1, "mean": 4.0840e+00}, "entropy_coef": {"n": 1, "mean": 7.3897e-02}, "action_stds": {"n": 6, "mean": 7.8042e-01, "std": 1.1629e-01, "min_value": 5.8058e-01, "max_value": 8.9992e-01, "data": [[[5.8058e-01, 7.2058e-01, 8.3344e-01, 7.8394e-01, 8.6406e-01, 8.9992e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.0077e-01, "std": 2.9663e-01, "min_value": 4.6811e-04, "max_value": 9.9926e-01}, "num_gradient_steps": 9000, "step_time": 7.9302e+00, "total_time": 7.4986e+01, "__timestamp": "2024-10-07 14:28:03.221158"}, {"step": 20000, "num_env_steps": 20000, "scores": {"n": 1, "mean": -2.9723e+02}, "actor_loss": {"n": 1, "mean": -2.7817e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.8405e+01}, "critic_loss": {"n": 1, "mean": 2.6868e+00}, "entropy_coef": {"n": 1, "mean": 5.6390e-02}, "action_stds": {"n": 6, "mean": 7.1307e-01, "std": 9.9572e-02, "min_value": 6.4340e-01, "max_value": 9.0925e-01, "data": [[[6.7027e-01, 6.4340e-01, 6.5231e-01, 7.1650e-01, 6.8668e-01, 9.0925e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.9999e-01, "std": 2.9586e-01, "min_value": 1.6659e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 10000, "step_time": 7.9590e+00, "total_time": 8.2945e+01, "__timestamp": "2024-10-07 14:28:11.180179"}, {"step": 21000, "num_env_steps": 21000, "scores": {"n": 1, "mean": -2.7201e+02}, "actor_loss": {"n": 1, "mean": -2.6152e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6047e+01}, "critic_loss": {"n": 1, "mean": 3.6616e+00}, "entropy_coef": {"n": 1, "mean": 4.3216e-02}, "action_stds": {"n": 6, "mean": 5.7247e-01, "std": 9.0705e-02, "min_value": 4.6661e-01, "max_value": 7.2145e-01, "data": [[[4.6661e-01, 5.7118e-01, 6.0998e-01, 4.9292e-01, 5.7269e-01, 7.2145e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.0804e-01, "std": 2.9519e-01, "min_value": 2.1362e-04, "max_value": 9.9980e-01}, "num_gradient_steps": 11000, "step_time": 8.0240e+00, "total_time": 9.0969e+01, "__timestamp": "2024-10-07 14:28:19.204730"}, {"step": 22000, "num_env_steps": 22000, "scores": {"n": 1, "mean": -3.5221e+02}, "actor_loss": {"n": 1, "mean": -2.5207e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.6062e+01}, "critic_loss": {"n": 1, "mean": 1.4594e+00}, "entropy_coef": {"n": 1, "mean": 3.3335e-02}, "action_stds": {"n": 6, "mean": 5.5350e-01, "std": 9.0226e-02, "min_value": 4.5292e-01, "max_value": 6.6325e-01, "data": [[[4.8836e-01, 4.9384e-01, 6.5635e-01, 4.5292e-01, 5.6626e-01, 6.6325e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1020e-01, "std": 2.9768e-01, "min_value": 5.2750e-06, "max_value": 9.9920e-01}, "num_gradient_steps": 12000, "step_time": 7.9704e+00, "total_time": 9.8939e+01, "__timestamp": "2024-10-07 14:28:27.173541"}, {"step": 23000, "num_env_steps": 23000, "scores": {"n": 1, "mean": -3.1632e+02}, "actor_loss": {"n": 1, "mean": -2.2912e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.5559e+01}, "critic_loss": {"n": 1, "mean": 3.6327e+00}, "entropy_coef": {"n": 1, "mean": 2.5703e-02}, "action_stds": {"n": 6, "mean": 4.5322e-01, "std": 4.9637e-02, "min_value": 3.9034e-01, "max_value": 5.2701e-01, "data": [[[3.9034e-01, 4.2323e-01, 5.2701e-01, 4.2823e-01, 4.6066e-01, 4.8985e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.9614e-01, "std": 2.9751e-01, "min_value": 1.3133e-04, "max_value": 9.9995e-01}, "num_gradient_steps": 13000, "step_time": 7.9425e+00, "total_time": 1.0688e+02, "__timestamp": "2024-10-07 14:28:35.116999"}, {"step": 24000, "num_env_steps": 24000, "scores": {"n": 1, "mean": -2.2230e+02}, "actor_loss": {"n": 1, "mean": -2.1226e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3337e+01}, "critic_loss": {"n": 1, "mean": 1.6106e+00}, "entropy_coef": {"n": 1, "mean": 1.9952e-02}, "action_stds": {"n": 6, "mean": 5.2382e-01, "std": 4.1742e-02, "min_value": 4.5887e-01, "max_value": 5.7209e-01, "data": [[[4.9550e-01, 5.1990e-01, 5.5691e-01, 4.5887e-01, 5.7209e-01, 5.3968e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.0065e-01, "std": 2.9489e-01, "min_value": 3.7879e-05, "max_value": 9.9992e-01}, "num_gradient_steps": 14000, "step_time": 8.0129e+00, "total_time": 1.1489e+02, "__timestamp": "2024-10-07 14:28:43.128902"}, {"step": 25000, "num_env_steps": 25000, "scores": {"n": 1, "mean": -2.1507e+02}, "actor_loss": {"n": 1, "mean": -2.0497e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.3014e+01}, "critic_loss": {"n": 1, "mean": 1.5346e+00}, "entropy_coef": {"n": 1, "mean": 1.5595e-02}, "action_stds": {"n": 6, "mean": 3.6801e-01, "std": 9.8301e-02, "min_value": 2.7338e-01, "max_value": 5.2921e-01, "data": [[[3.3982e-01, 2.7338e-01, 4.3982e-01, 2.8692e-01, 3.3891e-01, 5.2921e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.2102e-01, "std": 2.9405e-01, "min_value": 1.5248e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 15000, "step_time": 8.0661e+00, "total_time": 1.2296e+02, "__timestamp": "2024-10-07 14:28:51.194957"}, {"step": 26000, "num_env_steps": 26000, "scores": {"n": 1, "mean": 9.7068e+01}, "actor_loss": {"n": 1, "mean": -1.9682e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.0081e+01}, "critic_loss": {"n": 1, "mean": 2.4476e+00}, "entropy_coef": {"n": 1, "mean": 1.2372e-02}, "action_stds": {"n": 6, "mean": 3.8435e-01, "std": 5.3641e-02, "min_value": 3.2162e-01, "max_value": 4.7662e-01, "data": [[[3.8566e-01, 3.6994e-01, 4.7662e-01, 3.2162e-01, 3.4791e-01, 4.0436e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8413e-01, "std": 2.9660e-01, "min_value": 3.0607e-04, "max_value": 9.9992e-01}, "num_gradient_steps": 16000, "step_time": 7.9359e+00, "total_time": 1.3090e+02, "__timestamp": "2024-10-07 14:28:59.131856"}, {"step": 27000, "num_env_steps": 27000, "scores": {"n": 1, "mean": -1.5374e+02}, "actor_loss": {"n": 1, "mean": -1.7803e+01}, "entropy_coef_loss": {"n": 1, "mean": -6.7956e+00}, "critic_loss": {"n": 1, "mean": 1.8888e+00}, "entropy_coef": {"n": 1, "mean": 1.0043e-02}, "action_stds": {"n": 6, "mean": 3.3564e-01, "std": 6.0677e-02, "min_value": 2.3867e-01, "max_value": 3.9377e-01, "data": [[[3.1718e-01, 3.7162e-01, 3.8967e-01, 2.3867e-01, 3.0290e-01, 3.9377e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8784e-01, "std": 2.9929e-01, "min_value": 3.1613e-04, "max_value": 9.9980e-01}, "num_gradient_steps": 17000, "step_time": 8.0177e+00, "total_time": 1.3891e+02, "__timestamp": "2024-10-07 14:29:07.149576"}, {"step": 28000, "num_env_steps": 28000, "scores": {"n": 1, "mean": -1.2882e+02}, "actor_loss": {"n": 1, "mean": -1.5244e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.6829e+00}, "critic_loss": {"n": 1, "mean": 1.1596e+00}, "entropy_coef": {"n": 1, "mean": 8.4213e-03}, "action_stds": {"n": 6, "mean": 3.5716e-01, "std": 8.8790e-02, "min_value": 2.3607e-01, "max_value": 4.6275e-01, "data": [[[3.2477e-01, 2.9909e-01, 4.5240e-01, 2.3607e-01, 3.6785e-01, 4.6275e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8403e-01, "std": 2.9574e-01, "min_value": 2.1815e-04, "max_value": 9.9974e-01}, "num_gradient_steps": 18000, "step_time": 8.0112e+00, "total_time": 1.4693e+02, "__timestamp": "2024-10-07 14:29:15.160755"}, {"step": 29000, "num_env_steps": 29000, "scores": {"n": 1, "mean": 2.4190e+02}, "actor_loss": {"n": 1, "mean": -1.5831e+01}, "entropy_coef_loss": {"n": 1, "mean": -3.4341e+00}, "critic_loss": {"n": 1, "mean": 1.2336e+00}, "entropy_coef": {"n": 1, "mean": 7.3599e-03}, "action_stds": {"n": 6, "mean": 2.1664e-01, "std": 5.2367e-02, "min_value": 1.4713e-01, "max_value": 2.8792e-01, "data": [[[2.0375e-01, 1.7380e-01, 2.8792e-01, 1.4713e-01, 2.3051e-01, 2.5672e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.6110e-01, "std": 3.0047e-01, "min_value": 1.7917e-04, "max_value": 9.9997e-01}, "num_gradient_steps": 19000, "step_time": 8.0414e+00, "total_time": 1.5497e+02, "__timestamp": "2024-10-07 14:29:23.202193"}, {"step": 30000, "num_env_steps": 30000, "scores": {"n": 1, "mean": 4.6950e+01}, "actor_loss": {"n": 1, "mean": -1.4937e+01}, "entropy_coef_loss": {"n": 1, "mean": -7.0754e-01}, "critic_loss": {"n": 1, "mean": 1.0874e+00}, "entropy_coef": {"n": 1, "mean": 6.8293e-03}, "action_stds": {"n": 6, "mean": 3.3715e-01, "std": 5.7833e-02, "min_value": 2.9013e-01, "max_value": 4.2287e-01, "data": [[[2.9096e-01, 3.9234e-01, 4.2287e-01, 2.9013e-01, 2.9270e-01, 3.3387e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1250e-01, "std": 2.9868e-01, "min_value": 1.3781e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 20000, "step_time": 8.0033e+00, "total_time": 1.6297e+02, "__timestamp": "2024-10-07 14:29:31.204475"}, {"step": 31000, "num_env_steps": 31000, "scores": {"n": 1, "mean": -1.3302e+02}, "actor_loss": {"n": 1, "mean": -1.4011e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.2358e+00}, "critic_loss": {"n": 1, "mean": 1.2640e+00}, "entropy_coef": {"n": 1, "mean": 6.7679e-03}, "action_stds": {"n": 6, "mean": 3.5218e-01, "std": 3.8094e-02, "min_value": 3.0381e-01, "max_value": 3.9668e-01, "data": [[[3.0381e-01, 3.1439e-01, 3.4338e-01, 3.9668e-01, 3.8627e-01, 3.6854e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1087e-01, "std": 3.0089e-01, "min_value": 1.4525e-04, "max_value": 9.9998e-01}, "num_gradient_steps": 21000, "step_time": 8.0563e+00, "total_time": 1.7103e+02, "__timestamp": "2024-10-07 14:29:39.261793"}, {"step": 32000, "num_env_steps": 32000, "scores": {"n": 1, "mean": -1.2969e+02}, "actor_loss": {"n": 1, "mean": -1.2867e+01}, "entropy_coef_loss": {"n": 1, "mean": 6.5612e-01}, "critic_loss": {"n": 1, "mean": 1.2295e+00}, "entropy_coef": {"n": 1, "mean": 6.5635e-03}, "action_stds": {"n": 6, "mean": 3.7381e-01, "std": 2.2222e-02, "min_value": 3.4935e-01, "max_value": 4.0433e-01, "data": [[[3.4935e-01, 3.5371e-01, 3.8906e-01, 3.8548e-01, 3.6093e-01, 4.0433e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.8734e-01, "std": 2.9882e-01, "min_value": 2.6636e-04, "max_value": 9.9996e-01}, "num_gradient_steps": 22000, "step_time": 7.9533e+00, "total_time": 1.7898e+02, "__timestamp": "2024-10-07 14:29:47.215118"}, {"step": 33000, "num_env_steps": 33000, "scores": {"n": 1, "mean": 2.7229e+02}, "actor_loss": {"n": 1, "mean": -1.3466e+01}, "entropy_coef_loss": {"n": 1, "mean": -8.0943e-01}, "critic_loss": {"n": 1, "mean": 1.1724e+00}, "entropy_coef": {"n": 1, "mean": 7.0334e-03}, "action_stds": {"n": 6, "mean": 2.8772e-01, "std": 1.0368e-01, "min_value": 1.8668e-01, "max_value": 4.4812e-01, "data": [[[1.8668e-01, 2.5510e-01, 4.4812e-01, 2.2562e-01, 2.2706e-01, 3.8371e-01]]]}, "action_magnitude": {"n": 6000, "mean": 5.9921e-01, "std": 3.0019e-01, "min_value": 2.8372e-04, "max_value": 9.9994e-01}, "num_gradient_steps": 23000, "step_time": 8.0015e+00, "total_time": 1.8698e+02, "__timestamp": "2024-10-07 14:29:55.216649"}, {"step": 34000, "num_env_steps": 34000, "scores": {"n": 1, "mean": 1.5945e+02}, "actor_loss": {"n": 1, "mean": -1.3513e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.9334e-01}, "critic_loss": {"n": 1, "mean": 1.2989e+00}, "entropy_coef": {"n": 1, "mean": 7.2276e-03}, "action_stds": {"n": 6, "mean": 2.5573e-01, "std": 7.9226e-02, "min_value": 1.8402e-01, "max_value": 4.0281e-01, "data": [[[2.3729e-01, 2.6668e-01, 4.0281e-01, 1.8402e-01, 1.9142e-01, 2.5216e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.0262e-01, "std": 3.0024e-01, "min_value": 4.9628e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 24000, "step_time": 8.0229e+00, "total_time": 1.9500e+02, "__timestamp": "2024-10-07 14:30:03.239584"}, {"step": 35000, "num_env_steps": 35000, "scores": {"n": 1, "mean": 1.1521e+03}, "actor_loss": {"n": 1, "mean": -1.3180e+01}, "entropy_coef_loss": {"n": 1, "mean": 7.0613e-01}, "critic_loss": {"n": 1, "mean": 1.4540e+00}, "entropy_coef": {"n": 1, "mean": 7.6485e-03}, "action_stds": {"n": 6, "mean": 2.1907e-01, "std": 3.7793e-02, "min_value": 1.7329e-01, "max_value": 2.8417e-01, "data": [[[2.1413e-01, 1.9365e-01, 2.8417e-01, 2.1839e-01, 1.7329e-01, 2.3078e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.5810e-01, "std": 3.0146e-01, "min_value": 9.3877e-07, "max_value": 1.0000e+00}, "num_gradient_steps": 25000, "step_time": 8.0378e+00, "total_time": 2.0304e+02, "__timestamp": "2024-10-07 14:30:11.276435"}, {"step": 36000, "num_env_steps": 36000, "scores": {"n": 1, "mean": 3.1208e+02}, "actor_loss": {"n": 1, "mean": -1.4030e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.1915e+00}, "critic_loss": {"n": 1, "mean": 1.4069e+00}, "entropy_coef": {"n": 1, "mean": 8.0549e-03}, "action_stds": {"n": 6, "mean": 3.0428e-01, "std": 6.1543e-02, "min_value": 2.3469e-01, "max_value": 4.0461e-01, "data": [[[2.9634e-01, 3.4669e-01, 4.0461e-01, 2.3469e-01, 2.6723e-01, 2.7610e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.1497e-01, "std": 3.0380e-01, "min_value": 3.4752e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 26000, "step_time": 8.0158e+00, "total_time": 2.1106e+02, "__timestamp": "2024-10-07 14:30:19.292278"}, {"step": 37000, "num_env_steps": 37000, "scores": {"n": 1, "mean": 1.6841e+02}, "actor_loss": {"n": 1, "mean": -1.4376e+01}, "entropy_coef_loss": {"n": 1, "mean": 5.5413e-01}, "critic_loss": {"n": 1, "mean": 1.4698e+00}, "entropy_coef": {"n": 1, "mean": 8.6572e-03}, "action_stds": {"n": 6, "mean": 3.0312e-01, "std": 6.5659e-02, "min_value": 1.9694e-01, "max_value": 3.6490e-01, "data": [[[3.3905e-01, 3.6440e-01, 3.6490e-01, 1.9694e-01, 2.7317e-01, 2.8027e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.0607e-01, "std": 2.9567e-01, "min_value": 1.7905e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 27000, "step_time": 8.1643e+00, "total_time": 2.1922e+02, "__timestamp": "2024-10-07 14:30:27.457540"}, {"step": 38000, "num_env_steps": 38000, "scores": {"n": 1, "mean": 1.3249e+03}, "actor_loss": {"n": 1, "mean": -1.5210e+01}, "entropy_coef_loss": {"n": 1, "mean": 9.9216e-01}, "critic_loss": {"n": 1, "mean": 1.6875e+00}, "entropy_coef": {"n": 1, "mean": 9.0508e-03}, "action_stds": {"n": 6, "mean": 1.7747e-01, "std": 6.7066e-02, "min_value": 1.1434e-01, "max_value": 3.0601e-01, "data": [[[1.7719e-01, 1.3654e-01, 3.0601e-01, 1.1434e-01, 1.6293e-01, 1.6782e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.7062e-01, "std": 3.0024e-01, "min_value": 1.2206e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 28000, "step_time": 7.9892e+00, "total_time": 2.2721e+02, "__timestamp": "2024-10-07 14:30:35.446746"}, {"step": 39000, "num_env_steps": 39000, "scores": {"n": 1, "mean": 1.1933e+03}, "actor_loss": {"n": 1, "mean": -1.4989e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4486e+00}, "critic_loss": {"n": 1, "mean": 1.7687e+00}, "entropy_coef": {"n": 1, "mean": 9.6390e-03}, "action_stds": {"n": 6, "mean": 2.7306e-01, "std": 1.1484e-01, "min_value": 1.5990e-01, "max_value": 4.8426e-01, "data": [[[2.5618e-01, 1.8253e-01, 4.8426e-01, 1.5990e-01, 2.7893e-01, 2.7657e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.4571e-01, "std": 3.0349e-01, "min_value": 1.3964e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 29000, "step_time": 8.1253e+00, "total_time": 2.3534e+02, "__timestamp": "2024-10-07 14:30:43.571040"}, {"step": 40000, "num_env_steps": 40000, "scores": {"n": 1, "mean": 5.2366e+02}, "actor_loss": {"n": 1, "mean": -1.9344e+01}, "entropy_coef_loss": {"n": 1, "mean": 2.2050e+00}, "critic_loss": {"n": 1, "mean": 2.2937e+00}, "entropy_coef": {"n": 1, "mean": 1.0931e-02}, "action_stds": {"n": 6, "mean": 3.5214e-01, "std": 3.8009e-02, "min_value": 3.0022e-01, "max_value": 3.9913e-01, "data": [[[3.3055e-01, 3.3192e-01, 3.9913e-01, 3.0022e-01, 3.8875e-01, 3.6229e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.4675e-01, "std": 3.0053e-01, "min_value": 1.1483e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 30000, "step_time": 7.9610e+00, "total_time": 2.4330e+02, "__timestamp": "2024-10-07 14:30:51.533043"}, {"step": 41000, "num_env_steps": 41000, "scores": {"n": 1, "mean": 1.2615e+03}, "actor_loss": {"n": 1, "mean": -1.9345e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.7556e+00}, "critic_loss": {"n": 1, "mean": 2.5453e+00}, "entropy_coef": {"n": 1, "mean": 1.1726e-02}, "action_stds": {"n": 6, "mean": 2.3078e-01, "std": 7.6218e-02, "min_value": 1.2924e-01, "max_value": 3.4361e-01, "data": [[[2.2624e-01, 1.7093e-01, 3.4361e-01, 1.2924e-01, 2.7958e-01, 2.3510e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.8650e-01, "std": 2.9748e-01, "min_value": 2.1297e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 31000, "step_time": 7.9619e+00, "total_time": 2.5126e+02, "__timestamp": "2024-10-07 14:30:59.494902"}, {"step": 42000, "num_env_steps": 42000, "scores": {"n": 1, "mean": 1.4979e+03}, "actor_loss": {"n": 1, "mean": -2.1159e+01}, "entropy_coef_loss": {"n": 1, "mean": -1.4348e+00}, "critic_loss": {"n": 1, "mean": 2.6515e+00}, "entropy_coef": {"n": 1, "mean": 1.2676e-02}, "action_stds": {"n": 6, "mean": 2.4319e-01, "std": 8.8800e-02, "min_value": 1.4377e-01, "max_value": 3.9024e-01, "data": [[[2.1499e-01, 1.7907e-01, 3.9024e-01, 1.4377e-01, 2.9726e-01, 2.3380e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.8718e-01, "std": 2.9795e-01, "min_value": 2.2370e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 32000, "step_time": 8.0742e+00, "total_time": 2.5933e+02, "__timestamp": "2024-10-07 14:31:07.569145"}, {"step": 43000, "num_env_steps": 43000, "scores": {"n": 1, "mean": 1.5274e+03}, "actor_loss": {"n": 1, "mean": -2.4156e+01}, "entropy_coef_loss": {"n": 1, "mean": 3.3288e+00}, "critic_loss": {"n": 1, "mean": 2.3227e+00}, "entropy_coef": {"n": 1, "mean": 1.3382e-02}, "action_stds": {"n": 6, "mean": 3.6306e-01, "std": 1.3296e-01, "min_value": 2.1899e-01, "max_value": 5.8856e-01, "data": [[[2.1899e-01, 3.5506e-01, 5.8856e-01, 2.4768e-01, 4.1957e-01, 3.4852e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.8444e-01, "std": 3.0216e-01, "min_value": 4.0509e-05, "max_value": 1.0000e+00}, "num_gradient_steps": 33000, "step_time": 8.0282e+00, "total_time": 2.6736e+02, "__timestamp": "2024-10-07 14:31:15.597339"}, {"step": 44000, "num_env_steps": 44000, "scores": {"n": 1, "mean": 1.6013e+03}, "actor_loss": {"n": 1, "mean": -2.4006e+01}, "entropy_coef_loss": {"n": 1, "mean": -9.8016e-01}, "critic_loss": {"n": 1, "mean": 2.5360e+00}, "entropy_coef": {"n": 1, "mean": 1.4977e-02}, "action_stds": {"n": 6, "mean": 3.4844e-01, "std": 1.1880e-01, "min_value": 1.7020e-01, "max_value": 5.3587e-01, "data": [[[3.1853e-01, 3.1360e-01, 5.3587e-01, 1.7020e-01, 3.7358e-01, 3.7889e-01]]]}, "action_magnitude": {"n": 6000, "mean": 6.7743e-01, "std": 2.9839e-01, "min_value": 4.4309e-04, "max_value": 1.0000e+00}, "num_gradient_steps": 34000, "step_time": 8.1937e+00, "total_time": 2.7556e+02, "__timestamp": "2024-10-07 14:31:23.791021"}]}}