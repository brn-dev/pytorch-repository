{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "\n",
    "from src.experiment_logging.experiment_log import ExperimentLogItem\n",
    "from src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\n",
    "from src.module_analysis import count_parameters\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACInfoStashConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.loss_config import LossInfoStashConfig\n",
    "from src.reinforcement_learning.core.policies.components.actor import Actor\n",
    "from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import maybe_compute_summary_statistics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T00:24:23.118791Z",
     "start_time": "2024-10-15T00:24:15.785718Z"
    }
   },
   "id": "ba8c59a3eba2f172",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "    \n",
    "def get_setup() -> dict[str, str]:\n",
    "    return {\n",
    "        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n",
    "    }\n",
    "\n",
    "step_stopwatch = Stopwatch()\n",
    "total_stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def save_experiment_state():\n",
    "    experiment_id = logger.experiment_log['experiment_id']\n",
    "    algo.save(\n",
    "        folder_location=f'models/{env_name}/{experiment_id}', \n",
    "        name=experiment_id, \n",
    "        latest_log_item=logger.get_latest_log_item()\n",
    "    )\n",
    "    \n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs, consider_truncated_as_done=True)\n",
    "    \n",
    "    if len(episode_scores) > 0:\n",
    "    \n",
    "        global best_iteration_score\n",
    "        iteration_score = episode_scores.mean()\n",
    "        if iteration_score >= best_iteration_score:\n",
    "            pass\n",
    "    \n",
    "    info['episode_scores'] = episode_scores\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    num_env_steps = step * rl.num_envs\n",
    "    \n",
    "    step_time = step_stopwatch.reset()\n",
    "    total_time = total_stopwatch.time_passed()\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    episode_scores = info.get('episode_scores')\n",
    "    \n",
    "    log_item: ExperimentLogItem = {\n",
    "        'step': step,\n",
    "        'num_env_steps': num_env_steps,\n",
    "        'scores': maybe_compute_summary_statistics(episode_scores),\n",
    "        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n",
    "        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n",
    "        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n",
    "        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n",
    "        'action_stds': maybe_compute_summary_statistics(info['rollout'].get('action_stds')),\n",
    "        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n",
    "        'num_gradient_steps': rl.gradient_steps_performed,\n",
    "        'step_time': step_time,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "    print(logger.format_log_item(log_item, mean_format='5.3f', std_format='5.3f', step_time='.2f', total_time='.2f'), end='\\n\\n')\n",
    "    logger.add_item(log_item)\n",
    "    if step % 10000 == 0:\n",
    "        logger.save_experiment_log()\n",
    "        \n",
    "        print()\n",
    "    print()\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "env_kwargs = {}\n",
    "num_envs = 1\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    make_single_env = lambda: gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "    \n",
    "    if num_envs == 1:\n",
    "        return make_single_env()\n",
    "        \n",
    "    return parallelize_env_async(make_single_env, num_envs)\n",
    "\n",
    "\n",
    "def create_policy():\n",
    "    in_size = 17\n",
    "    action_size = 6\n",
    "    \n",
    "    actor_net = nn.Sequential(\n",
    "        nn.Linear(in_size, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    critic = QCritic(\n",
    "        n_critics=2,\n",
    "        create_q_network=lambda: nn.Sequential(\n",
    "            nn.Linear(in_size + action_size, 256),\n",
    "            nn.ReLU(),\n",
    "            # BatchRenorm(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            # BatchRenorm(256),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return SACPolicy(\n",
    "        actor=Actor(actor_net, PredictedStdActionSelector(\n",
    "            latent_dim=256,\n",
    "            action_dim=action_size,\n",
    "            base_std=1.0,\n",
    "            squash_output=True,\n",
    "        )),\n",
    "        critic=critic\n",
    "    )\n",
    "\n",
    "\n",
    "env = create_env(render_mode=None)\n",
    "policy = create_policy()\n",
    "logger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n",
    "\n",
    "try:\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = }')\n",
    "        \n",
    "    with ((torch.autograd.set_detect_anomaly(False))):\n",
    "        algo = SAC(\n",
    "            env=env,\n",
    "            policy=policy,\n",
    "            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            buffer_size=1_000_000,\n",
    "            reward_scale=1,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n",
    "            entropy_coef=1.0,\n",
    "            rollout_steps=1,\n",
    "            gradient_steps=1,\n",
    "            warmup_steps=10_000,\n",
    "            optimization_batch_size=256,\n",
    "            target_update_interval=1,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            stash_config=SACInfoStashConfig(stash_rollout_infos=True, stash_rollout_action_stds=True,\n",
    "                                            stash_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossInfoStashConfig(stash_final=True),\n",
    "                                            actor_loss=LossInfoStashConfig(stash_final=True),\n",
    "                                            critic_loss=LossInfoStashConfig(stash_final=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        total_stopwatch.reset()\n",
    "        with log_experiment(\n",
    "            logger,\n",
    "            experiment_tags=algo.collect_tags(),\n",
    "            hyper_parameters=algo.collect_hyper_parameters(),\n",
    "            setup=get_setup(),\n",
    "        ) as x:\n",
    "            logger.save_experiment_log()\n",
    "            print('\\nStarting Training\\n\\n')\n",
    "            # import cProfile\n",
    "            # pr = cProfile.Profile()\n",
    "            # pr.enable()\n",
    "            algo.learn(3_500_000)\n",
    "            # pr.disable()  \n",
    "            # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T22:01:12.659642Z",
     "start_time": "2024-10-15T00:24:23.119786Z"
    }
   },
   "id": "f71efe062771e81b",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T20:26:08.622551Z",
     "start_time": "2024-10-12T20:26:08.202540Z"
    }
   },
   "id": "128e0218ec9cae23",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d1ae8571d73535c6",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ab51a61dd845",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
