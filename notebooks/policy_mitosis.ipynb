{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-19T15:21:22.816637Z",
     "start_time": "2024-06-19T15:21:21.101977Z"
    }
   },
   "source": [
    "import sys\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from notebooks.policy_mitosis import init_action_selector, init_policy, init_optimizer, wrap_env\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.np_functions import softmax\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.async_policy_mitosis import AsyncPolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis_base import TrainResultInfo, TrainInfo\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPOLoggingConfig, PPO\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.core.policy_construction import PolicyConstruction\n",
    "from src.reinforcement_learning.core.policy_evaluation import evaluate_policy\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.torch_device import get_torch_device\n",
    "from src.torch_functions import antisymmetric_power\n",
    "from src.trees import Forest\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def train_func(train_info: TrainInfo) -> TrainResultInfo:\n",
    "    policy = train_info['policy']\n",
    "    optimizer = train_info['optimizer']\n",
    "    env = train_info['env']\n",
    "    policy_info = train_info['policy_info']\n",
    "    \n",
    "    score = 0.0\n",
    "    score_ema = ExponentialMovingAverage(0.45)\n",
    "    rollout_stopwatch = Stopwatch()\n",
    "    def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):   \n",
    "        \n",
    "        rewards = rl.buffer.rewards\n",
    "        if 'raw_rewards' in info['rollout']:\n",
    "            rewards = info['rollout']['raw_rewards']\n",
    "        \n",
    "        episode_scores = compute_episode_returns(\n",
    "            rewards=rewards,\n",
    "            episode_starts=rl.buffer.episode_starts,\n",
    "            last_episode_starts=info['last_episode_starts'],\n",
    "        )\n",
    "        \n",
    "        nonlocal score, score_ema\n",
    "        score = episode_scores.mean()\n",
    "        current_score_ema = score_ema.update(score)\n",
    "        \n",
    "        rollout_time = rollout_stopwatch.reset()\n",
    "        \n",
    "        resets: np.ndarray = rl.buffer.episode_starts.astype(int).sum(axis=0)\n",
    "        resets_mean = resets.mean()\n",
    "        resets_min = resets.min()\n",
    "        \n",
    "        print(f'{policy_info[\"policy_id\"]}  {step:>6}: '\n",
    "              f'{score = :9.3f}, '\n",
    "              f'score_ema = {current_score_ema or score_ema.get():9.3f}, '\n",
    "              f'time = {rollout_time:5.2f}, '\n",
    "              f'resets = {resets_mean:5.2f} >= {resets_min:5.2f}')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    optimizations_done = 0\n",
    "    def on_optimization_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "        nonlocal optimizations_done\n",
    "        optimizations_done += 1\n",
    "    \n",
    "    policy_info_str = ('('\n",
    "          f'policy_id = {policy_info[\"policy_id\"]}, '\n",
    "          f'parent_id = {policy_info[\"parent_policy_id\"]}, '\n",
    "          f'num_parameters = {count_parameters(policy)}, '\n",
    "          f'previous_steps = {policy_info[\"steps_trained\"]}, '\n",
    "          f'previous_score = {policy_info[\"score\"]:9.3f}'\n",
    "          ')')\n",
    "    \n",
    "    print(f'Starting PPO with policy {policy_info_str:s} for {steps_per_iteration:_} steps')\n",
    "    mitosis_iteration_stopwatch = Stopwatch()\n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        policy_optimizer=optimizer,\n",
    "        buffer_size=5000,\n",
    "        gamma=0.995,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        weigh_and_reduce_actor_objective=lambda obj: antisymmetric_power(obj, 1.5).mean(),\n",
    "        weigh_and_reduce_entropy_objective=None,  # lambda obj: 1.0 * obj.mean(),\n",
    "        weigh_and_reduce_critic_objective=lambda obj: 0.5 * obj.mean(),\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.025,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.1,\n",
    "        grad_norm_clip_value=1.0,\n",
    "        callback=Callback(\n",
    "            on_rollout_done=on_rollout_done,\n",
    "            on_optimization_done=on_optimization_done,\n",
    "        ),\n",
    "        logging_config=PPOLoggingConfig(log_rollout_infos=True, log_last_obs=True),\n",
    "        torch_device=device,\n",
    "    ).train(steps_per_iteration)\n",
    "    \n",
    "    eval_scores = evaluate_policy(\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        num_steps=10_000,\n",
    "    )\n",
    "    eval_score = eval_scores.mean() \n",
    "    \n",
    "    print(f'Training finished for policy {policy_info_str:s}, '\n",
    "          f'evaluation_score = {eval_score:9.3f}, '\n",
    "          f'moving average score = {score_ema.get():9.3f}, '\n",
    "          f'time = {mitosis_iteration_stopwatch.time_passed():6.2f}')\n",
    "    \n",
    "    return {\n",
    "        'steps_trained': steps_per_iteration, \n",
    "        'optimizations_done': optimizations_done, \n",
    "        'score': eval_score,\n",
    "        'extra_infos': {\n",
    "            'score_ema': score_ema.get()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def select_policy_selection_probs(policy_infos: Iterable[MitosisPolicyInfo]) -> np.ndarray:\n",
    "    # TODO introduce score change momentum factor, average child score\n",
    "    policy_infos = list(policy_infos)\n",
    "    policy_info_forest = Forest(\n",
    "        policy_infos, \n",
    "        get_id=lambda pi: pi['policy_id'], \n",
    "        get_parent_id=lambda pi: pi['parent_policy_id']\n",
    "    )\n",
    "    \n",
    "    scores = np.array([policy_info['score'] for policy_info in policy_infos], dtype=float)\n",
    "    score_probs = softmax(scores, temperature=0.5 / np.log(len(scores) + 1), normalize=True)\n",
    "    \n",
    "    num_descendants = np.array([\n",
    "        policy_info_forest.compute_num_descendants(policy_info['policy_id'], discount_factor=0.5) \n",
    "        for policy_info in policy_infos\n",
    "    ], dtype=float)\n",
    "    num_descendants_probs = softmax(-num_descendants, temperature=0.5)\n",
    "    \n",
    "    steps_trained = np.array([policy_info['steps_trained'] for policy_info in policy_infos], dtype=float)\n",
    "    steps_trained_probs = softmax(-steps_trained, temperature=0.1, normalize=True)\n",
    "    \n",
    "    score_weight = 1.0\n",
    "    num_descendants_weight = 0.5\n",
    "    steps_trained_weight = 0.1\n",
    "    \n",
    "    probs = (\n",
    "        score_probs**score_weight * \n",
    "        num_descendants_probs**num_descendants_weight * \n",
    "        steps_trained_probs**steps_trained_weight\n",
    "    )\n",
    "    probs /= probs.sum()\n",
    "    \n",
    "    print('policy selection probs = \\n\\t' + '\\n\\t'.join(\n",
    "        f'{(policy_id := policy_infos[i][\"policy_id\"])}: {p = :8.6f}, '\n",
    "        f'score = {policy_infos[i][\"score\"]:7.3f}, '\n",
    "        f'score_prob = {score_probs[i]**score_weight:7.5f}, '\n",
    "        f'num_children = {len(policy_info_forest[policy_id].children)}, '\n",
    "        f'num_descendants = {num_descendants[i]:7.3f}, '\n",
    "        f'descendants_prob = {num_descendants_probs[i]**num_descendants_weight:7.5f}, '\n",
    "        f'steps = {policy_infos[i][\"steps_trained\"]}, '\n",
    "        f'steps_prob = {steps_trained_probs[i]**steps_trained_weight:7.5f}, '\n",
    "        for i, p\n",
    "        in enumerate(probs)\n",
    "    ))\n",
    "    \n",
    "    return probs\n",
    "\n",
    "device = get_torch_device(\"cuda:0\") if True else get_torch_device('cpu')\n",
    "\n",
    "steps_per_iteration = 50_000\n",
    "\n",
    "# env_name = 'Humanoid-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "env_name = 'Ant-v4'\n",
    "env_kwargs = {'healthy_reward': 0.001, 'ctrl_cost_weight': 0.05 }\n",
    "num_envs = 16\n",
    "\n",
    "mitosis_id = get_current_timestamp()\n",
    "# mitosis_id = '2024-06-10_19.43.13'\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'E:/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'C:/Users/domin/git/pytorch-starter/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "\n",
    "try:\n",
    "    print(f'Starting mitosis with id {mitosis_id}')\n",
    "    AsyncPolicyMitosis(\n",
    "        num_workers=3,\n",
    "        policy_db=policy_db,\n",
    "        train_policy_function=train_func,\n",
    "        create_env=lambda: parallelize_env_async(lambda: gym.make(env_name, render_mode=None, **env_kwargs), num_envs),\n",
    "        new_policy_initialization_info=PolicyConstruction.create_policy_initialization_info(\n",
    "            init_action_selector=init_action_selector,\n",
    "            init_policy=init_policy,\n",
    "            init_optimizer=init_optimizer,\n",
    "            wrap_env=wrap_env,\n",
    "        ),\n",
    "        new_policy_prob_function=lambda nr_policies, nr_primordial_ancestors: 0.0,\n",
    "        modify_policy=None,\n",
    "        select_policy_selection_probs=select_policy_selection_probs,\n",
    "        min_primordial_ancestors=1,\n",
    "        rng_seed=None,\n",
    "        initialization_delay=5,\n",
    "        delay_between_workers=5,\n",
    "        save_optimizer_state_dicts=True,\n",
    "        load_optimizer_state_dicts=True,\n",
    "    ).train_with_mitosis(1000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-19T15:21:22.816637Z"
    }
   },
   "id": "62b56460e99463f4",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "id": "bb9ea562feef0c28",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
