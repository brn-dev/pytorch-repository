{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.networks.multihead_self_attention import MultiheadSelfAttention\n",
    "from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.reinforcement_learning.algorithms.supervised_pre_training.zero_action_pre_training import ZeroActionPreTraining\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.objectives import ObjectiveLoggingConfig\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.gym.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "from src.reinforcement_learning.algorithms import policy_optimization_base\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "from src.weight_initialization import orthogonal_initialization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T16:59:58.351Z",
     "start_time": "2024-05-30T16:59:58.219086Z"
    }
   },
   "id": "ba8c59a3eba2f172",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8)\n"
     ]
    }
   ],
   "source": [
    "nr_carts = 2\n",
    "def make_multi_agent_cart_pole_env(render_mode: str | None = None):\n",
    "    from src.reinforcement_learning.gym.envs.multi_agent_cartpole3d import MultiAgentCartPole3D\n",
    "    return MultiAgentCartPole3D(\n",
    "        nr_carts=nr_carts,\n",
    "        cart_size=0.25,\n",
    "        force_magnitude=500,\n",
    "        physics_steps_per_step=15,\n",
    "        reset_position_radius=0.75,\n",
    "        reset_randomize_position_angle_offset=True,\n",
    "        reset_position_randomization_magnitude=0.3,\n",
    "        reset_hinge_randomization_magnitude=0.05,\n",
    "        slide_range=2,\n",
    "        hinge_range=0.99,\n",
    "        time_limit=120.0,\n",
    "        step_reward_function=lambda time_, action, state, prev_state: 0.01,\n",
    "        out_ouf_range_reward_function=lambda time_, action, state: 0.0,# -10 + time_ * 3,\n",
    "        time_limit_reward_function=lambda time_, action, state: 10,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "tmp_env = make_multi_agent_cart_pole_env()\n",
    "print(tmp_env.reset()[0].shape)\n",
    "tmp_env.step(np.zeros(2 * nr_carts))\n",
    "tmp_env.render(camera_id=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T17:00:02.108457Z",
     "start_time": "2024-05-30T17:00:01.966309Z"
    }
   },
   "id": "5069af9187794400",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "policy_id: str\n",
    "policy: Optional[BasePolicy]\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy = init_policy()\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy\n",
    "\n",
    "def init_policy():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.net import Net\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n",
    "        SquashedDiagGaussianActionSelector\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n",
    "    from src.weight_initialization import orthogonal_initialization\n",
    "    from src.networks.multihead_self_attention import MultiheadSelfAttention\n",
    "    \n",
    "    in_size = 8\n",
    "    action_size = 2\n",
    "    \n",
    "    actor_layers = 3\n",
    "    actor_features = 48\n",
    "    \n",
    "    critic_layers = 2\n",
    "    critic_features = 48\n",
    "\n",
    "    actor_hidden_activation_function = nn.ELU\n",
    "    critic_hidden_activation_function = nn.ELU\n",
    "    \n",
    "    actor_hidden_initialization = lambda module: orthogonal_initialization(module, gain=np.sqrt(2))\n",
    "    critic_hidden_initialization = lambda module: orthogonal_initialization(module, gain=np.sqrt(2))\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.actor_embedding = nn.Sequential(nn.Linear(in_size, actor_features), actor_hidden_activation_function())\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.sequential_net(\n",
    "                        actor_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        actor_hidden_activation_function(),\n",
    "                        actor_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        nn.Tanh() if is_last_layer else actor_hidden_activation_function(),\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=actor_layers,\n",
    "                num_features=actor_features,\n",
    "            )\n",
    "\n",
    "            self.critic_embedding = nn.Sequential(nn.Linear(in_size, critic_features), critic_hidden_activation_function())\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.sequential_net(\n",
    "                        critic_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        critic_hidden_activation_function(),\n",
    "                        critic_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        critic_hidden_activation_function(),\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=critic_layers,\n",
    "                num_features=critic_features,\n",
    "            )\n",
    "            self.critic_regressor = nn.Linear(critic_features, 1)\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            *batch_shape, nr_actors, nr_features = x.shape\n",
    "            x = torch.flatten(x, end_dim=-3)\n",
    "            \n",
    "            actor_out: torch.Tensor = self.actor(self.actor_embedding(x))\n",
    "            critic_out: torch.Tensor = self.critic_regressor(self.critic(self.critic_embedding(x)).sum(dim=-2))\n",
    "            \n",
    "            actor_out = actor_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            critic_out = critic_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            \n",
    "            return actor_out, critic_out\n",
    "    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n",
    "        latent_dim=actor_features,\n",
    "        action_dim=action_size,\n",
    "        std=0.1,\n",
    "        std_learnable=False,\n",
    "        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "    ))\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if 'raw_rewards' in info['rollout']:\n",
    "        raw_rewards = info['rollout']['raw_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "            rewards=raw_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_scores = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    global best_iteration_score\n",
    "    iteration_score = episode_scores.mean()\n",
    "    score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    if iteration_score >= best_iteration_score:\n",
    "        best_iteration_score = iteration_score\n",
    "        policy_db.save_model_state_dict(\n",
    "            model=policy,\n",
    "            model_id=policy_id,\n",
    "            parent_model_id=parent_policy_id,\n",
    "            model_info={\n",
    "                'score': iteration_score.item(),\n",
    "                'steps_trained': steps_trained,\n",
    "                'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                'init_policy_source_code': init_policy_source\n",
    "            },\n",
    "        )\n",
    "        \n",
    "    info['episode_scores'] = episode_scores\n",
    "    info['score_moving_average'] = score_moving_average\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    time_taken = stopwatch.reset()\n",
    "    \n",
    "    global steps_trained\n",
    "    steps_trained += rl.buffer.pos\n",
    "    \n",
    "    episode_scores = info['episode_scores']\n",
    "    score_moving_average = info['score_moving_average']\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    action_mags = format_summary_statics(\n",
    "        torch.stack(rl.buffer.actions).detach() ** 2,\n",
    "        mean_format='4.2f',\n",
    "        std_format='.2f',\n",
    "        max_value_format='4.2f',\n",
    "        \n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        torch.abs(info['raw_actor_objective']),  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_obj = None if info['reduced_entropy_objective'] is None else format_summary_statics(\n",
    "        info['reduced_entropy_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['reduced_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = format_summary_statics(\n",
    "        rl.buffer.episode_starts.astype(int).sum(axis=0), \n",
    "        mean_format='.2f',\n",
    "        std_format=None,\n",
    "        min_value_format='1d',\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    kl_div = info['actor_kl_divergence'][-1]\n",
    "    grad_norm = format_summary_statics(\n",
    "        info['grad_norm'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    rollout_action_stds = format_summary_statics(\n",
    "        info['rollout']['action_stds'],\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    ppo_epochs = info['nr_ppo_epochs']\n",
    "    ppo_updates = info['nr_ppo_updates']\n",
    "    expl_var = rl.buffer.compute_critic_explained_variance(info['returns'])\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          f\"{advantages = :s}, \"\n",
    "          f\"{action_mags = :s}, \"\n",
    "          f\"{abs_actor_obj = :s}, \"\n",
    "          +(f\"{entropy_obj = :s}, \" if entropy_obj is not None else '')+\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{critic_obj = :s}, \"\n",
    "          f\"{expl_var = :.3f}, \"\n",
    "          f\"{kl_div = :.4f}, \"\n",
    "          f\"{ppo_epochs = }, \"\n",
    "          f\"{ppo_updates = }, \"\n",
    "          f\"{grad_norm = :s}, \"\n",
    "          f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f} \\n\")\n",
    "    print()\n",
    "\n",
    "default_device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {default_device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return make_multi_agent_cart_pole_env(render_mode=render_mode)\n",
    "\n",
    "def wrap_env(env_):\n",
    "    return env_\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "num_envs = 32\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/MultiAgentCartPole/{nr_carts}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None #'2024-05-26_21.07.11'  # '2024-04-28_20.57.23'\n",
    "policy_id, policy = get_policy(create_new_if_exists=True)\n",
    "print(f'{count_parameters(policy) = }')\n",
    "\n",
    "env = parallelize_env_async(lambda: make_multi_agent_cart_pole_env(render_mode=None), num_envs)\n",
    "# env, _ = as_vec_env(create_env(render_mode=None))\n",
    "print(env.reset()[0].shape)\n",
    "try:\n",
    "    env = wrap_env(env)\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        # print('Running zero action pre-training')\n",
    "        # ZeroActionPreTraining(\n",
    "        #     env=env,\n",
    "        #     policy=policy.to(default_device),\n",
    "        #     policy_optimizer=lambda pol: optim.AdamW(pol.parameters(), lr=1e-4),\n",
    "        #     buffer_size=250,\n",
    "        #     callback=Callback(\n",
    "        #         on_optimization_done=lambda rl, step, info, scheduler_values: print(f\"{step:>5}: {info['zero_action_objective'].item():.4e}\")\n",
    "        #     ),\n",
    "        #     torch_device=default_device,\n",
    "        # ).train(50_000)\n",
    "        \n",
    "        print('Running PPO')\n",
    "        PPO(\n",
    "            env=env,\n",
    "            policy=policy.to(default_device),\n",
    "            policy_optimizer=lambda pol: optim.AdamW(pol.parameters(), lr=1e-5),\n",
    "            buffer_size=2500,\n",
    "            gamma=0.995,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=NormalizationType.Std,\n",
    "            weigh_and_reduce_actor_objective=lambda obj: antisymmetric_power(obj, 1.5).mean(),\n",
    "            weigh_and_reduce_entropy_objective=lambda obj: 0.1 * obj.mean(),\n",
    "            weigh_and_reduce_critic_objective=lambda obj: 0.5 * obj.mean(),\n",
    "            ppo_max_epochs=10,\n",
    "            ppo_kl_target=0.025,\n",
    "            ppo_batch_size=500,\n",
    "            action_ratio_clip_range=0.1,\n",
    "            grad_norm_clip_value=0.5,\n",
    "            # sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=PPOLoggingConfig(log_returns=True, log_advantages=True, log_grad_norm=True,\n",
    "                                            log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_actor_kl_divergence=True,\n",
    "                                            actor_objective=ObjectiveLoggingConfig(log_raw=True),\n",
    "                                            entropy_objective=ObjectiveLoggingConfig(log_reduced=True),\n",
    "                                            critic_objective=ObjectiveLoggingConfig(log_reduced=True), ),\n",
    "            torch_device=default_device,\n",
    "        ).train(5_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T17:03:33.984291Z",
     "start_time": "2024-05-30T17:02:52.803736Z"
    }
   },
   "id": "f71efe062771e81b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "New policy 2024-05-30_19.02.53 created\n",
      "Using policy 2024-05-30_19.02.53 with parent policy None\n",
      "count_parameters(policy) = 72531\n",
      "(32, 2, 8)\n",
      "env = AsyncVectorEnv(32), num_envs = 32 \n",
      "\n",
      "\n",
      "Running PPO\n",
      "step =    2500, scores =  0.197 ± 0.024 [ 0.000, 0.290], score_ema =  0.197, advantages =  1.198 ± 1.0 [ -3.969,  4.532], action_mags = 0.01 ± 0.01 [0.00, 0.18], abs_actor_obj =  1.765 ± 1.053, entropy_obj = 0.090 ± 0.000, rollout_stds = 0.100 ± 0.000, critic_obj = 0.474 ± 0.103, expl_var = -68.818, kl_div = 0.0011, ppo_epochs = 10, ppo_updates = 50, grad_norm =  9.734 ± 1.6 [  7.052, 12.494], resets = 119.94 ≥ 117, time = 37.6 \n",
      "\n",
      "\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 342\u001B[0m\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;66;03m# print('Running zero action pre-training')\u001B[39;00m\n\u001B[0;32m    299\u001B[0m         \u001B[38;5;66;03m# ZeroActionPreTraining(\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;66;03m#     torch_device=default_device,\u001B[39;00m\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;66;03m# ).train(50_000)\u001B[39;00m\n\u001B[0;32m    310\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRunning PPO\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    311\u001B[0m         \u001B[43mPPO\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m            \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefault_device\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpolicy_optimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpol\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAdamW\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.995\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgae_lambda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnormalize_rewards\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnormalize_advantages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNormalizationType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mStd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m            \u001B[49m\u001B[43mweigh_and_reduce_actor_objective\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mantisymmetric_power\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m            \u001B[49m\u001B[43mweigh_and_reduce_entropy_objective\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m            \u001B[49m\u001B[43mweigh_and_reduce_critic_objective\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m            \u001B[49m\u001B[43mppo_max_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m            \u001B[49m\u001B[43mppo_kl_target\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.025\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m            \u001B[49m\u001B[43mppo_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m            \u001B[49m\u001B[43maction_ratio_clip_range\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgrad_norm_clip_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    328\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# sde_noise_sample_freq=50,\u001B[39;49;00m\n\u001B[0;32m    329\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCallback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m                \u001B[49m\u001B[43mon_rollout_done\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon_rollout_done\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrollout_schedulers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m                \u001B[49m\u001B[43mon_optimization_done\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon_optimization_done\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m                \u001B[49m\u001B[43moptimization_schedulers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlogging_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mPPOLoggingConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_returns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_advantages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_grad_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mlog_rollout_infos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_rollout_action_stds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    337\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mlog_actor_kl_divergence\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    338\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mactor_objective\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mObjectiveLoggingConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_raw\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    339\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mentropy_objective\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mObjectiveLoggingConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_reduced\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    340\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mcritic_objective\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mObjectiveLoggingConfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_reduced\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtorch_device\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m--> 342\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5_000_000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m    344\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeyboard interrupt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Git\\pytorch-starter\\src\\reinforcement_learning\\algorithms\\policy_optimization_base.py:169\u001B[0m, in \u001B[0;36mPolicyOptimizationBase.train\u001B[1;34m(self, num_steps)\u001B[0m\n\u001B[0;32m    167\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpol.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    168\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_optimizer\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopt.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 169\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = wrap_env(record_env)\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env, _ = as_vec_env(record_env)\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            actions_dist, _ = policy.process_obs(torch.tensor(obs, device=default_device))\n",
    "            actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-28T17:04:44.249831Z",
     "start_time": "2024-05-28T17:04:41.067814Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-0.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-1.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-2.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-3.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-4.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-5.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-5.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-6.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-6.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-6.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-7.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-7.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-7.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-05-28_19.04.41\\rl-video-episode-8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyboard interrupt\n",
      "closing record_env\n",
      "record_env closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bba6ab51a61dd845",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
