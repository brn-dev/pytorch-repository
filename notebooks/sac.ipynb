{
 "cells": [
  {
   "cell_type": "code",
   "id": "ba8c59a3eba2f172",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T19:25:59.350648Z",
     "start_time": "2024-10-10T19:25:56.187452Z"
    }
   },
   "source": [
    "\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "\n",
    "from src.experiment_logging.experiment_log import ExperimentLogItem\n",
    "from src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.loss_config import LossLoggingConfig\n",
    "from src.reinforcement_learning.core.policies.components.actor import Actor\n",
    "from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import maybe_compute_summary_statistics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_setup() -> dict[str, str]:\n",
    "    return {\n",
    "        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n",
    "    }\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "step_stopwatch = Stopwatch()\n",
    "total_stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n",
    "    \n",
    "    if len(episode_scores) > 0:\n",
    "    \n",
    "        global best_iteration_score\n",
    "        iteration_score = episode_scores.mean()\n",
    "        if iteration_score >= best_iteration_score:\n",
    "            pass\n",
    "    \n",
    "    info['episode_scores'] = episode_scores\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    num_env_steps = step * rl.num_envs\n",
    "    \n",
    "    step_time = step_stopwatch.reset()\n",
    "    total_time = total_stopwatch.time_passed()\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    episode_scores = info.get('episode_scores')\n",
    "    \n",
    "    log_item: ExperimentLogItem = {\n",
    "        'step': step,\n",
    "        'num_env_steps': num_env_steps,\n",
    "        'scores': maybe_compute_summary_statistics(episode_scores),\n",
    "        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n",
    "        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n",
    "        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n",
    "        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n",
    "        'action_stds': maybe_compute_summary_statistics(info['rollout'].get('action_stds')),\n",
    "        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n",
    "        'num_gradient_steps': rl.gradient_steps_performed,\n",
    "        'step_time': step_time,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "    logger.add_item(log_item)\n",
    "    print(logger.format_log_item(log_item, mean_format='5.3f', std_format='5.3f'))\n",
    "    if step % 10000 == 0:\n",
    "        logger.save_experiment_log()\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "env_kwargs = {}\n",
    "num_envs = 1\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    make_single_env = lambda: gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "    \n",
    "    if num_envs == 1:\n",
    "        return make_single_env()\n",
    "        \n",
    "    return parallelize_env_async(make_single_env, num_envs)\n",
    "\n",
    "\n",
    "def create_policy():\n",
    "    in_size = 17\n",
    "    action_size = 6\n",
    "    \n",
    "    actor_net = nn.Sequential(\n",
    "        nn.Linear(in_size, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    critic = QCritic(\n",
    "        n_critics=2,\n",
    "        create_q_network=lambda: nn.Sequential(\n",
    "            nn.Linear(in_size + action_size, 256),\n",
    "            nn.ReLU(),\n",
    "            # BatchRenorm(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            # BatchRenorm(256),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return SACPolicy(\n",
    "        actor=Actor(actor_net, PredictedStdActionSelector(\n",
    "            latent_dim=256,\n",
    "            action_dim=action_size,\n",
    "            base_std=1.0,\n",
    "            squash_output=True,\n",
    "        )),\n",
    "        critic=critic\n",
    "    )\n",
    "\n",
    "\n",
    "env = create_env(render_mode=None)\n",
    "policy = create_policy()\n",
    "logger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n",
    "\n",
    "try:\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = }')\n",
    "        \n",
    "    with ((torch.autograd.set_detect_anomaly(False))):\n",
    "        algo = SAC(\n",
    "            env=env,\n",
    "            policy=policy,\n",
    "            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            buffer_size=1_000_000,\n",
    "            reward_scale=1,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n",
    "            entropy_coef=1.0,\n",
    "            rollout_steps=1,\n",
    "            gradient_steps=1,\n",
    "            warmup_steps=10_000,\n",
    "            optimization_batch_size=256,\n",
    "            target_update_interval=1,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n",
    "                                            actor_loss=LossLoggingConfig(log_final=True),\n",
    "                                            critic_loss=LossLoggingConfig(log_final=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        total_stopwatch.reset()\n",
    "        with log_experiment(\n",
    "            logger,\n",
    "            experiment_tags=algo.collect_tags(),\n",
    "            hyper_parameters=algo.collect_hyper_parameters(),\n",
    "            setup=get_setup(),\n",
    "        ) as x:\n",
    "            logger.save_experiment_log()\n",
    "            print('\\nStarting Training\\n\\n')\n",
    "            # import cProfile\n",
    "            # pr = cProfile.Profile()\n",
    "            # pr.enable()\n",
    "            algo.learn(5_000_000)\n",
    "            # pr.disable()  \n",
    "            # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T19:33:04.664538Z",
     "start_time": "2024-10-10T19:25:59.351649Z"
    }
   },
   "id": "f71efe062771e81b",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "No policy in RAM, creating a new one\n",
      "New policy 2024-10-10 19:25:59.513793 created\n",
      "Using policy 2024-10-10 19:25:59.513793 with parent policy None\n",
      "count_parameters(policy) = 217870\n",
      "env = <TimeLimit<OrderEnforcing<PassiveEnvChecker<HalfCheetahEnv<HalfCheetah-v4>>>>>, num_envs = 1\n",
      "Grabbing system information... done!\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "Starting Training\n",
      "\n",
      "step =   11000, num_env_steps =   11000, scores = -139.240 (n= 1), score_ema = -254.650, actor_loss = -15.036, critic_loss = 18.719, entropy_coef_loss = -2.811, entropy_coef = 0.745, rollout_stds = 0.943 ± 0.024, action_magnitude = 0.553 ± 0.289, n_updates = 1000, time = 13.6, total_time = 13.2 \n",
      "\n",
      "step =   12000, num_env_steps =   12000, scores = -137.261 (n= 1), score_ema = -225.303, actor_loss = -23.296, critic_loss = 15.917, entropy_coef_loss = -4.861, entropy_coef = 0.557, rollout_stds = 0.794 ± 0.171, action_magnitude = 0.573 ± 0.292, n_updates = 2000, time =  9.1, total_time = 22.3 \n",
      "\n",
      "step =   13000, num_env_steps =   13000, scores = -272.821 (n= 1), score_ema = -237.183, actor_loss = -29.715, critic_loss = 21.822, entropy_coef_loss = -6.886, entropy_coef = 0.421, rollout_stds = 0.844 ± 0.099, action_magnitude = 0.601 ± 0.293, n_updates = 3000, time =  9.2, total_time = 31.5 \n",
      "\n",
      "step =   14000, num_env_steps =   14000, scores = -536.847 (n= 1), score_ema = -312.099, actor_loss = -32.409, critic_loss = 31.942, entropy_coef_loss = -7.924, entropy_coef = 0.320, rollout_stds = 0.772 ± 0.042, action_magnitude = 0.582 ± 0.292, n_updates = 4000, time =  9.2, total_time = 40.7 \n",
      "\n",
      "step =   15000, num_env_steps =   15000, scores = -283.037 (n= 1), score_ema = -304.833, actor_loss = -35.095, critic_loss = 29.034, entropy_coef_loss = -8.690, entropy_coef = 0.244, rollout_stds = 0.656 ± 0.094, action_magnitude = 0.541 ± 0.293, n_updates = 5000, time =  9.7, total_time = 50.4 \n",
      "\n",
      "step =   16000, num_env_steps =   16000, scores = -258.312 (n= 1), score_ema = -293.203, actor_loss = -35.874, critic_loss = 50.492, entropy_coef_loss = -8.355, entropy_coef = 0.189, rollout_stds = 0.752 ± 0.051, action_magnitude = 0.577 ± 0.298, n_updates = 6000, time =  9.7, total_time = 60.0 \n",
      "\n",
      "step =   17000, num_env_steps =   17000, scores = -255.271 (n= 1), score_ema = -283.720, actor_loss = -40.022, critic_loss = 31.813, entropy_coef_loss = -6.968, entropy_coef = 0.149, rollout_stds = 0.658 ± 0.088, action_magnitude = 0.626 ± 0.295, n_updates = 7000, time =  9.6, total_time = 69.6 \n",
      "\n",
      "step =   18000, num_env_steps =   18000, scores = -274.856 (n= 1), score_ema = -281.504, actor_loss = -47.818, critic_loss = 37.832, entropy_coef_loss = -3.779, entropy_coef = 0.121, rollout_stds = 0.703 ± 0.127, action_magnitude = 0.612 ± 0.299, n_updates = 8000, time = 10.2, total_time = 79.7 \n",
      "\n",
      "step =   19000, num_env_steps =   19000, scores = -190.951 (n= 1), score_ema = -258.866, actor_loss = -51.805, critic_loss = 40.311, entropy_coef_loss = -2.475, entropy_coef = 0.101, rollout_stds = 0.702 ± 0.051, action_magnitude = 0.639 ± 0.300, n_updates = 9000, time = 10.2, total_time = 89.9 \n",
      "\n",
      "step =   20000, num_env_steps =   20000, scores = -281.141 (n= 1), score_ema = -264.435, actor_loss = -60.509, critic_loss = 41.798, entropy_coef_loss = -0.334, entropy_coef = 0.087, rollout_stds = 0.603 ± 0.084, action_magnitude = 0.695 ± 0.297, n_updates = 10000, time =  9.7, total_time = 99.6 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   21000, num_env_steps =   21000, scores = -225.209 (n= 1), score_ema = -254.628, actor_loss = -50.654, critic_loss = 47.000, entropy_coef_loss = -2.255, entropy_coef = 0.078, rollout_stds = 0.629 ± 0.079, action_magnitude = 0.723 ± 0.296, n_updates = 11000, time =  9.6, total_time = 109.2 \n",
      "\n",
      "step =   22000, num_env_steps =   22000, scores =  168.986 (n= 1), score_ema = -148.725, actor_loss = -59.542, critic_loss = 49.299, entropy_coef_loss = -2.084, entropy_coef = 0.070, rollout_stds = 0.401 ± 0.151, action_magnitude = 0.721 ± 0.292, n_updates = 12000, time =  9.5, total_time = 118.7 \n",
      "\n",
      "step =   23000, num_env_steps =   23000, scores = -45.413 (n= 1), score_ema = -122.897, actor_loss = -56.033, critic_loss = 33.902, entropy_coef_loss = -1.696, entropy_coef = 0.062, rollout_stds = 0.583 ± 0.074, action_magnitude = 0.702 ± 0.296, n_updates = 13000, time = 10.6, total_time = 129.3 \n",
      "\n",
      "step =   24000, num_env_steps =   24000, scores = -222.297 (n= 1), score_ema = -147.747, actor_loss = -67.075, critic_loss = 34.397, entropy_coef_loss = -0.288, entropy_coef = 0.058, rollout_stds = 0.610 ± 0.145, action_magnitude = 0.724 ± 0.289, n_updates = 14000, time =  9.6, total_time = 138.9 \n",
      "\n",
      "step =   25000, num_env_steps =   25000, scores =  478.157 (n= 1), score_ema =  8.729, actor_loss = -65.079, critic_loss = 36.743, entropy_coef_loss = 1.120, entropy_coef = 0.058, rollout_stds = 0.389 ± 0.152, action_magnitude = 0.700 ± 0.297, n_updates = 15000, time =  9.4, total_time = 148.3 \n",
      "\n",
      "step =   26000, num_env_steps =   26000, scores =  793.695 (n= 1), score_ema =  204.971, actor_loss = -74.468, critic_loss = 40.615, entropy_coef_loss = 1.102, entropy_coef = 0.059, rollout_stds = 0.369 ± 0.109, action_magnitude = 0.692 ± 0.295, n_updates = 16000, time =  9.8, total_time = 158.1 \n",
      "\n",
      "step =   27000, num_env_steps =   27000, scores =  1.820 (n= 1), score_ema =  154.183, actor_loss = -73.246, critic_loss = 38.841, entropy_coef_loss = -0.794, entropy_coef = 0.059, rollout_stds = 0.691 ± 0.134, action_magnitude = 0.707 ± 0.295, n_updates = 17000, time =  9.7, total_time = 167.9 \n",
      "\n",
      "step =   28000, num_env_steps =   28000, scores =  1552.657 (n= 1), score_ema =  503.801, actor_loss = -73.729, critic_loss = 33.808, entropy_coef_loss = 1.580, entropy_coef = 0.061, rollout_stds = 0.397 ± 0.181, action_magnitude = 0.724 ± 0.293, n_updates = 18000, time = 10.0, total_time = 177.8 \n",
      "\n",
      "step =   29000, num_env_steps =   29000, scores =  1840.126 (n= 1), score_ema =  837.883, actor_loss = -89.666, critic_loss = 60.047, entropy_coef_loss = 2.245, entropy_coef = 0.070, rollout_stds = 0.372 ± 0.196, action_magnitude = 0.744 ± 0.290, n_updates = 19000, time =  9.4, total_time = 187.2 \n",
      "\n",
      "step =   30000, num_env_steps =   30000, scores =  1690.662 (n= 1), score_ema =  1051.077, actor_loss = -99.582, critic_loss = 46.016, entropy_coef_loss = 2.932, entropy_coef = 0.083, rollout_stds = 0.439 ± 0.228, action_magnitude = 0.740 ± 0.294, n_updates = 20000, time =  9.2, total_time = 196.4 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   31000, num_env_steps =   31000, scores =  1613.961 (n= 1), score_ema =  1191.798, actor_loss = -115.636, critic_loss = 59.896, entropy_coef_loss = 0.119, entropy_coef = 0.099, rollout_stds = 0.480 ± 0.260, action_magnitude = 0.756 ± 0.285, n_updates = 21000, time =  9.2, total_time = 205.6 \n",
      "\n",
      "step =   32000, num_env_steps =   32000, scores =  2187.679 (n= 1), score_ema =  1440.768, actor_loss = -121.563, critic_loss = 61.442, entropy_coef_loss = 0.452, entropy_coef = 0.114, rollout_stds = 0.464 ± 0.224, action_magnitude = 0.776 ± 0.273, n_updates = 22000, time =  9.9, total_time = 215.5 \n",
      "\n",
      "step =   33000, num_env_steps =   33000, scores =  1725.022 (n= 1), score_ema =  1511.832, actor_loss = -153.678, critic_loss = 64.051, entropy_coef_loss = 1.562, entropy_coef = 0.129, rollout_stds = 0.469 ± 0.127, action_magnitude = 0.762 ± 0.280, n_updates = 23000, time = 10.7, total_time = 226.2 \n",
      "\n",
      "step =   34000, num_env_steps =   34000, scores =  1964.489 (n= 1), score_ema =  1624.996, actor_loss = -160.416, critic_loss = 66.722, entropy_coef_loss = -0.213, entropy_coef = 0.142, rollout_stds = 0.405 ± 0.052, action_magnitude = 0.788 ± 0.269, n_updates = 24000, time =  9.8, total_time = 236.0 \n",
      "\n",
      "step =   35000, num_env_steps =   35000, scores =  1367.221 (n= 1), score_ema =  1560.552, actor_loss = -168.835, critic_loss = 77.575, entropy_coef_loss = 0.503, entropy_coef = 0.149, rollout_stds = 0.634 ± 0.065, action_magnitude = 0.783 ± 0.269, n_updates = 25000, time =  9.8, total_time = 245.7 \n",
      "\n",
      "step =   36000, num_env_steps =   36000, scores =  586.057 (n= 1), score_ema =  1316.929, actor_loss = -198.375, critic_loss = 78.700, entropy_coef_loss = -0.488, entropy_coef = 0.147, rollout_stds = 0.684 ± 0.093, action_magnitude = 0.729 ± 0.289, n_updates = 26000, time = 10.1, total_time = 255.8 \n",
      "\n",
      "step =   37000, num_env_steps =   37000, scores =  507.539 (n= 1), score_ema =  1114.581, actor_loss = -180.607, critic_loss = 85.309, entropy_coef_loss = 0.094, entropy_coef = 0.140, rollout_stds = 0.652 ± 0.115, action_magnitude = 0.721 ± 0.292, n_updates = 27000, time =  9.6, total_time = 265.5 \n",
      "\n",
      "step =   38000, num_env_steps =   38000, scores =  2329.716 (n= 1), score_ema =  1418.365, actor_loss = -209.942, critic_loss = 111.281, entropy_coef_loss = 0.677, entropy_coef = 0.143, rollout_stds = 0.387 ± 0.110, action_magnitude = 0.808 ± 0.250, n_updates = 28000, time =  9.7, total_time = 275.2 \n",
      "\n",
      "step =   39000, num_env_steps =   39000, scores =  2118.213 (n= 1), score_ema =  1593.327, actor_loss = -242.239, critic_loss = 116.545, entropy_coef_loss = 0.670, entropy_coef = 0.149, rollout_stds = 0.455 ± 0.041, action_magnitude = 0.799 ± 0.257, n_updates = 29000, time =  9.5, total_time = 284.7 \n",
      "\n",
      "step =   40000, num_env_steps =   40000, scores =  2583.218 (n= 1), score_ema =  1840.800, actor_loss = -254.659, critic_loss = 107.048, entropy_coef_loss = 0.470, entropy_coef = 0.157, rollout_stds = 0.412 ± 0.067, action_magnitude = 0.815 ± 0.246, n_updates = 30000, time =  9.6, total_time = 294.4 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   41000, num_env_steps =   41000, scores =  1673.034 (n= 1), score_ema =  1798.858, actor_loss = -272.172, critic_loss = 122.704, entropy_coef_loss = 0.730, entropy_coef = 0.166, rollout_stds = 0.621 ± 0.109, action_magnitude = 0.774 ± 0.272, n_updates = 31000, time = 10.3, total_time = 304.6 \n",
      "\n",
      "step =   42000, num_env_steps =   42000, scores =  2398.796 (n= 1), score_ema =  1948.843, actor_loss = -241.564, critic_loss = 117.358, entropy_coef_loss = -1.406, entropy_coef = 0.171, rollout_stds = 0.478 ± 0.054, action_magnitude = 0.801 ± 0.253, n_updates = 32000, time =  9.6, total_time = 314.2 \n",
      "\n",
      "step =   43000, num_env_steps =   43000, scores =  2585.223 (n= 1), score_ema =  2107.938, actor_loss = -287.482, critic_loss = 114.754, entropy_coef_loss = -0.549, entropy_coef = 0.173, rollout_stds = 0.370 ± 0.109, action_magnitude = 0.814 ± 0.246, n_updates = 33000, time =  9.7, total_time = 323.9 \n",
      "\n",
      "step =   44000, num_env_steps =   44000, scores =  2800.912 (n= 1), score_ema =  2281.181, actor_loss = -291.821, critic_loss = 137.356, entropy_coef_loss = -0.151, entropy_coef = 0.182, rollout_stds = 0.387 ± 0.130, action_magnitude = 0.806 ± 0.254, n_updates = 34000, time =  9.6, total_time = 333.5 \n",
      "\n",
      "step =   45000, num_env_steps =   45000, scores =  2848.736 (n= 1), score_ema =  2423.070, actor_loss = -348.292, critic_loss = 125.454, entropy_coef_loss = 0.764, entropy_coef = 0.187, rollout_stds = 0.438 ± 0.177, action_magnitude = 0.817 ± 0.246, n_updates = 35000, time = 10.7, total_time = 344.2 \n",
      "\n",
      "step =   46000, num_env_steps =   46000, scores =  2655.914 (n= 1), score_ema =  2481.281, actor_loss = -318.955, critic_loss = 151.002, entropy_coef_loss = -0.199, entropy_coef = 0.191, rollout_stds = 0.409 ± 0.155, action_magnitude = 0.796 ± 0.259, n_updates = 36000, time = 10.4, total_time = 354.6 \n",
      "\n",
      "step =   47000, num_env_steps =   47000, scores =  2954.192 (n= 1), score_ema =  2599.509, actor_loss = -337.271, critic_loss = 117.074, entropy_coef_loss = -0.544, entropy_coef = 0.195, rollout_stds = 0.419 ± 0.152, action_magnitude = 0.818 ± 0.244, n_updates = 37000, time = 10.3, total_time = 364.9 \n",
      "\n",
      "step =   48000, num_env_steps =   48000, scores =  2608.721 (n= 1), score_ema =  2601.812, actor_loss = -379.160, critic_loss = 139.369, entropy_coef_loss = 0.360, entropy_coef = 0.205, rollout_stds = 0.674 ± 0.106, action_magnitude = 0.800 ± 0.257, n_updates = 38000, time = 10.6, total_time = 375.5 \n",
      "\n",
      "step =   49000, num_env_steps =   49000, scores =  2822.421 (n= 1), score_ema =  2656.964, actor_loss = -367.413, critic_loss = 138.678, entropy_coef_loss = -0.113, entropy_coef = 0.213, rollout_stds = 0.455 ± 0.108, action_magnitude = 0.813 ± 0.244, n_updates = 39000, time = 10.5, total_time = 386.0 \n",
      "\n",
      "step =   50000, num_env_steps =   50000, scores =  2853.957 (n= 1), score_ema =  2706.212, actor_loss = -371.719, critic_loss = 119.470, entropy_coef_loss = 0.089, entropy_coef = 0.220, rollout_stds = 0.448 ± 0.098, action_magnitude = 0.817 ± 0.245, n_updates = 40000, time = 10.3, total_time = 396.3 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   51000, num_env_steps =   51000, scores =  2950.891 (n= 1), score_ema =  2767.382, actor_loss = -433.668, critic_loss = 201.100, entropy_coef_loss = -0.491, entropy_coef = 0.225, rollout_stds = 0.415 ± 0.106, action_magnitude = 0.807 ± 0.247, n_updates = 41000, time =  9.8, total_time = 406.2 \n",
      "\n",
      "step =   52000, num_env_steps =   52000, scores =  2915.616 (n= 1), score_ema =  2804.440, actor_loss = -430.337, critic_loss = 185.565, entropy_coef_loss = -0.682, entropy_coef = 0.233, rollout_stds = 0.480 ± 0.130, action_magnitude = 0.808 ± 0.247, n_updates = 42000, time =  9.3, total_time = 415.5 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n",
      "done\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "128e0218ec9cae23",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d1ae8571d73535c6",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ab51a61dd845",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
