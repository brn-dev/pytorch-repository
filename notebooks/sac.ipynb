{
 "cells": [
  {
   "cell_type": "code",
   "id": "ba8c59a3eba2f172",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T23:08:31.291507Z",
     "start_time": "2024-09-24T23:08:28.050703Z"
    }
   },
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.np_functions import inv_symmetric_log\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\n",
    "from src.reinforcement_learning.gym.envs.test_env import TestEnv\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.torch_device import set_default_torch_device, optimizer_to_device\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "No policy in RAM, creating a new one\n",
      "New policy 2024-09-24 23:08:33.923925 created\n",
      "Using policy 2024-09-24 23:08:33.923925 with parent policy None\n",
      "count_parameters(policy) = 217870\n",
      "env = AsyncVectorEnv(16), num_envs = 16 \n",
      "\n",
      "step =    1000, total_step =   16000, scores = N/A, score_ema =  0.000, actor_loss = -16.563, entropy_coef_loss = -2.739, critic_loss = 1.868, entropy_coef = 0.763, rollout_stds = 0.902 ± 0.036, action_magnitude = 0.520 ± 0.286, n_updates = 901, time = 13.1, total_time = 10.2 \n",
      "\n",
      "step =    2000, total_step =   32000, scores = -250.101 ± 74.096 [-410.303, -153.189] (n=16), score_ema = -250.101, actor_loss = -24.748, entropy_coef_loss = -5.640, critic_loss = 3.969, entropy_coef = 0.565, rollout_stds = 0.896 ± 0.038, action_magnitude = 0.532 ± 0.288, n_updates = 1901, time = 11.5, total_time = 21.7 \n",
      "\n",
      "step =    3000, total_step =   48000, scores = -213.952 ± 64.134 [-290.927, -68.861] (n=16), score_ema = -241.064, actor_loss = -29.363, entropy_coef_loss = -8.530, critic_loss = 1.698, entropy_coef = 0.420, rollout_stds = 0.907 ± 0.039, action_magnitude = 0.536 ± 0.289, n_updates = 2901, time = 12.2, total_time = 33.9 \n",
      "\n",
      "step =    4000, total_step =   64000, scores = -199.737 ± 52.925 [-281.427, -122.441] (n=16), score_ema = -230.732, actor_loss = -31.811, entropy_coef_loss = -11.453, critic_loss = 1.751, entropy_coef = 0.312, rollout_stds = 0.900 ± 0.050, action_magnitude = 0.541 ± 0.289, n_updates = 3901, time = 12.1, total_time = 46.0 \n",
      "\n",
      "step =    5000, total_step =   80000, scores = -216.140 ± 49.171 [-297.610, -92.505] (n=16), score_ema = -227.084, actor_loss = -32.931, entropy_coef_loss = -13.745, critic_loss = 1.908, entropy_coef = 0.232, rollout_stds = 0.908 ± 0.044, action_magnitude = 0.545 ± 0.291, n_updates = 4901, time = 13.8, total_time = 59.8 \n",
      "\n",
      "step =    6000, total_step =   96000, scores = -220.646 ± 35.546 [-274.282, -145.972] (n=16), score_ema = -225.475, actor_loss = -31.989, entropy_coef_loss = -15.754, critic_loss = 1.450, entropy_coef = 0.173, rollout_stds = 0.912 ± 0.053, action_magnitude = 0.549 ± 0.291, n_updates = 5901, time = 14.5, total_time = 74.3 \n",
      "\n",
      "step =    7000, total_step =  112000, scores = -175.702 ± 29.880 [-244.047, -114.807] (n=16), score_ema = -213.032, actor_loss = -31.202, entropy_coef_loss = -17.760, critic_loss = 1.478, entropy_coef = 0.130, rollout_stds = 0.873 ± 0.065, action_magnitude = 0.560 ± 0.292, n_updates = 6901, time = 15.1, total_time = 89.4 \n",
      "\n",
      "step =    8000, total_step =  128000, scores = -151.457 ± 29.783 [-233.485, -108.133] (n=16), score_ema = -197.638, actor_loss = -30.097, entropy_coef_loss = -19.339, critic_loss = 1.673, entropy_coef = 0.098, rollout_stds = 0.846 ± 0.061, action_magnitude = 0.574 ± 0.295, n_updates = 7901, time = 14.7, total_time = 104.1 \n",
      "\n",
      "step =    9000, total_step =  144000, scores = -85.902 ± 28.910 [-137.052, -31.264] (n=16), score_ema = -169.704, actor_loss = -28.517, entropy_coef_loss = -19.986, critic_loss = 1.186, entropy_coef = 0.074, rollout_stds = 0.849 ± 0.084, action_magnitude = 0.590 ± 0.295, n_updates = 8901, time = 13.2, total_time = 117.3 \n",
      "\n",
      "step =   10000, total_step =  160000, scores = -24.619 ± 22.834 [-82.166, 12.167] (n=16), score_ema = -133.433, actor_loss = -26.256, entropy_coef_loss = -20.709, critic_loss = 3.272, entropy_coef = 0.056, rollout_stds = 0.790 ± 0.093, action_magnitude = 0.605 ± 0.295, n_updates = 9901, time = 12.5, total_time = 129.9 \n",
      "\n",
      "step =   11000, total_step =  176000, scores =  23.612 ± 31.621 [-19.310, 86.116] (n=16), score_ema = -94.172, actor_loss = -25.160, entropy_coef_loss = -20.214, critic_loss = 1.037, entropy_coef = 0.042, rollout_stds = 0.772 ± 0.100, action_magnitude = 0.631 ± 0.295, n_updates = 10901, time = 12.6, total_time = 142.5 \n",
      "\n",
      "step =   12000, total_step =  192000, scores =  74.245 ± 37.488 [ 21.726, 137.217] (n=16), score_ema = -52.068, actor_loss = -23.887, entropy_coef_loss = -19.037, critic_loss = 1.033, entropy_coef = 0.032, rollout_stds = 0.711 ± 0.109, action_magnitude = 0.658 ± 0.293, n_updates = 11901, time = 13.4, total_time = 155.9 \n",
      "\n",
      "step =   13000, total_step =  208000, scores =  158.285 ± 36.589 [ 75.702, 208.968] (n=16), score_ema =  0.521, actor_loss = -22.572, entropy_coef_loss = -12.207, critic_loss = 3.093, entropy_coef = 0.025, rollout_stds = 0.678 ± 0.103, action_magnitude = 0.692 ± 0.288, n_updates = 12901, time = 12.4, total_time = 168.2 \n",
      "\n",
      "step =   14000, total_step =  224000, scores =  159.489 ± 37.182 [ 85.474, 223.262] (n=16), score_ema =  40.263, actor_loss = -20.976, entropy_coef_loss = -15.363, critic_loss = 1.793, entropy_coef = 0.020, rollout_stds = 0.644 ± 0.102, action_magnitude = 0.717 ± 0.284, n_updates = 13901, time = 12.4, total_time = 180.7 \n",
      "\n",
      "step =   15000, total_step =  240000, scores =  172.336 ± 48.075 [ 46.307, 238.692] (n=16), score_ema =  73.281, actor_loss = -21.249, entropy_coef_loss = -10.237, critic_loss = 1.029, entropy_coef = 0.016, rollout_stds = 0.614 ± 0.109, action_magnitude = 0.731 ± 0.283, n_updates = 14901, time = 12.5, total_time = 193.2 \n",
      "\n",
      "step =   16000, total_step =  256000, scores =  121.970 ± 44.914 [ 9.165, 215.823] (n=16), score_ema =  85.453, actor_loss = -19.590, entropy_coef_loss = -5.983, critic_loss = 1.409, entropy_coef = 0.013, rollout_stds = 0.569 ± 0.097, action_magnitude = 0.745 ± 0.279, n_updates = 15901, time = 12.4, total_time = 205.5 \n",
      "\n",
      "step =   17000, total_step =  272000, scores =  22.952 ± 56.692 [-32.070, 207.361] (n=16), score_ema =  69.828, actor_loss = -18.748, entropy_coef_loss = -3.589, critic_loss = 0.886, entropy_coef = 0.011, rollout_stds = 0.552 ± 0.110, action_magnitude = 0.668 ± 0.300, n_updates = 16901, time = 12.5, total_time = 218.1 \n",
      "\n",
      "step =   18000, total_step =  288000, scores =  322.953 ± 62.063 [ 220.081, 455.384] (n=16), score_ema =  133.109, actor_loss = -18.939, entropy_coef_loss = 1.040, critic_loss = 1.322, entropy_coef = 0.010, rollout_stds = 0.519 ± 0.105, action_magnitude = 0.683 ± 0.298, n_updates = 17901, time = 12.3, total_time = 230.4 \n",
      "\n",
      "step =   19000, total_step =  304000, scores =  838.434 ± 229.804 [ 309.515, 1018.508] (n=16), score_ema =  309.440, actor_loss = -19.764, entropy_coef_loss = 2.314, critic_loss = 1.094, entropy_coef = 0.010, rollout_stds = 0.433 ± 0.127, action_magnitude = 0.721 ± 0.290, n_updates = 18901, time = 12.4, total_time = 242.8 \n",
      "\n",
      "step =   20000, total_step =  320000, scores =  963.553 ± 264.699 [ 395.797, 1212.562] (n=16), score_ema =  472.968, actor_loss = -21.059, entropy_coef_loss = 0.803, critic_loss = 1.388, entropy_coef = 0.011, rollout_stds = 0.451 ± 0.112, action_magnitude = 0.742 ± 0.286, n_updates = 19901, time = 12.3, total_time = 255.1 \n",
      "\n",
      "step =   21000, total_step =  336000, scores =  583.627 ± 102.360 [ 347.037, 712.742] (n=16), score_ema =  500.633, actor_loss = -83670197734696797863936.000, entropy_coef_loss = -31848529615874443807031296.000, critic_loss =   inf, entropy_coef = 0.012, rollout_stds = 0.000 ± 0.000, action_magnitude = 0.840 ± 0.259, n_updates = 20901, time = 12.4, total_time = 267.5 \n",
      "\n",
      "step =   22000, total_step =  352000, scores = -597.481 ± 3.066 [-600.368, -589.197] (n=16), score_ema =  226.105, actor_loss = -199694219163626220027904.000, entropy_coef_loss = -76012340771344324069687296.000, critic_loss =   inf, entropy_coef = 0.012, rollout_stds = 0.000 ± 0.000, action_magnitude = 1.000 ± 0.000, n_updates = 21901, time = 12.5, total_time = 280.0 \n",
      "\n",
      "\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Score too low, policy probably fucked :(",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 332\u001B[0m\n\u001B[0;32m    322\u001B[0m         total_stopwatch\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m    323\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m log_experiment(\n\u001B[0;32m    324\u001B[0m             logger,\n\u001B[0;32m    325\u001B[0m             experiment_tags\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mtype\u001B[39m(algo)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, env_name],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[38;5;66;03m# pr = cProfile.Profile()\u001B[39;00m\n\u001B[0;32m    331\u001B[0m             \u001B[38;5;66;03m# pr.enable()\u001B[39;00m\n\u001B[1;32m--> 332\u001B[0m             \u001B[43malgo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1_000_000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    333\u001B[0m             \u001B[38;5;66;03m# pr.disable()  \u001B[39;00m\n\u001B[0;32m    334\u001B[0m             \u001B[38;5;66;03m# pr.dump_stats('profile_stats.pstat')\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Git\\pytorch-repository\\src\\reinforcement_learning\\algorithms\\base\\base_algorithm.py:134\u001B[0m, in \u001B[0;36mBaseAlgorithm.learn\u001B[1;34m(self, total_timesteps)\u001B[0m\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mset_train_mode(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    132\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimize(obs, episode_starts, info)\n\u001B[1;32m--> 134\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_optimization_done\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\Git\\pytorch-repository\\src\\reinforcement_learning\\core\\callback.py:42\u001B[0m, in \u001B[0;36mCallback.on_optimization_done\u001B[1;34m(self, optim_algo, step, info)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_optimization_done\u001B[39m(\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     37\u001B[0m         optim_algo: Algo,\n\u001B[0;32m     38\u001B[0m         step: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m     39\u001B[0m         info: InfoDict,\n\u001B[0;32m     40\u001B[0m ):\n\u001B[0;32m     41\u001B[0m     scheduler_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollect_scheduler_values(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimization_schedulers)\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_optimization_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptim_algo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler_values\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 256\u001B[0m, in \u001B[0;36mon_optimization_done\u001B[1;34m(rl, step, info, scheduler_values)\u001B[0m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m episode_scores \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(episode_scores) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m episode_scores\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m500\u001B[39m:\n\u001B[0;32m    255\u001B[0m     logger\u001B[38;5;241m.\u001B[39msave_experiment()\n\u001B[1;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mScore too low, policy probably fucked :(\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Score too low, policy probably fucked :("
     ]
    }
   ],
   "source": [
    "from src.summary_statistics import maybe_compute_summary_statistics\n",
    "from src.reinforcement_learning.core.loss_config import LossLoggingConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\n",
    "def get_setup() -> dict[str, str]:\n",
    "    import inspect\n",
    "    import sac\n",
    "    return {\n",
    "        'sac.py': inspect.getsource(sac),\n",
    "        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n",
    "    }\n",
    "\n",
    "policy_id: str\n",
    "policy: BasePolicy\n",
    "optimizer: optim.Optimizer\n",
    "wrapped_env: Env\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n",
    "            env=env,\n",
    "            info=PolicyConstruction.create_policy_initialization_info(\n",
    "                init_action_selector=init_action_selector,\n",
    "                init_policy=init_policy,\n",
    "                init_optimizer=init_optimizer,\n",
    "                wrap_env=wrap_env,\n",
    "                hyper_parameters=policy_construction_hyper_parameter,\n",
    "            ),\n",
    "        )\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "step_stopwatch = Stopwatch()\n",
    "total_stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    # rewards = rl.buffer.rewards[tail_indices]\n",
    "    # if 'raw_rewards' in info['rollout']:\n",
    "    #     rewards = info['rollout']['raw_rewards']\n",
    "    \n",
    "    # episode_scores = compute_episode_returns(\n",
    "    #     rewards=rewards,\n",
    "    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n",
    "    #     last_episode_starts=info['last_episode_starts'],\n",
    "    #     gamma=1.0,\n",
    "    #     gae_lambda=1.0,\n",
    "    #     normalize_rewards=None,\n",
    "    #     remove_unfinished_episodes=True,\n",
    "    # )\n",
    "    \n",
    "    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n",
    "    \n",
    "    if len(episode_scores) > 0:\n",
    "    \n",
    "        global best_iteration_score\n",
    "        iteration_score = episode_scores.mean()\n",
    "        score_moving_average = score_mean_ema.update(iteration_score)\n",
    "        if iteration_score >= best_iteration_score:\n",
    "            best_iteration_score = iteration_score\n",
    "            policy_db.save_model_state_dict(\n",
    "                model_id=policy_id,\n",
    "                parent_model_id=parent_policy_id,\n",
    "                model_info={\n",
    "                    'score': iteration_score.item(),\n",
    "                    'steps_trained': steps_trained,\n",
    "                    'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                    'init_policy_source_code': init_policy_source\n",
    "                },\n",
    "                model=policy,\n",
    "                optimizer=optimizer,\n",
    "            )\n",
    "        info['score_moving_average'] = score_moving_average\n",
    "    \n",
    "    info['episode_scores'] = episode_scores\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    # global steps_trained\n",
    "    # steps_trained += rl.buffer.pos\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    total_step = step * rl.num_envs\n",
    "    \n",
    "    step_time = step_stopwatch.reset()\n",
    "    total_time = total_stopwatch.time_passed()\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    episode_scores = info.get('episode_scores')\n",
    "    score_moving_average = info.get('score_moving_average') or 0.0\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "        n_format='>2'\n",
    "    )\n",
    "    # scores2 = format_summary_statics(\n",
    "    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='4.3f',\n",
    "    #     min_value_format=' 6.3f',\n",
    "    #     max_value_format='5.3f',\n",
    "    #     n_format='>2'\n",
    "    # )\n",
    "    # advantages = format_summary_statics(\n",
    "    #     rl.buffer.advantages, \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    actor_loss = format_summary_statics(\n",
    "        info['final_actor_loss'],  \n",
    "        mean_format=' 5.3f',\n",
    "        # std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # actor_loss_raw = format_summary_statics(\n",
    "    #     info['raw_actor_loss'],  \n",
    "    #     mean_format=' 5.3f',\n",
    "    #     std_format='5.3f',\n",
    "    #     min_value_format=None,\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n",
    "        info['final_entropy_coef_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_loss = format_summary_statics(\n",
    "        info['final_critic_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_coef = format_summary_statics(\n",
    "        info['entropy_coef'],\n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # resets = format_summary_statics(\n",
    "    #     rl.buffer.dones.astype(int).sum(axis=0), \n",
    "    #     mean_format='.2f',\n",
    "    #     std_format=None,\n",
    "    #     min_value_format='1d',\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    # kl_div = info['actor_kl_divergence'][-1]\n",
    "    # grad_norm = format_summary_statics(\n",
    "    #     info['grad_norm'], \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    action_stds = info['rollout'].get('action_stds')\n",
    "    if action_stds is not None:\n",
    "        rollout_action_stds = format_summary_statics(\n",
    "            action_stds,\n",
    "            mean_format='5.3f',\n",
    "            std_format='5.3f',\n",
    "            min_value_format=None,\n",
    "            max_value_format=None,\n",
    "        )\n",
    "    else:\n",
    "        rollout_action_stds = 'N/A'\n",
    "    action_magnitude = format_summary_statics(\n",
    "        np.abs(rl.buffer.actions[tail_indices]),\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # ppo_epochs = info['nr_ppo_epochs']\n",
    "    # ppo_updates = info['nr_ppo_updates']\n",
    "    # expl_var = rl.buffer.compute_critic_explained_variance()\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{total_step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          # f\"{scores2 = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          # f\"{advantages = :s}, \"\n",
    "          f\"{actor_loss = :s}, \"\n",
    "          # f\"{actor_loss_raw = :s}, \"\n",
    "          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n",
    "          f\"{critic_loss = :s}, \"\n",
    "          f\"{entropy_coef = :s}, \"\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{action_magnitude = :s}, \"\n",
    "          # f\"{expl_var = :.3f}, \"\n",
    "          # f\"{kl_div = :.4f}, \"\n",
    "          # f\"{ppo_epochs = }, \"\n",
    "          # f\"{ppo_updates = }, \"\n",
    "          # f\"{grad_norm = :s}, \"\n",
    "          f\"n_updates = {rl.gradient_steps_performed}, \"\n",
    "          # f\"{resets = :s}, \"\n",
    "          f\"time = {step_time:4.1f}, \"\n",
    "          f\"total_time = {total_time:4.1f} \\n\"\n",
    "          )\n",
    "    logger.add_item({\n",
    "        'step': step,\n",
    "        'total_step': total_step,\n",
    "        'scores': maybe_compute_summary_statistics(episode_scores),\n",
    "        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n",
    "        'entropy_coef_loss': maybe_compute_summary_statistics(info['final_entropy_coef_loss']),\n",
    "        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n",
    "        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n",
    "        'action_stds': maybe_compute_summary_statistics(action_stds),\n",
    "        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n",
    "        'gradient_step': rl.gradient_steps_performed,\n",
    "        'step_time': step_time,\n",
    "        'total_time': total_time\n",
    "    })\n",
    "    if step % 10000 == 0:\n",
    "        logger.save_experiment()\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "    if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n",
    "        logger.save_experiment()\n",
    "        raise ValueError('Score too low, policy probably fucked :(')\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "env_kwargs = {}\n",
    "num_envs = 16\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "\n",
    "env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "\n",
    "logger = ExperimentLogger('experiment_logs/sac/')\n",
    "\n",
    "try:\n",
    "    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "        \n",
    "    with ((torch.autograd.set_detect_anomaly(False))):\n",
    "        algo = SAC(\n",
    "            env=wrapped_env,\n",
    "            policy=policy,\n",
    "            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n",
    "            weigh_critic_loss=lambda l: 1 * l,\n",
    "            buffer_size=50_000,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n",
    "            entropy_coef=1.0,\n",
    "            rollout_steps=1,\n",
    "            gradient_steps=1,\n",
    "            warmup_steps=100,\n",
    "            learning_starts=100,\n",
    "            optimization_batch_size=256,\n",
    "            target_update_interval=1,\n",
    "            # sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_last_obs=True, log_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n",
    "                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n",
    "                                            critic_loss=LossLoggingConfig(log_final=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        total_stopwatch.reset()\n",
    "        with log_experiment(\n",
    "            logger,\n",
    "            experiment_tags=[type(algo).__name__, env_name],\n",
    "            hyper_parameters=algo.collect_hyper_parameters(),\n",
    "            setup=get_setup(),\n",
    "        ) as x:\n",
    "            # import cProfile\n",
    "            # pr = cProfile.Profile()\n",
    "            # pr.enable()\n",
    "            algo.learn(1_000_000)\n",
    "            # pr.disable()  \n",
    "            # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T23:13:15.532960Z",
     "start_time": "2024-09-24T23:08:31.292506Z"
    }
   },
   "id": "f71efe062771e81b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_ih[1] + '\\n\\n' + _ih[2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "778fd25d2dc23013",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ReplayBufferSamples(observations=tensor([[ 3.6485e-02,  1.0740e-01, -4.7875e-02, -7.9375e-02, -4.9577e-01,\n          3.5744e-01, -1.5947e-01, -5.1551e-01,  4.7873e+00,  7.1960e-01,\n          1.3489e+00,  9.5294e+00, -1.6766e+01,  2.8030e+00, -1.7730e+00,\n          1.7621e+01,  5.8761e-01],\n        [ 8.5811e-03, -2.1452e-01,  1.5625e-01,  4.9634e-01, -4.1958e-01,\n          4.5376e-01,  4.9127e-01, -4.2792e-01,  4.9719e+00, -1.0670e+00,\n         -2.6811e+00,  1.2919e+01, -6.4337e+00,  2.6973e-01,  2.0420e+00,\n          1.0915e+01,  1.4056e+00],\n        [ 1.4482e-01,  9.8598e-01, -3.3468e-01, -2.8100e-01,  1.0499e-01,\n         -2.0172e-01,  3.8732e-01,  3.7418e-02,  3.6597e+00,  8.3203e-01,\n          1.4555e+00, -9.9579e+00, -6.2714e+00,  1.4224e+00, -2.3534e+01,\n          1.8326e+01,  1.5905e+01]], device='cuda:0'), actions=tensor([[ 0.9953, -0.9944,  0.9880,  0.9908, -0.7834, -0.9546],\n        [ 0.9570, -0.9661,  0.9195,  0.9698, -0.1049, -0.7983],\n        [ 0.8277, -0.9974,  0.9464, -0.9319,  0.9724,  0.2002]],\n       device='cuda:0'), next_observations=tensor([[ 1.7763e-02,  1.6559e-01,  4.1288e-01, -7.5234e-01,  3.1288e-01,\n          5.7161e-01,  5.8682e-02, -4.1182e-01,  4.8310e+00, -1.1274e+00,\n          6.4474e-01,  6.7049e+00, -7.0175e+00,  1.7974e+01,  7.2488e+00,\n         -2.9050e+00,  2.3756e+00],\n        [-2.8194e-02, -2.0854e-01,  6.4102e-01, -8.7492e-02,  1.1497e-01,\n          7.3500e-01,  3.2915e-01, -3.2520e-01,  5.2210e+00, -1.6230e-01,\n          2.6464e+00,  5.7071e+00, -5.7501e+00,  9.6984e+00,  5.8110e+00,\n         -8.7761e+00,  1.8374e+00],\n        [ 1.3676e-01,  1.0784e+00, -2.3591e-01, -8.8112e-01,  3.4428e-01,\n         -6.2197e-01,  9.3744e-01,  5.5867e-01,  5.1160e+00, -9.0701e-01,\n          1.7058e+00,  6.2866e+00, -9.7055e+00,  4.8871e+00,  2.5157e+00,\n          7.8773e-01,  3.4867e-02]], device='cuda:0'), dones=tensor([[0.],\n        [0.],\n        [0.]], device='cuda:0'), rewards=tensor([[4.4485],\n        [4.7180],\n        [4.1625]], device='cuda:0'))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = algo.buffer.tail_indices(10000)[:1000]\n",
    "algo.buffer.actions[indices].mean()\n",
    "\n",
    "algo.buffer.sample(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T17:39:27.620912Z",
     "start_time": "2024-09-24T17:39:27.479406Z"
    }
   },
   "id": "e39b8f4554a69cc8",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "SAC-256 1k steps:\n",
    "* num_envs = 16: 12.0\n",
    "\n",
    "CrossQ-256 1k steps:\n",
    "* num_envs = 16: 17.0\n",
    "* num_envs = 32: 18.5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64dec978abc8662"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc184a8a98ea506",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range(10):\n",
    "    print((np.abs(algo.buffer.actions[6000 + 100 * i:6100 + 100*i])).mean())\n",
    "    print((algo.buffer.actions[6000 + 100 * i:6100 + 100*i]).mean())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "algo.target_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d4d48d204b6f44",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1ae8571d73535c6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env = wrap_env(as_vec_env(record_env)[0], {})\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        with torch.no_grad():\n",
    "            obs, info = record_env.reset()\n",
    "            for step in range(max_steps):\n",
    "                actions_dist, _ = policy.process_obs(torch.tensor(obs, device=device))\n",
    "                actions = actions_dist.sample().detach().cpu().numpy()\n",
    "                obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ab51a61dd845",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
