{
 "cells": [
  {
   "cell_type": "code",
   "id": "ba8c59a3eba2f172",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T15:37:06.549656Z",
     "start_time": "2024-09-24T15:37:03.072583Z"
    }
   },
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.np_functions import inv_symmetric_log\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\n",
    "from src.reinforcement_learning.gym.envs.test_env import TestEnv\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.torch_device import set_default_torch_device, optimizer_to_device\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "No policy in RAM, creating a new one\n",
      "New policy 2024-09-24 15:37:09.222259 created\n",
      "Using policy 2024-09-24 15:37:09.222259 with parent policy None\n",
      "count_parameters(policy) = 217870\n",
      "env = AsyncVectorEnv(16), num_envs = 16 \n",
      "\n",
      "step =    1000, total_step =   16000, scores = -285.149 ± 76.399 [-453.123, -120.749] (n=16), score_ema = -285.149, actor_loss = -17.100 ±   nan, entropy_coef_loss = -2.720 ±   nan, critic_loss = 4.907 ±   nan, entropy_coef = 0.763 ±   nan, rollout_stds = 0.919 ± 0.048, action_magnitude = 0.520 ± 0.286, n_updates = 901, time = 14.4, total_time = 11.4 \n",
      "\n",
      "step =    2000, total_step =   32000, scores = -208.396 ± 40.530 [-277.805, -107.015] (n=16), score_ema = -265.961, actor_loss = -25.315 ±   nan, entropy_coef_loss = -5.642 ±   nan, critic_loss = 3.515 ±   nan, entropy_coef = 0.566 ±   nan, rollout_stds = 0.897 ± 0.042, action_magnitude = 0.534 ± 0.288, n_updates = 1901, time = 12.6, total_time = 24.1 \n",
      "\n",
      "step =    3000, total_step =   48000, scores = -215.098 ± 59.118 [-345.988, -107.661] (n=16), score_ema = -253.245, actor_loss = -29.287 ±   nan, entropy_coef_loss = -8.583 ±   nan, critic_loss = 3.447 ±   nan, entropy_coef = 0.420 ±   nan, rollout_stds = 0.926 ± 0.047, action_magnitude = 0.537 ± 0.290, n_updates = 2901, time = 12.8, total_time = 36.9 \n",
      "\n",
      "step =    4000, total_step =   64000, scores = -205.353 ± 48.930 [-287.549, -107.479] (n=16), score_ema = -241.272, actor_loss = -32.163 ±   nan, entropy_coef_loss = -11.373 ±   nan, critic_loss = 4.608 ±   nan, entropy_coef = 0.311 ±   nan, rollout_stds = 0.932 ± 0.037, action_magnitude = 0.542 ± 0.290, n_updates = 3901, time = 12.5, total_time = 49.4 \n",
      "\n",
      "step =    5000, total_step =   80000, scores = -209.298 ± 38.536 [-272.069, -128.335] (n=16), score_ema = -233.278, actor_loss = -32.494 ±   nan, entropy_coef_loss = -13.935 ±   nan, critic_loss = 3.651 ±   nan, entropy_coef = 0.231 ±   nan, rollout_stds = 0.921 ± 0.038, action_magnitude = 0.550 ± 0.291, n_updates = 4901, time = 11.9, total_time = 61.3 \n",
      "\n",
      "step =    6000, total_step =   96000, scores = -194.268 ± 41.362 [-258.562, -113.631] (n=16), score_ema = -223.526, actor_loss = -31.993 ±   nan, entropy_coef_loss = -16.459 ±   nan, critic_loss = 3.181 ±   nan, entropy_coef = 0.172 ±   nan, rollout_stds = 0.915 ± 0.049, action_magnitude = 0.554 ± 0.292, n_updates = 5901, time = 12.3, total_time = 73.6 \n",
      "\n",
      "step =    7000, total_step =  112000, scores = -208.664 ± 34.186 [-256.268, -133.852] (n=16), score_ema = -219.810, actor_loss = -30.941 ±   nan, entropy_coef_loss = -18.033 ±   nan, critic_loss = 3.234 ±   nan, entropy_coef = 0.129 ±   nan, rollout_stds = 0.918 ± 0.055, action_magnitude = 0.560 ± 0.293, n_updates = 6901, time = 12.9, total_time = 86.4 \n",
      "\n",
      "step =    8000, total_step =  128000, scores = -222.221 ± 36.938 [-303.880, -139.248] (n=16), score_ema = -220.413, actor_loss = -29.862 ±   nan, entropy_coef_loss = -19.496 ±   nan, critic_loss = 2.812 ±   nan, entropy_coef = 0.097 ±   nan, rollout_stds = 0.914 ± 0.058, action_magnitude = 0.572 ± 0.294, n_updates = 7901, time = 12.7, total_time = 99.1 \n",
      "\n",
      "step =    9000, total_step =  144000, scores = -204.323 ± 23.696 [-251.055, -161.261] (n=16), score_ema = -216.391, actor_loss = -27.535 ±   nan, entropy_coef_loss = -19.714 ±   nan, critic_loss = 6.489 ±   nan, entropy_coef = 0.073 ±   nan, rollout_stds = 0.854 ± 0.057, action_magnitude = 0.580 ± 0.295, n_updates = 8901, time = 12.0, total_time = 111.2 \n",
      "\n",
      "step =   10000, total_step =  160000, scores = -169.592 ± 27.349 [-233.351, -105.374] (n=16), score_ema = -204.691, actor_loss = -25.960 ±   nan, entropy_coef_loss = -20.441 ±   nan, critic_loss = 1.987 ±   nan, entropy_coef = 0.055 ±   nan, rollout_stds = 0.843 ± 0.085, action_magnitude = 0.592 ± 0.295, n_updates = 9901, time = 12.5, total_time = 123.7 \n",
      "\n",
      "step =   11000, total_step =  176000, scores = -167.785 ± 29.315 [-207.812, -116.775] (n=16), score_ema = -195.464, actor_loss = -23.782 ±   nan, entropy_coef_loss = -22.668 ±   nan, critic_loss = 2.455 ±   nan, entropy_coef = 0.042 ±   nan, rollout_stds = 0.807 ± 0.094, action_magnitude = 0.605 ± 0.295, n_updates = 10901, time = 13.6, total_time = 137.3 \n",
      "\n",
      "step =   12000, total_step =  192000, scores = -116.450 ± 29.710 [-177.377, -78.827] (n=16), score_ema = -175.711, actor_loss = -21.589 ±   nan, entropy_coef_loss = -20.733 ±   nan, critic_loss = 2.275 ±   nan, entropy_coef = 0.032 ±   nan, rollout_stds = 0.753 ± 0.088, action_magnitude = 0.620 ± 0.296, n_updates = 11901, time = 12.7, total_time = 150.0 \n",
      "\n",
      "step =   13000, total_step =  208000, scores = -85.550 ± 36.921 [-151.371, -5.826] (n=16), score_ema = -153.171, actor_loss = -19.488 ±   nan, entropy_coef_loss = -20.732 ±   nan, critic_loss = 2.111 ±   nan, entropy_coef = 0.025 ±   nan, rollout_stds = 0.737 ± 0.102, action_magnitude = 0.642 ± 0.296, n_updates = 12901, time = 12.0, total_time = 162.0 \n",
      "\n",
      "step =   14000, total_step =  224000, scores = -23.776 ± 38.423 [-106.982, 23.078] (n=16), score_ema = -120.822, actor_loss = -18.496 ±   nan, entropy_coef_loss = -15.730 ±   nan, critic_loss = 2.282 ±   nan, entropy_coef = 0.019 ±   nan, rollout_stds = 0.688 ± 0.102, action_magnitude = 0.659 ± 0.296, n_updates = 13901, time = 13.4, total_time = 175.3 \n",
      "\n",
      "step =   15000, total_step =  240000, scores =  26.501 ± 29.577 [-25.614, 67.916] (n=16), score_ema = -83.991, actor_loss = -17.407 ±   nan, entropy_coef_loss = -9.278 ±   nan, critic_loss = 2.445 ±   nan, entropy_coef = 0.015 ±   nan, rollout_stds = 0.701 ± 0.106, action_magnitude = 0.682 ± 0.294, n_updates = 14901, time = 12.4, total_time = 187.7 \n",
      "\n",
      "step =   16000, total_step =  256000, scores = -76.437 ± 189.026 [-333.549, 179.829] (n=16), score_ema = -82.103, actor_loss = -16.114 ±   nan, entropy_coef_loss = -13.412 ±   nan, critic_loss = 1.794 ±   nan, entropy_coef = 0.012 ±   nan, rollout_stds = 0.641 ± 0.090, action_magnitude = 0.679 ± 0.299, n_updates = 15901, time = 12.4, total_time = 200.1 \n",
      "\n",
      "step =   17000, total_step =  272000, scores =  166.315 ± 27.635 [ 126.047, 218.913] (n=16), score_ema = -19.998, actor_loss = -15.079 ±   nan, entropy_coef_loss = -3.253 ±   nan, critic_loss = 3.601 ±   nan, entropy_coef = 0.010 ±   nan, rollout_stds = 0.584 ± 0.087, action_magnitude = 0.719 ± 0.293, n_updates = 16901, time = 12.2, total_time = 212.3 \n",
      "\n",
      "step =   18000, total_step =  288000, scores =  113.477 ± 63.923 [-58.997, 204.074] (n=16), score_ema =  13.371, actor_loss = -12.862 ±   nan, entropy_coef_loss = -3.843 ±   nan, critic_loss = 2.343 ±   nan, entropy_coef = 0.008 ±   nan, rollout_stds = 0.557 ± 0.098, action_magnitude = 0.681 ± 0.300, n_updates = 17901, time = 12.8, total_time = 225.1 \n",
      "\n",
      "step =   19000, total_step =  304000, scores =  188.753 ± 41.237 [ 117.690, 274.910] (n=16), score_ema =  57.216, actor_loss = -13.310 ±   nan, entropy_coef_loss = -2.479 ±   nan, critic_loss = 1.650 ±   nan, entropy_coef = 0.008 ±   nan, rollout_stds = 0.517 ± 0.100, action_magnitude = 0.687 ± 0.302, n_updates = 18901, time = 12.0, total_time = 237.1 \n",
      "\n",
      "step =   20000, total_step =  320000, scores =  273.689 ± 125.089 [ 73.454, 523.062] (n=16), score_ema =  111.334, actor_loss = -13.697 ±   nan, entropy_coef_loss = -2.653 ±   nan, critic_loss = 2.253 ±   nan, entropy_coef = 0.008 ±   nan, rollout_stds = 0.467 ± 0.150, action_magnitude = 0.704 ± 0.297, n_updates = 19901, time = 11.9, total_time = 249.0 \n",
      "\n",
      "step =   21000, total_step =  336000, scores =  361.581 ± 208.973 [ 37.857, 653.784] (n=16), score_ema =  173.896, actor_loss = -14.037 ±   nan, entropy_coef_loss = 2.178 ±   nan, critic_loss = 2.422 ±   nan, entropy_coef = 0.009 ±   nan, rollout_stds = 0.483 ± 0.121, action_magnitude = 0.726 ± 0.289, n_updates = 20901, time = 12.5, total_time = 261.5 \n",
      "\n",
      "step =   22000, total_step =  352000, scores =  910.072 ± 222.932 [ 373.795, 1167.757] (n=16), score_ema =  357.940, actor_loss = -14.277 ±   nan, entropy_coef_loss = 3.079 ±   nan, critic_loss = 2.435 ±   nan, entropy_coef = 0.010 ±   nan, rollout_stds = 0.522 ± 0.134, action_magnitude = 0.722 ± 0.295, n_updates = 21901, time = 12.0, total_time = 273.5 \n",
      "\n",
      "step =   23000, total_step =  368000, scores =  1000.192 ± 162.550 [ 672.923, 1193.362] (n=16), score_ema =  518.503, actor_loss = -16.074 ±   nan, entropy_coef_loss = 0.820 ±   nan, critic_loss = 2.168 ±   nan, entropy_coef = 0.012 ±   nan, rollout_stds = 0.525 ± 0.142, action_magnitude = 0.757 ± 0.283, n_updates = 22901, time = 12.2, total_time = 285.6 \n",
      "\n",
      "step =   24000, total_step =  384000, scores =  1528.991 ± 444.137 [ 347.206, 1826.097] (n=16), score_ema =  771.125, actor_loss = -19.102 ±   nan, entropy_coef_loss = -1.195 ±   nan, critic_loss = 3.790 ±   nan, entropy_coef = 0.014 ±   nan, rollout_stds = 0.489 ± 0.141, action_magnitude = 0.772 ± 0.278, n_updates = 23901, time = 12.3, total_time = 297.9 \n",
      "\n",
      "step =   25000, total_step =  400000, scores =  1667.159 ± 431.085 [ 804.575, 2092.612] (n=16), score_ema =  995.134, actor_loss = -19.490 ±   nan, entropy_coef_loss = 1.458 ±   nan, critic_loss = 4.689 ±   nan, entropy_coef = 0.015 ±   nan, rollout_stds = 0.499 ± 0.122, action_magnitude = 0.786 ± 0.270, n_updates = 24901, time = 12.1, total_time = 310.1 \n",
      "\n",
      "step =   26000, total_step =  416000, scores =  1049.517 ± 585.989 [ 302.535, 2036.585] (n=16), score_ema =  1008.729, actor_loss = -24.640 ±   nan, entropy_coef_loss = 2.266 ±   nan, critic_loss = 5.255 ±   nan, entropy_coef = 0.017 ±   nan, rollout_stds = 0.522 ± 0.103, action_magnitude = 0.766 ± 0.277, n_updates = 25901, time = 12.8, total_time = 322.8 \n",
      "\n",
      "step =   27000, total_step =  432000, scores =  1591.805 ± 557.796 [ 395.506, 2176.141] (n=16), score_ema =  1154.498, actor_loss = -24.989 ±   nan, entropy_coef_loss = 1.361 ±   nan, critic_loss = 13.696 ±   nan, entropy_coef = 0.019 ±   nan, rollout_stds = 0.504 ± 0.106, action_magnitude = 0.791 ± 0.265, n_updates = 26901, time = 12.2, total_time = 335.0 \n",
      "\n",
      "step =   28000, total_step =  448000, scores =  1238.399 ± 660.800 [ 347.378, 2243.540] (n=16), score_ema =  1175.474, actor_loss = -26.046 ±   nan, entropy_coef_loss = 1.623 ±   nan, critic_loss = 5.264 ±   nan, entropy_coef = 0.020 ±   nan, rollout_stds = 0.543 ± 0.099, action_magnitude = 0.771 ± 0.274, n_updates = 27901, time = 12.3, total_time = 347.3 \n",
      "\n",
      "step =   29000, total_step =  464000, scores =  1983.347 ± 487.158 [ 426.306, 2349.675] (n=16), score_ema =  1377.442, actor_loss = -29.428 ±   nan, entropy_coef_loss = 2.729 ±   nan, critic_loss = 5.649 ±   nan, entropy_coef = 0.021 ±   nan, rollout_stds = 0.509 ± 0.096, action_magnitude = 0.810 ± 0.254, n_updates = 28901, time = 12.4, total_time = 359.7 \n",
      "\n",
      "step =   30000, total_step =  480000, scores =  1999.335 ± 524.180 [ 668.066, 2517.363] (n=16), score_ema =  1532.915, actor_loss = -32.135 ±   nan, entropy_coef_loss = 1.147 ±   nan, critic_loss = 5.096 ±   nan, entropy_coef = 0.022 ±   nan, rollout_stds = 0.494 ± 0.108, action_magnitude = 0.809 ± 0.253, n_updates = 29901, time = 12.5, total_time = 372.2 \n",
      "\n",
      "step =   31000, total_step =  496000, scores =  2187.211 ± 536.656 [ 575.300, 2619.647] (n=16), score_ema =  1696.489, actor_loss = -34.716 ±   nan, entropy_coef_loss = 0.170 ±   nan, critic_loss = 5.818 ±   nan, entropy_coef = 0.023 ±   nan, rollout_stds = 0.498 ± 0.113, action_magnitude = 0.817 ± 0.249, n_updates = 30901, time = 12.0, total_time = 384.2 \n",
      "\n",
      "step =   32000, total_step =  512000, scores =  1928.092 ± 699.470 [ 584.041, 2525.884] (n=16), score_ema =  1754.390, actor_loss = -36.421 ±   nan, entropy_coef_loss = 1.128 ±   nan, critic_loss = 69.026 ±   nan, entropy_coef = 0.024 ±   nan, rollout_stds = 0.491 ± 0.112, action_magnitude = 0.799 ± 0.258, n_updates = 31901, time = 12.0, total_time = 396.2 \n",
      "\n",
      "step =   33000, total_step =  528000, scores =  2292.248 ± 623.610 [ 462.093, 2824.233] (n=16), score_ema =  1888.854, actor_loss = -42.447 ±   nan, entropy_coef_loss = 0.691 ±   nan, critic_loss = 7.145 ±   nan, entropy_coef = 0.026 ±   nan, rollout_stds = 0.457 ± 0.110, action_magnitude = 0.815 ± 0.249, n_updates = 32901, time = 12.3, total_time = 408.5 \n",
      "\n",
      "step =   34000, total_step =  544000, scores =  2579.490 ± 166.485 [ 2083.112, 2810.963] (n=16), score_ema =  2061.513, actor_loss = -47.817 ±   nan, entropy_coef_loss = 1.772 ±   nan, critic_loss = 8.653 ±   nan, entropy_coef = 0.027 ±   nan, rollout_stds = 0.490 ± 0.093, action_magnitude = 0.830 ± 0.240, n_updates = 33901, time = 12.1, total_time = 420.6 \n",
      "\n",
      "step =   35000, total_step =  560000, scores =  2512.285 ± 539.976 [ 458.735, 2839.382] (n=16), score_ema =  2174.206, actor_loss = -54.026 ±   nan, entropy_coef_loss = 0.636 ±   nan, critic_loss = 9.833 ±   nan, entropy_coef = 0.028 ±   nan, rollout_stds = 0.490 ± 0.110, action_magnitude = 0.818 ± 0.248, n_updates = 34901, time = 12.3, total_time = 432.8 \n",
      "\n",
      "step =   36000, total_step =  576000, scores =  2529.166 ± 438.618 [ 1414.588, 2912.733] (n=16), score_ema =  2262.946, actor_loss = -53.028 ±   nan, entropy_coef_loss = 0.201 ±   nan, critic_loss = 9.209 ±   nan, entropy_coef = 0.030 ±   nan, rollout_stds = 0.491 ± 0.114, action_magnitude = 0.818 ± 0.248, n_updates = 35901, time = 12.5, total_time = 445.3 \n",
      "\n",
      "step =   37000, total_step =  592000, scores =  2303.250 ± 601.585 [ 636.614, 2779.555] (n=16), score_ema =  2273.022, actor_loss = -61.537 ±   nan, entropy_coef_loss = -0.824 ±   nan, critic_loss = 9.550 ±   nan, entropy_coef = 0.031 ±   nan, rollout_stds = 0.513 ± 0.091, action_magnitude = 0.802 ± 0.256, n_updates = 36901, time = 12.7, total_time = 458.0 \n",
      "\n",
      "step =   38000, total_step =  608000, scores =  2048.020 ± 718.559 [ 584.175, 2896.924] (n=16), score_ema =  2216.772, actor_loss = -58.647 ±   nan, entropy_coef_loss = -0.598 ±   nan, critic_loss = 97.362 ±   nan, entropy_coef = 0.033 ±   nan, rollout_stds = 0.506 ± 0.106, action_magnitude = 0.790 ± 0.263, n_updates = 37901, time = 12.0, total_time = 470.0 \n",
      "\n",
      "step =   39000, total_step =  624000, scores =  2081.499 ± 804.394 [ 773.876, 2933.662] (n=16), score_ema =  2182.954, actor_loss = -64.448 ±   nan, entropy_coef_loss = -0.210 ±   nan, critic_loss = 11.766 ±   nan, entropy_coef = 0.033 ±   nan, rollout_stds = 0.511 ± 0.117, action_magnitude = 0.794 ± 0.261, n_updates = 38901, time = 12.1, total_time = 482.1 \n",
      "\n",
      "step =   40000, total_step =  640000, scores =  2088.250 ± 778.151 [ 426.458, 2888.760] (n=16), score_ema =  2159.278, actor_loss = -68.855 ±   nan, entropy_coef_loss = -0.549 ±   nan, critic_loss = 12.290 ±   nan, entropy_coef = 0.035 ±   nan, rollout_stds = 0.527 ± 0.105, action_magnitude = 0.792 ± 0.261, n_updates = 39901, time = 12.3, total_time = 494.4 \n",
      "\n",
      "step =   41000, total_step =  656000, scores =  2583.952 ± 475.831 [ 1346.823, 2982.094] (n=16), score_ema =  2265.446, actor_loss = -73.794 ±   nan, entropy_coef_loss = 0.752 ±   nan, critic_loss = 8.474 ±   nan, entropy_coef = 0.036 ±   nan, rollout_stds = 0.549 ± 0.102, action_magnitude = 0.813 ± 0.250, n_updates = 40901, time = 12.0, total_time = 506.4 \n",
      "\n",
      "step =   42000, total_step =  672000, scores =  2228.530 ± 978.537 [ 455.425, 3088.708] (n=16), score_ema =  2256.217, actor_loss = -70.170 ±   nan, entropy_coef_loss = -0.039 ±   nan, critic_loss = 14.355 ±   nan, entropy_coef = 0.037 ±   nan, rollout_stds = 0.531 ± 0.109, action_magnitude = 0.792 ± 0.260, n_updates = 41901, time = 12.4, total_time = 518.8 \n",
      "\n",
      "step =   43000, total_step =  688000, scores =  2526.105 ± 690.702 [ 650.508, 3129.329] (n=16), score_ema =  2323.689, actor_loss = -74.033 ±   nan, entropy_coef_loss = -0.525 ±   nan, critic_loss = 14.085 ±   nan, entropy_coef = 0.038 ±   nan, rollout_stds = 0.539 ± 0.115, action_magnitude = 0.811 ± 0.251, n_updates = 42901, time = 12.2, total_time = 531.0 \n",
      "\n",
      "step =   44000, total_step =  704000, scores =  2347.335 ± 801.988 [ 800.171, 3062.349] (n=16), score_ema =  2329.601, actor_loss = -78.318 ±   nan, entropy_coef_loss = 0.025 ±   nan, critic_loss = 19.536 ±   nan, entropy_coef = 0.039 ±   nan, rollout_stds = 0.531 ± 0.104, action_magnitude = 0.795 ± 0.259, n_updates = 43901, time = 12.1, total_time = 543.1 \n",
      "\n",
      "step =   45000, total_step =  720000, scores =  2663.853 ± 796.633 [ 746.316, 3260.610] (n=16), score_ema =  2413.164, actor_loss = -76.774 ±   nan, entropy_coef_loss = -0.940 ±   nan, critic_loss = 13.153 ±   nan, entropy_coef = 0.040 ±   nan, rollout_stds = 0.517 ± 0.089, action_magnitude = 0.809 ± 0.251, n_updates = 44901, time = 12.3, total_time = 555.5 \n",
      "\n",
      "step =   46000, total_step =  736000, scores =  3066.731 ± 111.176 [ 2858.409, 3276.381] (n=16), score_ema =  2576.555, actor_loss = -79.890 ±   nan, entropy_coef_loss = -2.171 ±   nan, critic_loss = 11.348 ±   nan, entropy_coef = 0.040 ±   nan, rollout_stds = 0.501 ± 0.096, action_magnitude = 0.826 ± 0.241, n_updates = 45901, time = 12.4, total_time = 567.9 \n",
      "\n",
      "step =   47000, total_step =  752000, scores =  3049.278 ± 347.404 [ 1769.554, 3292.176] (n=16), score_ema =  2694.736, actor_loss = -95.471 ±   nan, entropy_coef_loss = -0.518 ±   nan, critic_loss = 14.718 ±   nan, entropy_coef = 0.041 ±   nan, rollout_stds = 0.503 ± 0.093, action_magnitude = 0.816 ± 0.248, n_updates = 46901, time = 12.4, total_time = 580.3 \n",
      "\n",
      "step =   48000, total_step =  768000, scores =  2798.584 ± 862.224 [ 486.119, 3345.136] (n=16), score_ema =  2720.698, actor_loss = -94.004 ±   nan, entropy_coef_loss = 0.232 ±   nan, critic_loss = 11.017 ±   nan, entropy_coef = 0.042 ±   nan, rollout_stds = 0.517 ± 0.104, action_magnitude = 0.796 ± 0.260, n_updates = 47901, time = 12.5, total_time = 592.7 \n",
      "\n",
      "step =   49000, total_step =  784000, scores =  2800.437 ± 612.550 [ 1311.194, 3259.625] (n=16), score_ema =  2740.633, actor_loss = -89.918 ±   nan, entropy_coef_loss = -1.276 ±   nan, critic_loss = 241.282 ±   nan, entropy_coef = 0.042 ±   nan, rollout_stds = 0.539 ± 0.111, action_magnitude = 0.797 ± 0.259, n_updates = 48901, time = 12.2, total_time = 605.0 \n",
      "\n",
      "step =   50000, total_step =  800000, scores =  2908.782 ± 781.161 [ 936.238, 3499.279] (n=16), score_ema =  2782.670, actor_loss = -105.370 ±   nan, entropy_coef_loss = 1.959 ±   nan, critic_loss = 14.022 ±   nan, entropy_coef = 0.045 ±   nan, rollout_stds = 0.520 ± 0.099, action_magnitude = 0.801 ± 0.257, n_updates = 49901, time = 12.4, total_time = 617.4 \n",
      "\n",
      "step =   51000, total_step =  816000, scores =  3309.827 ± 119.274 [ 3050.551, 3476.343] (n=16), score_ema =  2914.459, actor_loss = -104.726 ±   nan, entropy_coef_loss = 2.132 ±   nan, critic_loss = 13.130 ±   nan, entropy_coef = 0.046 ±   nan, rollout_stds = 0.514 ± 0.084, action_magnitude = 0.815 ± 0.249, n_updates = 50901, time = 12.4, total_time = 629.8 \n",
      "\n",
      "step =   52000, total_step =  832000, scores =  3250.265 ± 394.471 [ 1773.472, 3513.992] (n=16), score_ema =  2998.411, actor_loss = -103.993 ±   nan, entropy_coef_loss = -0.423 ±   nan, critic_loss = 8.028 ±   nan, entropy_coef = 0.047 ±   nan, rollout_stds = 0.511 ± 0.097, action_magnitude = 0.807 ± 0.255, n_updates = 51901, time = 12.3, total_time = 642.1 \n",
      "\n",
      "step =   53000, total_step =  848000, scores =  3308.054 ± 68.084 [ 3163.241, 3419.066] (n=16), score_ema =  3075.822, actor_loss = -115.929 ±   nan, entropy_coef_loss = 0.611 ±   nan, critic_loss = 8.671 ±   nan, entropy_coef = 0.049 ±   nan, rollout_stds = 0.520 ± 0.088, action_magnitude = 0.815 ± 0.249, n_updates = 52901, time = 12.2, total_time = 654.3 \n",
      "\n",
      "step =   54000, total_step =  864000, scores =  3161.781 ± 451.906 [ 1707.098, 3438.543] (n=16), score_ema =  3097.312, actor_loss = -115.110 ±   nan, entropy_coef_loss = -0.157 ±   nan, critic_loss = 15.532 ±   nan, entropy_coef = 0.050 ±   nan, rollout_stds = 0.534 ± 0.095, action_magnitude = 0.801 ± 0.259, n_updates = 53901, time = 12.1, total_time = 666.4 \n",
      "\n",
      "step =   55000, total_step =  880000, scores =  3033.788 ± 627.845 [ 1350.629, 3534.184] (n=16), score_ema =  3081.431, actor_loss = -124.193 ±   nan, entropy_coef_loss = 0.282 ±   nan, critic_loss = 14.196 ±   nan, entropy_coef = 0.053 ±   nan, rollout_stds = 0.550 ± 0.112, action_magnitude = 0.802 ± 0.258, n_updates = 54901, time = 12.2, total_time = 678.6 \n",
      "\n",
      "step =   56000, total_step =  896000, scores =  3172.081 ± 605.260 [ 1586.080, 3696.108] (n=16), score_ema =  3104.093, actor_loss = -124.673 ±   nan, entropy_coef_loss = -1.104 ±   nan, critic_loss = 9.472 ±   nan, entropy_coef = 0.055 ±   nan, rollout_stds = 0.535 ± 0.106, action_magnitude = 0.804 ± 0.256, n_updates = 55901, time = 12.0, total_time = 690.6 \n",
      "\n",
      "step =   57000, total_step =  912000, scores =  3360.452 ± 181.637 [ 2754.521, 3569.025] (n=16), score_ema =  3168.183, actor_loss = -122.739 ±   nan, entropy_coef_loss = -0.787 ±   nan, critic_loss = 11.401 ±   nan, entropy_coef = 0.054 ±   nan, rollout_stds = 0.557 ± 0.128, action_magnitude = 0.809 ± 0.254, n_updates = 56901, time = 12.5, total_time = 703.1 \n",
      "\n",
      "step =   58000, total_step =  928000, scores =  3291.120 ± 619.674 [ 922.785, 3607.378] (n=16), score_ema =  3198.917, actor_loss = -126.706 ±   nan, entropy_coef_loss = -1.349 ±   nan, critic_loss = 13.039 ±   nan, entropy_coef = 0.059 ±   nan, rollout_stds = 0.534 ± 0.110, action_magnitude = 0.804 ± 0.255, n_updates = 57901, time = 12.1, total_time = 715.3 \n",
      "\n",
      "step =   59000, total_step =  944000, scores =  3317.563 ± 422.537 [ 1720.396, 3632.087] (n=16), score_ema =  3228.578, actor_loss = -143.806 ±   nan, entropy_coef_loss = 0.123 ±   nan, critic_loss = 15.620 ±   nan, entropy_coef = 0.060 ±   nan, rollout_stds = 0.514 ± 0.103, action_magnitude = 0.807 ± 0.253, n_updates = 58901, time = 12.4, total_time = 727.7 \n",
      "\n",
      "step =   60000, total_step =  960000, scores =  2984.400 ± 871.693 [ 660.745, 3504.093] (n=16), score_ema =  3167.534, actor_loss = -135.533 ±   nan, entropy_coef_loss = -1.627 ±   nan, critic_loss = 10.032 ±   nan, entropy_coef = 0.063 ±   nan, rollout_stds = 0.542 ± 0.126, action_magnitude = 0.790 ± 0.262, n_updates = 59901, time = 12.0, total_time = 739.7 \n",
      "\n",
      "step =   61000, total_step =  976000, scores =  3335.423 ± 477.025 [ 1501.989, 3538.986] (n=16), score_ema =  3209.506, actor_loss = -143.341 ±   nan, entropy_coef_loss = 0.313 ±   nan, critic_loss = 18.583 ±   nan, entropy_coef = 0.065 ±   nan, rollout_stds = 0.527 ± 0.104, action_magnitude = 0.802 ± 0.255, n_updates = 60901, time = 12.4, total_time = 752.1 \n",
      "\n",
      "step =   62000, total_step =  992000, scores =  3263.989 ± 603.443 [ 990.130, 3607.831] (n=16), score_ema =  3223.127, actor_loss = -160.314 ±   nan, entropy_coef_loss = 3.267 ±   nan, critic_loss = 21.195 ±   nan, entropy_coef = 0.068 ±   nan, rollout_stds = 0.522 ± 0.092, action_magnitude = 0.799 ± 0.256, n_updates = 61901, time = 12.0, total_time = 764.1 \n",
      "\n",
      "step =   63000, total_step = 1008000, scores =  3403.710 ± 219.957 [ 2640.723, 3710.849] (n=16), score_ema =  3268.273, actor_loss = -148.061 ±   nan, entropy_coef_loss = -0.600 ±   nan, critic_loss = 18.084 ±   nan, entropy_coef = 0.071 ±   nan, rollout_stds = 0.553 ± 0.175, action_magnitude = 0.807 ± 0.252, n_updates = 62901, time = 11.9, total_time = 776.0 \n",
      "\n",
      "step =   64000, total_step = 1024000, scores =  3457.444 ± 122.344 [ 3247.552, 3670.594] (n=16), score_ema =  3315.566, actor_loss = -154.629 ±   nan, entropy_coef_loss = -2.148 ±   nan, critic_loss = 15.471 ±   nan, entropy_coef = 0.073 ±   nan, rollout_stds = 0.546 ± 0.098, action_magnitude = 0.807 ± 0.249, n_updates = 63901, time = 12.0, total_time = 788.1 \n",
      "\n",
      "step =   65000, total_step = 1040000, scores =  3544.680 ± 91.370 [ 3310.831, 3691.525] (n=16), score_ema =  3372.844, actor_loss = -166.025 ±   nan, entropy_coef_loss = -0.974 ±   nan, critic_loss = 11.293 ±   nan, entropy_coef = 0.077 ±   nan, rollout_stds = 0.512 ± 0.092, action_magnitude = 0.807 ± 0.249, n_updates = 64901, time = 12.0, total_time = 800.0 \n",
      "\n",
      "step =   66000, total_step = 1056000, scores =  3444.598 ± 383.218 [ 2049.371, 3682.125] (n=16), score_ema =  3390.783, actor_loss = -166.353 ±   nan, entropy_coef_loss = 0.214 ±   nan, critic_loss = 15.872 ±   nan, entropy_coef = 0.078 ±   nan, rollout_stds = 0.565 ± 0.119, action_magnitude = 0.798 ± 0.254, n_updates = 65901, time = 12.1, total_time = 812.1 \n",
      "\n",
      "step =   67000, total_step = 1072000, scores =  3496.321 ± 170.537 [ 2943.134, 3754.506] (n=16), score_ema =  3417.167, actor_loss = -176.294 ±   nan, entropy_coef_loss = -1.554 ±   nan, critic_loss = 371.713 ±   nan, entropy_coef = 0.079 ±   nan, rollout_stds = 0.555 ± 0.112, action_magnitude = 0.801 ± 0.252, n_updates = 66901, time = 12.2, total_time = 824.3 \n",
      "\n",
      "step =   68000, total_step = 1088000, scores =  3618.264 ± 85.094 [ 3427.061, 3733.350] (n=16), score_ema =  3467.441, actor_loss = -178.501 ±   nan, entropy_coef_loss = -0.842 ±   nan, critic_loss = 677.827 ±   nan, entropy_coef = 0.081 ±   nan, rollout_stds = 0.538 ± 0.089, action_magnitude = 0.798 ± 0.253, n_updates = 67901, time = 12.1, total_time = 836.5 \n",
      "\n",
      "step =   69000, total_step = 1104000, scores =  3547.277 ± 416.538 [ 1971.421, 3789.754] (n=16), score_ema =  3487.400, actor_loss = -184.464 ±   nan, entropy_coef_loss = -1.177 ±   nan, critic_loss = 12.973 ±   nan, entropy_coef = 0.084 ±   nan, rollout_stds = 0.558 ± 0.114, action_magnitude = 0.789 ± 0.259, n_updates = 68901, time = 12.1, total_time = 848.5 \n",
      "\n",
      "step =   70000, total_step = 1120000, scores =  3516.680 ± 307.875 [ 2398.993, 3769.495] (n=16), score_ema =  3494.720, actor_loss = -189.040 ±   nan, entropy_coef_loss = 0.695 ±   nan, critic_loss = 13.285 ±   nan, entropy_coef = 0.086 ±   nan, rollout_stds = 0.547 ± 0.102, action_magnitude = 0.793 ± 0.257, n_updates = 69901, time = 12.3, total_time = 860.9 \n",
      "\n",
      "step =   71000, total_step = 1136000, scores =  3655.774 ± 65.374 [ 3540.373, 3806.062] (n=16), score_ema =  3534.984, actor_loss = -185.329 ±   nan, entropy_coef_loss = -0.287 ±   nan, critic_loss = 713.235 ±   nan, entropy_coef = 0.088 ±   nan, rollout_stds = 0.504 ± 0.107, action_magnitude = 0.797 ± 0.254, n_updates = 70901, time = 12.3, total_time = 873.2 \n",
      "\n",
      "step =   72000, total_step = 1152000, scores =  3657.793 ± 98.821 [ 3520.666, 3818.724] (n=16), score_ema =  3565.686, actor_loss = -193.918 ±   nan, entropy_coef_loss = -0.613 ±   nan, critic_loss = 14.943 ±   nan, entropy_coef = 0.088 ±   nan, rollout_stds = 0.529 ± 0.106, action_magnitude = 0.792 ± 0.257, n_updates = 71901, time = 12.2, total_time = 885.3 \n",
      "\n",
      "step =   73000, total_step = 1168000, scores =  3617.740 ± 225.048 [ 2798.019, 3833.703] (n=16), score_ema =  3578.700, actor_loss = -196.063 ±   nan, entropy_coef_loss = -0.181 ±   nan, critic_loss = 28.540 ±   nan, entropy_coef = 0.089 ±   nan, rollout_stds = 0.558 ± 0.119, action_magnitude = 0.791 ± 0.257, n_updates = 72901, time = 12.4, total_time = 897.7 \n",
      "\n",
      "step =   74000, total_step = 1184000, scores =  3687.544 ± 115.720 [ 3460.816, 3866.069] (n=16), score_ema =  3605.911, actor_loss = -199.951 ±   nan, entropy_coef_loss = -0.458 ±   nan, critic_loss = 15.421 ±   nan, entropy_coef = 0.094 ±   nan, rollout_stds = 0.541 ± 0.110, action_magnitude = 0.796 ± 0.255, n_updates = 73901, time = 12.5, total_time = 910.2 \n",
      "\n",
      "step =   75000, total_step = 1200000, scores =  3728.489 ± 101.309 [ 3570.085, 3916.875] (n=16), score_ema =  3636.555, actor_loss = -196.783 ±   nan, entropy_coef_loss = 0.284 ±   nan, critic_loss = 18.968 ±   nan, entropy_coef = 0.097 ±   nan, rollout_stds = 0.542 ± 0.099, action_magnitude = 0.792 ± 0.258, n_updates = 74901, time = 12.2, total_time = 922.4 \n",
      "\n",
      "step =   76000, total_step = 1216000, scores =  3725.239 ± 167.791 [ 3218.857, 3906.541] (n=16), score_ema =  3658.726, actor_loss = -200.687 ±   nan, entropy_coef_loss = -1.687 ±   nan, critic_loss = 11.062 ±   nan, entropy_coef = 0.099 ±   nan, rollout_stds = 0.531 ± 0.110, action_magnitude = 0.792 ± 0.256, n_updates = 75901, time = 12.1, total_time = 934.5 \n",
      "step =   77000, total_step = 1232000, scores =  3777.185 ± 81.491 [ 3514.026, 3901.872] (n=16), score_ema =  3688.341, actor_loss = -206.363 ±   nan, entropy_coef_loss = 0.555 ±   nan, critic_loss = 17.237 ±   nan, entropy_coef = 0.098 ±   nan, rollout_stds = 0.537 ± 0.119, action_magnitude = 0.790 ± 0.257, n_updates = 76901, time = 12.5, total_time = 947.1 \n",
      "step =   78000, total_step = 1248000, scores =  3826.781 ± 92.800 [ 3628.849, 3946.633] (n=16), score_ema =  3722.951, actor_loss = -211.920 ±   nan, entropy_coef_loss = -0.052 ±   nan, critic_loss = 420.537 ±   nan, entropy_coef = 0.101 ±   nan, rollout_stds = 0.529 ± 0.128, action_magnitude = 0.790 ± 0.256, n_updates = 77901, time = 12.7, total_time = 959.8 \n",
      "\n",
      "step =   79000, total_step = 1264000, scores =  3821.729 ± 53.057 [ 3718.401, 3902.062] (n=16), score_ema =  3747.645, actor_loss = -207.597 ±   nan, entropy_coef_loss = 0.208 ±   nan, critic_loss = 456.644 ±   nan, entropy_coef = 0.101 ±   nan, rollout_stds = 0.541 ± 0.118, action_magnitude = 0.787 ± 0.258, n_updates = 78901, time = 12.0, total_time = 971.8 \n",
      "\n",
      "step =   80000, total_step = 1280000, scores =  3861.556 ± 77.645 [ 3689.496, 3988.072] (n=16), score_ema =  3776.123, actor_loss = -207.187 ±   nan, entropy_coef_loss = -0.177 ±   nan, critic_loss = 13.637 ±   nan, entropy_coef = 0.103 ±   nan, rollout_stds = 0.536 ± 0.109, action_magnitude = 0.788 ± 0.259, n_updates = 79901, time = 11.8, total_time = 983.6 \n",
      "\n",
      "step =   81000, total_step = 1296000, scores =  3880.151 ± 129.041 [ 3445.028, 4072.267] (n=16), score_ema =  3802.130, actor_loss = -212.776 ±   nan, entropy_coef_loss = 0.848 ±   nan, critic_loss = 355.272 ±   nan, entropy_coef = 0.102 ±   nan, rollout_stds = 0.533 ± 0.119, action_magnitude = 0.782 ± 0.261, n_updates = 80901, time = 11.9, total_time = 995.6 \n",
      "\n",
      "step =   82000, total_step = 1312000, scores =  3850.773 ± 179.856 [ 3222.550, 4038.088] (n=16), score_ema =  3814.291, actor_loss = -221.879 ±   nan, entropy_coef_loss = 0.778 ±   nan, critic_loss = 15.322 ±   nan, entropy_coef = 0.102 ±   nan, rollout_stds = 0.535 ± 0.135, action_magnitude = 0.786 ± 0.259, n_updates = 81901, time = 12.4, total_time = 1008.0 \n",
      "\n",
      "step =   83000, total_step = 1328000, scores =  3936.963 ± 84.229 [ 3768.643, 4054.154] (n=16), score_ema =  3844.959, actor_loss = -215.713 ±   nan, entropy_coef_loss = 0.169 ±   nan, critic_loss = 12.679 ±   nan, entropy_coef = 0.105 ±   nan, rollout_stds = 0.529 ± 0.131, action_magnitude = 0.782 ± 0.261, n_updates = 82901, time = 11.9, total_time = 1019.9 \n",
      "\n",
      "step =   84000, total_step = 1344000, scores =  4035.660 ± 76.262 [ 3869.799, 4161.442] (n=16), score_ema =  3892.634, actor_loss = -222.727 ±   nan, entropy_coef_loss = 0.767 ±   nan, critic_loss = 15.194 ±   nan, entropy_coef = 0.103 ±   nan, rollout_stds = 0.529 ± 0.114, action_magnitude = 0.779 ± 0.264, n_updates = 83901, time = 12.4, total_time = 1032.4 \n",
      "\n",
      "step =   85000, total_step = 1360000, scores =  3930.562 ± 266.080 [ 2932.694, 4110.999] (n=16), score_ema =  3902.116, actor_loss = -225.816 ±   nan, entropy_coef_loss = 0.538 ±   nan, critic_loss = 12.338 ±   nan, entropy_coef = 0.104 ±   nan, rollout_stds = 0.535 ± 0.120, action_magnitude = 0.778 ± 0.264, n_updates = 84901, time = 12.8, total_time = 1045.1 \n",
      "\n",
      "step =   86000, total_step = 1376000, scores =  4001.475 ± 71.855 [ 3906.197, 4164.650] (n=16), score_ema =  3926.956, actor_loss = -216.066 ±   nan, entropy_coef_loss = -1.278 ±   nan, critic_loss = 9.095 ±   nan, entropy_coef = 0.107 ±   nan, rollout_stds = 0.528 ± 0.113, action_magnitude = 0.779 ± 0.264, n_updates = 85901, time = 12.6, total_time = 1057.7 \n",
      "\n",
      "step =   87000, total_step = 1392000, scores =  4024.275 ± 96.236 [ 3836.634, 4226.369] (n=16), score_ema =  3951.286, actor_loss = -223.956 ±   nan, entropy_coef_loss = 1.766 ±   nan, critic_loss = 501.602 ±   nan, entropy_coef = 0.106 ±   nan, rollout_stds = 0.522 ± 0.136, action_magnitude = 0.777 ± 0.267, n_updates = 86901, time = 12.1, total_time = 1069.8 \n",
      "\n",
      "step =   88000, total_step = 1408000, scores =  4028.089 ± 49.449 [ 3951.470, 4103.711] (n=16), score_ema =  3970.487, actor_loss = -233.992 ±   nan, entropy_coef_loss = 0.524 ±   nan, critic_loss = 42.574 ±   nan, entropy_coef = 0.110 ±   nan, rollout_stds = 0.538 ± 0.115, action_magnitude = 0.774 ± 0.268, n_updates = 87901, time = 12.5, total_time = 1082.3 \n",
      "\n",
      "step =   89000, total_step = 1424000, scores =  4076.923 ± 70.442 [ 3961.432, 4253.175] (n=16), score_ema =  3997.096, actor_loss = -231.522 ±   nan, entropy_coef_loss = 0.441 ±   nan, critic_loss = 8.895 ±   nan, entropy_coef = 0.110 ±   nan, rollout_stds = 0.526 ± 0.126, action_magnitude = 0.771 ± 0.269, n_updates = 88901, time = 12.1, total_time = 1094.4 \n",
      "\n",
      "step =   90000, total_step = 1440000, scores =  4096.543 ± 95.324 [ 3919.743, 4281.268] (n=16), score_ema =  4021.957, actor_loss = -235.684 ±   nan, entropy_coef_loss = 0.201 ±   nan, critic_loss = 22.396 ±   nan, entropy_coef = 0.114 ±   nan, rollout_stds = 0.528 ± 0.144, action_magnitude = 0.773 ± 0.268, n_updates = 89901, time = 12.4, total_time = 1106.8 \n",
      "\n",
      "step =   91000, total_step = 1456000, scores =  4125.984 ± 67.214 [ 4008.228, 4280.482] (n=16), score_ema =  4047.964, actor_loss = -227.749 ±   nan, entropy_coef_loss = 0.147 ±   nan, critic_loss = 11.337 ±   nan, entropy_coef = 0.118 ±   nan, rollout_stds = 0.512 ± 0.123, action_magnitude = 0.775 ± 0.265, n_updates = 90901, time = 12.4, total_time = 1119.3 \n",
      "\n",
      "step =   92000, total_step = 1472000, scores =  4147.965 ± 87.210 [ 3946.909, 4270.332] (n=16), score_ema =  4072.964, actor_loss = -240.094 ±   nan, entropy_coef_loss = 0.991 ±   nan, critic_loss = 15.496 ±   nan, entropy_coef = 0.121 ±   nan, rollout_stds = 0.512 ± 0.127, action_magnitude = 0.771 ± 0.268, n_updates = 91901, time = 12.5, total_time = 1131.8 \n",
      "\n",
      "step =   93000, total_step = 1488000, scores =  4126.156 ± 82.826 [ 3920.964, 4282.272] (n=16), score_ema =  4086.262, actor_loss = -239.001 ±   nan, entropy_coef_loss = 0.573 ±   nan, critic_loss = 30.971 ±   nan, entropy_coef = 0.122 ±   nan, rollout_stds = 0.510 ± 0.129, action_magnitude = 0.770 ± 0.269, n_updates = 92901, time = 12.5, total_time = 1144.3 \n",
      "\n",
      "step =   94000, total_step = 1504000, scores =  4102.948 ± 72.420 [ 3986.542, 4205.731] (n=16), score_ema =  4090.434, actor_loss = -241.291 ±   nan, entropy_coef_loss = 0.424 ±   nan, critic_loss = 15.220 ±   nan, entropy_coef = 0.123 ±   nan, rollout_stds = 0.510 ± 0.132, action_magnitude = 0.768 ± 0.269, n_updates = 93901, time = 12.3, total_time = 1156.6 \n",
      "\n",
      "step =   95000, total_step = 1520000, scores =  4148.381 ± 105.295 [ 3953.229, 4356.594] (n=16), score_ema =  4104.921, actor_loss = -249.218 ±   nan, entropy_coef_loss = -0.148 ±   nan, critic_loss = 15.312 ±   nan, entropy_coef = 0.125 ±   nan, rollout_stds = 0.490 ± 0.137, action_magnitude = 0.765 ± 0.271, n_updates = 94901, time = 12.3, total_time = 1168.9 \n",
      "\n",
      "step =   96000, total_step = 1536000, scores =  4152.369 ± 71.963 [ 4016.358, 4318.735] (n=16), score_ema =  4116.783, actor_loss = -251.243 ±   nan, entropy_coef_loss = 0.411 ±   nan, critic_loss = 596.313 ±   nan, entropy_coef = 0.125 ±   nan, rollout_stds = 0.498 ± 0.134, action_magnitude = 0.765 ± 0.271, n_updates = 95901, time = 12.3, total_time = 1181.2 \n",
      "\n",
      "step =   97000, total_step = 1552000, scores =  1349.504 ± 39.063 [ 1280.630, 1461.798] (n=16), score_ema =  3424.963, actor_loss = -84602632008746840096768.000 ±   nan, entropy_coef_loss = -1462525892532465056088064.000 ±   nan, critic_loss =   inf ±   nan, entropy_coef = 0.122 ±   nan, rollout_stds = 0.000 ± 0.000, action_magnitude = 0.904 ± 0.207, n_updates = 96901, time = 12.0, total_time = 1193.2 \n",
      "\n",
      "step =   98000, total_step = 1568000, scores = -601.151 ± 0.803 [-603.024, -599.730] (n=16), score_ema =  2418.434, actor_loss = -128861108828384048185344.000 ±   nan, entropy_coef_loss = -2227622185042513963778048.000 ±   nan, critic_loss =   inf ±   nan, entropy_coef = 0.122 ±   nan, rollout_stds = 0.000 ± 0.000, action_magnitude = 1.000 ± 0.000, n_updates = 97901, time = 12.2, total_time = 1205.4 \n",
      "\n",
      "\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Score too low, policy probably fucked :(",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 324\u001B[0m\n\u001B[0;32m    314\u001B[0m         total_stopwatch\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m log_experiment(\n\u001B[0;32m    316\u001B[0m             logger,\n\u001B[0;32m    317\u001B[0m             experiment_tags\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mtype\u001B[39m(algo)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, env_name],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    322\u001B[0m             \u001B[38;5;66;03m# pr = cProfile.Profile()\u001B[39;00m\n\u001B[0;32m    323\u001B[0m             \u001B[38;5;66;03m# pr.enable()\u001B[39;00m\n\u001B[1;32m--> 324\u001B[0m             \u001B[43malgo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1_000_000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m             \u001B[38;5;66;03m# pr.disable()  \u001B[39;00m\n\u001B[0;32m    326\u001B[0m             \u001B[38;5;66;03m# pr.dump_stats('profile_stats.pstat')\u001B[39;00m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Git\\pytorch-repository\\src\\reinforcement_learning\\algorithms\\base\\base_algorithm.py:134\u001B[0m, in \u001B[0;36mBaseAlgorithm.learn\u001B[1;34m(self, total_timesteps)\u001B[0m\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mset_train_mode(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    132\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimize(obs, episode_starts, info)\n\u001B[1;32m--> 134\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_optimization_done\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\Git\\pytorch-repository\\src\\reinforcement_learning\\core\\callback.py:42\u001B[0m, in \u001B[0;36mCallback.on_optimization_done\u001B[1;34m(self, optim_algo, step, info)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_optimization_done\u001B[39m(\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     37\u001B[0m         optim_algo: Algo,\n\u001B[0;32m     38\u001B[0m         step: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m     39\u001B[0m         info: InfoDict,\n\u001B[0;32m     40\u001B[0m ):\n\u001B[0;32m     41\u001B[0m     scheduler_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollect_scheduler_values(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimization_schedulers)\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_optimization_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptim_algo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler_values\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 248\u001B[0m, in \u001B[0;36mon_optimization_done\u001B[1;34m(rl, step, info, scheduler_values)\u001B[0m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m episode_scores\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m500\u001B[39m:\n\u001B[0;32m    247\u001B[0m     logger\u001B[38;5;241m.\u001B[39msave_experiment()\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mScore too low, policy probably fucked :(\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Score too low, policy probably fucked :("
     ]
    }
   ],
   "source": [
    "from src.summary_statistics import compute_summary_statistics\n",
    "from src.reinforcement_learning.core.loss_config import LossLoggingConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\n",
    "def get_setup() -> dict[str, str]:\n",
    "    import inspect\n",
    "    import sac\n",
    "    return {\n",
    "        'sac.py': inspect.getsource(sac),\n",
    "        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n",
    "    }\n",
    "\n",
    "policy_id: str\n",
    "policy: BasePolicy\n",
    "optimizer: optim.Optimizer\n",
    "wrapped_env: Env\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n",
    "            env=env,\n",
    "            info=PolicyConstruction.create_policy_initialization_info(\n",
    "                init_action_selector=init_action_selector,\n",
    "                init_policy=init_policy,\n",
    "                init_optimizer=init_optimizer,\n",
    "                wrap_env=wrap_env,\n",
    "                hyper_parameters=policy_construction_hyper_parameter,\n",
    "            ),\n",
    "        )\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "step_stopwatch = Stopwatch()\n",
    "total_stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    rewards = rl.buffer.rewards[tail_indices]\n",
    "    # if 'raw_rewards' in info['rollout']:\n",
    "    #     rewards = info['rollout']['raw_rewards']\n",
    "    \n",
    "    episode_scores = compute_episode_returns(\n",
    "        rewards=rewards,\n",
    "        episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n",
    "        last_episode_starts=info['last_episode_starts'],\n",
    "        gamma=1.0,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        remove_unfinished_episodes=True,\n",
    "    )\n",
    "    \n",
    "    global best_iteration_score\n",
    "    iteration_score = episode_scores.mean()\n",
    "    score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    if iteration_score >= best_iteration_score:\n",
    "        best_iteration_score = iteration_score\n",
    "        policy_db.save_model_state_dict(\n",
    "            model_id=policy_id,\n",
    "            parent_model_id=parent_policy_id,\n",
    "            model_info={\n",
    "                'score': iteration_score.item(),\n",
    "                'steps_trained': steps_trained,\n",
    "                'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                'init_policy_source_code': init_policy_source\n",
    "            },\n",
    "            model=policy,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "    \n",
    "    info['episode_scores'] = episode_scores\n",
    "    info['score_moving_average'] = score_moving_average\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    # global steps_trained\n",
    "    # steps_trained += rl.buffer.pos\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    total_step = step * rl.num_envs\n",
    "    \n",
    "    time_taken = step_stopwatch.reset()\n",
    "    total_time = total_stopwatch.time_passed()\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    episode_scores = info['episode_scores']\n",
    "    score_moving_average = info['score_moving_average']\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "        n_format='>2'\n",
    "    )\n",
    "    # scores2 = format_summary_statics(\n",
    "    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='4.3f',\n",
    "    #     min_value_format=' 6.3f',\n",
    "    #     max_value_format='5.3f',\n",
    "    #     n_format='>2'\n",
    "    # )\n",
    "    # advantages = format_summary_statics(\n",
    "    #     rl.buffer.advantages, \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    actor_loss = format_summary_statics(\n",
    "        info['final_actor_loss'],  \n",
    "        mean_format=' 5.3f',\n",
    "        # std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # actor_loss_raw = format_summary_statics(\n",
    "    #     info['raw_actor_loss'],  \n",
    "    #     mean_format=' 5.3f',\n",
    "    #     std_format='5.3f',\n",
    "    #     min_value_format=None,\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n",
    "        info['final_entropy_coef_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_loss = format_summary_statics(\n",
    "        info['final_critic_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_coef = format_summary_statics(\n",
    "        info['entropy_coef'],\n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # resets = format_summary_statics(\n",
    "    #     rl.buffer.dones.astype(int).sum(axis=0), \n",
    "    #     mean_format='.2f',\n",
    "    #     std_format=None,\n",
    "    #     min_value_format='1d',\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    # kl_div = info['actor_kl_divergence'][-1]\n",
    "    # grad_norm = format_summary_statics(\n",
    "    #     info['grad_norm'], \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    action_stds = info['rollout'].get('action_stds')\n",
    "    if action_stds is not None:\n",
    "        rollout_action_stds = format_summary_statics(\n",
    "            action_stds,\n",
    "            mean_format='5.3f',\n",
    "            std_format='5.3f',\n",
    "            min_value_format=None,\n",
    "            max_value_format=None,\n",
    "        )\n",
    "    else:\n",
    "        rollout_action_stds = 'N/A'\n",
    "    action_magnitude = format_summary_statics(\n",
    "        np.abs(rl.buffer.actions[tail_indices]),\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # ppo_epochs = info['nr_ppo_epochs']\n",
    "    # ppo_updates = info['nr_ppo_updates']\n",
    "    # expl_var = rl.buffer.compute_critic_explained_variance()\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{total_step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          # f\"{scores2 = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          # f\"{advantages = :s}, \"\n",
    "          f\"{actor_loss = :s}, \"\n",
    "          # f\"{actor_loss_raw = :s}, \"\n",
    "          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n",
    "          f\"{critic_loss = :s}, \"\n",
    "          f\"{entropy_coef = :s}, \"\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{action_magnitude = :s}, \"\n",
    "          # f\"{expl_var = :.3f}, \"\n",
    "          # f\"{kl_div = :.4f}, \"\n",
    "          # f\"{ppo_epochs = }, \"\n",
    "          # f\"{ppo_updates = }, \"\n",
    "          # f\"{grad_norm = :s}, \"\n",
    "          f\"n_updates = {rl.gradient_steps_performed}, \"\n",
    "          # f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f}, \"\n",
    "          f\"total_time = {total_time:4.1f} \\n\"\n",
    "          )\n",
    "    logger.add_item({\n",
    "        'step': step,\n",
    "        'total_step': total_step,\n",
    "        'scores': compute_summary_statistics(episode_scores),\n",
    "        'actor_loss': compute_summary_statistics(info['final_actor_loss']),\n",
    "        'entropy_coef_loss': compute_summary_statistics(info['final_entropy_coef_loss']),\n",
    "        'critic_loss': compute_summary_statistics(info['final_critic_loss']),\n",
    "        'entropy_coef': compute_summary_statistics(info['entropy_coef']),\n",
    "        'action_stds': compute_summary_statistics(action_stds),\n",
    "        'action_magnitude': compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n",
    "        'gradient_step': rl.gradient_steps_performed,\n",
    "        'time_taken': time_taken,\n",
    "        'total_time': total_time\n",
    "    })\n",
    "    if step % 10000 == 0:\n",
    "        logger.save_experiment()\n",
    "    print()\n",
    "    \n",
    "    if episode_scores.mean().item() < -500:\n",
    "        logger.save_experiment()\n",
    "        raise ValueError('Score too low, policy probably fucked :(')\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "env_kwargs = {}\n",
    "num_envs = 16\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "\n",
    "env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "\n",
    "logger = ExperimentLogger('experiment_logs/sac/')\n",
    "\n",
    "try:\n",
    "    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "        \n",
    "    with ((torch.autograd.set_detect_anomaly(False))):\n",
    "        algo = SAC(\n",
    "            env=wrapped_env,\n",
    "            policy=policy,\n",
    "            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n",
    "            weigh_critic_loss=lambda l: 1 * l,\n",
    "            buffer_size=50_000,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n",
    "            entropy_coef=1.0,\n",
    "            rollout_steps=1,\n",
    "            gradient_steps=1,\n",
    "            warmup_steps=100,\n",
    "            learning_starts=100,\n",
    "            optimization_batch_size=256,\n",
    "            target_update_interval=1,\n",
    "            # sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_last_obs=True, log_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n",
    "                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n",
    "                                            critic_loss=LossLoggingConfig(log_final=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        total_stopwatch.reset()\n",
    "        with log_experiment(\n",
    "            logger,\n",
    "            experiment_tags=[type(algo).__name__, env_name],\n",
    "            hyper_parameters=algo.collect_hyper_parameters(),\n",
    "            setup=get_setup(),\n",
    "        ) as x:\n",
    "            # import cProfile\n",
    "            # pr = cProfile.Profile()\n",
    "            # pr.enable()\n",
    "            algo.learn(1_000_000)\n",
    "            # pr.disable()  \n",
    "            # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T15:57:16.197058Z",
     "start_time": "2024-09-24T15:37:06.550658Z"
    }
   },
   "id": "f71efe062771e81b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "_ih[1] + '\\n\\n' + _ih[2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "778fd25d2dc23013",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.17493404"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = algo.buffer.tail_indices(10000)[:1000]\n",
    "algo.buffer.actions[indices].mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-24T14:21:04.302382Z",
     "start_time": "2024-09-24T14:21:04.209398Z"
    }
   },
   "id": "e39b8f4554a69cc8",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "SAC-256 1k steps:\n",
    "* num_envs = 16: 12.0\n",
    "\n",
    "CrossQ-256 1k steps:\n",
    "* num_envs = 16: 17.0\n",
    "* num_envs = 32: 18.5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64dec978abc8662"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc184a8a98ea506",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range(10):\n",
    "    print((np.abs(algo.buffer.actions[6000 + 100 * i:6100 + 100*i])).mean())\n",
    "    print((algo.buffer.actions[6000 + 100 * i:6100 + 100*i]).mean())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "algo.target_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d4d48d204b6f44",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1ae8571d73535c6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env = wrap_env(as_vec_env(record_env)[0], {})\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        with torch.no_grad():\n",
    "            obs, info = record_env.reset()\n",
    "            for step in range(max_steps):\n",
    "                actions_dist, _ = policy.process_obs(torch.tensor(obs, device=device))\n",
    "                actions = actions_dist.sample().detach().cpu().numpy()\n",
    "                obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ab51a61dd845",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
