{
 "cells": [
  {
   "cell_type": "code",
   "id": "ba8c59a3eba2f172",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T19:25:59.350648Z",
     "start_time": "2024-10-10T19:25:56.187452Z"
    }
   },
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from sac import init_action_selector, init_policy, init_optimizer, wrap_env, policy_construction_hyper_parameter\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.np_functions import inv_symmetric_log\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\n",
    "from src.reinforcement_learning.gym.envs.test_env import TestEnv\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.torch_device import set_default_torch_device, optimizer_to_device\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from src.summary_statistics import maybe_compute_summary_statistics\n",
    "from src.reinforcement_learning.core.loss_config import LossLoggingConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\n",
    "def get_setup() -> dict[str, str]:\n",
    "    import inspect\n",
    "    import sac\n",
    "    return {\n",
    "        'sac.py': inspect.getsource(sac),\n",
    "        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n",
    "    }\n",
    "\n",
    "policy_id: str\n",
    "policy: BasePolicy\n",
    "optimizer: optim.Optimizer\n",
    "wrapped_env: Env\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n",
    "            env=env,\n",
    "            info=PolicyConstruction.create_policy_initialization_info(\n",
    "                init_action_selector=init_action_selector,\n",
    "                init_policy=init_policy,\n",
    "                init_optimizer=init_optimizer,\n",
    "                wrap_env=wrap_env,\n",
    "                hyper_parameters=policy_construction_hyper_parameter,\n",
    "            ),\n",
    "        )\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "step_stopwatch = Stopwatch()\n",
    "total_stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    # rewards = rl.buffer.rewards[tail_indices]\n",
    "    # if 'raw_rewards' in info['rollout']:\n",
    "    #     rewards = info['rollout']['raw_rewards']\n",
    "    \n",
    "    # episode_scores = compute_episode_returns(\n",
    "    #     rewards=rewards,\n",
    "    #     episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n",
    "    #     last_episode_starts=info['last_episode_starts'],\n",
    "    #     gamma=1.0,\n",
    "    #     gae_lambda=1.0,\n",
    "    #     normalize_rewards=None,\n",
    "    #     remove_unfinished_episodes=True,\n",
    "    # )\n",
    "    \n",
    "    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs)\n",
    "    \n",
    "    if len(episode_scores) > 0:\n",
    "    \n",
    "        global best_iteration_score\n",
    "        iteration_score = episode_scores.mean()\n",
    "        score_moving_average = score_mean_ema.update(iteration_score)\n",
    "        if iteration_score >= best_iteration_score:\n",
    "            best_iteration_score = iteration_score\n",
    "            policy_db.save_model_state_dict(\n",
    "                model_id=policy_id,\n",
    "                parent_model_id=parent_policy_id,\n",
    "                model_info={\n",
    "                    'score': iteration_score.item(),\n",
    "                    'steps_trained': steps_trained,\n",
    "                    'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                    'init_policy_source_code': init_policy_source\n",
    "                },\n",
    "                model=policy,\n",
    "                optimizer=optimizer,\n",
    "            )\n",
    "        info['score_moving_average'] = score_moving_average\n",
    "    \n",
    "    info['episode_scores'] = episode_scores\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    # global steps_trained\n",
    "    # steps_trained += rl.buffer.pos\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    num_env_steps = step * rl.num_envs\n",
    "    \n",
    "    step_time = step_stopwatch.reset()\n",
    "    total_time = total_stopwatch.time_passed()\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    episode_scores = info.get('episode_scores')\n",
    "    score_moving_average = info.get('score_moving_average') or 0.0\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "        n_format='>2'\n",
    "    )\n",
    "    # scores2 = format_summary_statics(\n",
    "    #     rl.buffer.compute_most_recent_episode_scores(rl.num_envs, lambda r: 1 * r), \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='4.3f',\n",
    "    #     min_value_format=' 6.3f',\n",
    "    #     max_value_format='5.3f',\n",
    "    #     n_format='>2'\n",
    "    # )\n",
    "    # advantages = format_summary_statics(\n",
    "    #     rl.buffer.advantages, \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    actor_loss = format_summary_statics(\n",
    "        info['final_actor_loss'],  \n",
    "        mean_format=' 5.3f',\n",
    "        # std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # actor_loss_raw = format_summary_statics(\n",
    "    #     info['raw_actor_loss'],  \n",
    "    #     mean_format=' 5.3f',\n",
    "    #     std_format='5.3f',\n",
    "    #     min_value_format=None,\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    entropy_coef_loss = None if 'final_entropy_coef_loss' not in info else format_summary_statics(\n",
    "        info['final_entropy_coef_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_loss = format_summary_statics(\n",
    "        info['final_critic_loss'], \n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_coef = format_summary_statics(\n",
    "        info['entropy_coef'],\n",
    "        mean_format='5.3f',\n",
    "#         std_format='5.3f',\n",
    "        std_format=None,\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # resets = format_summary_statics(\n",
    "    #     rl.buffer.dones.astype(int).sum(axis=0), \n",
    "    #     mean_format='.2f',\n",
    "    #     std_format=None,\n",
    "    #     min_value_format='1d',\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    # kl_div = info['actor_kl_divergence'][-1]\n",
    "    # grad_norm = format_summary_statics(\n",
    "    #     info['grad_norm'], \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    action_stds = info['rollout'].get('action_stds')\n",
    "    if action_stds is not None:\n",
    "        rollout_action_stds = format_summary_statics(\n",
    "            action_stds,\n",
    "            mean_format='5.3f',\n",
    "            std_format='5.3f',\n",
    "            min_value_format=None,\n",
    "            max_value_format=None,\n",
    "        )\n",
    "    else:\n",
    "        rollout_action_stds = 'N/A'\n",
    "    action_magnitude = format_summary_statics(\n",
    "        np.abs(rl.buffer.actions[tail_indices]),\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # ppo_epochs = info['nr_ppo_epochs']\n",
    "    # ppo_updates = info['nr_ppo_updates']\n",
    "    # expl_var = rl.buffer.compute_critic_explained_variance()\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{num_env_steps = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          # f\"{scores2 = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          # f\"{advantages = :s}, \"\n",
    "          f\"{actor_loss = :s}, \"\n",
    "          # f\"{actor_loss_raw = :s}, \"\n",
    "          f\"{critic_loss = :s}, \"\n",
    "          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n",
    "          f\"{entropy_coef = :s}, \"\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{action_magnitude = :s}, \"\n",
    "          # f\"{expl_var = :.3f}, \"\n",
    "          # f\"{kl_div = :.4f}, \"\n",
    "          # f\"{ppo_epochs = }, \"\n",
    "          # f\"{ppo_updates = }, \"\n",
    "          # f\"{grad_norm = :s}, \"\n",
    "          f\"n_updates = {rl.gradient_steps_performed}, \"\n",
    "          # f\"{resets = :s}, \"\n",
    "          f\"time = {step_time:4.1f}, \"\n",
    "          f\"total_time = {total_time:4.1f} \\n\"\n",
    "          )\n",
    "    logger.add_item({\n",
    "        'step': step,\n",
    "        'num_env_steps': num_env_steps,\n",
    "        'scores': maybe_compute_summary_statistics(episode_scores),\n",
    "        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n",
    "        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n",
    "        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n",
    "        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n",
    "        'action_stds': maybe_compute_summary_statistics(action_stds),\n",
    "        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n",
    "        'num_gradient_steps': rl.gradient_steps_performed,\n",
    "        'step_time': step_time,\n",
    "        'total_time': total_time\n",
    "    })\n",
    "    if step % 10000 == 0:\n",
    "        logger.save_experiment_log()\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "    # if episode_scores is not None and len(episode_scores) > 0 and episode_scores.mean().item() < -500:\n",
    "    #     logger.save_experiment_log()\n",
    "    #     raise ValueError('Score too low, policy probably fucked :(')\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.1 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "env_kwargs = {}\n",
    "num_envs = 1\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "\n",
    "# TODO\n",
    "# env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "env = create_env(render_mode=None)\n",
    "\n",
    "logger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n",
    "\n",
    "try:\n",
    "    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=False)\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = }')\n",
    "        \n",
    "    with ((torch.autograd.set_detect_anomaly(False))):\n",
    "        algo = SAC(\n",
    "            env=wrapped_env,\n",
    "            policy=policy,\n",
    "            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            # weigh_and_reduce_actor_loss=lambda l: 1 * l.mean(),\n",
    "            # weigh_critic_loss=lambda l: 1 * l,\n",
    "            buffer_size=1_000_000,\n",
    "            reward_scale=1,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n",
    "            entropy_coef=1.0,\n",
    "            rollout_steps=1,\n",
    "            gradient_steps=1,\n",
    "            warmup_steps=10_000,\n",
    "            optimization_batch_size=256,\n",
    "            target_update_interval=1,\n",
    "            # sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_last_obs=True, log_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossLoggingConfig(log_final=True),\n",
    "                                            actor_loss=LossLoggingConfig(log_final=True, log_raw=True),\n",
    "                                            critic_loss=LossLoggingConfig(log_final=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        total_stopwatch.reset()\n",
    "        with log_experiment(\n",
    "            logger,\n",
    "            experiment_tags=algo.collect_tags(),\n",
    "            hyper_parameters=algo.collect_hyper_parameters(),\n",
    "            setup=get_setup(),\n",
    "        ) as x:\n",
    "            logger.save_experiment_log()\n",
    "            print('\\nStarting Training\\n\\n')\n",
    "            # import cProfile\n",
    "            # pr = cProfile.Profile()\n",
    "            # pr.enable()\n",
    "            algo.learn(5_000_000)\n",
    "            # pr.disable()  \n",
    "            # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T19:33:04.664538Z",
     "start_time": "2024-10-10T19:25:59.351649Z"
    }
   },
   "id": "f71efe062771e81b",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "No policy in RAM, creating a new one\n",
      "New policy 2024-10-10 19:25:59.513793 created\n",
      "Using policy 2024-10-10 19:25:59.513793 with parent policy None\n",
      "count_parameters(policy) = 217870\n",
      "env = <TimeLimit<OrderEnforcing<PassiveEnvChecker<HalfCheetahEnv<HalfCheetah-v4>>>>>, num_envs = 1\n",
      "Grabbing system information... done!\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "Starting Training\n",
      "\n",
      "step =   11000, num_env_steps =   11000, scores = -139.240 (n= 1), score_ema = -254.650, actor_loss = -15.036, critic_loss = 18.719, entropy_coef_loss = -2.811, entropy_coef = 0.745, rollout_stds = 0.943 ± 0.024, action_magnitude = 0.553 ± 0.289, n_updates = 1000, time = 13.6, total_time = 13.2 \n",
      "\n",
      "step =   12000, num_env_steps =   12000, scores = -137.261 (n= 1), score_ema = -225.303, actor_loss = -23.296, critic_loss = 15.917, entropy_coef_loss = -4.861, entropy_coef = 0.557, rollout_stds = 0.794 ± 0.171, action_magnitude = 0.573 ± 0.292, n_updates = 2000, time =  9.1, total_time = 22.3 \n",
      "\n",
      "step =   13000, num_env_steps =   13000, scores = -272.821 (n= 1), score_ema = -237.183, actor_loss = -29.715, critic_loss = 21.822, entropy_coef_loss = -6.886, entropy_coef = 0.421, rollout_stds = 0.844 ± 0.099, action_magnitude = 0.601 ± 0.293, n_updates = 3000, time =  9.2, total_time = 31.5 \n",
      "\n",
      "step =   14000, num_env_steps =   14000, scores = -536.847 (n= 1), score_ema = -312.099, actor_loss = -32.409, critic_loss = 31.942, entropy_coef_loss = -7.924, entropy_coef = 0.320, rollout_stds = 0.772 ± 0.042, action_magnitude = 0.582 ± 0.292, n_updates = 4000, time =  9.2, total_time = 40.7 \n",
      "\n",
      "step =   15000, num_env_steps =   15000, scores = -283.037 (n= 1), score_ema = -304.833, actor_loss = -35.095, critic_loss = 29.034, entropy_coef_loss = -8.690, entropy_coef = 0.244, rollout_stds = 0.656 ± 0.094, action_magnitude = 0.541 ± 0.293, n_updates = 5000, time =  9.7, total_time = 50.4 \n",
      "\n",
      "step =   16000, num_env_steps =   16000, scores = -258.312 (n= 1), score_ema = -293.203, actor_loss = -35.874, critic_loss = 50.492, entropy_coef_loss = -8.355, entropy_coef = 0.189, rollout_stds = 0.752 ± 0.051, action_magnitude = 0.577 ± 0.298, n_updates = 6000, time =  9.7, total_time = 60.0 \n",
      "\n",
      "step =   17000, num_env_steps =   17000, scores = -255.271 (n= 1), score_ema = -283.720, actor_loss = -40.022, critic_loss = 31.813, entropy_coef_loss = -6.968, entropy_coef = 0.149, rollout_stds = 0.658 ± 0.088, action_magnitude = 0.626 ± 0.295, n_updates = 7000, time =  9.6, total_time = 69.6 \n",
      "\n",
      "step =   18000, num_env_steps =   18000, scores = -274.856 (n= 1), score_ema = -281.504, actor_loss = -47.818, critic_loss = 37.832, entropy_coef_loss = -3.779, entropy_coef = 0.121, rollout_stds = 0.703 ± 0.127, action_magnitude = 0.612 ± 0.299, n_updates = 8000, time = 10.2, total_time = 79.7 \n",
      "\n",
      "step =   19000, num_env_steps =   19000, scores = -190.951 (n= 1), score_ema = -258.866, actor_loss = -51.805, critic_loss = 40.311, entropy_coef_loss = -2.475, entropy_coef = 0.101, rollout_stds = 0.702 ± 0.051, action_magnitude = 0.639 ± 0.300, n_updates = 9000, time = 10.2, total_time = 89.9 \n",
      "\n",
      "step =   20000, num_env_steps =   20000, scores = -281.141 (n= 1), score_ema = -264.435, actor_loss = -60.509, critic_loss = 41.798, entropy_coef_loss = -0.334, entropy_coef = 0.087, rollout_stds = 0.603 ± 0.084, action_magnitude = 0.695 ± 0.297, n_updates = 10000, time =  9.7, total_time = 99.6 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   21000, num_env_steps =   21000, scores = -225.209 (n= 1), score_ema = -254.628, actor_loss = -50.654, critic_loss = 47.000, entropy_coef_loss = -2.255, entropy_coef = 0.078, rollout_stds = 0.629 ± 0.079, action_magnitude = 0.723 ± 0.296, n_updates = 11000, time =  9.6, total_time = 109.2 \n",
      "\n",
      "step =   22000, num_env_steps =   22000, scores =  168.986 (n= 1), score_ema = -148.725, actor_loss = -59.542, critic_loss = 49.299, entropy_coef_loss = -2.084, entropy_coef = 0.070, rollout_stds = 0.401 ± 0.151, action_magnitude = 0.721 ± 0.292, n_updates = 12000, time =  9.5, total_time = 118.7 \n",
      "\n",
      "step =   23000, num_env_steps =   23000, scores = -45.413 (n= 1), score_ema = -122.897, actor_loss = -56.033, critic_loss = 33.902, entropy_coef_loss = -1.696, entropy_coef = 0.062, rollout_stds = 0.583 ± 0.074, action_magnitude = 0.702 ± 0.296, n_updates = 13000, time = 10.6, total_time = 129.3 \n",
      "\n",
      "step =   24000, num_env_steps =   24000, scores = -222.297 (n= 1), score_ema = -147.747, actor_loss = -67.075, critic_loss = 34.397, entropy_coef_loss = -0.288, entropy_coef = 0.058, rollout_stds = 0.610 ± 0.145, action_magnitude = 0.724 ± 0.289, n_updates = 14000, time =  9.6, total_time = 138.9 \n",
      "\n",
      "step =   25000, num_env_steps =   25000, scores =  478.157 (n= 1), score_ema =  8.729, actor_loss = -65.079, critic_loss = 36.743, entropy_coef_loss = 1.120, entropy_coef = 0.058, rollout_stds = 0.389 ± 0.152, action_magnitude = 0.700 ± 0.297, n_updates = 15000, time =  9.4, total_time = 148.3 \n",
      "\n",
      "step =   26000, num_env_steps =   26000, scores =  793.695 (n= 1), score_ema =  204.971, actor_loss = -74.468, critic_loss = 40.615, entropy_coef_loss = 1.102, entropy_coef = 0.059, rollout_stds = 0.369 ± 0.109, action_magnitude = 0.692 ± 0.295, n_updates = 16000, time =  9.8, total_time = 158.1 \n",
      "\n",
      "step =   27000, num_env_steps =   27000, scores =  1.820 (n= 1), score_ema =  154.183, actor_loss = -73.246, critic_loss = 38.841, entropy_coef_loss = -0.794, entropy_coef = 0.059, rollout_stds = 0.691 ± 0.134, action_magnitude = 0.707 ± 0.295, n_updates = 17000, time =  9.7, total_time = 167.9 \n",
      "\n",
      "step =   28000, num_env_steps =   28000, scores =  1552.657 (n= 1), score_ema =  503.801, actor_loss = -73.729, critic_loss = 33.808, entropy_coef_loss = 1.580, entropy_coef = 0.061, rollout_stds = 0.397 ± 0.181, action_magnitude = 0.724 ± 0.293, n_updates = 18000, time = 10.0, total_time = 177.8 \n",
      "\n",
      "step =   29000, num_env_steps =   29000, scores =  1840.126 (n= 1), score_ema =  837.883, actor_loss = -89.666, critic_loss = 60.047, entropy_coef_loss = 2.245, entropy_coef = 0.070, rollout_stds = 0.372 ± 0.196, action_magnitude = 0.744 ± 0.290, n_updates = 19000, time =  9.4, total_time = 187.2 \n",
      "\n",
      "step =   30000, num_env_steps =   30000, scores =  1690.662 (n= 1), score_ema =  1051.077, actor_loss = -99.582, critic_loss = 46.016, entropy_coef_loss = 2.932, entropy_coef = 0.083, rollout_stds = 0.439 ± 0.228, action_magnitude = 0.740 ± 0.294, n_updates = 20000, time =  9.2, total_time = 196.4 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   31000, num_env_steps =   31000, scores =  1613.961 (n= 1), score_ema =  1191.798, actor_loss = -115.636, critic_loss = 59.896, entropy_coef_loss = 0.119, entropy_coef = 0.099, rollout_stds = 0.480 ± 0.260, action_magnitude = 0.756 ± 0.285, n_updates = 21000, time =  9.2, total_time = 205.6 \n",
      "\n",
      "step =   32000, num_env_steps =   32000, scores =  2187.679 (n= 1), score_ema =  1440.768, actor_loss = -121.563, critic_loss = 61.442, entropy_coef_loss = 0.452, entropy_coef = 0.114, rollout_stds = 0.464 ± 0.224, action_magnitude = 0.776 ± 0.273, n_updates = 22000, time =  9.9, total_time = 215.5 \n",
      "\n",
      "step =   33000, num_env_steps =   33000, scores =  1725.022 (n= 1), score_ema =  1511.832, actor_loss = -153.678, critic_loss = 64.051, entropy_coef_loss = 1.562, entropy_coef = 0.129, rollout_stds = 0.469 ± 0.127, action_magnitude = 0.762 ± 0.280, n_updates = 23000, time = 10.7, total_time = 226.2 \n",
      "\n",
      "step =   34000, num_env_steps =   34000, scores =  1964.489 (n= 1), score_ema =  1624.996, actor_loss = -160.416, critic_loss = 66.722, entropy_coef_loss = -0.213, entropy_coef = 0.142, rollout_stds = 0.405 ± 0.052, action_magnitude = 0.788 ± 0.269, n_updates = 24000, time =  9.8, total_time = 236.0 \n",
      "\n",
      "step =   35000, num_env_steps =   35000, scores =  1367.221 (n= 1), score_ema =  1560.552, actor_loss = -168.835, critic_loss = 77.575, entropy_coef_loss = 0.503, entropy_coef = 0.149, rollout_stds = 0.634 ± 0.065, action_magnitude = 0.783 ± 0.269, n_updates = 25000, time =  9.8, total_time = 245.7 \n",
      "\n",
      "step =   36000, num_env_steps =   36000, scores =  586.057 (n= 1), score_ema =  1316.929, actor_loss = -198.375, critic_loss = 78.700, entropy_coef_loss = -0.488, entropy_coef = 0.147, rollout_stds = 0.684 ± 0.093, action_magnitude = 0.729 ± 0.289, n_updates = 26000, time = 10.1, total_time = 255.8 \n",
      "\n",
      "step =   37000, num_env_steps =   37000, scores =  507.539 (n= 1), score_ema =  1114.581, actor_loss = -180.607, critic_loss = 85.309, entropy_coef_loss = 0.094, entropy_coef = 0.140, rollout_stds = 0.652 ± 0.115, action_magnitude = 0.721 ± 0.292, n_updates = 27000, time =  9.6, total_time = 265.5 \n",
      "\n",
      "step =   38000, num_env_steps =   38000, scores =  2329.716 (n= 1), score_ema =  1418.365, actor_loss = -209.942, critic_loss = 111.281, entropy_coef_loss = 0.677, entropy_coef = 0.143, rollout_stds = 0.387 ± 0.110, action_magnitude = 0.808 ± 0.250, n_updates = 28000, time =  9.7, total_time = 275.2 \n",
      "\n",
      "step =   39000, num_env_steps =   39000, scores =  2118.213 (n= 1), score_ema =  1593.327, actor_loss = -242.239, critic_loss = 116.545, entropy_coef_loss = 0.670, entropy_coef = 0.149, rollout_stds = 0.455 ± 0.041, action_magnitude = 0.799 ± 0.257, n_updates = 29000, time =  9.5, total_time = 284.7 \n",
      "\n",
      "step =   40000, num_env_steps =   40000, scores =  2583.218 (n= 1), score_ema =  1840.800, actor_loss = -254.659, critic_loss = 107.048, entropy_coef_loss = 0.470, entropy_coef = 0.157, rollout_stds = 0.412 ± 0.067, action_magnitude = 0.815 ± 0.246, n_updates = 30000, time =  9.6, total_time = 294.4 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   41000, num_env_steps =   41000, scores =  1673.034 (n= 1), score_ema =  1798.858, actor_loss = -272.172, critic_loss = 122.704, entropy_coef_loss = 0.730, entropy_coef = 0.166, rollout_stds = 0.621 ± 0.109, action_magnitude = 0.774 ± 0.272, n_updates = 31000, time = 10.3, total_time = 304.6 \n",
      "\n",
      "step =   42000, num_env_steps =   42000, scores =  2398.796 (n= 1), score_ema =  1948.843, actor_loss = -241.564, critic_loss = 117.358, entropy_coef_loss = -1.406, entropy_coef = 0.171, rollout_stds = 0.478 ± 0.054, action_magnitude = 0.801 ± 0.253, n_updates = 32000, time =  9.6, total_time = 314.2 \n",
      "\n",
      "step =   43000, num_env_steps =   43000, scores =  2585.223 (n= 1), score_ema =  2107.938, actor_loss = -287.482, critic_loss = 114.754, entropy_coef_loss = -0.549, entropy_coef = 0.173, rollout_stds = 0.370 ± 0.109, action_magnitude = 0.814 ± 0.246, n_updates = 33000, time =  9.7, total_time = 323.9 \n",
      "\n",
      "step =   44000, num_env_steps =   44000, scores =  2800.912 (n= 1), score_ema =  2281.181, actor_loss = -291.821, critic_loss = 137.356, entropy_coef_loss = -0.151, entropy_coef = 0.182, rollout_stds = 0.387 ± 0.130, action_magnitude = 0.806 ± 0.254, n_updates = 34000, time =  9.6, total_time = 333.5 \n",
      "\n",
      "step =   45000, num_env_steps =   45000, scores =  2848.736 (n= 1), score_ema =  2423.070, actor_loss = -348.292, critic_loss = 125.454, entropy_coef_loss = 0.764, entropy_coef = 0.187, rollout_stds = 0.438 ± 0.177, action_magnitude = 0.817 ± 0.246, n_updates = 35000, time = 10.7, total_time = 344.2 \n",
      "\n",
      "step =   46000, num_env_steps =   46000, scores =  2655.914 (n= 1), score_ema =  2481.281, actor_loss = -318.955, critic_loss = 151.002, entropy_coef_loss = -0.199, entropy_coef = 0.191, rollout_stds = 0.409 ± 0.155, action_magnitude = 0.796 ± 0.259, n_updates = 36000, time = 10.4, total_time = 354.6 \n",
      "\n",
      "step =   47000, num_env_steps =   47000, scores =  2954.192 (n= 1), score_ema =  2599.509, actor_loss = -337.271, critic_loss = 117.074, entropy_coef_loss = -0.544, entropy_coef = 0.195, rollout_stds = 0.419 ± 0.152, action_magnitude = 0.818 ± 0.244, n_updates = 37000, time = 10.3, total_time = 364.9 \n",
      "\n",
      "step =   48000, num_env_steps =   48000, scores =  2608.721 (n= 1), score_ema =  2601.812, actor_loss = -379.160, critic_loss = 139.369, entropy_coef_loss = 0.360, entropy_coef = 0.205, rollout_stds = 0.674 ± 0.106, action_magnitude = 0.800 ± 0.257, n_updates = 38000, time = 10.6, total_time = 375.5 \n",
      "\n",
      "step =   49000, num_env_steps =   49000, scores =  2822.421 (n= 1), score_ema =  2656.964, actor_loss = -367.413, critic_loss = 138.678, entropy_coef_loss = -0.113, entropy_coef = 0.213, rollout_stds = 0.455 ± 0.108, action_magnitude = 0.813 ± 0.244, n_updates = 39000, time = 10.5, total_time = 386.0 \n",
      "\n",
      "step =   50000, num_env_steps =   50000, scores =  2853.957 (n= 1), score_ema =  2706.212, actor_loss = -371.719, critic_loss = 119.470, entropy_coef_loss = 0.089, entropy_coef = 0.220, rollout_stds = 0.448 ± 0.098, action_magnitude = 0.817 ± 0.245, n_updates = 40000, time = 10.3, total_time = 396.3 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "\n",
      "step =   51000, num_env_steps =   51000, scores =  2950.891 (n= 1), score_ema =  2767.382, actor_loss = -433.668, critic_loss = 201.100, entropy_coef_loss = -0.491, entropy_coef = 0.225, rollout_stds = 0.415 ± 0.106, action_magnitude = 0.807 ± 0.247, n_updates = 41000, time =  9.8, total_time = 406.2 \n",
      "\n",
      "step =   52000, num_env_steps =   52000, scores =  2915.616 (n= 1), score_ema =  2804.440, actor_loss = -430.337, critic_loss = 185.565, entropy_coef_loss = -0.682, entropy_coef = 0.233, rollout_stds = 0.480 ± 0.130, action_magnitude = 0.808 ± 0.247, n_updates = 42000, time =  9.3, total_time = 415.5 \n",
      "\n",
      "saved experiment log 2024-10-10_19-25-59_944200~cmHCgQ at experiment_logs/HalfCheetah-v4/sac/2024-10-10_19-25-59_944200~cmHCgQ.json\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n",
      "done\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(algo.policy, 'tmp.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T22:16:46.561781Z",
     "start_time": "2024-10-02T22:16:46.340445Z"
    }
   },
   "id": "200ecfe4159e6786",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "128e0218ec9cae23",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "_ih[1] + '\\n\\n' + _ih[2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-27T12:36:05.325183Z"
    }
   },
   "id": "778fd25d2dc23013",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "indices = algo.buffer.tail_indices(10000)[:1000]\n",
    "algo.buffer.actions[indices].mean()\n",
    "\n",
    "algo.buffer.sample(3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-27T12:36:05.325183Z"
    }
   },
   "id": "e39b8f4554a69cc8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "SAC-256 1k steps:\n",
    "* num_envs = 16: 12.0\n",
    "\n",
    "CrossQ-256 1k steps:\n",
    "* num_envs = 16: 17.0\n",
    "* num_envs = 32: 18.5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64dec978abc8662"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc184a8a98ea506",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range(10):\n",
    "    print((np.abs(algo.buffer.actions[6000 + 100 * i:6100 + 100*i])).mean())\n",
    "    print((algo.buffer.actions[6000 + 100 * i:6100 + 100*i]).mean())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "algo.target_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d4d48d204b6f44",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d1ae8571d73535c6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env = wrap_env(as_vec_env(record_env)[0], {})\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        with torch.no_grad():\n",
    "            obs, info = record_env.reset()\n",
    "            for step in range(max_steps):\n",
    "                actions_dist, _ = policy.process_obs(torch.tensor(obs, device=device))\n",
    "                actions = actions_dist.sample().detach().cpu().numpy()\n",
    "                obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ab51a61dd845",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
