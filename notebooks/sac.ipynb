{
 "cells": [
  {
   "cell_type": "code",
   "id": "ba8c59a3eba2f172",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T19:47:24.854774Z",
     "start_time": "2024-10-12T19:47:21.109021Z"
    }
   },
   "source": [
    "\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "\n",
    "from src.experiment_logging.experiment_log import ExperimentLogItem\n",
    "from src.experiment_logging.experiment_logger import ExperimentLogger, log_experiment\n",
    "from src.module_analysis import count_parameters\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACInfoStashConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac_policy import SACPolicy\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.loss_config import LossInfoStashConfig\n",
    "from src.reinforcement_learning.core.policies.components.actor import Actor\n",
    "from src.reinforcement_learning.core.policies.components.q_critic import QCritic\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import maybe_compute_summary_statistics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_setup() -> dict[str, str]:\n",
    "    return {\n",
    "        'notebook': _ih[1] + '\\n\\n' + _ih[-1], # first and last cell input (imports and this cell)\n",
    "    }\n",
    "\n",
    "step_stopwatch = Stopwatch()\n",
    "total_stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def save_experiment_state():\n",
    "    experiment_id = logger.experiment_log['experiment_id']\n",
    "    algo.save(\n",
    "        folder_location=f'models/{env_name}/{experiment_id}', \n",
    "        name=experiment_id, \n",
    "        latest_log_item=logger.get_latest_log_item()\n",
    "    )\n",
    "    \n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    episode_scores = rl.buffer.compute_most_recent_episode_scores(rl.num_envs, consider_truncated_as_done=True)\n",
    "    \n",
    "    if len(episode_scores) > 0:\n",
    "    \n",
    "        global best_iteration_score\n",
    "        iteration_score = episode_scores.mean()\n",
    "        if iteration_score >= best_iteration_score:\n",
    "            pass\n",
    "    \n",
    "    info['episode_scores'] = episode_scores\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    num_env_steps = step * rl.num_envs\n",
    "    \n",
    "    step_time = step_stopwatch.reset()\n",
    "    total_time = total_stopwatch.time_passed()\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    episode_scores = info.get('episode_scores')\n",
    "    \n",
    "    log_item: ExperimentLogItem = {\n",
    "        'step': step,\n",
    "        'num_env_steps': num_env_steps,\n",
    "        'scores': maybe_compute_summary_statistics(episode_scores),\n",
    "        'actor_loss': maybe_compute_summary_statistics(info['final_actor_loss']),\n",
    "        'entropy_coef_loss': maybe_compute_summary_statistics(info.get('final_entropy_coef_loss')),\n",
    "        'critic_loss': maybe_compute_summary_statistics(info['final_critic_loss']),\n",
    "        'entropy_coef': maybe_compute_summary_statistics(info['entropy_coef']),\n",
    "        'action_stds': maybe_compute_summary_statistics(info['rollout'].get('action_stds')),\n",
    "        'action_magnitude': maybe_compute_summary_statistics(np.abs(rl.buffer.actions[tail_indices])),\n",
    "        'num_gradient_steps': rl.gradient_steps_performed,\n",
    "        'step_time': step_time,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "    print(logger.format_log_item(log_item, mean_format='5.3f', std_format='5.3f', step_time='.2f', total_time='.2f'), end='\\n\\n')\n",
    "    logger.add_item(log_item)\n",
    "    if step % 10000 == 0:\n",
    "        logger.save_experiment_log()\n",
    "        \n",
    "        print()\n",
    "    print()\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "env_kwargs = {}\n",
    "num_envs = 1\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    make_single_env = lambda: gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "    \n",
    "    if num_envs == 1:\n",
    "        return make_single_env()\n",
    "        \n",
    "    return parallelize_env_async(make_single_env, num_envs)\n",
    "\n",
    "\n",
    "def create_policy():\n",
    "    in_size = 17\n",
    "    action_size = 6\n",
    "    \n",
    "    actor_net = nn.Sequential(\n",
    "        nn.Linear(in_size, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    critic = QCritic(\n",
    "        n_critics=2,\n",
    "        create_q_network=lambda: nn.Sequential(\n",
    "            nn.Linear(in_size + action_size, 256),\n",
    "            nn.ReLU(),\n",
    "            # BatchRenorm(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            # BatchRenorm(256),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return SACPolicy(\n",
    "        actor=Actor(actor_net, PredictedStdActionSelector(\n",
    "            latent_dim=256,\n",
    "            action_dim=action_size,\n",
    "            base_std=1.0,\n",
    "            squash_output=True,\n",
    "        )),\n",
    "        critic=critic\n",
    "    )\n",
    "\n",
    "\n",
    "env = create_env(render_mode=None)\n",
    "policy = create_policy()\n",
    "logger = ExperimentLogger(f'experiment_logs/{env_name}/sac/')\n",
    "\n",
    "try:\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = }')\n",
    "        \n",
    "    with ((torch.autograd.set_detect_anomaly(False))):\n",
    "        algo = SAC(\n",
    "            env=env,\n",
    "            policy=policy,\n",
    "            actor_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            critic_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),  # (params, lr=3e-4, betas=(0.5, 0.999)),\n",
    "            buffer_size=1_000_000,\n",
    "            reward_scale=1,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=lambda params: optim.Adam(params, lr=3e-4),\n",
    "            entropy_coef=1.0,\n",
    "            rollout_steps=1,\n",
    "            gradient_steps=1,\n",
    "            warmup_steps=10_000,\n",
    "            optimization_batch_size=256,\n",
    "            target_update_interval=1,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            stash_config=SACInfoStashConfig(stash_rollout_infos=True, stash_rollout_action_stds=True,\n",
    "                                            stash_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossInfoStashConfig(stash_final=True),\n",
    "                                            actor_loss=LossInfoStashConfig(stash_final=True),\n",
    "                                            critic_loss=LossInfoStashConfig(stash_final=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        total_stopwatch.reset()\n",
    "        with log_experiment(\n",
    "            logger,\n",
    "            experiment_tags=algo.collect_tags(),\n",
    "            hyper_parameters=algo.collect_hyper_parameters(),\n",
    "            setup=get_setup(),\n",
    "        ) as x:\n",
    "            logger.save_experiment_log()\n",
    "            print('\\nStarting Training\\n\\n')\n",
    "            # import cProfile\n",
    "            # pr = cProfile.Profile()\n",
    "            # pr.enable()\n",
    "            algo.learn(5_000_000)\n",
    "            # pr.disable()  \n",
    "            # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T20:46:37.549783Z",
     "start_time": "2024-10-12T20:40:17.154371Z"
    }
   },
   "id": "f71efe062771e81b",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "count_parameters(policy) = 217870\n",
      "env = <TimeLimit<OrderEnforcing<PassiveEnvChecker<HalfCheetahEnv<HalfCheetah-v4>>>>>, num_envs = 1\n",
      "Grabbing system information... done!\n",
      "saved experiment log 2024-10-12_20-40-17_520453~vYvmRJ at experiment_logs/HalfCheetah-v4/sac/2024-10-12_20-40-17_520453~vYvmRJ.json\n",
      "\n",
      "Starting Training\n",
      "\n",
      "step = 11000, num_env_steps = 11000, scores = -341.764, actor_loss = -17.909, entropy_coef_loss = -2.994, critic_loss = 0.951, entropy_coef = 0.741, action_stds = 0.912 ± 0.023, action_magnitude = 0.531 ± 0.286, num_gradient_steps = 1000, step_time = 23.28, total_time = 23.27\n",
      "\n",
      "step = 12000, num_env_steps = 12000, scores = -260.059, actor_loss = -25.711, entropy_coef_loss = -5.930, critic_loss = 1.307, entropy_coef = 0.549, action_stds = 0.891 ± 0.036, action_magnitude = 0.528 ± 0.287, num_gradient_steps = 2000, step_time = 18.45, total_time = 41.72\n",
      "\n",
      "step = 13000, num_env_steps = 13000, scores = -211.489, actor_loss = -30.281, entropy_coef_loss = -8.950, critic_loss = 1.355, entropy_coef = 0.407, action_stds = 0.926 ± 0.030, action_magnitude = 0.536 ± 0.290, num_gradient_steps = 3000, step_time = 19.06, total_time = 60.78\n",
      "\n",
      "step = 14000, num_env_steps = 14000, scores = -197.351, actor_loss = -33.063, entropy_coef_loss = -11.641, critic_loss = 1.220, entropy_coef = 0.302, action_stds = 0.881 ± 0.054, action_magnitude = 0.538 ± 0.289, num_gradient_steps = 4000, step_time = 18.66, total_time = 79.44\n",
      "\n",
      "step = 15000, num_env_steps = 15000, scores = -412.035, actor_loss = -33.590, entropy_coef_loss = -14.163, critic_loss = 1.362, entropy_coef = 0.225, action_stds = 0.876 ± 0.075, action_magnitude = 0.546 ± 0.287, num_gradient_steps = 5000, step_time = 18.68, total_time = 98.12\n",
      "\n",
      "step = 16000, num_env_steps = 16000, scores = -254.171, actor_loss = -33.821, entropy_coef_loss = -15.812, critic_loss = 1.690, entropy_coef = 0.169, action_stds = 0.920 ± 0.041, action_magnitude = 0.559 ± 0.290, num_gradient_steps = 6000, step_time = 18.66, total_time = 116.78\n",
      "\n",
      "step = 17000, num_env_steps = 17000, scores = -213.173, actor_loss = -33.170, entropy_coef_loss = -16.218, critic_loss = 1.500, entropy_coef = 0.127, action_stds = 0.842 ± 0.058, action_magnitude = 0.567 ± 0.293, num_gradient_steps = 7000, step_time = 18.95, total_time = 135.73\n",
      "\n",
      "step = 18000, num_env_steps = 18000, scores = -208.391, actor_loss = -32.138, entropy_coef_loss = -17.349, critic_loss = 1.638, entropy_coef = 0.096, action_stds = 0.824 ± 0.088, action_magnitude = 0.569 ± 0.295, num_gradient_steps = 8000, step_time = 18.74, total_time = 154.47\n",
      "\n",
      "step = 19000, num_env_steps = 19000, scores = -156.284, actor_loss = -31.410, entropy_coef_loss = -17.420, critic_loss = 1.334, entropy_coef = 0.073, action_stds = 0.732 ± 0.056, action_magnitude = 0.578 ± 0.292, num_gradient_steps = 9000, step_time = 18.60, total_time = 173.07\n",
      "\n",
      "step = 20000, num_env_steps = 20000, scores = -149.362, actor_loss = -30.279, entropy_coef_loss = -17.171, critic_loss = 1.228, entropy_coef = 0.056, action_stds = 0.684 ± 0.075, action_magnitude = 0.591 ± 0.295, num_gradient_steps = 10000, step_time = 18.68, total_time = 191.75\n",
      "\n",
      "saved experiment log 2024-10-12_20-40-17_520453~vYvmRJ at experiment_logs/HalfCheetah-v4/sac/2024-10-12_20-40-17_520453~vYvmRJ.json\n",
      "\n",
      "step = 21000, num_env_steps = 21000, scores = -43.068, actor_loss = -30.082, entropy_coef_loss = -14.943, critic_loss = 1.660, entropy_coef = 0.043, action_stds = 0.654 ± 0.065, action_magnitude = 0.594 ± 0.297, num_gradient_steps = 11000, step_time = 18.75, total_time = 210.50\n",
      "\n",
      "step = 22000, num_env_steps = 22000, scores = -360.120, actor_loss = -28.955, entropy_coef_loss = -15.124, critic_loss = 1.400, entropy_coef = 0.034, action_stds = 0.496 ± 0.081, action_magnitude = 0.633 ± 0.294, num_gradient_steps = 12000, step_time = 18.70, total_time = 229.20\n",
      "\n",
      "step = 23000, num_env_steps = 23000, scores = -202.690, actor_loss = -27.488, entropy_coef_loss = -14.617, critic_loss = 1.582, entropy_coef = 0.026, action_stds = 0.512 ± 0.076, action_magnitude = 0.574 ± 0.299, num_gradient_steps = 13000, step_time = 18.67, total_time = 247.86\n",
      "\n",
      "step = 24000, num_env_steps = 24000, scores = 40.515, actor_loss = -26.612, entropy_coef_loss = -11.041, critic_loss = 1.631, entropy_coef = 0.020, action_stds = 0.433 ± 0.038, action_magnitude = 0.618 ± 0.299, num_gradient_steps = 14000, step_time = 18.69, total_time = 266.55\n",
      "\n",
      "step = 25000, num_env_steps = 25000, scores = 840.788, actor_loss = -26.516, entropy_coef_loss = -5.036, critic_loss = 1.496, entropy_coef = 0.016, action_stds = 0.350 ± 0.108, action_magnitude = 0.719 ± 0.292, num_gradient_steps = 15000, step_time = 18.68, total_time = 285.23\n",
      "\n",
      "step = 26000, num_env_steps = 26000, scores = 881.220, actor_loss = -27.520, entropy_coef_loss = -4.783, critic_loss = 2.114, entropy_coef = 0.014, action_stds = 0.313 ± 0.097, action_magnitude = 0.700 ± 0.298, num_gradient_steps = 16000, step_time = 18.67, total_time = 303.90\n",
      "\n",
      "step = 27000, num_env_steps = 27000, scores = 917.482, actor_loss = -27.287, entropy_coef_loss = -4.547, critic_loss = 1.775, entropy_coef = 0.012, action_stds = 0.429 ± 0.101, action_magnitude = 0.717 ± 0.292, num_gradient_steps = 17000, step_time = 18.77, total_time = 322.67\n",
      "\n",
      "step = 28000, num_env_steps = 28000, scores = 1324.699, actor_loss = -29.840, entropy_coef_loss = 2.004, critic_loss = 2.234, entropy_coef = 0.012, action_stds = 0.401 ± 0.183, action_magnitude = 0.736 ± 0.295, num_gradient_steps = 18000, step_time = 18.53, total_time = 341.21\n",
      "\n",
      "step = 29000, num_env_steps = 29000, scores = 335.906, actor_loss = -29.032, entropy_coef_loss = 0.130, critic_loss = 2.167, entropy_coef = 0.012, action_stds = 0.434 ± 0.125, action_magnitude = 0.657 ± 0.301, num_gradient_steps = 19000, step_time = 18.61, total_time = 359.82\n",
      "\n",
      "step = 30000, num_env_steps = 30000, scores = 1783.196, actor_loss = -32.515, entropy_coef_loss = 0.253, critic_loss = 2.881, entropy_coef = 0.014, action_stds = 0.527 ± 0.285, action_magnitude = 0.800 ± 0.274, num_gradient_steps = 20000, step_time = 19.13, total_time = 378.95\n",
      "\n",
      "saved experiment log 2024-10-12_20-40-17_520453~vYvmRJ at experiment_logs/HalfCheetah-v4/sac/2024-10-12_20-40-17_520453~vYvmRJ.json\n",
      "\n",
      "saved experiment log 2024-10-12_20-40-17_520453~vYvmRJ at experiment_logs/HalfCheetah-v4/sac/2024-10-12_20-40-17_520453~vYvmRJ.json\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "done\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T20:26:08.622551Z",
     "start_time": "2024-10-12T20:26:08.202540Z"
    }
   },
   "id": "128e0218ec9cae23",
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d1ae8571d73535c6",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ab51a61dd845",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
