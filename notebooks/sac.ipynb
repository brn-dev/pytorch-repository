{
 "cells": [
  {
   "cell_type": "code",
   "id": "ba8c59a3eba2f172",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-20T17:16:10.065090Z",
     "start_time": "2024-09-20T17:16:09.968931Z"
    }
   },
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from sac import init_action_selector, init_policy, init_optimizer, wrap_env\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.reinforcement_learning.core.action_selectors.action_selector import ActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_episode_returns, compute_returns\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.core.policy_construction import InitActionSelectorFunction, PolicyConstruction\n",
    "from src.reinforcement_learning.gym.envs.test_env import TestEnv\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SAC_DEFAULT_OPTIMIZER_PROVIDER\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.torch_device import set_default_torch_device, optimizer_to_device\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "f71efe062771e81b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-20T17:17:44.537385Z",
     "start_time": "2024-09-20T17:16:10.066085Z"
    }
   },
   "source": [
    "from src.reinforcement_learning.core.loss_config import LossLoggingConfig\n",
    "from src.reinforcement_learning.algorithms.sac.sac import SAC, SACLoggingConfig\n",
    "\n",
    "policy_id: str\n",
    "policy: BasePolicy\n",
    "optimizer: optim.Optimizer\n",
    "wrapped_env: Env\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy, optimizer, wrapped_env = PolicyConstruction.init_from_info(\n",
    "            env=env,\n",
    "            info=PolicyConstruction.create_policy_initialization_info(\n",
    "                init_action_selector=init_action_selector,\n",
    "                init_policy=init_policy,\n",
    "                init_optimizer=init_optimizer,\n",
    "                wrap_env=wrap_env,\n",
    "            ),\n",
    "        )\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy, optimizer, wrapped_env, steps_trained\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    tail_indices = rl.buffer.tail_indices(1000)\n",
    "    \n",
    "    rewards = rl.buffer.rewards[tail_indices]\n",
    "    # if 'raw_rewards' in info['rollout']:\n",
    "    #     rewards = info['rollout']['raw_rewards']\n",
    "    \n",
    "    episode_scores = compute_episode_returns(\n",
    "        rewards=rewards,\n",
    "        episode_starts=np.repeat(np.arange(len(tail_indices)).reshape(-1, 1), num_envs, axis=1) % 1000 == 0,\n",
    "        last_episode_starts=info['last_episode_starts'],\n",
    "        gamma=1.0,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        remove_unfinished_episodes=True,\n",
    "    )\n",
    "    \n",
    "    global best_iteration_score\n",
    "    iteration_score = episode_scores.mean()\n",
    "    score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    if iteration_score >= best_iteration_score:\n",
    "        best_iteration_score = iteration_score\n",
    "        policy_db.save_model_state_dict(\n",
    "            model_id=policy_id,\n",
    "            parent_model_id=parent_policy_id,\n",
    "            model_info={\n",
    "                'score': iteration_score.item(),\n",
    "                'steps_trained': steps_trained,\n",
    "                'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                'init_policy_source_code': init_policy_source\n",
    "            },\n",
    "            model=policy,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "    \n",
    "    info['episode_scores'] = episode_scores\n",
    "    info['score_moving_average'] = score_moving_average\n",
    "        \n",
    "def on_optimization_done(rl: SAC, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    global steps_trained\n",
    "    steps_trained += rl.buffer.pos\n",
    "    \n",
    "    if step % 1000 != 0:\n",
    "        return\n",
    "    \n",
    "    time_taken = stopwatch.reset()\n",
    "    \n",
    "    episode_scores = info['episode_scores']\n",
    "    score_moving_average = info['score_moving_average']\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "        n_format='>2'\n",
    "    )\n",
    "    # advantages = format_summary_statics(\n",
    "    #     rl.buffer.advantages, \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    actor_loss = format_summary_statics(\n",
    "        torch.abs(info['reduced_actor_loss']),  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_coef_loss = None if 'reduced_entropy_coef_loss' not in info else format_summary_statics(\n",
    "        info['reduced_entropy_coef_loss'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_loss = format_summary_statics(\n",
    "        info['reduced_critic_loss'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_coef = format_summary_statics(\n",
    "        info['entropy_coef'],\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    # resets = format_summary_statics(\n",
    "    #     rl.buffer.dones.astype(int).sum(axis=0), \n",
    "    #     mean_format='.2f',\n",
    "    #     std_format=None,\n",
    "    #     min_value_format='1d',\n",
    "    #     max_value_format=None,\n",
    "    # )\n",
    "    # kl_div = info['actor_kl_divergence'][-1]\n",
    "    # grad_norm = format_summary_statics(\n",
    "    #     info['grad_norm'], \n",
    "    #     mean_format=' 6.3f',\n",
    "    #     std_format='.1f',\n",
    "    #     min_value_format=' 7.3f',\n",
    "    #     max_value_format='6.3f',\n",
    "    # )\n",
    "    action_stds = info['rollout'].get('action_stds')\n",
    "    if action_stds is not None:\n",
    "        rollout_action_stds = format_summary_statics(\n",
    "            action_stds,\n",
    "            mean_format='5.3f',\n",
    "            std_format='5.3f',\n",
    "            min_value_format=None,\n",
    "            max_value_format=None,\n",
    "        )\n",
    "    else:\n",
    "        rollout_action_stds = 'N/A'\n",
    "    # ppo_epochs = info['nr_ppo_epochs']\n",
    "    # ppo_updates = info['nr_ppo_updates']\n",
    "    # expl_var = rl.buffer.compute_critic_explained_variance()\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          # f\"{advantages = :s}, \"\n",
    "          f\"{actor_loss = :s}, \"\n",
    "          +(f\"{entropy_coef_loss = :s}, \" if entropy_coef_loss is not None else '')+\n",
    "          f\"{critic_loss = :s}, \"\n",
    "          f\"{entropy_coef = :s}, \"\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          # f\"{expl_var = :.3f}, \"\n",
    "          # f\"{kl_div = :.4f}, \"\n",
    "          # f\"{ppo_epochs = }, \"\n",
    "          # f\"{ppo_updates = }, \"\n",
    "          # f\"{grad_norm = :s}, \"\n",
    "          f\"n_updates = {rl.gradient_steps_performed}, \"\n",
    "          # f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f} \\n\"\n",
    "          )\n",
    "    print()\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "env_name = 'HalfCheetah-v4'\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.001 }\n",
    "# env_kwargs = {'forward_reward_weight': 1.25, 'ctrl_cost_weight': 0.05 }\n",
    "num_envs = 16\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None  # '2024-04-28_20.57.23'\n",
    "\n",
    "env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "\n",
    "\n",
    "try:\n",
    "    policy_id, policy, optimizer, wrapped_env, steps_trained = get_policy(create_new_if_exists=True)\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "        \n",
    "    with (torch.autograd.set_detect_anomaly(False)):\n",
    "        algo = SAC(\n",
    "            env=wrapped_env,\n",
    "            policy=policy,\n",
    "            buffer_size=50_000,\n",
    "            gamma=0.99,\n",
    "            tau=0.005,\n",
    "            entropy_coef_optimizer_provider=SAC_DEFAULT_OPTIMIZER_PROVIDER,\n",
    "            entropy_coef=0.1,\n",
    "            rollout_steps=1,\n",
    "            warmup_steps=50,\n",
    "            learning_starts=50,\n",
    "            optimization_batch_size=256,\n",
    "            gradient_steps=1,\n",
    "            target_update_interval=1,\n",
    "            # sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=SACLoggingConfig(log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_last_obs=True, log_entropy_coef=True,\n",
    "                                            entropy_coef_loss=LossLoggingConfig(log_reduced=True),\n",
    "                                            actor_loss=LossLoggingConfig(log_reduced=True),\n",
    "                                            critic_loss=LossLoggingConfig(log_reduced=True)),\n",
    "            torch_device=device,\n",
    "        )\n",
    "        # import cProfile\n",
    "        # pr = cProfile.Profile()\n",
    "        # pr.enable()\n",
    "        algo.learn(1_000_000)\n",
    "        # pr.disable()\n",
    "        # pr.dump_stats('profile_stats.pstat')\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(0.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "New policy 2024-09-20_19.16.12 created\n",
      "Using policy 2024-09-20_19.16.12 with parent policy None\n",
      "count_parameters(policy) = 217870\n",
      "env = AsyncVectorEnv(16), num_envs = 16 \n",
      "\n",
      "step =    1000, scores =  19.550 ± 31.158 [-65.371, 78.159] (n=16), score_ema =  19.550, actor_loss =  1.493 ±   nan, entropy_coef_loss = -257.626 ±   nan, critic_loss = 0.331 ±   nan, entropy_coef = 0.075 ±   nan, rollout_stds = 0.870 ± 0.039, n_updates = 951, time = 12.0 \n",
      "\n",
      "step =    2000, scores =  0.268 ± 27.246 [-42.615, 48.489] (n=16), score_ema =  14.730, actor_loss =  2.559 ±   nan, entropy_coef_loss = -282.480 ±   nan, critic_loss = 0.605 ±   nan, entropy_coef = 0.056 ±   nan, rollout_stds = 0.804 ± 0.108, n_updates = 1951, time = 10.0 \n",
      "\n",
      "step =    3000, scores =  29.252 ± 29.438 [-36.147, 67.347] (n=16), score_ema =  18.360, actor_loss =  3.090 ±   nan, entropy_coef_loss = -312.586 ±   nan, critic_loss = 0.388 ±   nan, entropy_coef = 0.041 ±   nan, rollout_stds = 0.841 ± 0.059, n_updates = 2951, time = 10.3 \n",
      "\n",
      "step =    4000, scores =  46.782 ± 22.859 [ 7.583, 98.593] (n=16), score_ema =  25.466, actor_loss =  3.683 ±   nan, entropy_coef_loss = -338.699 ±   nan, critic_loss = 0.449 ±   nan, entropy_coef = 0.031 ±   nan, rollout_stds = 0.819 ± 0.070, n_updates = 3951, time = 10.1 \n",
      "\n",
      "step =    5000, scores =  65.595 ± 18.271 [ 21.205, 97.422] (n=16), score_ema =  35.498, actor_loss =  4.160 ±   nan, entropy_coef_loss = -364.324 ±   nan, critic_loss = 0.547 ±   nan, entropy_coef = 0.023 ±   nan, rollout_stds = 0.796 ± 0.078, n_updates = 4951, time = 10.1 \n",
      "\n",
      "step =    6000, scores =  134.010 ± 23.252 [ 72.552, 171.302] (n=16), score_ema =  60.126, actor_loss =  4.497 ±   nan, entropy_coef_loss = -388.744 ±   nan, critic_loss = 0.566 ±   nan, entropy_coef = 0.017 ±   nan, rollout_stds = 0.720 ± 0.090, n_updates = 5951, time = 10.2 \n",
      "\n",
      "step =    7000, scores =  284.455 ± 25.079 [ 234.934, 321.585] (n=16), score_ema =  116.208, actor_loss =  4.997 ±   nan, entropy_coef_loss = -408.096 ±   nan, critic_loss = 0.715 ±   nan, entropy_coef = 0.013 ±   nan, rollout_stds = 0.665 ± 0.081, n_updates = 6951, time = 10.1 \n",
      "\n",
      "step =    8000, scores =  419.633 ± 30.344 [ 374.300, 480.886] (n=16), score_ema =  192.065, actor_loss =  5.684 ±   nan, entropy_coef_loss = -422.771 ±   nan, critic_loss = 0.575 ±   nan, entropy_coef = 0.009 ±   nan, rollout_stds = 0.570 ± 0.078, n_updates = 7951, time = 10.1 \n",
      "\n",
      "step =    9000, scores =  544.618 ± 29.368 [ 497.216, 590.818] (n=16), score_ema =  280.203, actor_loss =  7.269 ±   nan, entropy_coef_loss = -434.045 ±   nan, critic_loss = 0.816 ±   nan, entropy_coef = 0.007 ±   nan, rollout_stds = 0.528 ± 0.073, n_updates = 8951, time = 10.4 \n",
      "\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n",
      "done\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea59700a7662567",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-18T22:04:53.035324Z",
     "start_time": "2024-09-18T22:04:52.948657Z"
    }
   },
   "source": [],
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc184a8a98ea506",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d1ae8571d73535c6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T16:06:44.130912Z",
     "start_time": "2024-06-17T16:06:43.483543Z"
    }
   },
   "source": [
    "from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env = wrap_env(as_vec_env(record_env)[0], {})\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        with torch.no_grad():\n",
    "            obs, info = record_env.reset()\n",
    "            for step in range(max_steps):\n",
    "                actions_dist, _ = policy.process_obs(torch.tensor(obs, device=device))\n",
    "                actions = actions_dist.sample().detach().cpu().numpy()\n",
    "                obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6ab51a61dd845",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
