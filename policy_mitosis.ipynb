{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-16T16:30:57.763728Z",
     "start_time": "2024-05-16T16:30:55.528152Z"
    }
   },
   "source": [
    "import sys\n",
    "import time\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.np_functions import softmax\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.async_policy_mitosis import AsyncPolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis import PolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis_base import PolicyWithEnvAndInfo\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPOLoggingConfig, PPO\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.core.policy_info import PolicyInfo\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.torch_device import set_default_torch_device, get_torch_device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-16T16:30:57.763728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def init_policy():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n",
    "        SquashedDiagGaussianActionSelector\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "\n",
    "    in_size = 376\n",
    "    action_size = 17\n",
    "    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n",
    "    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n",
    "\n",
    "    hidden_activation_function = nn.ELU()\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    hidden_activation_function\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.Identity() if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            return self.actor(x), self.critic(x)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n",
    "        latent_dim=actor_out_sizes[-1],\n",
    "        action_dim=action_size,\n",
    "        std=0.1,\n",
    "        std_learnable=False,\n",
    "    ))\n",
    "\n",
    "def wrap_env(_env):\n",
    "    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n",
    "    from gymnasium.wrappers import RescaleAction\n",
    "\n",
    "    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n",
    "    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n",
    "\n",
    "    return _env\n",
    "\n",
    "def train_func(policy_with_env_and_info: PolicyWithEnvAndInfo) -> tuple[int, float]:\n",
    "    policy = policy_with_env_and_info['policy']\n",
    "    env = policy_with_env_and_info['env']\n",
    "    \n",
    "    score = 0.0\n",
    "    score_ema = ExponentialMovingAverage(0.45)\n",
    "    rollout_stopwatch = Stopwatch()\n",
    "    def on_rollout_done(rl: PPO, step: int, info: dict[str, Any]):   \n",
    "        \n",
    "        if 'raw_rewards' in info['rollout']:\n",
    "            raw_rewards = info['rollout']['raw_rewards']\n",
    "            _, gamma_1_returns = compute_gae_and_returns(\n",
    "                value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "                rewards=raw_rewards,\n",
    "                episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "                last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_rewards=None,\n",
    "                normalize_advantages=None,\n",
    "            )\n",
    "        else:\n",
    "            _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "                last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_advantages=None,\n",
    "                normalize_rewards=None,\n",
    "            )\n",
    "        \n",
    "        episode_scores = gamma_1_returns[\n",
    "            rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "        ]\n",
    "        \n",
    "        nonlocal score, score_ema\n",
    "        score = episode_scores.mean()\n",
    "        current_score_ema = score_ema.update(score)\n",
    "        \n",
    "        rollout_time = rollout_stopwatch.reset()\n",
    "        \n",
    "        resets: np.ndarray = rl.buffer.episode_starts.astype(int).sum(axis=0)\n",
    "        resets_mean = resets.mean()\n",
    "        resets_min = resets.min()\n",
    "        print(f'{step:>6}: '\n",
    "              f'{score = :9.3f}, '\n",
    "              f'score_ema = {current_score_ema:9.3f}, '\n",
    "              f'time = {rollout_time:5.2f}, '\n",
    "              f'resets = {resets_mean:5.2f} >= {resets_min:5.2f}')\n",
    "    \n",
    "    policy_info = policy_with_env_and_info['policy_info']\n",
    "    policy_info_str = ('('\n",
    "          f'policy_id = {policy_info[\"policy_id\"]}, '\n",
    "          f'parent_id = {policy_info[\"parent_policy_id\"]}, '\n",
    "          f'num_parameters = {count_parameters(policy)}, '\n",
    "          f'previous_steps = {policy_info[\"steps_trained\"]}, '\n",
    "          f'previous_score = {policy_info[\"score\"]:9.3f}'\n",
    "          ')')\n",
    "    \n",
    "    print(f'Starting PPO with policy {policy_info_str:s} for {steps_per_iteration:_} steps')\n",
    "    mitosis_iteration_stopwatch = Stopwatch()\n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters(), lr=1e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=0.995,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "        weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.01,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.05,\n",
    "        grad_norm_clip_value=2.0,\n",
    "        callback=Callback(on_rollout_done=on_rollout_done),\n",
    "        logging_config=PPOLoggingConfig(log_rollout_infos=True),\n",
    "        torch_device=device,\n",
    "    ).train(steps_per_iteration)\n",
    "    \n",
    "    \n",
    "    print(f'Training finished for policy {policy_info_str:s}, end score = {score:9.3f}, time = {mitosis_iteration_stopwatch.time_passed():6.2f}')\n",
    "    \n",
    "    return steps_per_iteration, score_ema.get()\n",
    "\n",
    "def select_policy_selection_probs(policy_infos: Iterable[PolicyInfo]) -> np.ndarray:\n",
    "    scores = np.array([policy_info['score'] for policy_info in policy_infos])\n",
    "    scores = scores / scores.std()\n",
    "    scores = softmax(scores, temperature=0.9)\n",
    "    return scores\n",
    "\n",
    "device = get_torch_device(\"cuda:0\") if True else get_torch_device('cpu')\n",
    "\n",
    "policy_action_std = 0.15\n",
    "steps_per_iteration = 100_000\n",
    "\n",
    "env_name = 'Humanoid-v4'\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "num_envs = 4\n",
    "\n",
    "mitosis_id = get_current_timestamp()\n",
    "# mitosis_id = '2024-05-03_19.31.53'\n",
    "# policy_db = TinyModelDB[PolicyInfo](base_path=f'E:/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "policy_db = TinyModelDB[PolicyInfo](base_path=f'C:/Users/domin/git/pytorch-starter/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "\n",
    "unwrapped_env = parallelize_env_async(lambda: gym.make(env_name, **env_kwargs), num_envs)\n",
    "try:\n",
    "    print(f'Starting mitosis with id {mitosis_id}')\n",
    "    AsyncPolicyMitosis(\n",
    "        num_workers=2,\n",
    "        policy_db=policy_db,\n",
    "        train_policy_function=train_func,\n",
    "        create_env=lambda: parallelize_env_async(lambda: gym.make(env_name, **env_kwargs), num_envs),\n",
    "        new_init_policy_function=init_policy,\n",
    "        new_wrap_env_function=wrap_env,\n",
    "        select_policy_selection_probs=select_policy_selection_probs,\n",
    "        min_base_ancestors=5,\n",
    "        rng_seed=None,\n",
    "    ).train_with_mitosis(1000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    unwrapped_env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "id": "90c056a126f17111",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mitosis with id 2024-05-16_18.30.57\n",
      "Started training iteration for policy: 2024-05-16_18.30.58~UeQRBr, parent policy id: None\n",
      "Started training iteration for policy: 2024-05-16_18.30.58~AWrUA7, parent policy id: None\n",
      "Finished training iteration for policy: 2024-05-16_18.30.58~UeQRBr\n",
      "Started training iteration for policy: 2024-05-16_18.31.06~htveIW, parent policy id: None\n",
      "Finished training iteration for policy: 2024-05-16_18.30.58~AWrUA7\n",
      "Started training iteration for policy: 2024-05-16_18.31.06~q6nQTS, parent policy id: None\n",
      "Finished training iteration for policy: 2024-05-16_18.31.06~htveIW\n",
      "Started training iteration for policy: 2024-05-16_18.31.10~EYR5xp, parent policy id: None\n",
      "Finished training iteration for policy: 2024-05-16_18.31.06~q6nQTS\n",
      "Started training iteration for policy: 2024-05-16_18.31.10~iSbh4i, parent policy id: None\n",
      "Finished training iteration for policy: 2024-05-16_18.31.10~EYR5xp\n",
      "policy selection probs = \n",
      "\t2024-05-16_18.30.58~UeQRBr: p = 0.625930, scores =  36.687, steps = 1000\n",
      "\t2024-05-16_18.30.58~AWrUA7: p = 0.112769, scores =  26.347, steps = 1000\n",
      "\t2024-05-16_18.31.06~htveIW: p = 0.159348, scores =  28.433, steps = 1000\n",
      "\t2024-05-16_18.31.06~q6nQTS: p = 0.082087, scores =  24.431, steps = 1000\n",
      "\t2024-05-16_18.31.10~EYR5xp: p = 0.019865, scores =  15.872, steps = 1000\n",
      "Started training iteration for policy: 2024-05-16_18.31.14~NbsQb2, parent policy id: 2024-05-16_18.30.58~UeQRBr\n",
      "Finished training iteration for policy: 2024-05-16_18.31.10~iSbh4i\n",
      "policy selection probs = \n",
      "\t2024-05-16_18.30.58~UeQRBr: p = 0.608430, scores =  36.687, steps = 1000\n",
      "\t2024-05-16_18.30.58~AWrUA7: p = 0.107825, scores =  26.347, steps = 1000\n",
      "\t2024-05-16_18.31.06~htveIW: p = 0.152869, scores =  28.433, steps = 1000\n",
      "\t2024-05-16_18.31.06~q6nQTS: p = 0.078249, scores =  24.431, steps = 1000\n",
      "\t2024-05-16_18.31.10~EYR5xp: p = 0.018680, scores =  15.872, steps = 1000\n",
      "\t2024-05-16_18.31.10~iSbh4i: p = 0.033946, scores =  19.441, steps = 1000\n",
      "Started training iteration for policy: 2024-05-16_18.31.15~xMt7Iv, parent policy id: 2024-05-16_18.31.06~q6nQTS\n",
      "Finished training iteration for policy: 2024-05-16_18.31.15~xMt7Iv\n",
      "policy selection probs = \n",
      "\t2024-05-16_18.30.58~UeQRBr: p = 0.555979, scores =  36.687, steps = 1000\n",
      "\t2024-05-16_18.30.58~AWrUA7: p = 0.116107, scores =  26.347, steps = 1000\n",
      "\t2024-05-16_18.31.06~htveIW: p = 0.159250, scores =  28.433, steps = 1000\n",
      "\t2024-05-16_18.31.06~q6nQTS: p = 0.086861, scores =  24.431, steps = 1000\n",
      "\t2024-05-16_18.31.10~EYR5xp: p = 0.023754, scores =  15.872, steps = 1000\n",
      "\t2024-05-16_18.31.10~iSbh4i: p = 0.040790, scores =  19.441, steps = 1000\n",
      "\t2024-05-16_18.31.15~xMt7Iv: p = 0.017259, scores =  13.764, steps = 2000\n",
      "Started training iteration for policy: 2024-05-16_18.31.19~WfwfiB, parent policy id: 2024-05-16_18.31.06~q6nQTS\n",
      "Finished training iteration for policy: 2024-05-16_18.31.14~NbsQb2\n",
      "policy selection probs = \n",
      "\t2024-05-16_18.30.58~UeQRBr: p = 0.533226, scores =  36.687, steps = 1000\n",
      "\t2024-05-16_18.30.58~AWrUA7: p = 0.116401, scores =  26.347, steps = 1000\n",
      "\t2024-05-16_18.31.06~htveIW: p = 0.158231, scores =  28.433, steps = 1000\n",
      "\t2024-05-16_18.31.06~q6nQTS: p = 0.087799, scores =  24.431, steps = 1000\n",
      "\t2024-05-16_18.31.10~EYR5xp: p = 0.024907, scores =  15.872, steps = 1000\n",
      "\t2024-05-16_18.31.10~iSbh4i: p = 0.042121, scores =  19.441, steps = 1000\n",
      "\t2024-05-16_18.31.15~xMt7Iv: p = 0.018261, scores =  13.764, steps = 2000\n",
      "\t2024-05-16_18.31.14~NbsQb2: p = 0.019054, scores =  14.052, steps = 2000\n",
      "Started training iteration for policy: 2024-05-16_18.31.19~4tx1q5, parent policy id: 2024-05-16_18.30.58~UeQRBr\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T16:16:56.328740Z",
     "start_time": "2024-05-16T16:16:56.325484Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d23de72c3bf3e561",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb9ea562feef0c28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
