{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T00:04:34.897720Z",
     "start_time": "2024-05-18T00:04:31.088208Z"
    }
   },
   "source": [
    "import sys\n",
    "import time\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.np_functions import softmax\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.async_policy_mitosis import AsyncPolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis import PolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis_base import PolicyWithEnvAndInfo\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPOLoggingConfig, PPO\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.torch_device import set_default_torch_device, get_torch_device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-18T00:04:34.898721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def init_policy():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n",
    "        SquashedDiagGaussianActionSelector\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "\n",
    "    in_size = 376\n",
    "    action_size = 17\n",
    "    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n",
    "    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n",
    "\n",
    "    hidden_activation_function = nn.ELU()\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    hidden_activation_function\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.Identity() if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            return self.actor(x), self.critic(x)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n",
    "        latent_dim=actor_out_sizes[-1],\n",
    "        action_dim=action_size,\n",
    "        std=0.1,\n",
    "        std_learnable=False,\n",
    "    ))\n",
    "\n",
    "def wrap_env(_env):\n",
    "    from src.reinforcement_learning.gym.transform_reward_wrapper import TransformRewardWrapper\n",
    "    from gymnasium.wrappers import RescaleAction\n",
    "\n",
    "    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n",
    "    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n",
    "\n",
    "    return _env\n",
    "\n",
    "def train_func(policy_with_env_and_info: PolicyWithEnvAndInfo) -> tuple[int, float]:\n",
    "    policy = policy_with_env_and_info['policy']\n",
    "    env = policy_with_env_and_info['env']\n",
    "    \n",
    "    score = 0.0\n",
    "    score_ema = ExponentialMovingAverage(0.45)\n",
    "    rollout_stopwatch = Stopwatch()\n",
    "    def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):   \n",
    "        \n",
    "        if 'raw_rewards' in info['rollout']:\n",
    "            raw_rewards = info['rollout']['raw_rewards']\n",
    "            _, gamma_1_returns = compute_gae_and_returns(\n",
    "                value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "                rewards=raw_rewards,\n",
    "                episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "                last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_rewards=None,\n",
    "                normalize_advantages=None,\n",
    "            )\n",
    "        else:\n",
    "            _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "                last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_advantages=None,\n",
    "                normalize_rewards=None,\n",
    "            )\n",
    "        \n",
    "        episode_scores = gamma_1_returns[\n",
    "            rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "        ]\n",
    "        \n",
    "        nonlocal score, score_ema\n",
    "        score = episode_scores.mean()\n",
    "        current_score_ema = score_ema.update(score)\n",
    "        \n",
    "        rollout_time = rollout_stopwatch.reset()\n",
    "        \n",
    "        resets: np.ndarray = rl.buffer.episode_starts.astype(int).sum(axis=0)\n",
    "        resets_mean = resets.mean()\n",
    "        resets_min = resets.min()\n",
    "        print(f'{step:>6}: '\n",
    "              f'{score = :9.3f}, '\n",
    "              f'score_ema = {current_score_ema:9.3f}, '\n",
    "              f'time = {rollout_time:5.2f}, '\n",
    "              f'resets = {resets_mean:5.2f} >= {resets_min:5.2f}')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    policy_info = policy_with_env_and_info['policy_info']\n",
    "    policy_info_str = ('('\n",
    "          f'policy_id = {policy_info[\"policy_id\"]}, '\n",
    "          f'parent_id = {policy_info[\"parent_policy_id\"]}, '\n",
    "          f'num_parameters = {count_parameters(policy)}, '\n",
    "          f'previous_steps = {policy_info[\"steps_trained\"]}, '\n",
    "          f'previous_score = {policy_info[\"score\"]:9.3f}'\n",
    "          ')')\n",
    "    \n",
    "    print(f'Starting PPO with policy {policy_info_str:s} for {steps_per_iteration:_} steps')\n",
    "    mitosis_iteration_stopwatch = Stopwatch()\n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters(), lr=1e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=0.995,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "        weigh_entropy_objective=lambda obj: 0.0 * obj,\n",
    "        weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.01,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.05,\n",
    "        grad_norm_clip_value=2.0,\n",
    "        callback=Callback(on_rollout_done=on_rollout_done),\n",
    "        logging_config=PPOLoggingConfig(log_rollout_infos=True),\n",
    "        torch_device=device,\n",
    "    ).train(steps_per_iteration)\n",
    "    \n",
    "    \n",
    "    print(f'Training finished for policy {policy_info_str:s}, end score = {score:9.3f}, time = {mitosis_iteration_stopwatch.time_passed():6.2f}')\n",
    "    \n",
    "    return steps_per_iteration, score_ema.get()\n",
    "\n",
    "def select_policy_selection_probs(policy_infos: Iterable[MitosisPolicyInfo]) -> np.ndarray:\n",
    "    scores = np.array([policy_info['score'] for policy_info in policy_infos])\n",
    "    scores = scores / scores.mean()\n",
    "    scores = softmax(scores, temperature=0.9 / len(scores)**0.5)\n",
    "    return scores\n",
    "\n",
    "device = get_torch_device(\"cuda:0\") if True else get_torch_device('cpu')\n",
    "\n",
    "steps_per_iteration = 100_000\n",
    "\n",
    "env_name = 'Humanoid-v4'\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "num_envs = 32\n",
    "\n",
    "mitosis_id = get_current_timestamp()\n",
    "# mitosis_id = '2024-05-17_19.14.43'\n",
    "policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'E:/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "# policy_db = TinyModelDB[PolicyInfo](base_path=f'C:/Users/domin/git/pytorch-starter/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "\n",
    "unwrapped_env = parallelize_env_async(lambda: gym.make(env_name, **env_kwargs), num_envs)\n",
    "try:\n",
    "    print(f'Starting mitosis with id {mitosis_id}')\n",
    "    AsyncPolicyMitosis(\n",
    "        num_workers=3,\n",
    "        policy_db=policy_db,\n",
    "        train_policy_function=train_func,\n",
    "        # env=parallelize_env_async(lambda: gym.make(env_name, **env_kwargs), num_envs),\n",
    "        create_env=lambda: parallelize_env_async(lambda: gym.make(env_name, **env_kwargs), num_envs),\n",
    "        new_init_policy_function=init_policy,\n",
    "        new_wrap_env_function=wrap_env,\n",
    "        select_policy_selection_probs=select_policy_selection_probs,\n",
    "        min_primordial_ancestors=5,\n",
    "        rng_seed=None,\n",
    "    ).train_with_mitosis(1000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    unwrapped_env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "id": "90c056a126f17111",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "id": "bb9ea562feef0c28",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
