{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T21:56:34.335400Z",
     "start_time": "2024-05-05T21:56:31.553207Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "from gymnasium.vector import VectorEnv\n",
    "from gymnasium.wrappers import RescaleAction, ClipAction\n",
    "from torch import optim, nn\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.np_functions import softmax\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.async_policy_mitosis import AsyncPolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis import PolicyMitosis, PolicyWithEnvAndInfo\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from src.reinforcement_learning.core.policy_info import PolicyInfo\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.torch_device import set_default_torch_device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mitosis with id 2024-05-03_19.31.53\n",
      "Worker 0 starting iteration 1\n",
      "Worker 1 starting iteration 2\n",
      "Worker 2 starting iteration 3\n",
      "Worker 3 starting iteration 4\n",
      "policy selection probs = \n",
      "\t2024-05-03_19.31.56~YOi2Kr: p = 0.003371, scores =  78.815, steps = 100000\n",
      "\t2024-05-03_19.41.26~ONGcgY: p = 0.002850, scores =  76.237, steps = 100000\n",
      "\t2024-05-03_19.50.38~rpp1TN: p = 0.003296, scores =  78.466, steps = 100000\n",
      "\t2024-05-03_19.59.45~iOUmuV: p = 0.001695, scores =  68.256, steps = 100000\n",
      "\t2024-05-03_20.08.31~qVIc0m: p = 0.003239, scores =  78.197, steps = 100000\n",
      "\t2024-05-03_20.23.19~jkSCqb: p = 0.004759, scores =  84.109, steps = 200000\n",
      "\t2024-05-03_20.32.46~UETzfs: p = 0.006956, scores =  89.935, steps = 300000\n",
      "\t2024-05-03_20.42.03~AAbsq9: p = 0.006533, scores =  88.973, steps = 300000\n",
      "\t2024-05-03_20.51.29~B2PjVU: p = 0.002693, scores =  75.363, steps = 100000\n",
      "\t2024-05-03_21.00.17~O81lZo: p = 0.004296, scores =  82.537, steps = 200000\n",
      "\t2024-05-03_21.09.57~l028kP: p = 0.010269, scores =  95.916, steps = 400000\n",
      "\t2024-05-03_21.19.14~Ha74nA: p = 0.008054, scores =  92.187, steps = 300000\n",
      "\t2024-05-03_21.30.22~qZu9T3: p = 0.011488, scores =  97.640, steps = 400000\n",
      "\t2024-05-03_21.43.20~RfUGC7: p = 0.024136, scores = 109.038, steps = 500000\n",
      "\t2024-05-03_21.52.38~IWwG2v: p = 0.007584, scores =  91.264, steps = 400000\n",
      "\t2024-05-03_22.01.54~ACeu6l: p = 0.006245, scores =  88.281, steps = 100000\n",
      "\t2024-05-03_22.10.48~pYUxFh: p = 0.008042, scores =  92.164, steps = 200000\n",
      "\t2024-05-03_22.20.12~h4k0Gh: p = 0.022644, scores = 108.059, steps = 600000\n",
      "\t2024-05-03_22.29.29~Rs0F3r: p = 0.007703, scores =  91.502, steps = 300000\n",
      "\t2024-05-03_22.38.52~Xu8Xy5: p = 0.011847, scores =  98.112, steps = 400000\n",
      "\t2024-05-03_22.48.06~ZTYgTN: p = 0.046647, scores = 119.155, steps = 600000\n",
      "\t2024-05-03_22.57.17~SjuQMY: p = 0.009560, scores =  94.818, steps = 400000\n",
      "\t2024-05-03_23.06.42~60oZB5: p = 0.006854, scores =  89.709, steps = 400000\n",
      "\t2024-05-03_23.16.07~Ntmb7N: p = 0.046761, scores = 119.193, steps = 700000\n",
      "\t2024-05-03_23.25.23~F54ySp: p = 0.084728, scores = 128.320, steps = 700000\n",
      "\t2024-05-03_23.34.33~z9rEfe: p = 0.069441, scores = 125.265, steps = 800000\n",
      "\t2024-05-03_23.43.45~3uRz5g: p = 0.101578, scores = 131.105, steps = 800000\n",
      "\t2024-05-03_23.52.58~bSXC4u: p = 0.043064, scores = 117.928, steps = 600000\n",
      "\t2024-05-04_00.02.10~DQW8KB: p = 0.020958, scores = 106.870, steps = 700000\n",
      "\t2024-05-04_00.11.21~MQ4D7Q: p = 0.031686, scores = 113.217, steps = 800000\n",
      "\t2024-05-04_00.20.23~MV6RVg: p = 0.282498, scores = 146.810, steps = 900000\n",
      "\t2024-05-04_18.03.48~Ql7TTA: p = 0.009569, scores =  94.833, steps = 300000\n",
      "\t2024-05-04_18.13.24~WUZKMx: p = 0.008165, scores =  92.396, steps = 400000\n",
      "\t2024-05-04_18.22.47~VSYMLU: p = 0.012003, scores =  98.313, steps = 500000\n",
      "\t2024-05-04_18.32.05~vcAUuK: p = 0.012996, scores =  99.533, steps = 600000\n",
      "\t2024-05-04_18.41.28~yKmJMk: p = 0.016044, scores = 102.768, steps = 700000\n",
      "\t2024-05-04_18.50.58~LXSqBi: p = 0.021090, scores = 106.967, steps = 800000\n",
      "\t2024-05-04_19.00.29~pquFIW: p = 0.018657, scores = 105.085, steps = 900000\n",
      "Starting PPO with policy (policy_id = 2024-05-05_23.56.36~TMFCg7, parent_id = 2024-05-03_22.01.54~ACeu6l, num_parameters = 1639186, previous_steps = 100000, previous_score =    88.281) for 100_000 steps\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 187\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStarting mitosis with id \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmitosis_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    176\u001B[0m     \u001B[43mAsyncPolicyMitosis\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_db\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpolicy_db\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpolicy_train_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munwrapped_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_init_policy_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minit_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_wrap_env_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwrap_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mselect_policy_selection_probs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mselect_policy_selection_probs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_base_ancestors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_globals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrng_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m--> 187\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_with_mitosis\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeyboard interrupt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Git\\pytorch-starter\\src\\reinforcement_learning\\algorithms\\policy_mitosis\\async_policy_mitosis.py:62\u001B[0m, in \u001B[0;36mAsyncPolicyMitosis.train_with_mitosis\u001B[1;34m(self, nr_iterations)\u001B[0m\n\u001B[0;32m     56\u001B[0m futures \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     57\u001B[0m     pool\u001B[38;5;241m.\u001B[39msubmit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworker_task, worker_nr, nr_iterations)\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m worker_nr\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_workers)\n\u001B[0;32m     60\u001B[0m ]\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m future \u001B[38;5;129;01min\u001B[39;00m futures:\n\u001B[1;32m---> 62\u001B[0m     \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[1;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[0;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[1;32m~\\Git\\pytorch-starter\\src\\reinforcement_learning\\algorithms\\policy_mitosis\\async_policy_mitosis.py:74\u001B[0m, in \u001B[0;36mAsyncPolicyMitosis.worker_task\u001B[1;34m(self, worker_nr, nr_iterations)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m i_iteration \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m nr_iterations:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWorker \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworker_nr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m starting iteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi_iteration\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 74\u001B[0m     policy_with_env_and_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpick_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_iteration(policy_with_env_and_info)\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWorker \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mworker_nr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m finished iteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi_iteration\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Git\\pytorch-starter\\src\\reinforcement_learning\\algorithms\\policy_mitosis\\policy_mitosis.py:96\u001B[0m, in \u001B[0;36mPolicyMitosis.pick_policy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_new_policy()\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 96\u001B[0m     selected_parent_policy_entry \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_parent_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_child_policy(selected_parent_policy_entry)\n",
      "File \u001B[1;32m~\\Git\\pytorch-starter\\src\\reinforcement_learning\\algorithms\\policy_mitosis\\policy_mitosis.py:121\u001B[0m, in \u001B[0;36mPolicyMitosis.select_parent_policy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect_parent_policy\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ModelEntry[PolicyInfo]:\n\u001B[1;32m--> 121\u001B[0m     policy_entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_db\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_entries\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m     nr_policies \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(policy_entries)\n\u001B[0;32m    124\u001B[0m     policy_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect_policy_selection_probs(entry[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_info\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m policy_entries)\n",
      "File \u001B[1;32m~\\Git\\pytorch-starter\\src\\model_db\\multiprocessing_sync_wrapper.py:45\u001B[0m, in \u001B[0;36mMultiprocessingSyncWrapper.all_entries\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mall_entries\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[ModelEntry[ModelInfoType]]:\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccess_lock:\n\u001B[1;32m---> 45\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_db\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_entries\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Git\\pytorch-starter\\src\\model_db\\tiny_model_db.py:84\u001B[0m, in \u001B[0;36mTinyModelDB.all_entries\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mall_entries\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[ModelEntry[ModelInfoType]]:\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tinydb\\table.py:233\u001B[0m, in \u001B[0;36mTable.all\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124;03mGet all documents stored in the table.\u001B[39;00m\n\u001B[0;32m    224\u001B[0m \n\u001B[0;32m    225\u001B[0m \u001B[38;5;124;03m:returns: a list with all documents.\u001B[39;00m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;66;03m# iter(self) (implemented in Table.__iter__ provides an iterator\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;66;03m# that returns all documents in this table. We use it to get a list\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;66;03m# of all documents by using the ``list`` constructor to perform the\u001B[39;00m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;66;03m# conversion.\u001B[39;00m\n\u001B[1;32m--> 233\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tinydb\\table.py:655\u001B[0m, in \u001B[0;36mTable.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    649\u001B[0m \u001B[38;5;124;03mIterate over all documents stored in the table.\u001B[39;00m\n\u001B[0;32m    650\u001B[0m \n\u001B[0;32m    651\u001B[0m \u001B[38;5;124;03m:returns: an iterator over all documents.\u001B[39;00m\n\u001B[0;32m    652\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    654\u001B[0m \u001B[38;5;66;03m# Iterate all documents and their IDs\u001B[39;00m\n\u001B[1;32m--> 655\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc_id, doc \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m    656\u001B[0m     \u001B[38;5;66;03m# Convert documents to the document class\u001B[39;00m\n\u001B[0;32m    657\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocument_class(doc, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocument_id_class(doc_id))\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tinydb\\table.py:704\u001B[0m, in \u001B[0;36mTable._read_table\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    695\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;124;03mRead the table data from the underlying storage.\u001B[39;00m\n\u001B[0;32m    697\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    700\u001B[0m \u001B[38;5;124;03monly one document for example.\u001B[39;00m\n\u001B[0;32m    701\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;66;03m# Retrieve the tables from the storage\u001B[39;00m\n\u001B[1;32m--> 704\u001B[0m tables \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_storage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    706\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tables \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;66;03m# The database is empty\u001B[39;00m\n\u001B[0;32m    708\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tinydb\\storages.py:136\u001B[0m, in \u001B[0;36mJSONStorage.read\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    135\u001B[0m \u001B[38;5;66;03m# Load the JSON contents of the file\u001B[39;00m\n\u001B[1;32m--> 136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\json\\__init__.py:293\u001B[0m, in \u001B[0;36mload\u001B[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(fp, \u001B[38;5;241m*\u001B[39m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_float\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    275\u001B[0m         parse_int\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_constant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_pairs_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[0;32m    276\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001B[39;00m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;124;03m    a JSON document) to a Python object.\u001B[39;00m\n\u001B[0;32m    278\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001B[39;00m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobject_hook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobject_hook\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_float\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_int\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_int\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_constant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_constant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobject_pairs_hook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobject_pairs_hook\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\json\\__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[1;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[1;34m(self, s, _w)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[0;32m    333\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \n\u001B[0;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
      "File \u001B[1;32mC:\\Program Files\\Python311\\Lib\\json\\decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[1;34m(self, s, idx)\u001B[0m\n\u001B[0;32m    353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscan_once(s, idx)\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
      "\u001B[1;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def init_policy():\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            in_size = 376\n",
    "            shared_out_sizes = []\n",
    "            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n",
    "            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n",
    "            \n",
    "            hidden_activation_function = nn.ELU()\n",
    "            actor_out_activation_function = nn.Tanh()\n",
    "            critic_out_activation_function = nn.Identity()\n",
    "            \n",
    "            self.has_shared = len(shared_out_sizes) > 0\n",
    "            \n",
    "            if self.has_shared:\n",
    "                self.shared = SeqNet.from_layer_provider(\n",
    "                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                        nn.Linear(in_features, out_features),\n",
    "                        hidden_activation_function\n",
    "                    ),\n",
    "                    in_size=in_size,\n",
    "                    out_sizes=shared_out_sizes\n",
    "                )\n",
    "            else:\n",
    "                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    actor_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    critic_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            if self.has_shared:\n",
    "                shared_out = self.shared(x)\n",
    "            else:\n",
    "                shared_out = x\n",
    "\n",
    "            return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n",
    "\n",
    "def wrap_env(_env: VectorEnv):\n",
    "    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n",
    "    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n",
    "    _env = ClipAction(_env)\n",
    "    return _env\n",
    "\n",
    "def train_func(policy_with_env_and_info: PolicyWithEnvAndInfo) -> tuple[int, float]:\n",
    "    policy = policy_with_env_and_info['policy']\n",
    "    env = policy_with_env_and_info['env']\n",
    "    \n",
    "    score = 0.0\n",
    "    score_ema = ExponentialMovingAverage(0.45)\n",
    "    rollout_stopwatch = Stopwatch()\n",
    "    def on_rollout_done(rl: PPO, step: int, info: dict[str, Any]):   \n",
    "        \n",
    "        if 'raw_rewards' in info['rollout']:\n",
    "            raw_rewards = info['rollout']['raw_rewards']\n",
    "            _, gamma_1_returns = compute_gae_and_returns(\n",
    "                value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "                rewards=raw_rewards,\n",
    "                episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "                last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_rewards=None,\n",
    "                normalize_advantages=None,\n",
    "            )\n",
    "        else:\n",
    "            _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "                last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_advantages=None,\n",
    "                normalize_rewards=None,\n",
    "            )\n",
    "        \n",
    "        episode_scores = gamma_1_returns[\n",
    "            rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "        ]\n",
    "        \n",
    "        nonlocal score, score_ema\n",
    "        score = episode_scores.mean()\n",
    "        current_score_ema = score_ema.update(score)\n",
    "        \n",
    "        rollout_time = rollout_stopwatch.reset()\n",
    "        \n",
    "        resets = format_summary_statics(\n",
    "            rl.buffer.episode_starts.astype(int).sum(axis=0), \n",
    "            mean_format='.2f',\n",
    "            std_format=None,\n",
    "            min_value_format='1d',\n",
    "            max_value_format=None,\n",
    "        )\n",
    "        print(f'{step:>6}: '\n",
    "              f'{score = :9.3f}, '\n",
    "              f'score_ema = {current_score_ema:9.3f}, '\n",
    "              f'time = {rollout_time:5.2f}, '\n",
    "              f'{resets = :s}')\n",
    "    \n",
    "    policy_info_str = ('('\n",
    "          f'policy_id = {policy_with_env_and_info[\"policy_id\"]}, '\n",
    "          f'parent_id = {policy_with_env_and_info[\"parent_policy_id\"]}, '\n",
    "          f'num_parameters = {count_parameters(policy)}, '\n",
    "          f'previous_steps = {policy_with_env_and_info[\"policy_info\"][\"steps_trained\"]}, '\n",
    "          f'previous_score = {policy_with_env_and_info[\"policy_info\"][\"score\"]:9.3f}'\n",
    "          ')')\n",
    "    \n",
    "    print(f'Starting PPO with policy {policy_info_str:s} for {steps_per_iteration:_} steps')\n",
    "    mitosis_iteration_stopwatch = Stopwatch()\n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters(), lr=1e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=0.995,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "        weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.01,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.05,\n",
    "        grad_norm_clip_value=2.0,\n",
    "        callback=Callback(on_rollout_done=on_rollout_done),\n",
    "        logging_config=PPOLoggingConfig(log_rollout_infos=True)\n",
    "    ).train(steps_per_iteration)\n",
    "    \n",
    "    \n",
    "    print(f'Training finished for policy {policy_info_str:s}, end score = {score:9.3f}, time = {mitosis_iteration_stopwatch.time_passed():6.2f}')\n",
    "    \n",
    "    return steps_per_iteration, score_ema.get()\n",
    "\n",
    "def select_policy_selection_probs(policy_infos: Iterable[PolicyInfo]) -> np.ndarray:\n",
    "    scores = np.array([policy_info['score'] for policy_info in policy_infos])\n",
    "    scores = scores / scores.std()\n",
    "    scores = softmax(scores, temperature=0.9)\n",
    "    return scores\n",
    "\n",
    "device = set_default_torch_device(\"cuda:0\") if True else set_default_torch_device('cpu')\n",
    "\n",
    "policy_action_std = 0.15\n",
    "steps_per_iteration = 100_000\n",
    "\n",
    "env_name = 'Humanoid-v4'\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "num_envs = 32\n",
    "\n",
    "# mitosis_id = get_current_timestamp()\n",
    "mitosis_id = '2024-05-03_19.31.53'\n",
    "policy_db = TinyModelDB[PolicyInfo](base_path=f'E:/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "\n",
    "unwrapped_env = parallelize_env_async(lambda: gym.make(env_name, **env_kwargs), num_envs)\n",
    "try:\n",
    "    print(f'Starting mitosis with id {mitosis_id}')\n",
    "    AsyncPolicyMitosis(\n",
    "        num_workers=4,\n",
    "        policy_db=policy_db,\n",
    "        policy_train_function=train_func,\n",
    "        env=unwrapped_env,\n",
    "        new_init_policy_function=init_policy,\n",
    "        new_wrap_env_function=wrap_env,\n",
    "        select_policy_selection_probs=select_policy_selection_probs,\n",
    "        min_base_ancestors=5,\n",
    "        _globals=globals(),\n",
    "        rng_seed=None,\n",
    "    ).train_with_mitosis(1000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    unwrapped_env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T21:56:43.690546Z",
     "start_time": "2024-05-05T21:56:34.336397Z"
    }
   },
   "id": "8c51ce897912a70",
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
