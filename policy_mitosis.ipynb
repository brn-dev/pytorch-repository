{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T11:33:21.010956Z",
     "start_time": "2024-05-03T11:33:12.125873Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "from gymnasium.vector import VectorEnv\n",
    "from gymnasium.wrappers import RescaleAction, ClipAction\n",
    "from torch import optim, nn\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from src.reinforcement_learning.core.policy_info import PolicyInfo\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n",
    "from src.torch_device import set_default_torch_device\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO with policy (policy_id = 2024-05-03_13.55.08, parent_id = None) for 100_000 steps\n",
      "  2500: score =   21.6012\n",
      "  5000: score =   21.1634\n",
      "  7500: score =   23.8943\n",
      " 10000: score =   31.0903\n",
      " 12500: score =   39.0746\n",
      " 15000: score =   57.1511\n",
      " 17500: score =   50.3929\n",
      " 20000: score =   65.1771\n",
      " 22500: score =   67.2442\n",
      " 25000: score =   70.2096\n",
      " 27500: score =   68.8788\n",
      " 30000: score =   71.6042\n",
      " 32500: score =   70.7416\n",
      " 35000: score =   68.0738\n",
      " 37500: score =   68.5171\n",
      " 40000: score =   69.4280\n",
      " 42500: score =   71.8161\n",
      " 45000: score =   71.5567\n",
      " 47500: score =   71.1147\n",
      " 50000: score =   73.0481\n",
      " 52500: score =   74.0450\n",
      " 55000: score =   75.7549\n",
      " 57500: score =   75.9361\n",
      " 60000: score =   75.9568\n",
      " 62500: score =   77.5204\n",
      " 65000: score =   75.2102\n",
      " 67500: score =   74.4111\n",
      " 70000: score =   76.0839\n"
     ]
    }
   ],
   "source": [
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis import PolicyMitosis, PolicyWithEnvAndInfo\n",
    "\n",
    "\n",
    "def init_policy():\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            \n",
    "            in_size = 376\n",
    "            shared_out_sizes = []\n",
    "            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n",
    "            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n",
    "            \n",
    "            hidden_activation_function = nn.ELU()\n",
    "            actor_out_activation_function = nn.Tanh()\n",
    "            critic_out_activation_function = nn.Identity()\n",
    "            \n",
    "            self.has_shared = len(shared_out_sizes) > 0\n",
    "            \n",
    "            if self.has_shared:\n",
    "                self.shared = SeqNet.from_layer_provider(\n",
    "                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                        nn.Linear(in_features, out_features),\n",
    "                        hidden_activation_function\n",
    "                    ),\n",
    "                    in_size=in_size,\n",
    "                    out_sizes=shared_out_sizes\n",
    "                )\n",
    "            else:\n",
    "                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    actor_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    critic_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            if self.has_shared:\n",
    "                shared_out = self.shared(x)\n",
    "            else:\n",
    "                shared_out = x\n",
    "\n",
    "            return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n",
    "\n",
    "def wrap_env(_env: VectorEnv):\n",
    "    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n",
    "    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n",
    "    _env = ClipAction(_env)\n",
    "    return _env\n",
    "\n",
    "def train_func(policy_with_env_and_info: PolicyWithEnvAndInfo) -> tuple[int, float]:\n",
    "    policy = policy_with_env_and_info['policy']\n",
    "    env = policy_with_env_and_info['env']\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    def on_rollout_done(rl: PPO, step: int, info: dict[str, Any]):   \n",
    "        \n",
    "        if 'raw_rewards' in info['rollout']:\n",
    "            raw_rewards = info['rollout']['raw_rewards']\n",
    "            _, gamma_1_returns = compute_gae_and_returns(\n",
    "                value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "                rewards=raw_rewards,\n",
    "                episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "                last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_rewards=None,\n",
    "                normalize_advantages=None,\n",
    "            )\n",
    "        else:\n",
    "            _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "                last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_advantages=None,\n",
    "                normalize_rewards=None,\n",
    "            )\n",
    "        \n",
    "        episode_scores = gamma_1_returns[\n",
    "            rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "        ]\n",
    "        \n",
    "        nonlocal score\n",
    "        score = episode_scores.mean()\n",
    "        \n",
    "        print(f'{step:>6}: {score = :9.4f}')\n",
    "    \n",
    "    print(f'Starting PPO with policy (policy_id = {policy_with_env_and_info[\"policy_id\"]}, parent_id = {policy_with_env_and_info[\"parent_policy_id\"]}) for {steps_per_iteration:_} steps')\n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters(), lr=1e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=0.995,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "        weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.01,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.05,\n",
    "        grad_norm_clip_value=2.0,\n",
    "        callback=Callback(on_rollout_done=on_rollout_done),\n",
    "        logging_config=PPOLoggingConfig(log_rollout_infos=True)\n",
    "    ).train(steps_per_iteration)\n",
    "    \n",
    "    return steps_per_iteration, score\n",
    "\n",
    "\n",
    "device = set_default_torch_device(\"cuda:0\") if True else set_default_torch_device('cpu')\n",
    "\n",
    "policy_action_std = 0.15\n",
    "steps_per_iteration = 100_000\n",
    "\n",
    "env_name = 'Humanoid-v4'\n",
    "env_kwargs = {'forward_reward_weight': 1.25, 'healthy_reward': 0.5, 'ctrl_cost_weight': 0.001 }\n",
    "num_envs = 4\n",
    "\n",
    "policy_db = TinyModelDB[PolicyInfo](base_path=f'saved_models/rl/{env_name}/mitosis-{get_current_timestamp()}')\n",
    "\n",
    "unwrapped_env = parallelize_env_async(lambda: gym.make(env_name, **env_kwargs), num_envs)\n",
    "try:\n",
    "    PolicyMitosis(\n",
    "        policy_db=policy_db,\n",
    "        policy_train_function=train_func,\n",
    "        env=unwrapped_env,\n",
    "        new_init_policy_function=init_policy,\n",
    "        new_wrap_env_function=wrap_env,\n",
    "        _globals=globals(),\n",
    "        rng_seed=42,\n",
    "    ).train(1000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    unwrapped_env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-03T11:55:07.493132Z"
    }
   },
   "id": "8c51ce897912a70",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
