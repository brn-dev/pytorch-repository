{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T17:08:11.405344Z",
     "start_time": "2024-05-27T17:08:08.282835Z"
    }
   },
   "source": [
    "import sys\n",
    "import time\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.np_functions import softmax\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.async_policy_mitosis import AsyncPolicyMitosis\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.policy_mitosis_base import PolicyWithEnvAndInfo\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPOLoggingConfig, PPO\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.core.objectives import ObjectiveLoggingConfig\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.torch_device import get_torch_device\n",
    "from src.torch_functions import antisymmetric_power\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-27T17:08:11.406341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "nr_carts = 2\n",
    "\n",
    "def make_multi_agent_cart_pole_env(render_mode: str | None = None):\n",
    "    from src.reinforcement_learning.gym.envs.multi_agent_cartpole3d import MultiAgentCartPole3D\n",
    "    return MultiAgentCartPole3D(\n",
    "        nr_carts=nr_carts,\n",
    "        cart_size=0.25,\n",
    "        force_magnitude=500,\n",
    "        physics_steps_per_step=25,\n",
    "        reset_position_radius=0.75,\n",
    "        reset_randomize_position_angle_offset=True,\n",
    "        reset_position_randomization_magnitude=0.3,\n",
    "        reset_hinge_randomization_magnitude=0.05,\n",
    "        slide_range=2,\n",
    "        hinge_range=0.8,\n",
    "        time_limit=120.0,\n",
    "        step_reward_function=lambda time_, action, state, prev_state: 0.01,\n",
    "        out_ouf_range_reward_function=lambda time_, action, state: 0.0,# -10 + time_ * 3,\n",
    "        time_limit_reward_function=lambda time_, action, state: 10,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "\n",
    "\n",
    "def init_policy():\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.net import Net\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n",
    "        SquashedDiagGaussianActionSelector\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "    from src.networks.skip_nets.additive_skip_net import AdditiveSkipNet, FullyConnectedAdditiveSkipNet, \\\n",
    "        FullyConnectedUnweightedAdditiveSkipNet\n",
    "    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n",
    "    from src.weight_initialization import orthogonal_initialization\n",
    "    from src.networks.multihead_self_attention import MultiheadSelfAttention\n",
    "    \n",
    "    in_size = 8\n",
    "    action_size = 2\n",
    "    \n",
    "    actor_layers = 3\n",
    "    actor_features = 48\n",
    "    \n",
    "    critic_layers = 2\n",
    "    critic_features = 48\n",
    "\n",
    "    actor_hidden_activation_function = nn.ELU\n",
    "    critic_hidden_activation_function = nn.ELU\n",
    "    \n",
    "    actor_hidden_initialization = lambda module: orthogonal_initialization(module, gain=np.sqrt(2))\n",
    "    critic_hidden_initialization = lambda module: orthogonal_initialization(module, gain=np.sqrt(2))\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.actor_embedding = nn.Sequential(nn.Linear(in_size, actor_features), actor_hidden_activation_function())\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.sequential_net(\n",
    "                        actor_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        actor_hidden_activation_function(),\n",
    "                        actor_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        nn.Tanh() if is_last_layer else actor_hidden_activation_function(),\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=actor_layers,\n",
    "                num_features=actor_features,\n",
    "            )\n",
    "\n",
    "            self.critic_embedding = nn.Sequential(nn.Linear(in_size, critic_features), critic_hidden_activation_function())\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.sequential_net(\n",
    "                        critic_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        critic_hidden_activation_function(),\n",
    "                        critic_hidden_initialization(nn.Linear(in_features, out_features)),\n",
    "                        critic_hidden_activation_function(),\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=critic_layers,\n",
    "                num_features=critic_features,\n",
    "            )\n",
    "            self.critic_regressor = nn.Linear(critic_features, 1)\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            *batch_shape, nr_actors, nr_features = x.shape\n",
    "            x = torch.flatten(x, end_dim=-3)\n",
    "            \n",
    "            actor_out: torch.Tensor = self.actor(self.actor_embedding(x))\n",
    "            critic_out: torch.Tensor = self.critic_regressor(self.critic(self.critic_embedding(x)).sum(dim=-2))\n",
    "            \n",
    "            actor_out = actor_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            critic_out = critic_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            \n",
    "            return actor_out, critic_out\n",
    "    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n",
    "        latent_dim=actor_features,\n",
    "        action_dim=action_size,\n",
    "        std=0.15,\n",
    "        std_learnable=False,\n",
    "        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "    ))\n",
    "\n",
    "def wrap_env(env_):\n",
    "    return env_\n",
    "\n",
    "def train_func(policy_with_env_and_info: PolicyWithEnvAndInfo) -> tuple[int, float]:\n",
    "    policy = policy_with_env_and_info['policy']\n",
    "    env = policy_with_env_and_info['env']\n",
    "    \n",
    "    score = 0.0\n",
    "    score_ema = ExponentialMovingAverage(0.45)\n",
    "    rollout_stopwatch = Stopwatch()\n",
    "    def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):   \n",
    "        \n",
    "        if 'raw_rewards' in info['rollout']:\n",
    "            raw_rewards = info['rollout']['raw_rewards']\n",
    "            _, gamma_1_returns = compute_gae_and_returns(\n",
    "                value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "                rewards=raw_rewards,\n",
    "                episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "                last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_rewards=None,\n",
    "                normalize_advantages=None,\n",
    "            )\n",
    "        else:\n",
    "            _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "                last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "                last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "                gamma=1.0,\n",
    "                gae_lambda=1.0,\n",
    "                normalize_advantages=None,\n",
    "                normalize_rewards=None,\n",
    "            )\n",
    "        \n",
    "        episode_scores = gamma_1_returns[\n",
    "            rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "        ]\n",
    "        \n",
    "        nonlocal score, score_ema\n",
    "        score = episode_scores.mean()\n",
    "        current_score_ema = score_ema.update(score)\n",
    "        \n",
    "        rollout_time = rollout_stopwatch.reset()\n",
    "        \n",
    "        resets: np.ndarray = rl.buffer.episode_starts.astype(int).sum(axis=0)\n",
    "        resets_mean = resets.mean()\n",
    "        resets_min = resets.min()\n",
    "        print(f'{step:>6}: '\n",
    "              f'{score = :9.3f}, '\n",
    "              f'score_ema = {current_score_ema:9.3f}, '\n",
    "              f'time = {rollout_time:5.2f}, '\n",
    "              f'resets = {resets_mean:5.2f} >= {resets_min:5.2f}')\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    policy_info = policy_with_env_and_info['policy_info']\n",
    "    policy_info_str = ('('\n",
    "          f'policy_id = {policy_info[\"policy_id\"]}, '\n",
    "          f'parent_id = {policy_info[\"parent_policy_id\"]}, '\n",
    "          f'num_parameters = {count_parameters(policy)}, '\n",
    "          f'previous_steps = {policy_info[\"steps_trained\"]}, '\n",
    "          f'previous_score = {policy_info[\"score\"]:9.3f}'\n",
    "          ')')\n",
    "    \n",
    "    print(f'Starting PPO with policy {policy_info_str:s} for {steps_per_iteration:_} steps')\n",
    "    mitosis_iteration_stopwatch = Stopwatch()\n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.AdamW(pol.parameters(), lr=1e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=0.995,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        reduce_actor_objective=lambda obj: antisymmetric_power(obj, 1.5).mean(),\n",
    "        weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "        weigh_entropy_objective=lambda obj: 1.0 * obj.exp(),\n",
    "        weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "        ppo_max_epochs=10,\n",
    "        ppo_kl_target=0.025,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.1,\n",
    "        grad_norm_clip_value=0.5,\n",
    "        # sde_noise_sample_freq=50,\n",
    "        callback=Callback(\n",
    "            on_rollout_done=on_rollout_done,\n",
    "            rollout_schedulers={},\n",
    "            optimization_schedulers={},\n",
    "        ),\n",
    "        logging_config=PPOLoggingConfig(log_returns=True, log_advantages=True, log_grad_norm=True,\n",
    "                                        log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                        log_actor_kl_divergence=True,\n",
    "                                        actor_objective=ObjectiveLoggingConfig(log_raw=True),\n",
    "                                        entropy_objective=ObjectiveLoggingConfig(log_weighted=True),\n",
    "                                        critic_objective=ObjectiveLoggingConfig(log_weighted=True), ),\n",
    "            torch_device=device,\n",
    "        ).train(steps_per_iteration)\n",
    "    \n",
    "    \n",
    "    print(f'Training finished for policy {policy_info_str:s}, end score = {score:9.3f}, time = {mitosis_iteration_stopwatch.time_passed():6.2f}')\n",
    "    \n",
    "    return steps_per_iteration, score_ema.get()\n",
    "\n",
    "def select_policy_selection_probs(policy_infos: Iterable[MitosisPolicyInfo]) -> np.ndarray:\n",
    "    scores = np.array([policy_info['score'] for policy_info in policy_infos])\n",
    "    scores = scores / scores.mean()\n",
    "    scores = softmax(scores, temperature=0.5 / len(scores)**0.5)\n",
    "    return scores\n",
    "\n",
    "device = get_torch_device(\"cuda:0\") if True else get_torch_device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "steps_per_iteration = 100_000\n",
    "\n",
    "num_envs = 32\n",
    "\n",
    "# mitosis_id = get_current_timestamp()\n",
    "mitosis_id = '2024-05-27_19.00.00'\n",
    "policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'E:/saved_models/rl/MultiAgentCartPole/{nr_carts}/mitosis-{mitosis_id}')\n",
    "# policy_db = TinyModelDB[PolicyInfo](base_path=f'C:/Users/domin/git/pytorch-starter/saved_models/rl/{env_name}/mitosis-{mitosis_id}')\n",
    "\n",
    "try:\n",
    "    print(f'Starting mitosis with id {mitosis_id}')\n",
    "    AsyncPolicyMitosis(\n",
    "        num_workers=3,\n",
    "        policy_db=policy_db,\n",
    "        train_policy_function=train_func,\n",
    "        create_env=lambda: parallelize_env_async(lambda: make_multi_agent_cart_pole_env(None), num_envs),\n",
    "        new_init_policy_function=init_policy,\n",
    "        new_wrap_env_function=wrap_env,\n",
    "        select_policy_selection_probs=select_policy_selection_probs,\n",
    "        min_base_ancestors=5,\n",
    "        rng_seed=None,\n",
    "        initialization_delay=15,\n",
    "        delay_between_workers=10,\n",
    "    ).train_with_mitosis(1000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "id": "90c056a126f17111",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "Starting mitosis with id 2024-05-27_19.00.00\n",
      "Starting worker 0 with delay = 0\n",
      "Started training iteration for policy: 2024-05-27_19.08.26~zC9AVO, parent policy id: None\n",
      "Starting worker 1 with delay = 15\n",
      "Started training iteration for policy: 2024-05-27_19.08.41~aOPPI5, parent policy id: None\n",
      "Starting worker 2 with delay = 30\n",
      "Started training iteration for policy: 2024-05-27_19.08.56~xYsZyL, parent policy id: None\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "id": "bb9ea562feef0c28",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
