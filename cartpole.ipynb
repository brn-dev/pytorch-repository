{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from gymnasium import Env\n",
    "from gymnasium.vector import VectorEnv\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.model_db.model_db import ModelDB\n",
    "from src.model_db.dummy_model_db import DummyModelDB\n",
    "from src.networks.multihead_self_attention import MultiheadSelfAttention\n",
    "from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n",
    "from src.reinforcement_learning.algorithms.policy_mitosis.mitosis_policy_info import MitosisPolicyInfo\n",
    "from src.model_db.tiny_model_db import TinyModelDB\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.reinforcement_learning.core.action_selectors.predicted_std_action_selector import PredictedStdActionSelector\n",
    "from src.reinforcement_learning.core.action_selectors.state_dependent_noise_action_selector import \\\n",
    "    StateDependentNoiseActionSelector\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.core.objectives import ObjectiveLoggingConfig\n",
    "from src.reinforcement_learning.core.policies.base_policy import BasePolicy\n",
    "from src.reinforcement_learning.gym.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "from src.schedulers import FixedValueScheduler, OneStepRecursiveScheduler\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation, ClipAction\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.algorithms.a2c.a2c import A2C\n",
    "from src.reinforcement_learning.algorithms.ppo.ppo import PPO, PPOLoggingConfig\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.gym.singleton_vector_env import as_vec_env\n",
    "from src.reinforcement_learning.algorithms import policy_optimization_base\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.distributions as dist\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from src.torch_functions import antisymmetric_power\n",
    "from src.weight_initialization import orthogonal_initialization\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T15:59:51.383379Z",
     "start_time": "2024-05-27T15:59:46.882630Z"
    }
   },
   "id": "ba8c59a3eba2f172",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 8)\n"
     ]
    }
   ],
   "source": [
    "nr_carts = 2\n",
    "def make_multi_agent_cart_pole_env(render_mode: str | None = None):\n",
    "    from src.reinforcement_learning.gym.envs.multi_agent_cartpole3d import MultiAgentCartPole3D\n",
    "    return MultiAgentCartPole3D(\n",
    "        nr_carts=nr_carts,\n",
    "        cart_size=0.25,\n",
    "        force_magnitude=500,\n",
    "        physics_steps_per_step=25,\n",
    "        reset_position_radius=0.75,\n",
    "        reset_randomize_position_angle_offset=True,\n",
    "        reset_position_randomization_magnitude=0.3,\n",
    "        reset_hinge_randomization_magnitude=0.05,\n",
    "        slide_range=2,\n",
    "        hinge_range=0.8,\n",
    "        time_limit=120.0,\n",
    "        step_reward_function=lambda time_, action, state, prev_state: 0.01,\n",
    "        out_ouf_range_reward_function=lambda time_, action, state: 0.0,# -10 + time_ * 3,\n",
    "        time_limit_reward_function=lambda time_, action, state: 10,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "tmp_env = make_multi_agent_cart_pole_env()\n",
    "print(tmp_env.reset()[0].shape)\n",
    "tmp_env.render(camera_id=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T15:59:51.533916Z",
     "start_time": "2024-05-27T15:59:51.384376Z"
    }
   },
   "id": "5069af9187794400",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "policy_id: str\n",
    "policy: Optional[BasePolicy]\n",
    "steps_trained: int\n",
    "def get_policy(create_new_if_exists: bool):\n",
    "    \n",
    "    global policy_id, policy, steps_trained\n",
    "    \n",
    "    policy_in_ram = 'policy' in globals()\n",
    "    if not policy_in_ram or create_new_if_exists:\n",
    "        if not policy_in_ram:\n",
    "            print('No policy in RAM, creating a new one')\n",
    "        \n",
    "        policy_id = get_current_timestamp()\n",
    "        policy = init_policy()\n",
    "        steps_trained = 0\n",
    "        print(f'New policy {policy_id} created')\n",
    "    \n",
    "    if parent_policy_id is not None:\n",
    "        model_entry = policy_db.load_model_state_dict(policy, parent_policy_id)\n",
    "        steps_trained = model_entry['model_info']['steps_trained']\n",
    "        print(f'Loading state dict from policy {parent_policy_id}')\n",
    "    \n",
    "    print(f'Using policy {policy_id} with parent policy {parent_policy_id}')\n",
    "    return policy_id, policy\n",
    "\n",
    "def init_policy():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    from src.networks.core.net import Net\n",
    "    from src.networks.core.seq_net import SeqNet\n",
    "    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n",
    "        SquashedDiagGaussianActionSelector\n",
    "    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "    from src.networks.skip_nets.additive_skip_net import AdditiveSkipNet, FullyConnectedAdditiveSkipNet, \\\n",
    "        FullyConnectedUnweightedAdditiveSkipNet\n",
    "    from src.networks.skip_nets.additive_skip_connection import AdditiveSkipConnection\n",
    "    \n",
    "    in_size = 8\n",
    "    action_size = 2\n",
    "    \n",
    "    actor_layers = 3\n",
    "    actor_features = 96\n",
    "    \n",
    "    critic_layers = 2\n",
    "    critic_features = 96\n",
    "\n",
    "    # hidden_activation_function = nn.ELU()\n",
    "    actor_hidden_activation_function = nn.Tanh()\n",
    "    critic_hidden_activation_function = nn.ReLU()\n",
    "\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.actor_embedding = nn.Sequential(nn.Linear(in_size, actor_features), actor_hidden_activation_function)\n",
    "            self.actor = FullyConnectedUnweightedAdditiveSkipNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.as_net(nn.Sequential(\n",
    "                        orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                        actor_hidden_activation_function,\n",
    "                        orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                        nn.Tanh() if is_last_layer else actor_hidden_activation_function,\n",
    "                    ))),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=actor_layers,\n",
    "                num_features=actor_features,\n",
    "            )\n",
    "\n",
    "            self.critic_embedding = nn.Sequential(nn.Linear(in_size, critic_features), critic_hidden_activation_function)\n",
    "            self.critic = FullyConnectedUnweightedAdditiveSkipNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    AdditiveSkipConnection(MultiheadSelfAttention(\n",
    "                        embed_dim=in_features,\n",
    "                        num_heads=4,\n",
    "                        batch_first=True,\n",
    "                    )),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                    AdditiveSkipConnection(Net.as_net(nn.Sequential(\n",
    "                        orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                        actor_hidden_activation_function,\n",
    "                        orthogonal_initialization(nn.Linear(in_features, out_features), gain=np.sqrt(2)),\n",
    "                        nn.Tanh() if is_last_layer else actor_hidden_activation_function,\n",
    "                    ))),\n",
    "                    nn.LayerNorm(in_features),\n",
    "                ),\n",
    "                num_layers=critic_layers,\n",
    "                num_features=critic_features,\n",
    "            )\n",
    "            self.critic_regressor = nn.Linear(critic_features, 1)\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            *batch_shape, nr_actors, nr_features = x.shape\n",
    "            x = torch.flatten(x, end_dim=-3)\n",
    "            \n",
    "            actor_out: torch.Tensor = self.actor(self.actor_embedding(x))\n",
    "            critic_out: torch.Tensor = self.critic_regressor(self.critic(self.critic_embedding(x)).sum(dim=-2))\n",
    "            \n",
    "            actor_out = actor_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            critic_out = critic_out.unflatten(dim=0, sizes=batch_shape)\n",
    "            \n",
    "            return actor_out, critic_out\n",
    "        \n",
    "    return ActorCriticPolicy(A2CNetwork(), PredictedStdActionSelector(\n",
    "        latent_dim=actor_features,\n",
    "        action_dim=action_size,\n",
    "        base_std=0.5,\n",
    "        squash_output=True,\n",
    "        action_net_initialization=lambda module: orthogonal_initialization(module, gain=0.01),\n",
    "        log_std_net_initialization=lambda module: orthogonal_initialization(module, gain=0.1),\n",
    "    ))\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.25)\n",
    "stopwatch = Stopwatch()\n",
    "best_iteration_score = -1e6\n",
    "\n",
    "def on_rollout_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    \n",
    "    if 'raw_rewards' in info['rollout']:\n",
    "        raw_rewards = info['rollout']['raw_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards[:len(raw_rewards)]),\n",
    "            rewards=raw_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(raw_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_scores = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    global best_iteration_score\n",
    "    iteration_score = episode_scores.mean()\n",
    "    score_moving_average = score_mean_ema.update(iteration_score)\n",
    "    if iteration_score >= best_iteration_score:\n",
    "        best_iteration_score = iteration_score\n",
    "        policy_db.save_model_state_dict(\n",
    "            model=policy,\n",
    "            model_id=policy_id,\n",
    "            parent_model_id=parent_policy_id,\n",
    "            model_info={\n",
    "                'score': iteration_score.item(),\n",
    "                'steps_trained': steps_trained,\n",
    "                'wrap_env_source_code': wrap_env_source_code_source,\n",
    "                'init_policy_source_code': init_policy_source\n",
    "            },\n",
    "        )\n",
    "        \n",
    "    info['episode_scores'] = episode_scores\n",
    "    info['score_moving_average'] = score_moving_average\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any], scheduler_values: dict[str, Any]):\n",
    "    time_taken = stopwatch.reset()\n",
    "    \n",
    "    global steps_trained\n",
    "    steps_trained += rl.buffer.pos\n",
    "    \n",
    "    episode_scores = info['episode_scores']\n",
    "    score_moving_average = info['score_moving_average']\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_scores, \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='4.3f',\n",
    "        min_value_format=' 6.3f',\n",
    "        max_value_format='5.3f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        rl.weigh_actor_objective(torch.abs(info['raw_actor_objective'])),  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    entropy_obj = None if info['weighted_entropy_objective'] is None else format_summary_statics(\n",
    "        info['weighted_entropy_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['weighted_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = format_summary_statics(\n",
    "        rl.buffer.episode_starts.astype(int).sum(axis=0), \n",
    "        mean_format='.2f',\n",
    "        std_format=None,\n",
    "        min_value_format='1d',\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    kl_div = info['actor_kl_divergence'][-1]\n",
    "    grad_norm = format_summary_statics(\n",
    "        info['grad_norm'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    rollout_action_stds = format_summary_statics(\n",
    "        info['rollout']['action_stds'],\n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    ppo_epochs = info['nr_ppo_epochs']\n",
    "    ppo_updates = info['nr_ppo_updates']\n",
    "    expl_var = rl.buffer.compute_critic_explained_variance(info['returns'])\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f\"{scores = :s}, \"\n",
    "          f'score_ema = {score_moving_average: 6.3f}, '\n",
    "          f\"{advantages = :s}, \"\n",
    "          f\"{abs_actor_obj = :s}, \"\n",
    "          +(f\"{entropy_obj = :s}, \" if entropy_obj is not None else '')+\n",
    "          f\"rollout_stds = {rollout_action_stds:s}, \"\n",
    "          f\"{critic_obj = :s}, \"\n",
    "          f\"{expl_var = :.3f}, \"\n",
    "          f\"{kl_div = :.4f}, \"\n",
    "          f\"{ppo_epochs = }, \"\n",
    "          f\"{ppo_updates = }, \"\n",
    "          f\"{grad_norm = :s}, \"\n",
    "          f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f} \\n\")\n",
    "    print()\n",
    "    if not wandb_run.disabled:\n",
    "        wandb_run.log({\n",
    "            'scores': episode_scores,\n",
    "            'advantages': wandb.Histogram(info['advantages']),\n",
    "            'actor_obj': wandb.Histogram(rl.weigh_actor_objective(info['raw_actor_objective'])),\n",
    "            'abs_actor_obj': wandb.Histogram(rl.weigh_actor_objective(torch.abs(info['raw_actor_objective']))),\n",
    "            'critic_obj': wandb.Histogram(info['weighted_critic_objective']),\n",
    "            'expl_var': expl_var,\n",
    "            'kl_div': kl_div,\n",
    "            'ppo_epochs': ppo_epochs,\n",
    "            'ppo_updates': ppo_updates,\n",
    "            'grad_norm': wandb.Histogram(info['grad_norm']),\n",
    "            'resets': wandb.Histogram(rl.buffer.episode_starts.astype(int).sum(axis=0)),\n",
    "            'time_taken': time_taken,\n",
    "        }, step=step)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if True else torch.device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return make_multi_agent_cart_pole_env(render_mode=render_mode)\n",
    "\n",
    "def wrap_env(env_):\n",
    "    from src.reinforcement_learning.gym.transform_reward_wrapper import TransformRewardWrapper\n",
    "    \n",
    "    # env_ = TransformRewardWrapper(env_, lambda reward_: 0.01 * reward_)\n",
    "    return env_\n",
    "\n",
    "wrap_env_source_code_source = inspect.getsource(wrap_env)\n",
    "init_policy_source = inspect.getsource(init_policy)\n",
    "\n",
    "num_envs = 64\n",
    "    \n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/MultiAgentCartPole/{nr_carts}')\n",
    "policy_db = DummyModelDB[MitosisPolicyInfo]()\n",
    "print(f'{policy_db = }')\n",
    "\n",
    "parent_policy_id=None #'2024-05-26_21.07.11'  # '2024-04-28_20.57.23'\n",
    "policy_action_std=0.15\n",
    "policy_id, policy = get_policy(create_new_if_exists=False)\n",
    "print(f'{count_parameters(policy) = }')\n",
    "\n",
    "# wandb.init(project=f'rl-{env_name}', config={'policy_id': policy_id})\n",
    "wandb_run = wandb.init(mode='disabled')\n",
    "\n",
    "env = parallelize_env_async(lambda: make_multi_agent_cart_pole_env(render_mode=None), num_envs)\n",
    "# env, _ = as_vec_env(create_env(render_mode=None))\n",
    "# print(dict(policy.named_parameters()))\n",
    "print(env.reset()[0].shape)\n",
    "try:\n",
    "    env = wrap_env(env)\n",
    "    print(f'{env = }, {num_envs = } \\n\\n')\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        PPO(\n",
    "            env=env,\n",
    "            policy=policy.to(device),\n",
    "            policy_optimizer=lambda pol: optim.AdamW(pol.parameters(), lr=1e-5),\n",
    "            buffer_size=2500,\n",
    "            gamma=0.995,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=NormalizationType.Std,\n",
    "            reduce_actor_objective=lambda obj: antisymmetric_power(obj, 1.5).mean(),\n",
    "            weigh_actor_objective=lambda obj: 1.0 * obj,\n",
    "            weigh_entropy_objective=lambda obj: 1.0 * obj.exp(),\n",
    "            weigh_critic_objective=lambda obj: 0.5 * obj,\n",
    "            ppo_max_epochs=10,\n",
    "            ppo_kl_target=0.025,\n",
    "            ppo_batch_size=500,\n",
    "            action_ratio_clip_range=0.1,\n",
    "            grad_norm_clip_value=0.5,\n",
    "            # sde_noise_sample_freq=50,\n",
    "            callback=Callback(\n",
    "                on_rollout_done=on_rollout_done,\n",
    "                rollout_schedulers={},\n",
    "                on_optimization_done=on_optimization_done,\n",
    "                optimization_schedulers={},\n",
    "            ),\n",
    "            logging_config=PPOLoggingConfig(log_returns=True, log_advantages=True, log_grad_norm=True,\n",
    "                                            log_rollout_infos=True, log_rollout_action_stds=True,\n",
    "                                            log_actor_kl_divergence=True,\n",
    "                                            actor_objective=ObjectiveLoggingConfig(log_raw=True),\n",
    "                                            entropy_objective=ObjectiveLoggingConfig(log_weighted=True),\n",
    "                                            critic_objective=ObjectiveLoggingConfig(log_weighted=True), ),\n",
    "            torch_device=device,\n",
    "        ).train(5_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    time.sleep(2.5)\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "    policy_db.close()\n",
    "    print('model db closed')\n",
    "    \n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T16:28:39.993202Z",
     "start_time": "2024-05-27T16:08:02.364395Z"
    }
   },
   "id": "f71efe062771e81b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "policy_db = DummyModelDB()\n",
      "No policy in RAM, creating a new one\n",
      "New policy 2024-05-27_18.08.02 created\n",
      "Using policy 2024-05-27_18.08.02 with parent policy None\n",
      "count_parameters(policy) = 283493\n",
      "(64, 2, 8)\n",
      "env = AsyncVectorEnv(64), num_envs = 64 \n",
      "\n",
      "step =    2500, scores =  0.067 ± 0.014 [ 0.000, 0.170], score_ema =  0.067, advantages =  0.602 ± 1.0 [ -4.021,  4.161], abs_actor_obj =  1.676 ± 1.630, entropy_obj = 0.641 ± 0.003, rollout_stds = 0.468 ± 0.080, critic_obj = 1.194 ± 0.343, expl_var = -278.032, kl_div = 0.0143, ppo_epochs = 10, ppo_updates = 50, grad_norm =  24.435 ± 6.7 [ 14.556, 37.609], resets = 324.52 ≥ 315, time = 54.1 \n",
      "\n",
      "step =    5000, scores =  0.070 ± 0.015 [ 0.000, 0.170], score_ema =  0.068, advantages =  0.113 ± 1.0 [ -5.063,  3.560], abs_actor_obj =  1.870 ± 2.221, entropy_obj = 0.644 ± 0.005, rollout_stds = 0.465 ± 0.086, critic_obj = 0.424 ± 0.127, expl_var = -398.774, kl_div = 0.0261, ppo_epochs = 10, ppo_updates = 50, grad_norm =  9.073 ± 2.4 [  5.482, 13.825], resets = 313.56 ≥ 305, time = 45.3 \n",
      "\n",
      "step =    7500, scores =  0.072 ± 0.016 [ 0.000, 0.200], score_ema =  0.069, advantages =  0.055 ± 1.0 [ -5.359,  4.624], abs_actor_obj =  1.893 ± 2.181, entropy_obj = 0.670 ± 0.007, rollout_stds = 0.452 ± 0.115, critic_obj = 0.170 ± 0.036, expl_var = -286.118, kl_div = 0.0387, ppo_epochs = 8, ppo_updates = 43, grad_norm =  4.083 ± 0.7 [  2.930,  5.318], resets = 305.64 ≥ 297, time = 40.5 \n",
      "\n",
      "step =   10000, scores =  0.071 ± 0.015 [ 0.000, 0.160], score_ema =  0.069, advantages = -0.005 ± 1.0 [ -7.061,  5.119], abs_actor_obj =  1.967 ± 2.292, entropy_obj = 0.705 ± 0.008, rollout_stds = 0.434 ± 0.132, critic_obj = 0.080 ± 0.011, expl_var = -162.335, kl_div = 0.0399, ppo_epochs = 9, ppo_updates = 46, grad_norm =  4.132 ± 0.9 [  2.990,  6.954], resets = 306.36 ≥ 299, time = 42.7 \n",
      "\n",
      "step =   12500, scores =  0.066 ± 0.012 [ 0.000, 0.150], score_ema =  0.068, advantages = -0.169 ± 1.0 [ -7.015,  6.425], abs_actor_obj =  2.198 ± 2.545, entropy_obj = 0.728 ± 0.008, rollout_stds = 0.420 ± 0.130, critic_obj = 0.048 ± 0.005, expl_var = -180.003, kl_div = 0.0402, ppo_epochs = 8, ppo_updates = 44, grad_norm =  3.745 ± 0.7 [  2.844,  5.355], resets = 330.19 ≥ 325, time = 41.4 \n",
      "\n",
      "step =   15000, scores =  0.066 ± 0.011 [ 0.000, 0.160], score_ema =  0.068, advantages = -0.051 ± 1.0 [ -8.226,  6.270], abs_actor_obj =  1.991 ± 2.449, entropy_obj = 0.788 ± 0.009, rollout_stds = 0.380 ± 0.132, critic_obj = 0.034 ± 0.003, expl_var = -133.464, kl_div = 0.0394, ppo_epochs = 7, ppo_updates = 39, grad_norm =  4.710 ± 1.3 [  2.466,  7.663], resets = 329.47 ≥ 324, time = 39.5 \n",
      "\n",
      "step =   17500, scores =  0.073 ± 0.013 [ 0.000, 0.150], score_ema =  0.069, advantages =  0.091 ± 1.0 [ -7.128,  6.037], abs_actor_obj =  1.804 ± 2.210, entropy_obj = 0.851 ± 0.010, rollout_stds = 0.343 ± 0.125, critic_obj = 0.030 ± 0.001, expl_var = -98.715, kl_div = 0.0390, ppo_epochs = 7, ppo_updates = 36, grad_norm =  5.214 ± 1.8 [  2.329,  8.886], resets = 299.27 ≥ 295, time = 37.8 \n",
      "\n",
      "step =   20000, scores =  0.082 ± 0.017 [ 0.000, 0.170], score_ema =  0.072, advantages =  0.110 ± 1.0 [ -6.441,  7.075], abs_actor_obj =  1.780 ± 2.081, entropy_obj = 0.844 ± 0.009, rollout_stds = 0.355 ± 0.141, critic_obj = 0.033 ± 0.002, expl_var = -78.125, kl_div = 0.0405, ppo_epochs = 8, ppo_updates = 43, grad_norm =  5.289 ± 1.7 [  3.017,  9.185], resets = 272.45 ≥ 264, time = 41.5 \n",
      "\n",
      "step =   22500, scores =  0.094 ± 0.022 [ 0.000, 0.240], score_ema =  0.078, advantages =  0.078 ± 1.0 [ -7.147,  6.911], abs_actor_obj =  1.811 ± 2.093, entropy_obj = 0.889 ± 0.009, rollout_stds = 0.342 ± 0.152, critic_obj = 0.037 ± 0.002, expl_var = -67.602, kl_div = 0.0378, ppo_epochs = 7, ppo_updates = 35, grad_norm =  5.953 ± 2.4 [  2.964, 12.577], resets = 240.59 ≥ 233, time = 37.1 \n",
      "\n",
      "step =   25000, scores =  0.102 ± 0.024 [ 0.000, 0.230], score_ema =  0.084, advantages = -0.027 ± 1.0 [ -5.651,  6.654], abs_actor_obj =  1.969 ± 2.177, entropy_obj = 0.936 ± 0.010, rollout_stds = 0.335 ± 0.172, critic_obj = 0.042 ± 0.003, expl_var = -61.627, kl_div = 0.0388, ppo_epochs = 9, ppo_updates = 47, grad_norm =  5.026 ± 1.1 [  3.481,  8.034], resets = 223.70 ≥ 215, time = 43.4 \n",
      "\n",
      "step =   27500, scores =  0.117 ± 0.026 [ 0.000, 0.270], score_ema =  0.092, advantages =  0.054 ± 1.0 [ -5.258,  6.206], abs_actor_obj =  1.838 ± 2.017, entropy_obj = 1.100 ± 0.012, rollout_stds = 0.280 ± 0.158, critic_obj = 0.045 ± 0.003, expl_var = -50.283, kl_div = 0.0385, ppo_epochs = 7, ppo_updates = 38, grad_norm =  5.308 ± 1.6 [  2.968,  9.014], resets = 196.45 ≥ 190, time = 36.9 \n",
      "\n",
      "step =   30000, scores =  0.130 ± 0.027 [ 0.000, 0.310], score_ema =  0.101, advantages =  0.086 ± 1.0 [ -5.140,  5.917], abs_actor_obj =  1.800 ± 1.967, entropy_obj = 1.309 ± 0.014, rollout_stds = 0.224 ± 0.121, critic_obj = 0.045 ± 0.001, expl_var = -41.917, kl_div = 0.0390, ppo_epochs = 4, ppo_updates = 21, grad_norm =  16.996 ± 8.2 [  5.319, 33.061], resets = 178.58 ≥ 174, time = 29.0 \n",
      "\n",
      "step =   32500, scores =  0.143 ± 0.031 [ 0.000, 0.320], score_ema =  0.112, advantages =  0.106 ± 1.0 [ -6.476,  6.820], abs_actor_obj =  1.768 ± 1.966, entropy_obj = 1.395 ± 0.015, rollout_stds = 0.202 ± 0.096, critic_obj = 0.041 ± 0.001, expl_var = -30.668, kl_div = 0.0380, ppo_epochs = 4, ppo_updates = 21, grad_norm =  13.291 ± 7.0 [  3.536, 27.237], resets = 163.45 ≥ 158, time = 28.8 \n",
      "\n",
      "step =   35000, scores =  0.152 ± 0.033 [ 0.000, 0.340], score_ema =  0.122, advantages =  0.003 ± 1.0 [ -6.162,  6.193], abs_actor_obj =  1.905 ± 2.142, entropy_obj = 1.368 ± 0.015, rollout_stds = 0.204 ± 0.093, critic_obj = 0.038 ± 0.001, expl_var = -26.778, kl_div = 0.0387, ppo_epochs = 4, ppo_updates = 20, grad_norm =  14.451 ± 6.4 [  4.509, 25.115], resets = 153.92 ≥ 148, time = 28.5 \n",
      "\n",
      "step =   37500, scores =  0.142 ± 0.028 [ 0.000, 0.280], score_ema =  0.127, advantages = -0.112 ± 1.0 [ -6.504,  5.998], abs_actor_obj =  2.094 ± 2.346, entropy_obj = 1.321 ± 0.015, rollout_stds = 0.213 ± 0.100, critic_obj = 0.031 ± 0.002, expl_var = -26.055, kl_div = 0.0417, ppo_epochs = 5, ppo_updates = 27, grad_norm =  10.677 ± 5.3 [  5.013, 23.093], resets = 163.92 ≥ 158, time = 31.6 \n",
      "\n",
      "step =   40000, scores =  0.128 ± 0.022 [ 0.000, 0.240], score_ema =  0.127, advantages = -0.278 ± 1.0 [ -7.239,  5.127], abs_actor_obj =  2.406 ± 2.675, entropy_obj = 1.340 ± 0.014, rollout_stds = 0.211 ± 0.100, critic_obj = 0.022 ± 0.002, expl_var = -25.674, kl_div = 0.0407, ppo_epochs = 6, ppo_updates = 32, grad_norm =  9.070 ± 4.1 [  4.928, 21.308], resets = 180.12 ≥ 176, time = 34.8 \n",
      "\n",
      "step =   42500, scores =  0.119 ± 0.017 [ 0.000, 0.190], score_ema =  0.125, advantages = -0.167 ± 1.0 [ -6.667,  4.919], abs_actor_obj =  2.193 ± 2.585, entropy_obj = 1.407 ± 0.014, rollout_stds = 0.200 ± 0.090, critic_obj = 0.016 ± 0.001, expl_var = -22.346, kl_div = 0.0399, ppo_epochs = 7, ppo_updates = 36, grad_norm =  9.274 ± 4.9 [  3.961, 23.141], resets = 193.19 ≥ 189, time = 36.5 \n",
      "\n",
      "step =   45000, scores =  0.113 ± 0.015 [ 0.000, 0.180], score_ema =  0.122, advantages = -0.114 ± 1.0 [ -5.932,  5.033], abs_actor_obj =  2.133 ± 2.486, entropy_obj = 1.533 ± 0.017, rollout_stds = 0.177 ± 0.073, critic_obj = 0.014 ± 0.000, expl_var = -20.315, kl_div = 0.0399, ppo_epochs = 3, ppo_updates = 19, grad_norm =  15.129 ± 7.3 [  4.196, 28.571], resets = 202.88 ≥ 199, time = 27.8 \n",
      "\n",
      "step =   47500, scores =  0.108 ± 0.014 [ 0.000, 0.170], score_ema =  0.119, advantages = -0.186 ± 1.0 [ -5.655,  5.135], abs_actor_obj =  2.276 ± 2.593, entropy_obj = 1.653 ± 0.017, rollout_stds = 0.160 ± 0.055, critic_obj = 0.015 ± 0.000, expl_var = -25.044, kl_div = 0.0375, ppo_epochs = 3, ppo_updates = 18, grad_norm =  17.209 ± 8.1 [  5.169, 33.576], resets = 211.92 ≥ 209, time = 27.4 \n",
      "\n",
      "step =   50000, scores =  0.110 ± 0.014 [ 0.000, 0.170], score_ema =  0.116, advantages =  0.109 ± 1.0 [ -5.550,  7.717], abs_actor_obj =  1.849 ± 2.053, entropy_obj = 1.732 ± 0.021, rollout_stds = 0.151 ± 0.046, critic_obj = 0.014 ± 0.000, expl_var = -22.788, kl_div = 0.0422, ppo_epochs = 3, ppo_updates = 16, grad_norm =  25.684 ± 14.4 [  4.673, 49.963], resets = 207.55 ≥ 205, time = 26.2 \n",
      "\n",
      "step =   52500, scores =  0.115 ± 0.015 [ 0.000, 0.190], score_ema =  0.116, advantages =  0.154 ± 1.0 [ -5.685,  8.211], abs_actor_obj =  1.681 ± 1.840, entropy_obj = 1.776 ± 0.020, rollout_stds = 0.146 ± 0.041, critic_obj = 0.017 ± 0.000, expl_var = -24.577, kl_div = 0.0381, ppo_epochs = 2, ppo_updates = 13, grad_norm =  28.094 ± 14.8 [  4.118, 53.934], resets = 199.41 ≥ 195, time = 24.3 \n",
      "\n",
      "step =   55000, scores =  0.121 ± 0.018 [ 0.000, 0.230], score_ema =  0.117, advantages =  0.138 ± 1.0 [ -5.318,  7.771], abs_actor_obj =  1.654 ± 1.816, entropy_obj = 1.791 ± 0.020, rollout_stds = 0.146 ± 0.042, critic_obj = 0.019 ± 0.001, expl_var = -25.302, kl_div = 0.0391, ppo_epochs = 2, ppo_updates = 12, grad_norm =  29.512 ± 15.9 [  7.106, 53.994], resets = 190.53 ≥ 185, time = 24.2 \n",
      "\n",
      "step =   57500, scores =  0.127 ± 0.021 [ 0.000, 0.240], score_ema =  0.120, advantages =  0.034 ± 1.0 [ -5.131,  7.458], abs_actor_obj =  1.823 ± 1.978, entropy_obj = 1.791 ± 0.020, rollout_stds = 0.147 ± 0.046, critic_obj = 0.020 ± 0.001, expl_var = -25.024, kl_div = 0.0384, ppo_epochs = 2, ppo_updates = 13, grad_norm =  24.129 ± 11.6 [  8.409, 45.266], resets = 182.33 ≥ 177, time = 24.6 \n",
      "\n",
      "step =   60000, scores =  0.138 ± 0.025 [ 0.000, 0.290], score_ema =  0.124, advantages =  0.039 ± 1.0 [ -5.866,  6.846], abs_actor_obj =  1.853 ± 2.027, entropy_obj = 1.782 ± 0.020, rollout_stds = 0.150 ± 0.053, critic_obj = 0.022 ± 0.001, expl_var = -20.429, kl_div = 0.0397, ppo_epochs = 2, ppo_updates = 12, grad_norm =  29.411 ± 13.4 [  6.670, 51.536], resets = 168.52 ≥ 163, time = 23.9 \n",
      "\n",
      "step =   62500, scores =  0.150 ± 0.031 [ 0.000, 0.370], score_ema =  0.131, advantages =  0.027 ± 1.0 [ -5.287,  7.014], abs_actor_obj =  1.890 ± 2.107, entropy_obj = 1.752 ± 0.021, rollout_stds = 0.155 ± 0.064, critic_obj = 0.026 ± 0.001, expl_var = -19.710, kl_div = 0.0427, ppo_epochs = 2, ppo_updates = 13, grad_norm =  25.099 ± 13.4 [  5.960, 48.375], resets = 155.91 ≥ 150, time = 24.4 \n",
      "\n",
      "step =   65000, scores =  0.160 ± 0.032 [ 0.000, 0.330], score_ema =  0.138, advantages =  0.025 ± 1.0 [ -5.112,  6.810], abs_actor_obj =  1.888 ± 2.148, entropy_obj = 1.705 ± 0.021, rollout_stds = 0.165 ± 0.081, critic_obj = 0.032 ± 0.001, expl_var = -21.746, kl_div = 0.0439, ppo_epochs = 2, ppo_updates = 13, grad_norm =  26.591 ± 15.4 [  4.782, 52.733], resets = 146.27 ≥ 142, time = 24.5 \n",
      "\n",
      "step =   67500, scores =  0.161 ± 0.031 [ 0.000, 0.340], score_ema =  0.144, advantages =  0.035 ± 1.0 [ -4.639,  6.435], abs_actor_obj =  1.875 ± 2.108, entropy_obj = 1.635 ± 0.022, rollout_stds = 0.179 ± 0.101, critic_obj = 0.038 ± 0.001, expl_var = -26.599, kl_div = 0.0447, ppo_epochs = 2, ppo_updates = 13, grad_norm =  24.124 ± 13.5 [  5.197, 48.419], resets = 145.77 ≥ 141, time = 24.5 \n",
      "\n",
      "step =   70000, scores =  0.153 ± 0.026 [ 0.000, 0.260], score_ema =  0.146, advantages = -0.007 ± 1.0 [ -5.308,  5.488], abs_actor_obj =  1.951 ± 2.156, entropy_obj = 1.580 ± 0.018, rollout_stds = 0.194 ± 0.120, critic_obj = 0.040 ± 0.001, expl_var = -30.018, kl_div = 0.0411, ppo_epochs = 2, ppo_updates = 12, grad_norm =  20.422 ± 12.4 [  6.339, 46.034], resets = 153.17 ≥ 149, time = 31.2 \n",
      "\n",
      "step =   72500, scores =  0.142 ± 0.021 [ 0.000, 0.290], score_ema =  0.145, advantages = -0.034 ± 1.0 [ -4.929,  5.571], abs_actor_obj =  2.017 ± 2.190, entropy_obj = 1.545 ± 0.017, rollout_stds = 0.204 ± 0.134, critic_obj = 0.038 ± 0.001, expl_var = -36.645, kl_div = 0.0400, ppo_epochs = 2, ppo_updates = 13, grad_norm =  19.508 ± 11.4 [  6.026, 41.862], resets = 164.27 ≥ 161, time = 24.3 \n",
      "\n",
      "step =   75000, scores =  0.132 ± 0.018 [ 0.000, 0.220], score_ema =  0.142, advantages = -0.014 ± 1.0 [ -4.470,  5.060], abs_actor_obj =  2.010 ± 2.160, entropy_obj = 1.514 ± 0.018, rollout_stds = 0.214 ± 0.146, critic_obj = 0.035 ± 0.001, expl_var = -38.107, kl_div = 0.0424, ppo_epochs = 3, ppo_updates = 17, grad_norm =  17.711 ± 11.3 [  5.982, 41.867], resets = 176.03 ≥ 173, time = 26.9 \n",
      "\n",
      "step =   77500, scores =  0.122 ± 0.015 [ 0.000, 0.180], score_ema =  0.137, advantages = -0.040 ± 1.0 [ -4.746,  5.376], abs_actor_obj =  2.062 ± 2.194, entropy_obj = 1.482 ± 0.017, rollout_stds = 0.224 ± 0.156, critic_obj = 0.031 ± 0.001, expl_var = -40.393, kl_div = 0.0419, ppo_epochs = 3, ppo_updates = 16, grad_norm =  16.243 ± 9.4 [  5.137, 35.679], resets = 189.31 ≥ 186, time = 26.8 \n",
      "\n",
      "step =   80000, scores =  0.114 ± 0.013 [ 0.000, 0.170], score_ema =  0.131, advantages =  0.002 ± 1.0 [ -4.709,  6.280], abs_actor_obj =  2.004 ± 2.122, entropy_obj = 1.450 ± 0.015, rollout_stds = 0.234 ± 0.166, critic_obj = 0.029 ± 0.001, expl_var = -43.209, kl_div = 0.0402, ppo_epochs = 2, ppo_updates = 13, grad_norm =  16.810 ± 10.7 [  4.275, 35.475], resets = 201.88 ≥ 198, time = 31.1 \n",
      "\n",
      "step =   82500, scores =  0.106 ± 0.012 [ 0.000, 0.150], score_ema =  0.125, advantages = -0.055 ± 1.0 [ -5.224,  6.735], abs_actor_obj =  2.095 ± 2.204, entropy_obj = 1.435 ± 0.016, rollout_stds = 0.240 ± 0.174, critic_obj = 0.027 ± 0.001, expl_var = -46.822, kl_div = 0.0388, ppo_epochs = 2, ppo_updates = 12, grad_norm =  15.868 ± 9.4 [  4.910, 35.774], resets = 214.25 ≥ 212, time = 24.6 \n",
      "\n",
      "step =   85000, scores =  0.101 ± 0.011 [ 0.000, 0.150], score_ema =  0.119, advantages = -0.007 ± 1.0 [ -4.500,  6.776], abs_actor_obj =  2.031 ± 2.133, entropy_obj = 1.424 ± 0.015, rollout_stds = 0.247 ± 0.182, critic_obj = 0.025 ± 0.001, expl_var = -49.997, kl_div = 0.0390, ppo_epochs = 2, ppo_updates = 12, grad_norm =  15.062 ± 9.7 [  4.474, 35.857], resets = 225.39 ≥ 222, time = 24.5 \n",
      "\n",
      "step =   87500, scores =  0.096 ± 0.010 [ 0.000, 0.130], score_ema =  0.113, advantages = -0.040 ± 1.0 [ -4.465,  6.820], abs_actor_obj =  2.080 ± 2.174, entropy_obj = 1.420 ± 0.017, rollout_stds = 0.250 ± 0.187, critic_obj = 0.024 ± 0.001, expl_var = -49.912, kl_div = 0.0439, ppo_epochs = 2, ppo_updates = 14, grad_norm =  18.434 ± 10.4 [  7.025, 40.855], resets = 236.48 ≥ 233, time = 32.4 \n",
      "\n",
      "step =   90000, scores =  0.091 ± 0.009 [ 0.000, 0.130], score_ema =  0.107, advantages = -0.036 ± 1.0 [ -4.659,  7.288], abs_actor_obj =  2.070 ± 2.183, entropy_obj = 1.413 ± 0.016, rollout_stds = 0.255 ± 0.192, critic_obj = 0.022 ± 0.001, expl_var = -53.578, kl_div = 0.0405, ppo_epochs = 2, ppo_updates = 13, grad_norm =  14.575 ± 8.1 [  4.591, 33.685], resets = 247.80 ≥ 246, time = 24.8 \n",
      "\n",
      "step =   92500, scores =  0.087 ± 0.008 [ 0.000, 0.110], score_ema =  0.102, advantages = -0.033 ± 1.0 [ -5.091,  8.262], abs_actor_obj =  2.069 ± 2.183, entropy_obj = 1.413 ± 0.016, rollout_stds = 0.257 ± 0.195, critic_obj = 0.021 ± 0.001, expl_var = -52.782, kl_div = 0.0425, ppo_epochs = 2, ppo_updates = 13, grad_norm =  13.323 ± 8.8 [  4.640, 34.884], resets = 257.50 ≥ 255, time = 25.1 \n",
      "\n",
      "step =   95000, scores =  0.083 ± 0.008 [ 0.000, 0.120], score_ema =  0.098, advantages = -0.026 ± 1.0 [ -4.682,  6.129], abs_actor_obj =  2.057 ± 2.179, entropy_obj = 1.436 ± 0.016, rollout_stds = 0.252 ± 0.192, critic_obj = 0.019 ± 0.000, expl_var = -52.948, kl_div = 0.0421, ppo_epochs = 2, ppo_updates = 13, grad_norm =  16.340 ± 9.4 [  4.347, 36.981], resets = 267.83 ≥ 265, time = 25.4 \n",
      "\n",
      "step =   97500, scores =  0.080 ± 0.007 [ 0.000, 0.110], score_ema =  0.093, advantages = -0.042 ± 1.0 [ -4.264,  5.712], abs_actor_obj =  2.080 ± 2.199, entropy_obj = 1.460 ± 0.018, rollout_stds = 0.248 ± 0.191, critic_obj = 0.017 ± 0.000, expl_var = -50.255, kl_div = 0.0446, ppo_epochs = 2, ppo_updates = 11, grad_norm =  18.539 ± 11.2 [  4.284, 39.356], resets = 277.98 ≥ 275, time = 30.6 \n",
      "\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "model db closed\n",
      "done\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "\n",
    "# policy_db = TinyModelDB[MitosisPolicyInfo](base_path=f'saved_models/rl/{env_name}')\n",
    "# policy_db.load_model_state_dict(policy, model_id='2024-05-24_16.15.39')\n",
    "\n",
    "try:\n",
    "    if 'render_fps' not in record_env.metadata:\n",
    "        record_env.metadata['render_fps'] = 30\n",
    "    record_env = wrap_env(record_env)\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    record_env, _ = as_vec_env(record_env)\n",
    "    \n",
    "    policy.reset_sde_noise(1)\n",
    "    \n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            actions_dist, _ = policy.process_obs(torch.tensor(obs, device=device))\n",
    "            actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(5_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T19:06:27.651938Z",
     "start_time": "2024-05-26T19:05:51.003662Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closing record_env\n",
      "record_env closed\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bba6ab51a61dd845",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
