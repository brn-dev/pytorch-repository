{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.gym.envs.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.a2c.a2c import A2C\n",
    "from src.reinforcement_learning.ppo.ppo import PPO\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.envs.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.core.rl_base import RLBase\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T14:35:40.545736Z",
     "start_time": "2024-04-20T14:35:37.674754Z"
    }
   },
   "id": "ba8c59a3eba2f172"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "env = AsyncVectorEnv(128)\n",
      "count_parameters(policy) = 84357\n",
      "keyboard interrupt\n",
      "closing envs\n",
      "envs closed\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def init_policy(continuous_actions: bool, actions_std: Optional[float]):\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # in_size = 4\n",
    "            # shared_out_sizes = [64, 64]\n",
    "            # actor_out_sizes = [64, 2]\n",
    "            # critic_out_sizes = [64, 1]\n",
    "\n",
    "            # in_size = 24\n",
    "            # shared_out_sizes = [64, 128, 128]\n",
    "            # actor_out_sizes = [128, 64, 4]\n",
    "            # critic_out_sizes = [128, 64, 1]\n",
    "\n",
    "            in_size = 8\n",
    "            shared_out_sizes = [128, 128]\n",
    "            actor_out_sizes = [128, 128, 4]\n",
    "            critic_out_sizes = [128, 128, 1]\n",
    "\n",
    "            self.shared = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.CELU() if not is_last_layer else nn.CELU()\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=shared_out_sizes\n",
    "            )\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.CELU() if not is_last_layer else nn.Tanh()\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.CELU() if not is_last_layer else nn.Identity()\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            shared_out = self.shared(x)\n",
    "\n",
    "            return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), continuous_actions, actions_std)\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any]):\n",
    "    \n",
    "    if 'unnormalized_rewards' in info['rollout']:\n",
    "        unnormalized_rewards = info['rollout']['unnormalized_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards[:len(unnormalized_rewards)]),\n",
    "            rewards=unnormalized_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(unnormalized_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_start_gamma_1_returns = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_start_gamma_1_returns, \n",
    "        mean_format=' 6.1f',\n",
    "        std_format='4.1f',\n",
    "        min_value_format=' 6.1f',\n",
    "        max_value_format='5.1f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        info['actor_objective_unreduced'].abs() * rl.actor_objective_weight,  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['weighted_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = np.mean([round(r) for r in rl.buffer.episode_starts.astype(int).sum(axis=0)])\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f'scores = {scores}, '\n",
    "          f\"advantages = {advantages}, \"\n",
    "          f\"abs_actor_obj = {abs_actor_obj}, \"\n",
    "          f\"critic_obj = {critic_obj}, \"\n",
    "          f\"{resets = :.2f}\")\n",
    "    \n",
    "    # for param_name, param_grad in get_gradients_per_parameter(rl.policy, param_type='weight'):\n",
    "    #     print(f'{param_name + \".grad\":<50}: ' + format_summary_statics(\n",
    "    #         param_grad,\n",
    "    #         mean_format=' 8.5f',\n",
    "    #         std_format='.5f',\n",
    "    #         min_value_format=' 8.5f',\n",
    "    #         max_value_format='7.5f',\n",
    "    #     ))\n",
    "    # \n",
    "    # print('\\n')\n",
    "\n",
    "\n",
    "device = set_default_torch_device(\"cuda:0\") if True else set_default_torch_device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "# env = parallelize_env_async(lambda: gym.make(\"CartPole-v1\", max_episode_steps=1000, render_mode='rgb_array'), 16)\n",
    "# env = parallelize_env_async(lambda: StepSkipWrapper(gym.make(\"BipedalWalker-v3\", max_episode_steps=3000, render_mode='rgb_array'), steps_per_step=5), 8)\n",
    "env = parallelize_env_async(lambda: gym.make(\"LunarLander-v2\", render_mode='rgb_array'), 128)\n",
    "print(f'{env = }')\n",
    "    \n",
    "try:\n",
    "    # policy = init_policy(continuous_actions=False, actions_std=None)\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    \n",
    "    gamma = 0.995\n",
    "    \n",
    "    env = NormalizeRewardWrapper(env, gamma=gamma)\n",
    "    \n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters(), lr=1e-3),\n",
    "        buffer_size=1500,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        actor_objective_weight=1.0,\n",
    "        critic_objective_weight=1.0,\n",
    "        ppo_epochs=3,\n",
    "        ppo_batch_size=300,\n",
    "        action_ratio_clip_range=0.2,\n",
    "        log_unreduced=True,\n",
    "        callback=Callback(on_optimization_done=on_optimization_done)\n",
    "    ).train(5_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T21:27:24.653576Z",
     "start_time": "2024-04-20T21:27:07.867381Z"
    }
   },
   "id": "f71efe062771e81b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-0.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-1.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-2.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-3.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-4.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-5.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-5.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-6.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-6.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-6.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-7.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-7.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-7.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-8.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-9.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-9.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-9.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-10.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-10.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-10.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-11.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-11.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-11.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-12.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-12.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-12.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-13.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-13.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-20.1\\rl-video-episode-13.mp4\n"
     ]
    }
   ],
   "source": [
    "record_env = env.env_fns[0]()\n",
    "try:\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=r'C:\\Users\\domin\\Videos\\rl\\2024-04-20.1', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            actions_dist = policy.predict_actions(obs)\n",
    "            actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(10000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T21:28:24.406663Z",
     "start_time": "2024-04-20T21:27:26.858715Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(lambda: gym.make('CartPole-v1', render_mode='rgb_array'), n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=2)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266d91aeee84c4cd",
   "execution_count": 30
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
