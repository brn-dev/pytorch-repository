{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from src.module_analysis import count_parameters\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.gym.envs.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.a2c.a2c import A2C\n",
    "from src.reinforcement_learning.ppo.ppo import PPO\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.envs.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.core.rl_base import RLBase\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T23:28:07.108594Z",
     "start_time": "2024-04-19T23:28:07.013064Z"
    }
   },
   "id": "ba8c59a3eba2f172"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "env = AsyncVectorEnv(32)\n",
      "256133\n",
      "step =    1500, scores = -176.3 ± 108.2 [-540.9,  42.3], advantages = -0.358 ± 1.0 [-11.170, 10.815], abs_actor_obj =  0.648 ± 0.850, critic_obj = 0.443 ± 0.178, resets = 15.88\n",
      "step =    3000, scores = -147.8 ±  82.5 [-561.7,  29.2], advantages =  0.264 ± 1.0 [ -7.975, 10.013], abs_actor_obj =  0.677 ± 0.800, critic_obj = 0.249 ± 0.033, resets = 15.25\n",
      "step =    4500, scores = -141.6 ±  87.1 [-536.7,  22.8], advantages = -0.005 ± 1.0 [ -7.378, 10.583], abs_actor_obj =  0.626 ± 0.792, critic_obj = 0.235 ± 0.016, resets = 14.25\n",
      "step =    6000, scores = -131.9 ±  92.4 [-500.0,  54.9], advantages =  0.143 ± 1.0 [ -8.325,  9.710], abs_actor_obj =  0.649 ± 0.793, critic_obj = 0.220 ± 0.025, resets = 13.38\n",
      "step =    7500, scores = -126.7 ±  84.4 [-398.1,  66.2], advantages = -0.033 ± 1.0 [ -8.579, 10.242], abs_actor_obj =  0.645 ± 0.773, critic_obj = 0.191 ± 0.017, resets = 12.59\n",
      "step =    9000, scores = -124.1 ±  91.4 [-506.6,  65.0], advantages = -0.142 ± 1.0 [ -7.939, 12.280], abs_actor_obj =  0.672 ± 0.759, critic_obj = 0.187 ± 0.032, resets = 11.59\n",
      "step =   10500, scores =  -97.1 ±  81.1 [-362.3,  60.8], advantages =  0.009 ± 1.0 [ -8.883,  8.430], abs_actor_obj =  0.641 ± 0.773, critic_obj = 0.161 ± 0.012, resets = 10.06\n",
      "step =   12000, scores =  -95.8 ±  87.6 [-352.2,  71.6], advantages =  0.263 ± 1.0 [ -9.986, 10.358], abs_actor_obj =  0.665 ± 0.801, critic_obj = 0.158 ± 0.023, resets = 8.91\n",
      "step =   13500, scores =  -88.6 ±  76.0 [-272.1,  84.3], advantages = -0.162 ± 1.0 [-10.765,  9.148], abs_actor_obj =  0.636 ± 0.792, critic_obj = 0.131 ± 0.007, resets = 7.62\n",
      "step =   15000, scores =  -83.8 ±  82.2 [-351.2, 100.9], advantages =  0.029 ± 1.0 [-11.377,  9.239], abs_actor_obj =  0.593 ± 0.810, critic_obj = 0.116 ± 0.012, resets = 6.53\n",
      "step =   16500, scores =  -49.6 ±  57.7 [-263.1,  80.5], advantages =  0.068 ± 1.0 [-13.164,  9.608], abs_actor_obj =  0.598 ± 0.816, critic_obj = 0.105 ± 0.014, resets = 6.00\n",
      "step =   18000, scores =  -53.3 ±  66.4 [-274.7,  91.1], advantages =  0.042 ± 1.0 [-13.211, 10.426], abs_actor_obj =  0.602 ± 0.802, critic_obj = 0.097 ± 0.008, resets = 5.38\n",
      "step =   19500, scores =  -52.5 ±  61.2 [-281.6,  97.1], advantages =  0.014 ± 1.0 [-14.127,  8.999], abs_actor_obj =  0.601 ± 0.802, critic_obj = 0.087 ± 0.011, resets = 4.88\n",
      "step =   21000, scores =  -56.8 ±  59.7 [-239.3, 100.5], advantages = -0.148 ± 1.0 [-14.698,  9.210], abs_actor_obj =  0.617 ± 0.799, critic_obj = 0.084 ± 0.010, resets = 4.66\n",
      "step =   22500, scores =  -54.2 ±  68.7 [-303.9,  88.3], advantages =  0.036 ± 1.0 [-15.009, 11.388], abs_actor_obj =  0.611 ± 0.794, critic_obj = 0.082 ± 0.009, resets = 4.31\n",
      "step =   24000, scores =  -41.3 ±  60.4 [-272.5, 147.6], advantages =  0.069 ± 1.0 [-13.169,  8.493], abs_actor_obj =  0.599 ± 0.806, critic_obj = 0.088 ± 0.006, resets = 4.53\n",
      "step =   25500, scores =  -38.8 ±  57.2 [-184.1,  99.9], advantages = -0.085 ± 1.0 [-17.308,  8.441], abs_actor_obj =  0.605 ± 0.800, critic_obj = 0.073 ± 0.008, resets = 3.66\n",
      "step =   27000, scores =  -26.2 ±  67.1 [-215.8, 151.8], advantages =  0.050 ± 1.0 [-19.231,  9.951], abs_actor_obj =  0.584 ± 0.826, critic_obj = 0.067 ± 0.009, resets = 3.12\n",
      "step =   28500, scores =  -42.2 ±  59.2 [-220.2, 106.2], advantages = -0.192 ± 1.0 [-16.634,  7.101], abs_actor_obj =  0.610 ± 0.811, critic_obj = 0.083 ± 0.006, resets = 3.94\n",
      "step =   30000, scores =  -41.4 ±  77.9 [-267.1, 151.0], advantages =  0.162 ± 1.0 [-14.899, 10.096], abs_actor_obj =  0.653 ± 0.776, critic_obj = 0.077 ± 0.007, resets = 3.19\n",
      "step =   31500, scores =  -33.4 ±  61.7 [-243.0, 110.6], advantages = -0.033 ± 1.0 [-15.396, 10.088], abs_actor_obj =  0.600 ± 0.804, critic_obj = 0.078 ± 0.017, resets = 3.34\n",
      "step =   33000, scores =  -42.3 ±  73.8 [-261.6, 156.3], advantages = -0.018 ± 1.0 [-18.009,  9.150], abs_actor_obj =  0.617 ± 0.785, critic_obj = 0.076 ± 0.008, resets = 3.50\n",
      "step =   34500, scores =  -41.3 ±  66.0 [-298.7, 243.5], advantages = -0.046 ± 1.0 [-18.255, 14.306], abs_actor_obj =  0.598 ± 0.813, critic_obj = 0.082 ± 0.003, resets = 3.47\n",
      "step =   36000, scores =  -19.7 ±  59.7 [-134.2, 152.3], advantages = -0.029 ± 1.0 [-18.901,  8.434], abs_actor_obj =  0.581 ± 0.822, critic_obj = 0.077 ± 0.015, resets = 2.94\n",
      "step =   37500, scores =  -23.9 ±  68.7 [-158.4, 184.4], advantages = -0.071 ± 1.0 [-19.113,  9.817], abs_actor_obj =  0.619 ± 0.794, critic_obj = 0.072 ± 0.012, resets = 2.97\n",
      "step =   39000, scores =  -27.6 ±  73.5 [-215.0, 141.6], advantages =  0.034 ± 1.0 [-19.807, 10.808], abs_actor_obj =  0.592 ± 0.809, critic_obj = 0.069 ± 0.007, resets = 2.75\n",
      "step =   40500, scores =  -25.6 ±  72.6 [-237.4, 134.1], advantages = -0.162 ± 1.0 [-17.228,  8.256], abs_actor_obj =  0.594 ± 0.820, critic_obj = 0.076 ± 0.012, resets = 3.00\n",
      "step =   42000, scores =  -21.8 ±  68.3 [-216.7, 132.0], advantages =  0.039 ± 1.0 [-18.540, 10.962], abs_actor_obj =  0.604 ± 0.804, critic_obj = 0.067 ± 0.010, resets = 2.66\n",
      "step =   43500, scores =  -27.0 ±  63.9 [-199.9, 151.7], advantages = -0.090 ± 1.0 [-18.919, 10.277], abs_actor_obj =  0.596 ± 0.805, critic_obj = 0.082 ± 0.011, resets = 3.22\n",
      "step =   45000, scores =  -12.7 ±  87.5 [-259.2, 161.0], advantages = -0.106 ± 1.0 [-20.497, 10.948], abs_actor_obj =  0.619 ± 0.805, critic_obj = 0.062 ± 0.013, resets = 2.16\n",
      "step =   46500, scores =  -10.6 ±  67.1 [-154.9, 211.7], advantages = -0.302 ± 1.0 [-20.617,  8.559], abs_actor_obj =  0.661 ± 0.817, critic_obj = 0.068 ± 0.016, resets = 2.50\n",
      "step =   48000, scores =  -16.6 ±  74.7 [-238.3, 128.4], advantages = -0.049 ± 1.0 [-19.457,  8.776], abs_actor_obj =  0.629 ± 0.786, critic_obj = 0.062 ± 0.014, resets = 2.41\n",
      "step =   49500, scores =   -8.6 ±  78.6 [-215.5, 199.4], advantages = -0.143 ± 1.0 [-19.998,  8.734], abs_actor_obj =  0.645 ± 0.778, critic_obj = 0.065 ± 0.009, resets = 2.22\n",
      "step =   51000, scores =  -16.5 ±  77.0 [-211.1, 170.3], advantages = -0.103 ± 1.0 [-21.887, 14.103], abs_actor_obj =  0.641 ± 0.775, critic_obj = 0.071 ± 0.009, resets = 2.34\n",
      "step =   52500, scores =  -27.0 ±  67.8 [-192.5, 103.8], advantages = -0.118 ± 1.0 [-21.835, 11.385], abs_actor_obj =  0.650 ± 0.768, critic_obj = 0.053 ± 0.009, resets = 1.91\n",
      "step =   54000, scores =  -10.0 ±  83.4 [-238.5, 141.1], advantages = -0.153 ± 1.0 [-21.659, 13.668], abs_actor_obj =  0.629 ± 0.789, critic_obj = 0.057 ± 0.011, resets = 1.97\n",
      "step =   55500, scores =   -1.7 ±  64.7 [-153.3, 185.9], advantages = -0.013 ± 1.0 [-20.943, 12.011], abs_actor_obj =  0.596 ± 0.802, critic_obj = 0.062 ± 0.013, resets = 2.28\n",
      "step =   57000, scores =    4.2 ±  74.0 [-183.7, 166.5], advantages =  0.018 ± 1.0 [-20.760,  9.535], abs_actor_obj =  0.608 ± 0.788, critic_obj = 0.056 ± 0.009, resets = 2.03\n",
      "step =   58500, scores =   -4.3 ±  64.4 [-173.0, 114.6], advantages =  0.240 ± 1.0 [-22.279, 12.736], abs_actor_obj =  0.660 ± 0.788, critic_obj = 0.065 ± 0.024, resets = 2.22\n",
      "step =   60000, scores =  -12.7 ±  80.5 [-182.9, 165.5], advantages = -0.156 ± 1.0 [-23.629, 10.663], abs_actor_obj =  0.641 ± 0.789, critic_obj = 0.064 ± 0.013, resets = 2.22\n",
      "step =   61500, scores =   -7.0 ±  66.5 [-139.9, 196.4], advantages = -0.225 ± 1.0 [-22.613, 12.440], abs_actor_obj =  0.642 ± 0.798, critic_obj = 0.065 ± 0.011, resets = 2.28\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def init_policy(continuous_actions: bool, actions_std: Optional[float]):\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # in_size = 4\n",
    "            # shared_out_sizes = [64, 64]\n",
    "            # actor_out_sizes = [64, 2]\n",
    "            # critic_out_sizes = [64, 1]\n",
    "\n",
    "            # in_size = 24\n",
    "            # shared_out_sizes = [64, 128, 128]\n",
    "            # actor_out_sizes = [128, 64, 4]\n",
    "            # critic_out_sizes = [128, 64, 1]\n",
    "\n",
    "            in_size = 8\n",
    "            shared_out_sizes = [64, 128, 256]\n",
    "            actor_out_sizes = [256, 128, 64, 4]\n",
    "            critic_out_sizes = [256, 128, 64, 1]\n",
    "\n",
    "            self.shared = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.CELU() if not is_last_layer else nn.CELU()\n",
    "                ),\n",
    "                in_size=in_size,\n",
    "                out_sizes=shared_out_sizes\n",
    "            )\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.CELU() if not is_last_layer else nn.Tanh()\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.CELU() if not is_last_layer else nn.Identity()\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            shared_out = self.shared(x)\n",
    "\n",
    "            return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), continuous_actions, actions_std)\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any]):\n",
    "    \n",
    "    if 'unnormalized_rewards' in info['rollout']:\n",
    "        unnormalized_rewards = info['rollout']['unnormalized_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards),\n",
    "            rewards=unnormalized_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(unnormalized_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_start_gamma_1_returns = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_start_gamma_1_returns, \n",
    "        mean_format=' 6.1f',\n",
    "        std_format='4.1f',\n",
    "        min_value_format=' 6.1f',\n",
    "        max_value_format='5.1f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        info['actor_objective_unreduced'].abs(),  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['weighted_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = np.mean([round(r) for r in rl.buffer.episode_starts.astype(int).sum(axis=0)])\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f'scores = {scores}, '\n",
    "          f\"advantages = {advantages}, \"\n",
    "          f\"abs_actor_obj = {abs_actor_obj}, \"\n",
    "          f\"critic_obj = {critic_obj}, \"\n",
    "          f\"{resets = :.2f}\")\n",
    "\n",
    "\n",
    "device = set_default_torch_device(\"cuda:0\") if True else set_default_torch_device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "# env = parallelize_env_async(lambda: gym.make(\"CartPole-v1\", max_episode_steps=1000, render_mode='rgb_array'), 16)\n",
    "# env = parallelize_env_async(lambda: StepSkipWrapper(gym.make(\"BipedalWalker-v3\", max_episode_steps=3000, render_mode='rgb_array'), steps_per_step=5), 8)\n",
    "env = parallelize_env_async(lambda: gym.make(\"LunarLander-v2\", render_mode=None), 32)\n",
    "# env = gym.make(\"LunarLander-v2\", render_mode=None)\n",
    "print(f'{env = }')\n",
    "\n",
    "# policy = init_policy(continuous_actions=False, actions_std=None)\n",
    "print(count_parameters(policy))\n",
    "    \n",
    "try:\n",
    "    gamma = 0.9\n",
    "    \n",
    "    env = NormalizeRewardWrapper(env, gamma=gamma)\n",
    "    \n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters()),\n",
    "        buffer_size=1500,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        critic_objective_weight=0.5,\n",
    "        ppo_epochs=3,\n",
    "        ppo_batch_size=300,\n",
    "        action_ratio_clip_range=0.2,\n",
    "        log_unreduced=True,\n",
    "        callback=Callback(on_optimization_done=on_optimization_done)\n",
    "    ).train(1_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    env.close()\n",
    "    print('keyboard interrupt')\n",
    "except Exception as e:\n",
    "    env.close()\n",
    "    raise e\n",
    "\n",
    "if not env.closed:\n",
    "    env.close()\n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-19T23:37:28.636093Z"
    }
   },
   "id": "f71efe062771e81b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-0.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-1.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-2.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-3.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-4.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-4.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-5.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-5.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-5.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-6.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-6.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-6.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-7.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-7.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-7.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-8.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-9.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-9.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-9.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-10.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-10.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-10.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-11.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-11.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-18.6\\rl-video-episode-11.mp4\n"
     ]
    }
   ],
   "source": [
    "record_env = env.env_fns[0]()\n",
    "record_env.render_mode = 'rgb_array'\n",
    "record_env = AutoResetWrapper(\n",
    "    RecordVideo(record_env, video_folder=r'C:\\Users\\domin\\Videos\\rl\\2024-04-18.6', episode_trigger=lambda ep_nr: True)\n",
    ")\n",
    "def record(max_steps: int):\n",
    "    obs, info = record_env.reset()\n",
    "    for step in range(max_steps):\n",
    "        actions_dist = policy.predict_actions(obs)\n",
    "        actions = actions_dist.sample().detach().cpu().numpy()\n",
    "        obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "\n",
    "record(10000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-19T16:25:03.799306Z",
     "start_time": "2024-04-19T16:24:26.743031Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(lambda: gym.make('CartPole-v1', render_mode='rgb_array'), n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=2)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266d91aeee84c4cd",
   "execution_count": 30
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
