{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from gymnasium import Env\n",
    "\n",
    "from src.datetime import get_current_timestamp\n",
    "from src.module_analysis import count_parameters, get_gradients_per_parameter\n",
    "from src.moving_averages import ExponentialMovingAverage\n",
    "from src.networks.core.tensor_shape import TensorShape\n",
    "from src.networks.core.torch_wrappers.torch_net import TorchNet\n",
    "from src.reinforcement_learning.core.generalized_advantage_estimate import compute_gae_and_returns\n",
    "from src.reinforcement_learning.gym.envs.normalize_reward_wrapper import NormalizeRewardWrapper\n",
    "from src.stopwatch import Stopwatch\n",
    "from src.summary_statistics import format_summary_statics\n",
    "from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n",
    "from typing import Any, SupportsFloat, Optional\n",
    "from gymnasium.wrappers import RecordVideo, AutoResetWrapper, NormalizeReward, TransformReward, TransformObservation\n",
    "from src.reinforcement_learning.core.callback import Callback\n",
    "from src.reinforcement_learning.a2c.a2c import A2C\n",
    "from src.reinforcement_learning.ppo.ppo import PPO\n",
    "from src.reinforcement_learning.core.normalization import NormalizationType\n",
    "from src.reinforcement_learning.gym.envs.step_skip_wrapper import StepSkipWrapper\n",
    "from src.reinforcement_learning.core.rl_base import RLBase\n",
    "from src.torch_device import set_default_torch_device\n",
    "from src.reinforcement_learning.gym.envs.parallelize_env import parallelize_env_async\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from src.networks.core.seq_net import SeqNet\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T20:52:32.336954Z",
     "start_time": "2024-04-26T20:52:28.754775Z"
    }
   },
   "id": "ba8c59a3eba2f172"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "env = AsyncVectorEnv(32), num_envs = 32\n",
      "count_parameters(policy) = 72681\n",
      "envs reset\n",
      "step =    2500, scores =  -11.9 ± 12.6 [ -43.1,  18.0], score_ema =  -11.9, advantages = -0.513 ± 1.0 [ -5.048,  3.141], abs_actor_obj =  5.739 ± 5.978, critic_obj = 10.822 ± 1.972, resets = 2.00 ≥ 2, time = 20.6\n",
      "step =    5000, scores =   -9.1 ± 14.5 [ -54.8,  20.7], score_ema =  -11.7, advantages = -0.456 ± 1.0 [ -4.344,  3.358], abs_actor_obj =  5.536 ± 5.707, critic_obj = 12.053 ± 2.668, resets = 3.00 ≥ 3, time = 16.6\n",
      "step =    7500, scores =   -4.3 ± 11.9 [ -35.0,  24.9], score_ema =  -10.9, advantages = -0.289 ± 1.0 [ -3.999,  3.322], abs_actor_obj =  4.705 ± 5.360, critic_obj = 10.351 ± 1.789, resets = 2.00 ≥ 2, time = 16.2\n",
      "step =   10000, scores =   -7.2 ± 13.1 [ -44.1,  21.1], score_ema =  -10.6, advantages = -0.351 ± 1.0 [ -3.686,  3.388], abs_actor_obj =  5.035 ± 5.318, critic_obj = 11.478 ± 1.550, resets = 3.00 ≥ 3, time = 16.3\n",
      "step =   12500, scores =   -4.5 ± 13.8 [ -37.4,  32.5], score_ema =   -9.9, advantages = -0.164 ± 1.0 [ -3.848,  3.968], abs_actor_obj =  4.232 ± 4.794, critic_obj = 11.036 ± 2.437, resets = 2.00 ≥ 2, time = 16.2\n",
      "step =   15000, scores =   -4.6 ± 11.3 [ -47.0,  22.6], score_ema =   -9.4, advantages = -0.256 ± 1.0 [ -4.058,  3.660], abs_actor_obj =  4.569 ± 5.102, critic_obj = 8.740 ± 1.988, resets = 3.00 ≥ 3, time = 16.8\n",
      "step =   17500, scores =   -4.2 ± 13.4 [ -46.0,  23.1], score_ema =   -8.9, advantages = -0.194 ± 1.0 [ -5.036,  3.101], abs_actor_obj =  4.322 ± 5.026, critic_obj = 9.834 ± 1.270, resets = 2.00 ≥ 2, time = 15.0\n",
      "step =   20000, scores =   -2.0 ± 13.2 [ -41.9,  40.7], score_ema =   -8.2, advantages = -0.082 ± 1.0 [ -3.379,  3.654], abs_actor_obj =  3.871 ± 4.713, critic_obj = 9.502 ± 0.526, resets = 3.00 ≥ 3, time = 13.6\n",
      "step =   22500, scores =    0.7 ±  9.4 [ -17.0,  27.1], score_ema =   -7.3, advantages =  0.035 ± 1.0 [ -3.444,  3.628], abs_actor_obj =  3.493 ± 4.233, critic_obj = 7.424 ± 1.062, resets = 2.00 ≥ 2, time = 14.0\n",
      "step =   25000, scores =    1.2 ± 10.7 [ -32.3,  33.6], score_ema =   -6.5, advantages =  0.075 ± 1.0 [ -3.415,  4.083], abs_actor_obj =  3.286 ± 4.045, critic_obj = 7.813 ± 0.649, resets = 3.00 ≥ 3, time = 15.4\n",
      "step =   27500, scores =    3.9 ± 11.1 [ -20.1,  39.7], score_ema =   -5.4, advantages =  0.192 ± 1.0 [ -3.793,  4.462], abs_actor_obj =  2.975 ± 3.772, critic_obj = 8.347 ± 1.210, resets = 2.00 ≥ 2, time = 17.2\n",
      "step =   30000, scores =    5.2 ± 10.4 [ -12.6,  38.0], score_ema =   -4.3, advantages =  0.410 ± 1.0 [ -2.849,  5.337], abs_actor_obj =  2.435 ± 2.994, critic_obj = 7.756 ± 1.779, resets = 3.00 ≥ 3, time = 15.0\n",
      "step =   32500, scores =    8.1 ± 10.6 [ -12.1,  33.2], score_ema =   -3.1, advantages =  0.508 ± 1.0 [ -3.529,  4.226], abs_actor_obj =  2.222 ± 2.863, critic_obj = 7.862 ± 1.244, resets = 2.00 ≥ 2, time = 15.2\n",
      "step =   35000, scores =    5.8 ± 11.0 [ -19.9,  42.8], score_ema =   -2.2, advantages =  0.409 ± 1.0 [ -3.312,  4.967], abs_actor_obj =  2.403 ± 3.177, critic_obj = 7.272 ± 1.324, resets = 3.00 ≥ 3, time = 15.3\n",
      "step =   37500, scores =   10.0 ± 11.5 [ -18.3,  36.3], score_ema =   -1.0, advantages =  0.596 ± 1.0 [ -2.983,  4.821], abs_actor_obj =  2.152 ± 2.812, critic_obj = 9.428 ± 1.358, resets = 2.00 ≥ 2, time = 14.6\n",
      "step =   40000, scores =    8.3 ± 10.8 [  -5.2,  42.9], score_ema =   -0.1, advantages =  0.498 ± 1.0 [ -2.919,  5.052], abs_actor_obj =  2.265 ± 2.773, critic_obj = 7.360 ± 2.357, resets = 3.00 ≥ 3, time = 14.4\n",
      "step =   42500, scores =    9.9 ±  6.8 [  -4.1,  26.6], score_ema =    0.9, advantages =  0.686 ± 1.0 [ -3.133,  5.135], abs_actor_obj =  1.993 ± 2.310, critic_obj = 7.637 ± 2.478, resets = 2.00 ≥ 2, time = 14.0\n",
      "step =   45000, scores =    9.3 ± 10.7 [ -22.6,  35.2], score_ema =    1.8, advantages =  0.656 ± 1.0 [ -3.654,  4.797], abs_actor_obj =  2.097 ± 2.624, critic_obj = 7.671 ± 1.799, resets = 3.00 ≥ 3, time = 14.0\n",
      "step =   47500, scores =   10.7 ±  9.2 [  -9.5,  41.3], score_ema =    2.7, advantages =  0.670 ± 1.0 [ -3.090,  4.925], abs_actor_obj =  1.984 ± 2.455, critic_obj = 7.668 ± 1.354, resets = 2.00 ≥ 2, time = 14.2\n",
      "step =   50000, scores =   11.3 ± 11.9 [  -9.8,  39.9], score_ema =    3.5, advantages =  0.752 ± 1.0 [ -2.909,  4.551], abs_actor_obj =  1.936 ± 2.390, critic_obj = 8.306 ± 2.065, resets = 3.00 ≥ 3, time = 13.8\n",
      "step =   52500, scores =   11.7 ±  8.8 [ -12.1,  28.5], score_ema =    4.3, advantages =  0.728 ± 1.0 [ -4.194,  4.641], abs_actor_obj =  2.001 ± 2.582, critic_obj = 7.898 ± 1.146, resets = 2.00 ≥ 2, time = 14.0\n",
      "step =   55000, scores =   12.2 ± 11.4 [ -20.4,  36.3], score_ema =    5.1, advantages =  0.788 ± 1.0 [ -3.721,  4.638], abs_actor_obj =  1.940 ± 2.458, critic_obj = 8.375 ± 2.386, resets = 3.00 ≥ 3, time = 14.1\n",
      "step =   57500, scores =   14.3 ±  9.2 [  -7.1,  35.6], score_ema =    6.1, advantages =  0.889 ± 1.0 [ -2.785,  5.129], abs_actor_obj =  1.846 ± 1.902, critic_obj = 7.947 ± 1.721, resets = 2.00 ≥ 2, time = 13.9\n",
      "step =   60000, scores =   13.2 ± 11.8 [  -3.6,  42.6], score_ema =    6.8, advantages =  0.964 ± 1.0 [ -2.453,  4.947], abs_actor_obj =  1.785 ± 1.803, critic_obj = 9.118 ± 1.201, resets = 3.00 ≥ 3, time = 14.2\n",
      "step =   62500, scores =   13.9 ± 10.3 [  -8.3,  38.4], score_ema =    7.5, advantages =  0.912 ± 1.0 [ -2.968,  5.006], abs_actor_obj =  1.816 ± 1.944, critic_obj = 8.283 ± 2.509, resets = 2.00 ≥ 2, time = 14.1\n",
      "step =   65000, scores =   11.7 ± 12.0 [ -13.9,  49.6], score_ema =    7.9, advantages =  0.876 ± 1.0 [ -2.973,  5.420], abs_actor_obj =  1.840 ± 2.036, critic_obj = 7.469 ± 1.821, resets = 3.00 ≥ 3, time = 13.7\n",
      "step =   67500, scores =   13.7 ±  8.2 [  -4.1,  31.8], score_ema =    8.5, advantages =  0.931 ± 1.0 [ -2.941,  4.845], abs_actor_obj =  1.879 ± 2.014, critic_obj = 8.031 ± 2.227, resets = 2.00 ≥ 2, time = 13.8\n",
      "step =   70000, scores =   14.0 ± 12.1 [  -0.3,  40.0], score_ema =    9.0, advantages =  1.021 ± 1.0 [ -3.509,  5.357], abs_actor_obj =  1.802 ± 1.829, critic_obj = 8.734 ± 1.858, resets = 3.00 ≥ 3, time = 13.5\n",
      "step =   72500, scores =   15.5 ±  8.7 [  -2.3,  43.1], score_ema =    9.7, advantages =  0.986 ± 1.0 [ -2.629,  5.050], abs_actor_obj =  1.795 ± 1.755, critic_obj = 8.988 ± 2.299, resets = 2.00 ≥ 2, time = 13.5\n",
      "step =   75000, scores =   14.6 ± 13.2 [ -10.8,  42.8], score_ema =   10.2, advantages =  0.986 ± 1.0 [ -2.850,  5.361], abs_actor_obj =  1.858 ± 2.065, critic_obj = 8.757 ± 3.268, resets = 3.00 ≥ 3, time = 13.8\n",
      "step =   77500, scores =   16.0 ±  9.3 [  -2.6,  38.3], score_ema =   10.8, advantages =  1.112 ± 1.0 [ -2.604,  4.563], abs_actor_obj =  1.852 ± 1.687, critic_obj = 9.060 ± 3.081, resets = 2.00 ≥ 2, time = 13.6\n",
      "step =   80000, scores =   15.1 ± 13.0 [  -5.1,  42.0], score_ema =   11.2, advantages =  1.032 ± 1.0 [ -3.096,  5.030], abs_actor_obj =  1.810 ± 1.738, critic_obj = 9.111 ± 2.430, resets = 3.00 ≥ 3, time = 14.0\n",
      "step =   82500, scores =   17.7 ± 10.1 [  -2.6,  51.5], score_ema =   11.8, advantages =  1.071 ± 1.0 [ -2.492,  4.898], abs_actor_obj =  1.752 ± 1.587, critic_obj = 9.242 ± 2.034, resets = 2.00 ≥ 2, time = 13.8\n",
      "step =   85000, scores =   15.5 ± 13.0 [  -0.4,  40.2], score_ema =   12.2, advantages =  1.010 ± 1.0 [ -2.478,  4.646], abs_actor_obj =  1.797 ± 1.749, critic_obj = 9.415 ± 2.635, resets = 3.00 ≥ 3, time = 13.8\n",
      "step =   87500, scores =   19.9 ±  9.2 [  -1.5,  45.2], score_ema =   13.0, advantages =  1.255 ± 1.0 [ -2.255,  4.702], abs_actor_obj =  1.849 ± 1.463, critic_obj = 11.289 ± 2.928, resets = 2.00 ≥ 2, time = 13.6\n",
      "step =   90000, scores =   17.0 ± 14.1 [  -0.6,  45.2], score_ema =   13.4, advantages =  0.997 ± 1.0 [ -2.540,  6.033], abs_actor_obj =  1.862 ± 1.825, critic_obj = 10.229 ± 3.483, resets = 3.00 ≥ 3, time = 13.7\n",
      "step =   92500, scores =   19.5 ±  9.1 [   3.6,  44.3], score_ema =   14.0, advantages =  1.125 ± 1.0 [ -2.816,  5.022], abs_actor_obj =  1.865 ± 1.793, critic_obj = 10.331 ± 3.720, resets = 2.00 ≥ 2, time = 13.7\n",
      "step =   95000, scores =   17.9 ± 15.2 [  -0.5,  52.9], score_ema =   14.4, advantages =  1.065 ± 1.0 [ -2.015,  4.858], abs_actor_obj =  1.778 ± 1.517, critic_obj = 10.916 ± 4.479, resets = 3.00 ≥ 3, time = 13.6\n",
      "step =   97500, scores =   21.4 ±  8.6 [   2.4,  40.6], score_ema =   15.1, advantages =  1.151 ± 1.0 [ -2.929,  5.345], abs_actor_obj =  1.885 ± 1.737, critic_obj = 11.208 ± 2.895, resets = 2.00 ≥ 2, time = 13.6\n",
      "step =  100000, scores =   18.0 ± 14.4 [  -0.5,  42.3], score_ema =   15.4, advantages =  1.145 ± 1.0 [ -2.339,  5.370], abs_actor_obj =  1.850 ± 1.626, critic_obj = 11.011 ± 2.708, resets = 3.00 ≥ 3, time = 14.3\n",
      "step =  102500, scores =   18.9 ±  9.2 [   1.0,  36.5], score_ema =   15.7, advantages =  1.072 ± 1.0 [ -2.389,  5.537], abs_actor_obj =  1.850 ± 1.726, critic_obj = 9.829 ± 2.682, resets = 2.00 ≥ 2, time = 13.9\n",
      "step =  105000, scores =   17.7 ± 14.4 [  -0.4,  43.5], score_ema =   15.9, advantages =  1.048 ± 1.0 [ -2.497,  4.334], abs_actor_obj =  1.867 ± 1.709, critic_obj = 10.438 ± 3.489, resets = 3.00 ≥ 3, time = 13.3\n",
      "step =  107500, scores =   21.4 ± 10.7 [   0.8,  50.5], score_ema =   16.5, advantages =  1.060 ± 1.0 [ -2.444,  4.886], abs_actor_obj =  1.830 ± 1.733, critic_obj = 10.737 ± 3.815, resets = 2.00 ≥ 2, time = 14.4\n",
      "step =  110000, scores =   19.7 ± 15.8 [  -0.5,  50.6], score_ema =   16.8, advantages =  1.172 ± 1.0 [ -2.018,  4.596], abs_actor_obj =  1.866 ± 1.526, critic_obj = 12.563 ± 4.073, resets = 3.00 ≥ 3, time = 14.1\n",
      "step =  112500, scores =   22.3 ± 11.0 [   0.9,  46.4], score_ema =   17.3, advantages =  1.222 ± 1.0 [ -2.530,  5.296], abs_actor_obj =  1.797 ± 1.372, critic_obj = 13.270 ± 4.996, resets = 2.00 ≥ 2, time = 13.7\n",
      "step =  115000, scores =   22.2 ± 18.0 [  -0.7,  65.4], score_ema =   17.8, advantages =  1.146 ± 1.0 [ -2.785,  5.057], abs_actor_obj =  1.834 ± 1.594, critic_obj = 14.367 ± 4.296, resets = 3.00 ≥ 3, time = 13.8\n",
      "step =  117500, scores =   25.9 ± 11.7 [  -0.6,  56.7], score_ema =   18.6, advantages =  1.195 ± 1.0 [ -3.145,  5.142], abs_actor_obj =  1.821 ± 1.550, critic_obj = 14.635 ± 4.830, resets = 2.00 ≥ 2, time = 13.1\n",
      "step =  120000, scores =   23.2 ± 18.3 [  -0.5,  59.6], score_ema =   19.1, advantages =  1.141 ± 1.0 [ -2.455,  4.723], abs_actor_obj =  1.829 ± 1.480, critic_obj = 15.074 ± 5.982, resets = 3.00 ≥ 3, time = 14.3\n",
      "step =  122500, scores =   25.7 ± 12.3 [   3.4,  48.9], score_ema =   19.8, advantages =  1.310 ± 1.0 [ -1.919,  4.826], abs_actor_obj =  1.907 ± 1.422, critic_obj = 14.102 ± 5.277, resets = 2.00 ≥ 2, time = 14.6\n",
      "step =  125000, scores =   24.1 ± 18.8 [  -0.3,  57.9], score_ema =   20.2, advantages =  1.293 ± 1.0 [ -2.863,  5.055], abs_actor_obj =  1.855 ± 1.560, critic_obj = 13.654 ± 5.022, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  127500, scores =   28.9 ± 10.9 [  13.5,  57.3], score_ema =   21.1, advantages =  1.478 ± 1.0 [ -2.370,  5.431], abs_actor_obj =  1.914 ± 1.195, critic_obj = 15.649 ± 5.289, resets = 2.00 ≥ 2, time = 15.9\n",
      "step =  130000, scores =   26.5 ± 19.9 [  -0.4,  60.8], score_ema =   21.6, advantages =  1.424 ± 1.0 [ -1.754,  5.583], abs_actor_obj =  1.908 ± 1.260, critic_obj = 14.628 ± 6.211, resets = 3.00 ≥ 3, time = 16.9\n",
      "step =  132500, scores =   30.2 ± 12.9 [   8.1,  56.3], score_ema =   22.5, advantages =  1.452 ± 1.0 [ -2.013,  5.864], abs_actor_obj =  1.932 ± 1.270, critic_obj = 14.210 ± 5.563, resets = 2.00 ≥ 2, time = 16.2\n",
      "step =  135000, scores =   25.8 ± 19.6 [  -0.2,  59.8], score_ema =   22.8, advantages =  1.381 ± 1.0 [ -1.792,  6.554], abs_actor_obj =  1.907 ± 1.251, critic_obj = 12.800 ± 6.023, resets = 3.00 ≥ 3, time = 16.4\n",
      "step =  137500, scores =   29.4 ± 12.5 [   0.4,  60.4], score_ema =   23.5, advantages =  1.546 ± 1.0 [ -2.133,  6.011], abs_actor_obj =  1.971 ± 1.162, critic_obj = 12.909 ± 5.181, resets = 2.00 ≥ 2, time = 16.2\n",
      "step =  140000, scores =   24.6 ± 18.4 [  -0.3,  50.1], score_ema =   23.6, advantages =  1.391 ± 1.0 [ -2.806,  5.220], abs_actor_obj =  1.996 ± 1.431, critic_obj = 11.218 ± 4.992, resets = 3.00 ≥ 3, time = 17.8\n",
      "step =  142500, scores =   28.4 ± 11.8 [   7.3,  54.1], score_ema =   24.1, advantages =  1.496 ± 1.0 [ -2.335,  5.810], abs_actor_obj =  1.985 ± 1.260, critic_obj = 12.313 ± 4.801, resets = 2.00 ≥ 2, time = 18.3\n",
      "step =  145000, scores =   24.6 ± 18.8 [  -0.2,  53.6], score_ema =   24.1, advantages =  1.378 ± 1.0 [ -1.682,  5.625], abs_actor_obj =  1.927 ± 1.234, critic_obj = 10.773 ± 4.223, resets = 3.00 ≥ 3, time = 17.1\n",
      "step =  147500, scores =   27.8 ± 10.9 [   2.5,  53.2], score_ema =   24.5, advantages =  1.332 ± 1.0 [ -2.834,  5.985], abs_actor_obj =  1.904 ± 1.357, critic_obj = 10.101 ± 4.741, resets = 2.00 ≥ 2, time = 16.5\n",
      "step =  150000, scores =   24.7 ± 18.9 [  -0.3,  55.5], score_ema =   24.5, advantages =  1.197 ± 1.0 [ -2.526,  5.063], abs_actor_obj =  1.951 ± 1.597, critic_obj = 9.947 ± 4.853, resets = 3.00 ≥ 3, time = 17.1\n",
      "step =  152500, scores =   29.0 ± 12.1 [   6.1,  52.5], score_ema =   25.0, advantages =  1.385 ± 1.0 [ -2.754,  6.794], abs_actor_obj =  1.946 ± 1.373, critic_obj = 10.167 ± 4.155, resets = 2.00 ≥ 2, time = 19.2\n",
      "step =  155000, scores =   24.4 ± 18.5 [  -0.3,  52.1], score_ema =   24.9, advantages =  1.259 ± 1.0 [ -2.206,  6.278], abs_actor_obj =  1.948 ± 1.339, critic_obj = 8.947 ± 4.079, resets = 3.00 ≥ 3, time = 18.2\n",
      "step =  157500, scores =   29.8 ± 12.1 [   7.4,  60.4], score_ema =   25.4, advantages =  1.389 ± 1.0 [ -2.438,  5.961], abs_actor_obj =  1.967 ± 1.361, critic_obj = 10.614 ± 4.364, resets = 2.00 ≥ 2, time = 18.4\n",
      "step =  160000, scores =   26.5 ± 19.5 [  -0.1,  56.1], score_ema =   25.5, advantages =  1.334 ± 1.0 [ -1.617,  6.124], abs_actor_obj =  2.045 ± 1.397, critic_obj = 9.498 ± 4.181, resets = 3.00 ≥ 3, time = 19.6\n",
      "step =  162500, scores =   28.3 ± 10.2 [   6.6,  48.4], score_ema =   25.8, advantages =  1.340 ± 1.0 [ -2.839,  5.893], abs_actor_obj =  2.038 ± 1.550, critic_obj = 8.762 ± 3.811, resets = 2.00 ≥ 2, time = 18.1\n",
      "step =  165000, scores =   26.5 ± 19.5 [  -0.2,  56.2], score_ema =   25.9, advantages =  1.287 ± 1.0 [ -1.998,  5.119], abs_actor_obj =  2.086 ± 1.589, critic_obj = 8.835 ± 4.116, resets = 3.00 ≥ 3, time = 17.0\n",
      "step =  167500, scores =   30.4 ± 12.3 [   9.2,  56.0], score_ema =   26.3, advantages =  1.324 ± 1.0 [ -3.203,  7.174], abs_actor_obj =  2.001 ± 1.591, critic_obj = 8.946 ± 3.992, resets = 2.00 ≥ 2, time = 16.9\n",
      "step =  170000, scores =   26.8 ± 19.8 [  -0.3,  52.5], score_ema =   26.4, advantages =  1.193 ± 1.0 [ -2.103,  6.202], abs_actor_obj =  2.005 ± 1.593, critic_obj = 8.442 ± 3.801, resets = 3.00 ≥ 3, time = 16.9\n",
      "step =  172500, scores =   30.1 ± 10.5 [  12.9,  52.5], score_ema =   26.7, advantages =  1.328 ± 1.0 [ -2.588,  6.856], abs_actor_obj =  1.994 ± 1.571, critic_obj = 8.267 ± 3.766, resets = 2.00 ≥ 2, time = 17.5\n",
      "step =  175000, scores =   26.2 ± 19.5 [  -0.4,  59.0], score_ema =   26.7, advantages =  1.063 ± 1.0 [ -2.024,  6.720], abs_actor_obj =  2.055 ± 1.821, critic_obj = 7.937 ± 3.745, resets = 3.00 ≥ 3, time = 17.2\n",
      "step =  177500, scores =   31.0 ± 11.2 [  10.8,  55.2], score_ema =   27.1, advantages =  1.262 ± 1.0 [ -1.936,  6.595], abs_actor_obj =  2.007 ± 1.608, critic_obj = 8.117 ± 3.351, resets = 2.00 ≥ 2, time = 16.1\n",
      "step =  180000, scores =   27.0 ± 19.6 [  -0.2,  54.5], score_ema =   27.1, advantages =  1.095 ± 1.0 [ -1.895,  6.008], abs_actor_obj =  2.073 ± 1.854, critic_obj = 7.208 ± 3.396, resets = 3.00 ≥ 3, time = 16.3\n",
      "step =  182500, scores =   30.3 ± 11.3 [  11.8,  56.6], score_ema =   27.4, advantages =  1.179 ± 1.0 [ -2.053,  7.342], abs_actor_obj =  1.993 ± 1.771, critic_obj = 7.782 ± 3.719, resets = 2.00 ≥ 2, time = 16.8\n",
      "step =  185000, scores =   27.9 ± 20.3 [  -0.3,  56.5], score_ema =   27.4, advantages =  1.074 ± 1.0 [ -2.042,  6.522], abs_actor_obj =  2.126 ± 1.931, critic_obj = 7.684 ± 3.461, resets = 3.00 ≥ 3, time = 16.6\n",
      "step =  187500, scores =   31.0 ± 11.8 [  10.3,  56.4], score_ema =   27.8, advantages =  1.215 ± 1.0 [ -2.244,  5.682], abs_actor_obj =  2.040 ± 1.841, critic_obj = 7.274 ± 2.741, resets = 2.00 ≥ 2, time = 17.0\n",
      "step =  190000, scores =   27.2 ± 19.8 [  -0.3,  53.2], score_ema =   27.7, advantages =  1.009 ± 1.0 [ -2.055,  5.563], abs_actor_obj =  2.119 ± 2.101, critic_obj = 6.441 ± 3.603, resets = 3.00 ≥ 3, time = 16.1\n",
      "step =  192500, scores =   31.1 ± 11.7 [   6.4,  51.6], score_ema =   28.1, advantages =  1.050 ± 1.0 [ -2.238,  6.180], abs_actor_obj =  2.027 ± 1.937, critic_obj = 7.243 ± 3.084, resets = 2.00 ≥ 2, time = 16.2\n",
      "step =  195000, scores =   28.1 ± 20.5 [  -0.3,  55.8], score_ema =   28.1, advantages =  0.992 ± 1.0 [ -2.054,  5.911], abs_actor_obj =  2.146 ± 2.215, critic_obj = 6.518 ± 3.328, resets = 3.00 ≥ 3, time = 14.4\n",
      "step =  197500, scores =   32.0 ± 12.3 [   7.7,  54.7], score_ema =   28.5, advantages =  1.219 ± 1.0 [ -2.255,  6.050], abs_actor_obj =  2.076 ± 1.945, critic_obj = 7.370 ± 3.425, resets = 2.00 ≥ 2, time = 14.7\n",
      "step =  200000, scores =   29.7 ± 21.7 [  -0.2,  62.4], score_ema =   28.6, advantages =  0.954 ± 1.0 [ -2.280,  6.205], abs_actor_obj =  2.194 ± 2.286, critic_obj = 7.356 ± 3.631, resets = 3.00 ≥ 3, time = 15.0\n",
      "step =  202500, scores =   33.2 ± 12.4 [   4.9,  59.5], score_ema =   29.1, advantages =  1.062 ± 1.0 [ -2.700,  5.667], abs_actor_obj =  2.068 ± 2.096, critic_obj = 7.553 ± 3.469, resets = 2.00 ≥ 2, time = 15.0\n",
      "step =  205000, scores =   28.5 ± 21.0 [  -0.3,  63.0], score_ema =   29.0, advantages =  0.814 ± 1.0 [ -2.117,  5.685], abs_actor_obj =  2.250 ± 2.510, critic_obj = 6.345 ± 3.088, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  207500, scores =   32.8 ± 11.7 [  13.0,  54.5], score_ema =   29.4, advantages =  1.046 ± 1.0 [ -2.304,  6.895], abs_actor_obj =  2.075 ± 2.222, critic_obj = 6.442 ± 2.852, resets = 2.00 ≥ 2, time = 16.4\n",
      "step =  210000, scores =   28.6 ± 21.0 [  -0.2,  61.0], score_ema =   29.3, advantages =  0.820 ± 1.0 [ -2.233,  5.324], abs_actor_obj =  2.257 ± 2.560, critic_obj = 6.239 ± 2.833, resets = 3.00 ≥ 3, time = 15.8\n",
      "step =  212500, scores =   36.0 ± 13.2 [  13.7,  59.7], score_ema =   30.0, advantages =  1.065 ± 1.0 [ -2.216,  5.454], abs_actor_obj =  2.189 ± 2.208, critic_obj = 8.130 ± 3.630, resets = 2.00 ≥ 2, time = 15.8\n",
      "step =  215000, scores =   32.2 ± 23.5 [  -0.2,  68.9], score_ema =   30.2, advantages =  0.913 ± 1.0 [ -2.235,  6.229], abs_actor_obj =  2.138 ± 2.250, critic_obj = 8.378 ± 3.981, resets = 3.00 ≥ 3, time = 15.9\n",
      "step =  217500, scores =   36.5 ± 13.3 [  12.8,  62.5], score_ema =   30.8, advantages =  1.063 ± 1.0 [ -2.021,  6.336], abs_actor_obj =  2.106 ± 2.186, critic_obj = 8.647 ± 3.397, resets = 2.00 ≥ 2, time = 16.1\n",
      "step =  220000, scores =   34.4 ± 25.2 [  -0.4,  67.1], score_ema =   31.2, advantages =  0.902 ± 1.0 [ -2.352,  5.175], abs_actor_obj =  2.234 ± 2.390, critic_obj = 9.375 ± 4.368, resets = 3.00 ≥ 3, time = 16.4\n",
      "step =  222500, scores =   38.2 ± 15.0 [  12.1,  66.8], score_ema =   31.9, advantages =  1.020 ± 1.0 [ -3.509,  4.989], abs_actor_obj =  2.140 ± 2.348, critic_obj = 10.355 ± 4.427, resets = 2.00 ≥ 2, time = 15.3\n",
      "step =  225000, scores =   34.8 ± 25.4 [  -0.3,  67.5], score_ema =   32.2, advantages =  0.865 ± 1.0 [ -2.221,  4.588], abs_actor_obj =  2.225 ± 2.410, critic_obj = 9.147 ± 4.748, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  227500, scores =   39.7 ± 14.0 [  15.1,  64.3], score_ema =   32.9, advantages =  1.131 ± 1.0 [ -2.300,  5.487], abs_actor_obj =  2.155 ± 2.237, critic_obj = 9.299 ± 3.843, resets = 2.00 ≥ 2, time = 16.0\n",
      "step =  230000, scores =   34.1 ± 25.0 [  -0.2,  66.4], score_ema =   33.0, advantages =  0.794 ± 1.0 [ -2.910,  5.511], abs_actor_obj =  2.292 ± 2.714, critic_obj = 7.794 ± 3.744, resets = 3.00 ≥ 3, time = 16.0\n",
      "step =  232500, scores =   40.0 ± 15.2 [  13.8,  70.5], score_ema =   33.7, advantages =  1.027 ± 1.0 [ -2.540,  5.656], abs_actor_obj =  2.187 ± 2.405, critic_obj = 9.469 ± 4.513, resets = 2.00 ≥ 2, time = 15.9\n",
      "step =  235000, scores =   36.3 ± 26.2 [  -0.3,  67.3], score_ema =   34.0, advantages =  0.859 ± 1.0 [ -2.810,  4.618], abs_actor_obj =  2.312 ± 2.667, critic_obj = 8.940 ± 4.059, resets = 3.00 ≥ 3, time = 16.1\n",
      "step =  237500, scores =   40.7 ± 15.5 [  13.1,  67.1], score_ema =   34.7, advantages =  1.039 ± 1.0 [ -2.400,  5.101], abs_actor_obj =  2.175 ± 2.407, critic_obj = 9.452 ± 4.222, resets = 2.00 ≥ 2, time = 15.5\n",
      "step =  240000, scores =   37.5 ± 27.3 [  -0.2,  70.9], score_ema =   35.0, advantages =  0.761 ± 1.0 [ -2.378,  5.133], abs_actor_obj =  2.319 ± 2.782, critic_obj = 8.836 ± 3.999, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  242500, scores =   43.1 ± 15.2 [  16.7,  67.7], score_ema =   35.8, advantages =  1.020 ± 1.0 [ -2.771,  4.780], abs_actor_obj =  2.235 ± 2.624, critic_obj = 9.703 ± 4.291, resets = 2.00 ≥ 2, time = 15.9\n",
      "step =  245000, scores =   37.1 ± 26.7 [  -0.3,  68.2], score_ema =   35.9, advantages =  0.770 ± 1.0 [ -2.745,  5.131], abs_actor_obj =  2.354 ± 2.976, critic_obj = 7.818 ± 3.460, resets = 3.00 ≥ 3, time = 15.2\n",
      "step =  247500, scores =   42.5 ± 14.9 [  14.9,  69.6], score_ema =   36.6, advantages =  0.944 ± 1.0 [ -2.656,  5.279], abs_actor_obj =  2.237 ± 2.835, critic_obj = 7.974 ± 3.100, resets = 2.00 ≥ 2, time = 17.0\n",
      "step =  250000, scores =   37.5 ± 27.1 [  -0.2,  72.2], score_ema =   36.7, advantages =  0.676 ± 1.0 [ -2.313,  4.676], abs_actor_obj =  2.424 ± 3.089, critic_obj = 8.019 ± 3.745, resets = 3.00 ≥ 3, time = 17.1\n",
      "step =  252500, scores =   45.5 ± 15.8 [  20.2,  74.2], score_ema =   37.5, advantages =  0.853 ± 1.0 [ -2.721,  5.320], abs_actor_obj =  2.256 ± 2.801, critic_obj = 9.297 ± 3.512, resets = 2.00 ≥ 2, time = 16.4\n",
      "step =  255000, scores =   39.3 ± 28.2 [  -0.3,  74.5], score_ema =   37.7, advantages =  0.633 ± 1.0 [ -2.499,  4.393], abs_actor_obj =  2.479 ± 3.135, critic_obj = 9.132 ± 4.164, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  257500, scores =   44.1 ± 15.3 [  20.9,  68.4], score_ema =   38.3, advantages =  0.749 ± 1.0 [ -2.808,  4.560], abs_actor_obj =  2.330 ± 3.195, critic_obj = 8.224 ± 2.667, resets = 2.00 ≥ 2, time = 15.7\n",
      "step =  260000, scores =   39.1 ± 28.2 [  -0.2,  72.5], score_ema =   38.4, advantages =  0.594 ± 1.0 [ -2.817,  4.368], abs_actor_obj =  2.541 ± 3.286, critic_obj = 8.391 ± 3.358, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  262500, scores =   43.5 ± 15.4 [  19.5,  72.4], score_ema =   38.9, advantages =  0.749 ± 1.0 [ -2.505,  4.458], abs_actor_obj =  2.355 ± 3.260, critic_obj = 7.574 ± 2.530, resets = 2.00 ≥ 2, time = 15.8\n",
      "step =  265000, scores =   39.3 ± 28.2 [  -0.1,  72.7], score_ema =   39.0, advantages =  0.547 ± 1.0 [ -2.763,  4.191], abs_actor_obj =  2.584 ± 3.494, critic_obj = 7.616 ± 2.611, resets = 3.00 ≥ 3, time = 15.9\n",
      "step =  267500, scores =   45.0 ± 16.4 [  21.7,  75.2], score_ema =   39.6, advantages =  0.655 ± 1.0 [ -2.808,  4.908], abs_actor_obj =  2.380 ± 3.375, critic_obj = 8.084 ± 2.995, resets = 2.00 ≥ 2, time = 15.7\n",
      "step =  270000, scores =   40.0 ± 29.3 [  -0.5,  77.9], score_ema =   39.6, advantages =  0.483 ± 1.0 [ -2.700,  4.602], abs_actor_obj =  2.646 ± 3.552, critic_obj = 8.296 ± 2.076, resets = 3.00 ≥ 3, time = 13.6\n",
      "step =  272500, scores =   42.5 ± 16.4 [   9.0,  74.0], score_ema =   39.9, advantages =  0.551 ± 1.0 [ -2.825,  4.672], abs_actor_obj =  2.510 ± 3.513, critic_obj = 8.435 ± 2.359, resets = 2.00 ≥ 2, time = 14.3\n",
      "step =  275000, scores =   38.7 ± 28.1 [  -0.2,  71.3], score_ema =   39.8, advantages =  0.348 ± 1.0 [ -2.735,  3.441], abs_actor_obj =  2.858 ± 3.774, critic_obj = 8.279 ± 1.932, resets = 3.00 ≥ 3, time = 14.7\n",
      "step =  277500, scores =   46.8 ± 16.1 [  18.6,  81.8], score_ema =   40.5, advantages =  0.656 ± 1.0 [ -2.624,  4.752], abs_actor_obj =  2.292 ± 3.223, critic_obj = 9.247 ± 2.417, resets = 2.00 ≥ 2, time = 14.8\n",
      "step =  280000, scores =   38.5 ± 27.9 [  -0.2,  78.4], score_ema =   40.3, advantages =  0.348 ± 1.0 [ -2.706,  3.718], abs_actor_obj =  2.849 ± 3.840, critic_obj = 7.509 ± 1.894, resets = 3.00 ≥ 3, time = 14.0\n",
      "step =  282500, scores =   45.5 ± 16.7 [  18.2,  75.8], score_ema =   40.8, advantages =  0.513 ± 1.0 [ -3.815,  4.666], abs_actor_obj =  2.488 ± 3.716, critic_obj = 8.248 ± 1.962, resets = 2.00 ≥ 2, time = 13.9\n",
      "step =  285000, scores =   40.9 ± 29.9 [  -0.3,  82.2], score_ema =   40.8, advantages =  0.352 ± 1.0 [ -2.709,  4.048], abs_actor_obj =  2.857 ± 3.861, critic_obj = 8.606 ± 2.058, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  287500, scores =   48.1 ± 17.2 [  15.2,  78.9], score_ema =   41.5, advantages =  0.618 ± 1.0 [ -2.857,  4.457], abs_actor_obj =  2.377 ± 3.406, critic_obj = 8.928 ± 2.314, resets = 2.00 ≥ 2, time = 16.0\n",
      "step =  290000, scores =   39.7 ± 28.9 [  -0.3,  78.1], score_ema =   41.4, advantages =  0.307 ± 1.0 [ -3.038,  3.171], abs_actor_obj =  2.907 ± 3.990, critic_obj = 7.872 ± 1.448, resets = 3.00 ≥ 3, time = 16.3\n",
      "step =  292500, scores =   44.7 ± 15.7 [  20.4,  77.3], score_ema =   41.7, advantages =  0.416 ± 1.0 [ -3.079,  4.705], abs_actor_obj =  2.637 ± 3.968, critic_obj = 7.216 ± 1.316, resets = 2.00 ≥ 2, time = 16.9\n",
      "step =  295000, scores =   40.2 ± 29.1 [  -0.3,  76.5], score_ema =   41.6, advantages =  0.225 ± 1.0 [ -3.144,  3.986], abs_actor_obj =  3.109 ± 4.167, critic_obj = 7.657 ± 0.961, resets = 3.00 ≥ 3, time = 16.8\n",
      "step =  297500, scores =   46.0 ± 15.8 [  20.6,  75.3], score_ema =   42.0, advantages =  0.370 ± 1.0 [ -3.576,  4.243], abs_actor_obj =  2.707 ± 4.021, critic_obj = 6.440 ± 0.833, resets = 2.00 ≥ 2, time = 16.0\n",
      "step =  300000, scores =   39.3 ± 28.6 [  -0.3,  71.1], score_ema =   41.7, advantages =  0.139 ± 1.0 [ -4.916,  3.187], abs_actor_obj =  3.247 ± 4.798, critic_obj = 7.376 ± 0.897, resets = 3.00 ≥ 3, time = 13.8\n",
      "step =  302500, scores =   43.8 ± 15.8 [  19.9,  73.0], score_ema =   41.9, advantages =  0.248 ± 1.0 [ -3.150,  4.413], abs_actor_obj =  2.935 ± 4.323, critic_obj = 6.430 ± 1.251, resets = 2.00 ≥ 2, time = 14.6\n",
      "step =  305000, scores =   42.0 ± 30.4 [  -0.3,  91.9], score_ema =   41.9, advantages =  0.231 ± 1.0 [ -2.944,  4.692], abs_actor_obj =  3.057 ± 4.100, critic_obj = 8.713 ± 1.470, resets = 3.00 ≥ 3, time = 14.0\n",
      "step =  307500, scores =   48.2 ± 18.1 [  20.2,  78.2], score_ema =   42.6, advantages =  0.412 ± 1.0 [ -3.210,  4.669], abs_actor_obj =  2.624 ± 3.749, critic_obj = 8.590 ± 1.975, resets = 2.00 ≥ 2, time = 13.8\n",
      "step =  310000, scores =   42.4 ± 30.5 [  -0.4,  81.4], score_ema =   42.5, advantages =  0.251 ± 1.0 [ -3.366,  4.287], abs_actor_obj =  3.041 ± 4.171, critic_obj = 8.477 ± 0.622, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  312500, scores =   47.0 ± 18.1 [  12.1,  82.4], score_ema =   43.0, advantages =  0.348 ± 1.0 [ -4.365,  4.259], abs_actor_obj =  2.775 ± 4.122, critic_obj = 8.924 ± 1.542, resets = 2.00 ≥ 2, time = 16.6\n",
      "step =  315000, scores =   43.1 ± 31.2 [  -0.3,  83.0], score_ema =   43.0, advantages =  0.235 ± 1.0 [ -2.993,  3.587], abs_actor_obj =  3.101 ± 4.161, critic_obj = 9.484 ± 1.286, resets = 3.00 ≥ 3, time = 16.0\n",
      "step =  317500, scores =   47.3 ± 15.7 [   5.7,  80.1], score_ema =   43.4, advantages =  0.285 ± 1.0 [ -3.287,  3.944], abs_actor_obj =  2.895 ± 4.109, critic_obj = 8.068 ± 0.526, resets = 2.00 ≥ 2, time = 15.6\n",
      "step =  320000, scores =   41.3 ± 30.0 [  -0.4,  91.9], score_ema =   43.2, advantages =  0.094 ± 1.0 [ -3.322,  3.965], abs_actor_obj =  3.411 ± 4.532, critic_obj = 8.710 ± 0.494, resets = 3.00 ≥ 3, time = 16.1\n",
      "step =  322500, scores =   48.0 ± 17.8 [  15.1,  79.9], score_ema =   43.7, advantages =  0.355 ± 1.0 [ -2.918,  3.737], abs_actor_obj =  2.767 ± 3.923, critic_obj = 9.178 ± 1.604, resets = 2.00 ≥ 2, time = 16.2\n",
      "step =  325000, scores =   43.2 ± 31.2 [  -0.2,  90.7], score_ema =   43.6, advantages =  0.173 ± 1.0 [ -2.841,  3.494], abs_actor_obj =  3.214 ± 4.405, critic_obj = 8.911 ± 0.834, resets = 3.00 ≥ 3, time = 18.7\n",
      "step =  327500, scores =   48.1 ± 16.1 [  21.4,  73.1], score_ema =   44.1, advantages =  0.339 ± 1.0 [ -3.286,  5.051], abs_actor_obj =  2.770 ± 4.077, critic_obj = 8.350 ± 1.159, resets = 2.00 ≥ 2, time = 17.8\n",
      "step =  330000, scores =   43.0 ± 31.1 [  -0.5,  82.7], score_ema =   44.0, advantages =  0.164 ± 1.0 [ -2.714,  4.390], abs_actor_obj =  3.228 ± 4.432, critic_obj = 8.241 ± 0.391, resets = 3.00 ≥ 3, time = 16.1\n",
      "step =  332500, scores =   49.0 ± 17.7 [  19.9,  79.0], score_ema =   44.5, advantages =  0.322 ± 1.0 [ -3.527,  3.846], abs_actor_obj =  2.820 ± 4.243, critic_obj = 7.446 ± 1.126, resets = 2.00 ≥ 2, time = 16.0\n",
      "step =  335000, scores =   41.0 ± 29.6 [  -0.3,  77.6], score_ema =   44.1, advantages =  0.028 ± 1.0 [ -3.623,  3.194], abs_actor_obj =  3.609 ± 4.866, critic_obj = 7.759 ± 1.125, resets = 3.00 ≥ 3, time = 16.3\n",
      "step =  337500, scores =   49.7 ± 17.7 [  16.3,  80.8], score_ema =   44.7, advantages =  0.294 ± 1.0 [ -3.163,  4.190], abs_actor_obj =  2.882 ± 4.334, critic_obj = 8.048 ± 1.430, resets = 2.00 ≥ 2, time = 16.0\n",
      "step =  340000, scores =   42.2 ± 31.3 [  -0.5,  85.0], score_ema =   44.4, advantages =  0.055 ± 1.0 [ -3.027,  3.247], abs_actor_obj =  3.550 ± 4.693, critic_obj = 9.137 ± 0.591, resets = 3.00 ≥ 3, time = 17.8\n",
      "step =  342500, scores =   49.9 ± 17.1 [  24.0,  80.2], score_ema =   45.0, advantages =  0.318 ± 1.0 [ -3.043,  4.538], abs_actor_obj =  2.798 ± 4.327, critic_obj = 7.907 ± 1.259, resets = 2.00 ≥ 2, time = 16.6\n",
      "step =  345000, scores =   44.1 ± 32.1 [  -0.3,  89.5], score_ema =   44.9, advantages =  0.114 ± 1.0 [ -2.761,  4.655], abs_actor_obj =  3.336 ± 4.635, critic_obj = 7.889 ± 0.458, resets = 3.00 ≥ 3, time = 15.7\n",
      "step =  347500, scores =   49.7 ± 18.7 [  20.4,  88.6], score_ema =   45.4, advantages =  0.309 ± 1.0 [ -3.402,  4.564], abs_actor_obj =  2.827 ± 4.325, critic_obj = 7.077 ± 1.489, resets = 2.00 ≥ 2, time = 16.2\n",
      "step =  350000, scores =   41.9 ± 30.3 [  -0.3,  77.1], score_ema =   45.0, advantages =  0.005 ± 1.0 [ -3.078,  4.070], abs_actor_obj =  3.635 ± 5.047, critic_obj = 7.626 ± 1.712, resets = 3.00 ≥ 3, time = 16.0\n",
      "step =  352500, scores =   47.8 ± 16.6 [  17.0,  76.7], score_ema =   45.3, advantages =  0.237 ± 1.0 [ -3.759,  3.591], abs_actor_obj =  2.925 ± 4.601, critic_obj = 6.998 ± 1.600, resets = 2.00 ≥ 2, time = 17.2\n",
      "step =  355000, scores =   43.3 ± 31.3 [  -0.2,  82.8], score_ema =   45.1, advantages =  0.057 ± 1.0 [ -2.915,  3.606], abs_actor_obj =  3.491 ± 4.783, critic_obj = 7.465 ± 1.080, resets = 3.00 ≥ 3, time = 18.2\n",
      "step =  357500, scores =   49.4 ± 17.7 [  16.8,  80.4], score_ema =   45.5, advantages =  0.245 ± 1.0 [ -3.344,  4.227], abs_actor_obj =  2.870 ± 4.444, critic_obj = 6.988 ± 1.285, resets = 2.00 ≥ 2, time = 17.6\n",
      "step =  360000, scores =   42.4 ± 30.8 [  -0.3,  93.0], score_ema =   45.2, advantages = -0.033 ± 1.0 [ -2.831,  3.232], abs_actor_obj =  3.741 ± 4.873, critic_obj = 8.134 ± 1.410, resets = 3.00 ≥ 3, time = 16.0\n",
      "step =  362500, scores =   50.0 ± 17.8 [  25.5,  82.9], score_ema =   45.7, advantages =  0.249 ± 1.0 [ -3.118,  3.606], abs_actor_obj =  2.938 ± 4.619, critic_obj = 7.467 ± 1.725, resets = 2.00 ≥ 2, time = 16.4\n",
      "step =  365000, scores =   44.6 ± 32.6 [  -0.3,  84.5], score_ema =   45.6, advantages =  0.044 ± 1.0 [ -2.861,  3.637], abs_actor_obj =  3.575 ± 4.794, critic_obj = 9.103 ± 0.802, resets = 3.00 ≥ 3, time = 15.8\n",
      "step =  367500, scores =   48.9 ± 17.3 [  21.3,  81.5], score_ema =   45.9, advantages =  0.144 ± 1.0 [ -3.197,  3.962], abs_actor_obj =  3.150 ± 4.752, critic_obj = 6.962 ± 2.183, resets = 2.00 ≥ 2, time = 14.8\n",
      "step =  370000, scores =   42.7 ± 30.9 [  -0.2,  84.2], score_ema =   45.6, advantages = -0.035 ± 1.0 [ -3.248,  3.342], abs_actor_obj =  3.733 ± 5.141, critic_obj = 7.372 ± 1.689, resets = 3.00 ≥ 3, time = 16.2\n",
      "step =  372500, scores =   50.7 ± 18.3 [  19.8,  87.6], score_ema =   46.1, advantages =  0.223 ± 1.0 [ -3.486,  4.205], abs_actor_obj =  2.923 ± 4.643, critic_obj = 6.501 ± 1.729, resets = 2.00 ≥ 2, time = 17.2\n",
      "step =  375000, scores =   42.3 ± 30.5 [  -0.1,  80.5], score_ema =   45.7, advantages = -0.074 ± 1.0 [ -3.268,  3.310], abs_actor_obj =  3.870 ± 5.274, critic_obj = 7.654 ± 2.090, resets = 3.00 ≥ 3, time = 16.3\n",
      "step =  377500, scores =   50.3 ± 17.8 [  24.9,  81.2], score_ema =   46.2, advantages =  0.263 ± 1.0 [ -3.227,  3.314], abs_actor_obj =  2.946 ± 4.533, critic_obj = 7.605 ± 1.615, resets = 2.00 ≥ 2, time = 15.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def init_policy(continuous_actions: bool, actions_std: Optional[float]):\n",
    "    class A2CNetwork(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # in_size = 4\n",
    "            # shared_out_sizes = [64, 64]\n",
    "            # actor_out_sizes = [64, 2]\n",
    "            # critic_out_sizes = [64, 1]\n",
    "\n",
    "            # in_size = 24\n",
    "            # shared_out_sizes = [64, 128, 128]\n",
    "            # actor_out_sizes = [128, 64, 4]\n",
    "            # critic_out_sizes = [128, 64, 1]\n",
    "\n",
    "            # in_size = 8\n",
    "            # shared_out_sizes = [128, 256, 256]\n",
    "            # actor_out_sizes = [256, 256, 128, 128, 4]\n",
    "            # critic_out_sizes = [256, 256, 128, 128, 1]\n",
    "            \n",
    "            # in_size = 128\n",
    "            # shared_out_sizes = [512, 512, 512, 512, 512]\n",
    "            # actor_out_sizes = [512, 512, 512, 512, 512, 512, 5]\n",
    "            # critic_out_sizes = [512, 512, 512, 512, 512, 512, 1]\n",
    "\n",
    "            # in_size = 128\n",
    "            # shared_out_sizes = [256, 256, 256]\n",
    "            # actor_out_sizes = [256, 256, 256, 256, 256, 5]\n",
    "            # critic_out_sizes = [256, 256, 256, 256, 256, 1]\n",
    "\n",
    "            # in_size = 17\n",
    "            # shared_out_sizes = []\n",
    "            # actor_out_sizes = [64, 64, 64, 64, 64, 64, 6]\n",
    "            # critic_out_sizes = [64, 64, 64, 64, 64, 64, 1]\n",
    "            \n",
    "            in_size = 27\n",
    "            shared_out_sizes = []\n",
    "            actor_out_sizes = [96, 96, 96, 96, 96, 96, 8]\n",
    "            critic_out_sizes = [64, 64, 64, 64, 64, 64, 1]\n",
    "            \n",
    "            hidden_activation_function = nn.ELU()\n",
    "            actor_out_activation_function = nn.Tanh()\n",
    "            critic_out_activation_function = nn.Identity()\n",
    "            \n",
    "            self.has_shared = len(shared_out_sizes) > 0\n",
    "            \n",
    "            if self.has_shared:\n",
    "                self.shared = SeqNet.from_layer_provider(\n",
    "                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                        nn.Linear(in_features, out_features),\n",
    "                        hidden_activation_function\n",
    "                    ),\n",
    "                    in_size=in_size,\n",
    "                    out_sizes=shared_out_sizes\n",
    "                )\n",
    "            else:\n",
    "                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n",
    "\n",
    "            self.actor = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    actor_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=actor_out_sizes\n",
    "            )\n",
    "\n",
    "            self.critic = SeqNet.from_layer_provider(\n",
    "                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    critic_out_activation_function if is_last_layer else hidden_activation_function\n",
    "                ),\n",
    "                in_size=self.shared.out_shape.get_definite_features(),\n",
    "                out_sizes=critic_out_sizes\n",
    "            )\n",
    "\n",
    "        def forward(self, x: torch.Tensor):\n",
    "            if self.has_shared:\n",
    "                shared_out = self.shared(x)\n",
    "            else:\n",
    "                shared_out = x\n",
    "\n",
    "            return self.actor(shared_out), self.critic(shared_out)\n",
    "\n",
    "    return ActorCriticPolicy(A2CNetwork(), continuous_actions, actions_std)\n",
    "\n",
    "\n",
    "score_mean_ema = ExponentialMovingAverage(alpha=0.1)\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "def on_optimization_done(rl: PPO, step: int, info: dict[str, Any]):\n",
    "    time_taken = stopwatch.reset()\n",
    "    \n",
    "    if 'unnormalized_rewards' in info['rollout']:\n",
    "        unnormalized_rewards = info['rollout']['unnormalized_rewards']\n",
    "        _, gamma_1_returns = compute_gae_and_returns(\n",
    "            value_estimates=np.zeros_like(rl.buffer.rewards[:len(unnormalized_rewards)]),\n",
    "            rewards=unnormalized_rewards,\n",
    "            episode_starts=rl.buffer.episode_starts[:len(unnormalized_rewards)],\n",
    "            last_values=np.zeros_like(rl.buffer.rewards[0], dtype=float),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_rewards=None,\n",
    "            normalize_advantages=None,\n",
    "        )\n",
    "    else:\n",
    "        _, gamma_1_returns = rl.buffer.compute_gae_and_returns(\n",
    "            last_values=torch.zeros_like(rl.buffer.value_estimates[0]),\n",
    "            last_dones=np.zeros_like(rl.buffer.episode_starts[0], dtype=bool),\n",
    "            gamma=1.0,\n",
    "            gae_lambda=1.0,\n",
    "            normalize_advantages=None,\n",
    "            normalize_rewards=None,\n",
    "        )\n",
    "    \n",
    "    episode_start_gamma_1_returns = gamma_1_returns[\n",
    "        rl.buffer.episode_starts[:rl.buffer.pos]\n",
    "    ]\n",
    "    \n",
    "    score_moving_average = score_mean_ema.update(episode_start_gamma_1_returns.mean())\n",
    "    \n",
    "    scores = format_summary_statics(\n",
    "        episode_start_gamma_1_returns, \n",
    "        mean_format=' 6.1f',\n",
    "        std_format='4.1f',\n",
    "        min_value_format=' 6.1f',\n",
    "        max_value_format='5.1f',\n",
    "    )\n",
    "    advantages = format_summary_statics(\n",
    "        info['advantages'], \n",
    "        mean_format=' 6.3f',\n",
    "        std_format='.1f',\n",
    "        min_value_format=' 7.3f',\n",
    "        max_value_format='6.3f',\n",
    "    )\n",
    "    abs_actor_obj = format_summary_statics(\n",
    "        info['actor_objective_unreduced'].abs() * rl.actor_objective_weight,  \n",
    "        mean_format=' 5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    critic_obj = format_summary_statics(\n",
    "        info['weighted_critic_objective'], \n",
    "        mean_format='5.3f',\n",
    "        std_format='5.3f',\n",
    "        min_value_format=None,\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    resets = format_summary_statics(\n",
    "        rl.buffer.episode_starts.astype(int).sum(axis=0), \n",
    "        mean_format='.2f',\n",
    "        std_format=None,\n",
    "        min_value_format='1d',\n",
    "        max_value_format=None,\n",
    "    )\n",
    "    print(f\"{step = : >7}, \"\n",
    "          f'{scores = :s}, '\n",
    "          f'score_ema = {score_moving_average: 6.1f}, '\n",
    "          f\"{advantages = :s}, \"\n",
    "          f\"{abs_actor_obj = :s}, \"\n",
    "          f\"{critic_obj = :s}, \"\n",
    "          f\"{resets = :s}, \"\n",
    "          f\"time = {time_taken:4.1f}\")\n",
    "    \n",
    "    # for param_name, param_grad in get_gradients_per_parameter(rl.policy, param_type='weight'):\n",
    "    #     print(f'{param_name + \".grad\":<50}: ' + format_summary_statics(\n",
    "    #         param_grad,\n",
    "    #         mean_format=' 8.5f',\n",
    "    #         std_format='.5f',\n",
    "    #         min_value_format=' 8.5f',\n",
    "    #         max_value_format='7.5f',\n",
    "    #     ))\n",
    "    # \n",
    "    # print('\\n')\n",
    "\n",
    "\n",
    "device = set_default_torch_device(\"cuda:0\") if True else set_default_torch_device('cpu')\n",
    "print(f'using device {device}')\n",
    "\n",
    "# env = parallelize_env_async(lambda: gym.make(\"CartPole-v1\", max_episode_steps=1000, render_mode='rgb_array'), 16)\n",
    "# env = parallelize_env_async(lambda: StepSkipWrapper(gym.make(\"BipedalWalker-v3\", max_episode_steps=3000, render_mode='rgb_array'), steps_per_step=5), 8)\n",
    "# env = parallelize_env_async(lambda: gym.make(\"LunarLander-v2\", render_mode='rgb_array'), 128)\n",
    "# env = parallelize_env_async(lambda: gym.make(\"ALE/Pacman-ram-v5\", render_mode='rgb_array'), 128)\n",
    "# env = parallelize_env_async(lambda: gym.make(\"ALE/Asteroids-ram-v5\", render_mode='rgb_array'), 16)\n",
    "\n",
    "def create_env(render_mode: str | None):\n",
    "    return gym.make(env_name, render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "def wrap_env(_env: Env):\n",
    "    # _env = NormalizeRewardWrapper(_env, gamma=gamma)\n",
    "    # _env = TransformObservation(_env, lambda _obs: _obs / 255)\n",
    "    # _env = TransformReward(_env, lambda _reward: 0.01 * _reward) \n",
    "    return _env\n",
    "\n",
    "env_name = \"Ant-v4\"\n",
    "env_kwargs = {'ctrl_cost_weight': 0.001, 'healthy_reward': 0.001}\n",
    "num_envs = 32\n",
    "env = parallelize_env_async(lambda: create_env(render_mode=None), num_envs)\n",
    "gamma = 0.995\n",
    "    \n",
    "try:\n",
    "    env = wrap_env(env)\n",
    "    print(f'{env = }, {num_envs = }')\n",
    "    \n",
    "    # policy = init_policy(continuous_actions=True, actions_std=0.1)\n",
    "    # policy.load_state_dict(torch.load('saved_models/rl/Pacman-ram-v5/2024-04-25_22.49.18---3+5x128--state_dict.pth'))\n",
    "    print(f'{count_parameters(policy) = }')\n",
    "    \n",
    "    PPO(\n",
    "        env=env,\n",
    "        policy=policy.to(device),\n",
    "        policy_optimizer=lambda pol: optim.Adam(pol.parameters(), lr=1e-5),\n",
    "        buffer_size=2500,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=1.0,\n",
    "        normalize_rewards=None,\n",
    "        normalize_advantages=NormalizationType.Std,\n",
    "        actor_objective_weight=1.0,\n",
    "        critic_objective_weight=0.5,\n",
    "        ppo_epochs=3,\n",
    "        ppo_batch_size=500,\n",
    "        action_ratio_clip_range=0.13,\n",
    "        log_unreduced=True,\n",
    "        callback=Callback(on_optimization_done=on_optimization_done)\n",
    "    ).train(5_000_000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing envs')\n",
    "    env.close()\n",
    "    print('envs closed')\n",
    "\n",
    "print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-26T20:52:37.285212Z"
    }
   },
   "id": "f71efe062771e81b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-0.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-1.mp4\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-2.mp4\n",
      "keyboard interrupt\n",
      "closing record_env\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-26_22.06.41\\rl-video-episode-3.mp4\n",
      "record_env closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "record_env: gym.Env = create_env(render_mode='rgb_array')\n",
    "record_env.metadata['render_fps'] = 20\n",
    "try:\n",
    "    record_env = wrap_env(record_env)\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=rf'C:\\Users\\domin\\Videos\\rl\\{get_current_timestamp()}', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            actions_dist = policy.predict_actions(obs)\n",
    "            actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "    \n",
    "    record(10000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T20:07:07.230458Z",
     "start_time": "2024-04-26T20:06:40.947989Z"
    }
   },
   "id": "d1ae8571d73535c6",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), f'saved_models/rl/Pacman-ram-v5/{get_current_timestamp()}-3+5x128--state_dict.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:49:18.628973Z",
     "start_time": "2024-04-25T20:49:18.536723Z"
    }
   },
   "id": "459d7865a53e3600",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "\n",
    "# Parallel environments\n",
    "vec_env = make_vec_env(lambda: gym.make('CartPole-v1', render_mode='rgb_array'), n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=2)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "for _ in range(100_000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266d91aeee84c4cd",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-0.mp4\n",
      "closing record_env\n",
      "Moviepy - Building video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\domin\\Videos\\rl\\2024-04-24.1\\rl-video-episode-1.mp4\n",
      "record_env closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "record_env = gymnasium.make(\"ALE/Pacman-ram-v5\", render_mode='rgb_array')\n",
    "try:\n",
    "    record_env = AutoResetWrapper(\n",
    "        RecordVideo(record_env, video_folder=r'C:\\Users\\domin\\Videos\\rl\\2024-04-24.1', episode_trigger=lambda ep_nr: True)\n",
    "    )\n",
    "    def record(max_steps: int):\n",
    "        obs, info = record_env.reset()\n",
    "        for step in range(max_steps):\n",
    "            # actions_dist = policy.predict_actions(obs)\n",
    "            # actions = actions_dist.sample().detach().cpu().numpy()\n",
    "            \n",
    "            actions = 2\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = record_env.step(actions)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    \n",
    "    record(10000)\n",
    "except KeyboardInterrupt:\n",
    "    print('keyboard interrupt')\n",
    "finally:\n",
    "    print('closing record_env')\n",
    "    record_env.close()\n",
    "    print('record_env closed')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T15:57:07.218154Z",
     "start_time": "2024-04-24T15:57:06.685399Z"
    }
   },
   "id": "1a6d8096efe48ca3",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OrderEnforcing<PassiveEnvChecker<AtariEnv<ALE/Pacman-ram-v5>>>>\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T16:29:07.007390Z",
     "start_time": "2024-04-24T16:29:06.925013Z"
    }
   },
   "id": "bba6ab51a61dd845",
   "execution_count": 10
  }
 ],
 "metadata": {
  "autoscrollcelloutput": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
